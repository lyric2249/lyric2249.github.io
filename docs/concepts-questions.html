<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A.3 Concepts Questions | Self-Study</title>
  <meta name="description" content="A.3 Concepts Questions | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="A.3 Concepts Questions | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A.3 Concepts Questions | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orderlogit.html"/>
<link rel="next" href="about-cluster-gcn.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li><a href="index.html#intro" id="toc-intro">Intro<span></span></a></li>
<li><a href="#part-20-02" id="toc-part-20-02">(PART) 20-02<span></span></a></li>
<li><a href="categorical.html#categorical" id="toc-categorical"><span class="toc-section-number">1</span> Categorical<span></span></a>
<ul>
<li><a href="overview.html#overview" id="toc-overview"><span class="toc-section-number">1.1</span> Overview<span></span></a>
<ul>
<li><a href="overview.html#data-type-and-statistical-analysis" id="toc-data-type-and-statistical-analysis"><span class="toc-section-number">1.1.1</span> Data Type and Statistical Analysis<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="bayesian.html#bayesian" id="toc-bayesian"><span class="toc-section-number">2</span> Bayesian<span></span></a>
<ul>
<li><a href="abstract.html#abstract" id="toc-abstract"><span class="toc-section-number">2.1</span> Abstract<span></span></a>
<ul>
<li><a href="abstract.html#변수의-독립성" id="toc-변수의-독립성"><span class="toc-section-number">2.1.1</span> 변수의 독립성<span></span></a></li>
<li><a href="abstract.html#교환가능성" id="toc-교환가능성"><span class="toc-section-number">2.1.2</span> 교환가능성<span></span></a></li>
</ul></li>
<li><a href="continual-aeassessment-method.html#continual-aeassessment-method" id="toc-continual-aeassessment-method"><span class="toc-section-number">2.2</span> Continual Aeassessment Method<span></span></a></li>
<li><a href="horseshoe-prior.html#horseshoe-prior" id="toc-horseshoe-prior"><span class="toc-section-number">2.3</span> Horseshoe Prior<span></span></a></li>
</ul></li>
<li><a href="#part-21-01" id="toc-part-21-01">(PART) 21-01<span></span></a></li>
<li><a href="mathematical-stats.html#mathematical-stats" id="toc-mathematical-stats"><span class="toc-section-number">3</span> Mathematical Stats<span></span></a>
<ul>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">3.1</span> Inference<span></span></a>
<ul>
<li><a href="inference.html#rao-blackwell-thm." id="toc-rao-blackwell-thm."><span class="toc-section-number">3.1.1</span> Rao-Blackwell thm.<span></span></a></li>
<li><a href="inference.html#completeness" id="toc-completeness"><span class="toc-section-number">3.1.2</span> Completeness<span></span></a></li>
<li><a href="inference.html#레만-쉐페-thm." id="toc-레만-쉐페-thm."><span class="toc-section-number">3.1.3</span> 레만-쉐페 thm.<span></span></a></li>
<li><a href="inference.html#raoblack" id="toc-raoblack"><span class="toc-section-number">3.1.4</span> Rao-Blackwell thm.<span></span></a></li>
</ul></li>
<li><a href="hypothesis-test.html#hypothesis-test" id="toc-hypothesis-test"><span class="toc-section-number">3.2</span> Hypothesis Test<span></span></a></li>
<li><a href="power-fucntion.html#power-fucntion" id="toc-power-fucntion"><span class="toc-section-number">3.3</span> Power Fucntion<span></span></a>
<ul>
<li><a href="power-fucntion.html#significance-probability-p-value" id="toc-significance-probability-p-value"><span class="toc-section-number">3.3.1</span> Significance Probability (p-value)<span></span></a></li>
</ul></li>
<li><a href="optimal-testing-method.html#optimal-testing-method" id="toc-optimal-testing-method"><span class="toc-section-number">3.4</span> Optimal Testing Method<span></span></a></li>
<li><a href="data-reduction.html#data-reduction" id="toc-data-reduction"><span class="toc-section-number">3.5</span> Data Reduction<span></span></a>
<ul>
<li><a href="data-reduction.html#sufficiency-principle" id="toc-sufficiency-principle"><span class="toc-section-number">3.5.1</span> Sufficiency Principle<span></span></a></li>
</ul></li>
<li><a href="borel-paradox.html#borel-paradox" id="toc-borel-paradox"><span class="toc-section-number">3.6</span> Borel Paradox<span></span></a></li>
<li><a href="neymanpearson-lemma.html#neymanpearson-lemma" id="toc-neymanpearson-lemma"><span class="toc-section-number">3.7</span> Neyman–Pearson lemma<span></span></a>
<ul>
<li><a href="neymanpearson-lemma.html#overview-1" id="toc-overview-1"><span class="toc-section-number">3.7.1</span> Overview<span></span></a></li>
<li><a href="neymanpearson-lemma.html#generalized-lrt" id="toc-generalized-lrt"><span class="toc-section-number">3.7.2</span> Generalized LRT<span></span></a></li>
</ul></li>
<li><a href="개념.html#개념" id="toc-개념"><span class="toc-section-number">3.8</span> 개념<span></span></a></li>
</ul></li>
<li><a href="mcmc.html#mcmc" id="toc-mcmc"><span class="toc-section-number">4</span> MCMC<span></span></a>
<ul>
<li><a href="importance-sampling.html#importance-sampling" id="toc-importance-sampling"><span class="toc-section-number">4.1</span> Importance Sampling<span></span></a>
<ul>
<li><a href="importance-sampling.html#independent-monte-carlo" id="toc-independent-monte-carlo"><span class="toc-section-number">4.1.1</span> Independent Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo"><span class="toc-section-number">4.2</span> Markov Chain Monte Carlo<span></span></a>
<ul>
<li><a href="markov-chain-monte-carlo.html#mh-algorithm" id="toc-mh-algorithm"><span class="toc-section-number">4.2.1</span> MH Algorithm<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used" id="toc-random-walk-chains-most-widely-used"><span class="toc-section-number">4.2.2</span> Random Walk Chains (Most Widely Used)<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler" id="toc-basic-gibbs-sampler"><span class="toc-section-number">4.2.3</span> Basic Gibbs Sampler<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#implementation" id="toc-implementation"><span class="toc-section-number">4.2.4</span> Implementation<span></span></a></li>
</ul></li>
<li><a href="advanced-mcmc-wk08.html#advanced-mcmc-wk08" id="toc-advanced-mcmc-wk08"><span class="toc-section-number">4.3</span> Advanced MCMC (wk08)<span></span></a>
<ul>
<li><a href="advanced-mcmc-wk08.html#data-augmentation" id="toc-data-augmentation"><span class="toc-section-number">4.3.1</span> Data Augmentation<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm" id="toc-hit-and-run-algorithm"><span class="toc-section-number">4.3.2</span> Hit-and-Run Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm" id="toc-metropolis-adjusted-langevin-algorithm"><span class="toc-section-number">4.3.3</span> Metropolis-Adjusted Langevin Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm" id="toc-multiple-try-metropolis-algorithm"><span class="toc-section-number">4.3.4</span> Multiple-Try Metropolis Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm" id="toc-reversible-jump-mcmc-algorithm"><span class="toc-section-number">4.3.5</span> Reversible Jump MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="auxiliary-variable-mcmc.html#auxiliary-variable-mcmc" id="toc-auxiliary-variable-mcmc"><span class="toc-section-number">4.4</span> Auxiliary Variable MCMC<span></span></a>
<ul>
<li><a href="auxiliary-variable-mcmc.html#introduction" id="toc-introduction"><span class="toc-section-number">4.4.1</span> Introduction<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution" id="toc-multimodal-target-distribution"><span class="toc-section-number">4.4.2</span> Multimodal Target Distribution<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants" id="toc-doubly-intractable-normalizing-constants"><span class="toc-section-number">4.4.3</span> Doubly-intractable Normalizing Constants<span></span></a></li>
</ul></li>
<li><a href="approximate-bayesian-computation.html#approximate-bayesian-computation" id="toc-approximate-bayesian-computation"><span class="toc-section-number">4.5</span> Approximate Bayesian Computation<span></span></a>
<ul>
<li><a href="approximate-bayesian-computation.html#simulator-based-models" id="toc-simulator-based-models"><span class="toc-section-number">4.5.1</span> Simulator-Based Models<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods" id="toc-abcifying-monte-carlo-methods"><span class="toc-section-number">4.5.2</span> ABCifying Monte Carlo Methods<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm" id="toc-abc-mcmc-algorithm"><span class="toc-section-number">4.5.3</span> ABC-MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="hamiltonian-monte-carlo.html#hamiltonian-monte-carlo" id="toc-hamiltonian-monte-carlo"><span class="toc-section-number">4.6</span> Hamiltonian Monte Carlo<span></span></a>
<ul>
<li><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo" id="toc-introduction-to-hamiltonian-monte-carlo"><span class="toc-section-number">4.6.1</span> Introduction to Hamiltonian Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="population-monte-carlo.html#population-monte-carlo" id="toc-population-monte-carlo"><span class="toc-section-number">4.7</span> Population Monte Carlo<span></span></a>
<ul>
<li><a href="population-monte-carlo.html#adaptive-direction-sampling" id="toc-adaptive-direction-sampling"><span class="toc-section-number">4.7.1</span> Adaptive Direction Sampling<span></span></a></li>
<li><a href="population-monte-carlo.html#conjugate-gradient-mc" id="toc-conjugate-gradient-mc"><span class="toc-section-number">4.7.2</span> Conjugate Gradient MC<span></span></a></li>
<li><a href="population-monte-carlo.html#parallel-tempering" id="toc-parallel-tempering"><span class="toc-section-number">4.7.3</span> Parallel Tempering<span></span></a></li>
<li><a href="population-monte-carlo.html#evolutionary-mc" id="toc-evolutionary-mc"><span class="toc-section-number">4.7.4</span> Evolutionary MC<span></span></a></li>
<li><a href="population-monte-carlo.html#sequential-parallel-tempering" id="toc-sequential-parallel-tempering"><span class="toc-section-number">4.7.5</span> Sequential Parallel Tempering<span></span></a></li>
</ul></li>
<li><a href="stochastic-approximation-monte-carlo.html#stochastic-approximation-monte-carlo" id="toc-stochastic-approximation-monte-carlo"><span class="toc-section-number">4.8</span> Stochastic Approximation Monte Carlo<span></span></a></li>
<li><a href="review.html#review" id="toc-review"><span class="toc-section-number">4.9</span> Review<span></span></a>
<ul>
<li><a href="review.html#wk01" id="toc-wk01"><span class="toc-section-number">4.9.1</span> Wk01<span></span></a></li>
<li><a href="review.html#wk03" id="toc-wk03"><span class="toc-section-number">4.9.2</span> wk03<span></span></a></li>
<li><a href="review.html#wk04-05" id="toc-wk04-05"><span class="toc-section-number">4.9.3</span> wk04, 05<span></span></a></li>
</ul></li>
<li><a href="else.html#else" id="toc-else"><span class="toc-section-number">4.10</span> Else<span></span></a>
<ul>
<li><a href="else.html#hw4.-rasch-model" id="toc-hw4.-rasch-model"><span class="toc-section-number">4.10.1</span> Hw4. Rasch Model<span></span></a></li>
<li><a href="else.html#da-example-mvn" id="toc-da-example-mvn"><span class="toc-section-number">4.10.2</span> DA) Example: MVN<span></span></a></li>
<li><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes" id="toc-bayesian-adaptive-clinical-trial-with-delayed-outcomes"><span class="toc-section-number">4.10.3</span> Bayesian adaptive clinical trial with delayed outcomes<span></span></a></li>
<li><a href="else.html#nmar의-종류" id="toc-nmar의-종류"><span class="toc-section-number">4.10.4</span> NMAR의 종류<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-selection" id="toc-wk10-bayesian-model-selection"><span class="toc-section-number">4.10.5</span> wk10) Bayesian Model Selection<span></span></a></li>
<li><a href="else.html#autologistic-model" id="toc-autologistic-model"><span class="toc-section-number">4.10.6</span> Autologistic model<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-averaging" id="toc-wk10-bayesian-model-averaging"><span class="toc-section-number">4.10.7</span> wk10) Bayesian Model Averaging<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="mva.html#mva" id="toc-mva"><span class="toc-section-number">5</span> MVA<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#overview-of-mva-not-ended" id="toc-overview-of-mva-not-ended"><span class="toc-section-number">5.1</span> Overview of mva (not ended)<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#notation" id="toc-notation"><span class="toc-section-number">5.1.1</span> Notation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">5.1.2</span> Summary Statistics<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation" id="toc-statistical-inference-on-correlation"><span class="toc-section-number">5.1.3</span> Statistical Inference on Correlation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#standardization" id="toc-standardization"><span class="toc-section-number">5.1.4</span> Standardization<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#missing-value-treatment" id="toc-missing-value-treatment"><span class="toc-section-number">5.1.5</span> Missing Value Treatment<span></span></a></li>
</ul></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-nomral-wk2" id="toc-multivariate-nomral-wk2"><span class="toc-section-number">5.2</span> Multivariate Nomral (wk2)<span></span></a>
<ul>
<li><a href="multivariate-nomral-wk2.html#overview-2" id="toc-overview-2"><span class="toc-section-number">5.2.1</span> Overview<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">5.2.2</span> Spectral Decomposition<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">5.2.3</span> Properties of MVN<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#chi2-distribution" id="toc-chi2-distribution"><span class="toc-section-number">5.2.4</span> <span class="math inline">\(\Chi^2\)</span> distribution<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors" id="toc-linear-combination-of-random-vectors"><span class="toc-section-number">5.2.5</span> Linear Combination of Random Vectors<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood" id="toc-multivariate-normal-likelihood"><span class="toc-section-number">5.2.6</span> Multivariate Normal Likelihood<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s" id="toc-sampling-distribtion-of-bar-pmb-y-s"><span class="toc-section-number">5.2.7</span> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span><span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#assessing-normality" id="toc-assessing-normality"><span class="toc-section-number">5.2.8</span> Assessing Normality<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#power-transformation" id="toc-power-transformation"><span class="toc-section-number">5.2.9</span> Power Transformation<span></span></a></li>
</ul></li>
<li><a href="inference-about-mean-vector-wk3.html#inference-about-mean-vector-wk3" id="toc-inference-about-mean-vector-wk3"><span class="toc-section-number">5.3</span> Inference about Mean Vector (wk3)<span></span></a>
<ul>
<li><a href="inference-about-mean-vector-wk3.html#overview-3" id="toc-overview-3"><span class="toc-section-number">5.3.1</span> Overview<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#confidence-region" id="toc-confidence-region"><span class="toc-section-number">5.3.2</span> 1. Confidence Region<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#simultaneous-ci" id="toc-simultaneous-ci"><span class="toc-section-number">5.3.3</span> 2. Simultaneous CI<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison" id="toc-note-bonferroni-multiple-comparison"><span class="toc-section-number">5.3.4</span> 3. Note: Bonferroni Multiple Comparison<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector" id="toc-large-sample-inferences-about-a-mean-vector"><span class="toc-section-number">5.3.5</span> 4. Large Sample Inferences about a Mean Vector<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5" id="toc-profile-analysis-wk4-5"><span class="toc-section-number">5.3.6</span> 1. Profile Analysis (wk4, 5)<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend" id="toc-test-for-linear-trend"><span class="toc-section-number">5.3.7</span> 2. Test for Linear Trend<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix" id="toc-inferences-about-a-covariance-matrix"><span class="toc-section-number">5.3.8</span> 3. Inferences about a Covariance Matrix<span></span></a></li>
</ul></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparison-of-several-mv-means-wk5" id="toc-comparison-of-several-mv-means-wk5"><span class="toc-section-number">5.4</span> Comparison of Several MV Means (wk5)<span></span></a>
<ul>
<li><a href="comparison-of-several-mv-means-wk5.html#paired-comparison" id="toc-paired-comparison"><span class="toc-section-number">5.4.1</span> Paired Comparison<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations" id="toc-comparing-mean-vectors-from-two-populations"><span class="toc-section-number">5.4.2</span> Comparing Mean Vectors from Two Populations<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2" id="toc-profile-analysis-for-g2"><span class="toc-section-number">5.4.3</span> Profile Analysis (for <span class="math inline">\(g=2\)</span>)<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means" id="toc-comparing-several-multivariate-population-means"><span class="toc-section-number">5.4.4</span> Comparing Several Multivariate Population Means<span></span></a></li>
</ul></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression-wk6" id="toc-multivariate-multiple-regression-wk6"><span class="toc-section-number">5.5</span> Multivariate Multiple Regression (wk6)<span></span></a>
<ul>
<li><a href="multivariate-multiple-regression-wk6.html#overview-4" id="toc-overview-4"><span class="toc-section-number">5.5.1</span> Overview<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression" id="toc-multivariate-multiple-regression"><span class="toc-section-number">5.5.2</span> Multivariate Multiple Regression<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#example" id="toc-example"><span class="toc-section-number">5.5.3</span> Example)<span></span></a></li>
</ul></li>
<li><a href="pca.html#pca" id="toc-pca"><span class="toc-section-number">5.6</span> PCA<span></span></a></li>
<li><a href="factor.html#factor" id="toc-factor"><span class="toc-section-number">5.7</span> Factor<span></span></a>
<ul>
<li><a href="factor.html#method-of-estimation" id="toc-method-of-estimation"><span class="toc-section-number">5.7.1</span> Method of Estimation<span></span></a></li>
<li><a href="factor.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">5.7.2</span> Factor Rotation<span></span></a></li>
<li><a href="factor.html#varimax-criterion" id="toc-varimax-criterion"><span class="toc-section-number">5.7.3</span> Varimax Criterion<span></span></a></li>
<li><a href="factor.html#factor-scores" id="toc-factor-scores"><span class="toc-section-number">5.7.4</span> Factor Scores<span></span></a></li>
</ul></li>
<li><a href="discrimination-and-classification.html#discrimination-and-classification" id="toc-discrimination-and-classification"><span class="toc-section-number">5.8</span> Discrimination and Classification<span></span></a>
<ul>
<li><a href="discrimination-and-classification.html#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">5.8.1</span> Bayes Rule<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations" id="toc-classification-with-two-mv-n-populations"><span class="toc-section-number">5.8.2</span> Classification with Two mv <span class="math inline">\(N\)</span> Populations<span></span></a></li>
<li><a href="discrimination-and-classification.html#evaluating-classification-functions" id="toc-evaluating-classification-functions"><span class="toc-section-number">5.8.3</span> Evaluating Classification Functions<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-several-populations-wk13" id="toc-classification-with-several-populations-wk13"><span class="toc-section-number">5.8.4</span> Classification with several Populations (wk13)<span></span></a></li>
<li><a href="discrimination-and-classification.html#other-discriminant-analysis-methods" id="toc-other-discriminant-analysis-methods"><span class="toc-section-number">5.8.5</span> Other Discriminant Analysis Methods<span></span></a></li>
</ul></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-distance-methods-and-ordination" id="toc-clustering-distance-methods-and-ordination"><span class="toc-section-number">5.9</span> Clustering, Distance Methods, and Ordination<span></span></a>
<ul>
<li><a href="clustering-distance-methods-and-ordination.html#overview-5" id="toc-overview-5"><span class="toc-section-number">5.9.1</span> Overview<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">5.9.2</span> Hierarchical Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">5.9.3</span> K-means Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법" id="toc-군집의-평가방법"><span class="toc-section-number">5.9.4</span> 군집의 평가방법<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14" id="toc-clustering-using-density-estimation-wk14"><span class="toc-section-number">5.9.5</span> Clustering using Density Estimation (wk14)<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds" id="toc-multidimensional-scaling-mds"><span class="toc-section-number">5.9.6</span> Multidimensional Scaling (MDS)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linear.html#linear" id="toc-linear"><span class="toc-section-number">6</span> Linear<span></span></a>
<ul>
<li><a href="overview-svd.html#overview-svd" id="toc-overview-svd"><span class="toc-section-number">6.1</span> Overview &amp; SVD<span></span></a>
<ul>
<li><a href="overview-svd.html#spectral-decomposition-1" id="toc-spectral-decomposition-1"><span class="toc-section-number">6.1.1</span> Spectral Decomposition<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-general-version" id="toc-singular-value-decomposition-general-version"><span class="toc-section-number">6.1.2</span> Singular value Decomposition: General-version<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-another-version" id="toc-singular-value-decomposition-another-version"><span class="toc-section-number">6.1.3</span> Singular value Decomposition: Another-version<span></span></a></li>
<li><a href="overview-svd.html#quadratic-forms" id="toc-quadratic-forms"><span class="toc-section-number">6.1.4</span> Quadratic Forms<span></span></a></li>
<li><a href="overview-svd.html#partitioned-matrices" id="toc-partitioned-matrices"><span class="toc-section-number">6.1.5</span> Partitioned Matrices<span></span></a></li>
<li><a href="overview-svd.html#geometrical-aspects" id="toc-geometrical-aspects"><span class="toc-section-number">6.1.6</span> Geometrical Aspects<span></span></a></li>
<li><a href="overview-svd.html#column-row-and-null-space" id="toc-column-row-and-null-space"><span class="toc-section-number">6.1.7</span> Column, Row and Null Space<span></span></a></li>
</ul></li>
<li><a href="introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">6.2</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-1.html#what" id="toc-what"><span class="toc-section-number">6.2.1</span> What<span></span></a></li>
<li><a href="introduction-1.html#random-vectors-and-matrices" id="toc-random-vectors-and-matrices"><span class="toc-section-number">6.2.2</span> Random Vectors and Matrices<span></span></a></li>
<li><a href="introduction-1.html#multivariate-normal-distributions" id="toc-multivariate-normal-distributions"><span class="toc-section-number">6.2.3</span> Multivariate Normal Distributions<span></span></a></li>
<li><a href="introduction-1.html#distributions-of-quadratic-forms" id="toc-distributions-of-quadratic-forms"><span class="toc-section-number">6.2.4</span> Distributions of Quadratic Forms<span></span></a></li>
</ul></li>
<li><a href="estimation.html#estimation" id="toc-estimation"><span class="toc-section-number">6.3</span> Estimation<span></span></a>
<ul>
<li><a href="estimation.html#identifiability-and-estimability" id="toc-identifiability-and-estimability"><span class="toc-section-number">6.3.1</span> Identifiability and Estimability<span></span></a></li>
<li><a href="estimation.html#estimation-least-squares" id="toc-estimation-least-squares"><span class="toc-section-number">6.3.2</span> Estimation: Least Squares<span></span></a></li>
<li><a href="estimation.html#estimation-best-linear-unbiased" id="toc-estimation-best-linear-unbiased"><span class="toc-section-number">6.3.3</span> Estimation: Best Linear Unbiased<span></span></a></li>
<li><a href="estimation.html#estimation-maximum-likelihood" id="toc-estimation-maximum-likelihood"><span class="toc-section-number">6.3.4</span> Estimation: Maximum Likelihood<span></span></a></li>
<li><a href="estimation.html#estimation-minimum-variance-unbiased" id="toc-estimation-minimum-variance-unbiased"><span class="toc-section-number">6.3.5</span> Estimation: Minimum Variance Unbiased<span></span></a></li>
<li><a href="estimation.html#sampling-distributions-of-estimates" id="toc-sampling-distributions-of-estimates"><span class="toc-section-number">6.3.6</span> Sampling Distributions of Estimates<span></span></a></li>
<li><a href="estimation.html#generalized-least-squaresgls" id="toc-generalized-least-squaresgls"><span class="toc-section-number">6.3.7</span> Generalized Least Squares(GLS)<span></span></a></li>
</ul></li>
<li><a href="one-way-anova.html#one-way-anova" id="toc-one-way-anova"><span class="toc-section-number">6.4</span> One-Way ANOVA<span></span></a>
<ul>
<li><a href="one-way-anova.html#one-way-anova-1" id="toc-one-way-anova-1"><span class="toc-section-number">6.4.1</span> One-Way ANOVA<span></span></a></li>
<li><a href="one-way-anova.html#more-about-models" id="toc-more-about-models"><span class="toc-section-number">6.4.2</span> More About Models<span></span></a></li>
<li><a href="one-way-anova.html#estimating-and-testing-contrasts" id="toc-estimating-and-testing-contrasts"><span class="toc-section-number">6.4.3</span> Estimating and Testing Contrasts<span></span></a></li>
<li><a href="one-way-anova.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">6.4.4</span> Cochran’s Theorem<span></span></a></li>
</ul></li>
<li><a href="testing.html#testing" id="toc-testing"><span class="toc-section-number">6.5</span> Testing<span></span></a>
<ul>
<li><a href="testing.html#more-about-models-two-approaches-for-linear-model" id="toc-more-about-models-two-approaches-for-linear-model"><span class="toc-section-number">6.5.1</span> More About Models: Two approaches for linear model<span></span></a></li>
<li><a href="testing.html#testing-models" id="toc-testing-models"><span class="toc-section-number">6.5.2</span> Testing Models<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure" id="toc-a-generalized-test-procedure"><span class="toc-section-number">6.5.3</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-linear-parametric-functions" id="toc-testing-linear-parametric-functions"><span class="toc-section-number">6.5.4</span> Testing Linear Parametric Functions<span></span></a></li>
<li><a href="testing.html#theoretical-complements" id="toc-theoretical-complements"><span class="toc-section-number">6.5.5</span> Theoretical Complements<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure-1" id="toc-a-generalized-test-procedure-1"><span class="toc-section-number">6.5.6</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace" id="toc-testing-single-degrees-of-freedom-in-a-given-subspace"><span class="toc-section-number">6.5.7</span> Testing Single Degrees of Freedom in a Given Subspace<span></span></a></li>
<li><a href="testing.html#breaking-ss-into-independent-components" id="toc-breaking-ss-into-independent-components"><span class="toc-section-number">6.5.8</span> Breaking SS into Independent Components<span></span></a></li>
<li><a href="testing.html#general-theory" id="toc-general-theory"><span class="toc-section-number">6.5.9</span> General Theory<span></span></a></li>
<li><a href="testing.html#two-way-anova" id="toc-two-way-anova"><span class="toc-section-number">6.5.10</span> Two-Way ANOVA<span></span></a></li>
<li><a href="testing.html#confidence-regions" id="toc-confidence-regions"><span class="toc-section-number">6.5.11</span> Confidence Regions<span></span></a></li>
<li><a href="testing.html#tests-for-generalized-least-squares-models" id="toc-tests-for-generalized-least-squares-models"><span class="toc-section-number">6.5.12</span> Tests for Generalized Least Squares Models<span></span></a></li>
</ul></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">6.6</span> Generalized Least Squares<span></span></a>
<ul>
<li><a href="generalized-least-squares.html#a-direct-solution-via-inner-products" id="toc-a-direct-solution-via-inner-products"><span class="toc-section-number">6.6.1</span> A direct solution via inner products<span></span></a></li>
</ul></li>
<li><a href="flat.html#flat" id="toc-flat"><span class="toc-section-number">6.7</span> Flat<span></span></a>
<ul>
<li><a href="flat.html#flat-1" id="toc-flat-1"><span class="toc-section-number">6.7.1</span> 1.Flat<span></span></a></li>
<li><a href="flat.html#solutions-to-systems-of-linear-equations" id="toc-solutions-to-systems-of-linear-equations"><span class="toc-section-number">6.7.2</span> 2. Solutions to systems of linear equations<span></span></a></li>
</ul></li>
<li><a href="unified-approach-to-balanced-anova-models.html#unified-approach-to-balanced-anova-models" id="toc-unified-approach-to-balanced-anova-models"><span class="toc-section-number">6.8</span> Unified Approach to Balanced ANOVA Models<span></span></a></li>
</ul></li>
<li><a href="#part-21-02" id="toc-part-21-02">(PART) 21-02<span></span></a></li>
<li><a href="network-stats.html#network-stats" id="toc-network-stats"><span class="toc-section-number">7</span> Network Stats<span></span></a>
<ul>
<li><a href="introduction-2.html#introduction-2" id="toc-introduction-2"><span class="toc-section-number">7.1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-2.html#types-of-network-analysis" id="toc-types-of-network-analysis"><span class="toc-section-number">7.1.1</span> Types of Network Analysis<span></span></a></li>
<li><a href="introduction-2.html#network-modeling-and-inference" id="toc-network-modeling-and-inference"><span class="toc-section-number">7.1.2</span> Network Modeling and Inference<span></span></a></li>
<li><a href="introduction-2.html#network-processes" id="toc-network-processes"><span class="toc-section-number">7.1.3</span> Network Processes<span></span></a></li>
</ul></li>
<li><a href="descriptive-statistics-of-networks.html#descriptive-statistics-of-networks" id="toc-descriptive-statistics-of-networks"><span class="toc-section-number">7.2</span> Descriptive Statistics of Networks<span></span></a>
<ul>
<li><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics" id="toc-vertex-and-edge-characteristics"><span class="toc-section-number">7.2.1</span> Vertex and Edge Characteristics<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion" id="toc-characterizing-network-cohesion"><span class="toc-section-number">7.2.2</span> Characterizing Network Cohesion<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#graph-partitioning" id="toc-graph-partitioning"><span class="toc-section-number">7.2.3</span> Graph Partitioning<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing" id="toc-assortativity-and-mixing"><span class="toc-section-number">7.2.4</span> Assortativity and Mixing<span></span></a></li>
</ul></li>
<li><a href="data-collection-and-sampling.html#data-collection-and-sampling" id="toc-data-collection-and-sampling"><span class="toc-section-number">7.3</span> Data Collection and Sampling<span></span></a>
<ul>
<li><a href="data-collection-and-sampling.html#sampling-designs" id="toc-sampling-designs"><span class="toc-section-number">7.3.1</span> Sampling Designs<span></span></a></li>
<li><a href="data-collection-and-sampling.html#coping-strategies" id="toc-coping-strategies"><span class="toc-section-number">7.3.2</span> Coping Strategies<span></span></a></li>
<li><a href="data-collection-and-sampling.html#big-data-solves-nothing" id="toc-big-data-solves-nothing"><span class="toc-section-number">7.3.3</span> Big Data Solves Nothing<span></span></a></li>
</ul></li>
<li><a href="mathematical-models-for-network-graphs.html#mathematical-models-for-network-graphs" id="toc-mathematical-models-for-network-graphs"><span class="toc-section-number">7.4</span> Mathematical Models for Network Graphs<span></span></a>
<ul>
<li><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models" id="toc-classical-random-graph-models"><span class="toc-section-number">7.4.1</span> Classical Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models" id="toc-generalized-random-graph-models"><span class="toc-section-number">7.4.2</span> Generalized Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms" id="toc-network-graph-models-based-on-mechanisms"><span class="toc-section-number">7.4.3</span> Network Graph Models Based on Mechanisms<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics" id="toc-assessing-significance-of-network-graph-characteristics"><span class="toc-section-number">7.4.4</span> Assessing Significance of Network Graph Characteristics<span></span></a></li>
</ul></li>
<li><a href="introduction-to-ergm.html#introduction-to-ergm" id="toc-introduction-to-ergm"><span class="toc-section-number">7.5</span> Introduction to ERGM<span></span></a>
<ul>
<li><a href="introduction-to-ergm.html#exponential-random-graph-models" id="toc-exponential-random-graph-models"><span class="toc-section-number">7.5.1</span> Exponential Random Graph Models<span></span></a></li>
<li><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation" id="toc-difficulty-in-parameter-estimation"><span class="toc-section-number">7.5.2</span> Difficulty in Parameter Estimation<span></span></a></li>
</ul></li>
<li><a href="parameter-estimation-of-ergm.html#parameter-estimation-of-ergm" id="toc-parameter-estimation-of-ergm"><span class="toc-section-number">7.6</span> Parameter Estimation of ERGM<span></span></a>
<ul>
<li><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm" id="toc-current-methods-for-ergm"><span class="toc-section-number">7.6.1</span> Current Methods for ERGM<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm" id="toc-approximation-based-algorithm"><span class="toc-section-number">7.6.2</span> Approximation-based Algorithm<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches" id="toc-auxiliary-variable-mcmc-based-approaches"><span class="toc-section-number">7.6.3</span> Auxiliary Variable MCMC-based Approaches<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc" id="toc-varying-trunction-stochastic-approximation-mcmc"><span class="toc-section-number">7.6.4</span> Varying Trunction Stochastic Approximation MCMC<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#conclusion" id="toc-conclusion"><span class="toc-section-number">7.6.5</span> Conclusion<span></span></a></li>
</ul></li>
<li><a href="ergm-for-dynamic-networks.html#ergm-for-dynamic-networks" id="toc-ergm-for-dynamic-networks"><span class="toc-section-number">7.7</span> ERGM for Dynamic Networks<span></span></a>
<ul>
<li><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm" id="toc-temporal-ergm-tergm-t-ergm"><span class="toc-section-number">7.7.1</span> Temporal ERGM (TERGM, T ERGM)<span></span></a></li>
<li><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm" id="toc-separable-temporal-ergm-stergm-st-ergm"><span class="toc-section-number">7.7.2</span> Separable Temporal ERGM (STERGM, ST ERGM)<span></span></a></li>
</ul></li>
<li><a href="latent-network-models.html#latent-network-models" id="toc-latent-network-models"><span class="toc-section-number">7.8</span> Latent Network Models<span></span></a>
<ul>
<li><a href="latent-network-models.html#latent-position-model" id="toc-latent-position-model"><span class="toc-section-number">7.8.1</span> Latent Position Model<span></span></a></li>
<li><a href="latent-network-models.html#latent-position-cluster-model" id="toc-latent-position-cluster-model"><span class="toc-section-number">7.8.2</span> Latent Position Cluster Model<span></span></a></li>
</ul></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#additive-and-multiplicative-effects-network-models" id="toc-additive-and-multiplicative-effects-network-models"><span class="toc-section-number">7.9</span> Additive and Multiplicative Effects Network Models<span></span></a>
<ul>
<li><a href="additive-and-multiplicative-effects-network-models.html#introduction-3" id="toc-introduction-3"><span class="toc-section-number">7.9.1</span> Introduction<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression" id="toc-social-relations-regression"><span class="toc-section-number">7.9.2</span> Social Relations Regression<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models" id="toc-multiplicative-effects-models"><span class="toc-section-number">7.9.3</span> Multiplicative Effects Models<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation" id="toc-inference-via-posterior-approximation"><span class="toc-section-number">7.9.4</span> Inference via Posterior Approximation<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r" id="toc-discussion-and-example-with-r"><span class="toc-section-number">7.9.5</span> Discussion and Example with R<span></span></a></li>
</ul></li>
<li><a href="stochastic-block-models.html#stochastic-block-models" id="toc-stochastic-block-models"><span class="toc-section-number">7.10</span> Stochastic Block Models<span></span></a>
<ul>
<li><a href="stochastic-block-models.html#stochastic-block-model" id="toc-stochastic-block-model"><span class="toc-section-number">7.10.1</span> Stochastic Block Model<span></span></a></li>
<li><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm" id="toc-mixed-membership-block-model-mmbm"><span class="toc-section-number">7.10.2</span> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="high-dimension.html#high-dimension" id="toc-high-dimension"><span class="toc-section-number">8</span> High Dimension<span></span></a>
<ul>
<li><a href="introduction-4.html#introduction-4" id="toc-introduction-4"><span class="toc-section-number">8.1</span> Introduction<span></span></a></li>
<li><a href="concentration-inequalities.html#concentration-inequalities" id="toc-concentration-inequalities"><span class="toc-section-number">8.2</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities.html#motivation" id="toc-motivation"><span class="toc-section-number">8.2.1</span> Motivation<span></span></a></li>
<li><a href="concentration-inequalities.html#from-markov-to-chernoff" id="toc-from-markov-to-chernoff"><span class="toc-section-number">8.2.2</span> From Markov to Chernoff<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-variables" id="toc-sub-gaussian-random-variables"><span class="toc-section-number">8.2.3</span> sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables" id="toc-properties-of-sub-gaussian-random-variables"><span class="toc-section-number">8.2.4</span> Properties of sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#equivalent-definitions" id="toc-equivalent-definitions"><span class="toc-section-number">8.2.5</span> Equivalent definitions<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-vectors" id="toc-sub-gaussian-random-vectors"><span class="toc-section-number">8.2.6</span> Sub-Gaussian random vectors<span></span></a></li>
<li><a href="concentration-inequalities.html#hoeffdings-inequality" id="toc-hoeffdings-inequality"><span class="toc-section-number">8.2.7</span> Hoeffding’s inequality<span></span></a></li>
<li><a href="concentration-inequalities.html#maximal-inequalities" id="toc-maximal-inequalities"><span class="toc-section-number">8.2.8</span> Maximal inequalities<span></span></a></li>
<li><a href="concentration-inequalities.html#section" id="toc-section"><span class="toc-section-number">8.2.9</span> </a></li>
</ul></li>
<li><a href="concentration-inequalities-1.html#concentration-inequalities-1" id="toc-concentration-inequalities-1"><span class="toc-section-number">8.3</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities-1.html#sub-exponential-random-variables" id="toc-sub-exponential-random-variables"><span class="toc-section-number">8.3.1</span> Sub-exponential random variables<span></span></a></li>
<li><a href="concentration-inequalities-1.html#bernsteins-condition" id="toc-bernsteins-condition"><span class="toc-section-number">8.3.2</span> Bernstein’s condition<span></span></a></li>
<li><a href="concentration-inequalities-1.html#mcdiarmids-inequality" id="toc-mcdiarmids-inequality"><span class="toc-section-number">8.3.3</span> McDiarmid’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#levys-inequality" id="toc-levys-inequality"><span class="toc-section-number">8.3.4</span> Levy’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#quadratic-form" id="toc-quadratic-form"><span class="toc-section-number">8.3.5</span> Quadratic form<span></span></a></li>
<li><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma" id="toc-the-johnsonlindenstrauss-lemma"><span class="toc-section-number">8.3.6</span> The Johnson–Lindenstrauss Lemma<span></span></a></li>
</ul></li>
<li><a href="metric-entropy-and-its-uses.html#metric-entropy-and-its-uses" id="toc-metric-entropy-and-its-uses"><span class="toc-section-number">8.4</span> Metric entropy and its uses<span></span></a>
<ul>
<li><a href="metric-entropy-and-its-uses.html#metric-space" id="toc-metric-space"><span class="toc-section-number">8.4.1</span> Metric space<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy" id="toc-covering-numbers-and-metric-entropy"><span class="toc-section-number">8.4.2</span> Covering numbers and metric entropy<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#packing-numbers" id="toc-packing-numbers"><span class="toc-section-number">8.4.3</span> Packing numbers<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#section-1" id="toc-section-1"><span class="toc-section-number">8.4.4</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-2" id="toc-section-2"><span class="toc-section-number">8.4.5</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-3" id="toc-section-3"><span class="toc-section-number">8.4.6</span> </a></li>
</ul></li>
<li><a href="covariance-estimation.html#covariance-estimation" id="toc-covariance-estimation"><span class="toc-section-number">8.5</span> Covariance estimation<span></span></a>
<ul>
<li><a href="covariance-estimation.html#matrix-algebra-review" id="toc-matrix-algebra-review"><span class="toc-section-number">8.5.1</span> Matrix algebra review<span></span></a></li>
<li><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm" id="toc-covariance-matrix-estimation-in-the-operator-norm"><span class="toc-section-number">8.5.2</span> Covariance matrix estimation in the operator norm<span></span></a></li>
<li><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices" id="toc-bounds-for-structured-covariance-matrices"><span class="toc-section-number">8.5.3</span> Bounds for structured covariance matrices<span></span></a></li>
</ul></li>
<li><a href="matrix-concentration-inequalities.html#matrix-concentration-inequalities" id="toc-matrix-concentration-inequalities"><span class="toc-section-number">8.6</span> Matrix concentration inequalities<span></span></a>
<ul>
<li><a href="matrix-concentration-inequalities.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">8.6.1</span> Matrix calculus<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#matrix-chernoff" id="toc-matrix-chernoff"><span class="toc-section-number">8.6.2</span> Matrix Chernoff<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices" id="toc-sub-gaussian-and-sub-exponential-matrices"><span class="toc-section-number">8.6.3</span> Sub-Gaussian and sub-exponential matrices<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds" id="toc-랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><span class="toc-section-number">8.6.4</span> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds<span></span></a></li>
</ul></li>
<li><a href="principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.7</span> Principal Component Analysis<span></span></a>
<ul>
<li><a href="principal-component-analysis.html#pca-1" id="toc-pca-1"><span class="toc-section-number">8.7.1</span> PCA<span></span></a></li>
<li><a href="principal-component-analysis.html#matrix-perturbation" id="toc-matrix-perturbation"><span class="toc-section-number">8.7.2</span> Matrix Perturbation<span></span></a></li>
<li><a href="principal-component-analysis.html#spiked-cov-model" id="toc-spiked-cov-model"><span class="toc-section-number">8.7.3</span> Spiked Cov Model<span></span></a></li>
<li><a href="principal-component-analysis.html#sparse-pca" id="toc-sparse-pca"><span class="toc-section-number">8.7.4</span> sparse PCA<span></span></a></li>
</ul></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">8.8</span> Linear Regression<span></span></a>
<ul>
<li><a href="linear-regression.html#problem-formulation" id="toc-problem-formulation"><span class="toc-section-number">8.8.1</span> Problem formulation<span></span></a></li>
<li><a href="linear-regression.html#least-squares-estimator-in-high-dimensions" id="toc-least-squares-estimator-in-high-dimensions"><span class="toc-section-number">8.8.2</span> Least Squares Estimator in high dimensions<span></span></a></li>
<li><a href="linear-regression.html#sparse-linear-regression" id="toc-sparse-linear-regression"><span class="toc-section-number">8.8.3</span> Sparse linear regression<span></span></a></li>
</ul></li>
<li><a href="uniform-laws-of-large-numbers.html#uniform-laws-of-large-numbers" id="toc-uniform-laws-of-large-numbers"><span class="toc-section-number">8.9</span> Uniform laws of large numbers<span></span></a>
<ul>
<li><a href="uniform-laws-of-large-numbers.html#motivation-1" id="toc-motivation-1"><span class="toc-section-number">8.9.1</span> Motivation<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity" id="toc-a-uniform-law-via-rademacher-complexity"><span class="toc-section-number">8.9.2</span> A uniform law via Rademacher complexity<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity" id="toc-upper-bounds-on-the-rademacher-complexity"><span class="toc-section-number">8.9.3</span> Upper bounds on the Rademacher complexity<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="survival-analysis.html#survival-analysis" id="toc-survival-analysis"><span class="toc-section-number">9</span> Survival Analysis<span></span></a>
<ul>
<li><a href="introduction-5.html#introduction-5" id="toc-introduction-5"><span class="toc-section-number">9.1</span> Introduction<span></span></a></li>
<li><a href="section-4.html#section-4" id="toc-section-4"><span class="toc-section-number">9.2</span> </a></li>
<li><a href="counting-processes-and-martingales.html#counting-processes-and-martingales" id="toc-counting-processes-and-martingales"><span class="toc-section-number">9.3</span> Counting Processes and Martingales<span></span></a>
<ul>
<li><a href="counting-processes-and-martingales.html#conditional-expectation" id="toc-conditional-expectation"><span class="toc-section-number">9.3.1</span> Conditional Expectation<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#martingale" id="toc-martingale"><span class="toc-section-number">9.3.2</span> Martingale<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#key-martingales-properties" id="toc-key-martingales-properties"><span class="toc-section-number">9.3.3</span> Key Martingales Properties<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#section-5" id="toc-section-5"><span class="toc-section-number">9.3.4</span> </a></li>
<li><a href="counting-processes-and-martingales.html#section-6" id="toc-section-6"><span class="toc-section-number">9.3.5</span> </a></li>
</ul></li>
<li><a href="section-7.html#section-7" id="toc-section-7"><span class="toc-section-number">9.4</span> </a></li>
<li><a href="cox-regression.html#cox-regression" id="toc-cox-regression"><span class="toc-section-number">9.5</span> Cox Regression<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#filtration의-개념을-정복하자" id="toc-filtration의-개념을-정복하자"><span class="toc-section-number">9.6</span> Filtration의 개념을 정복하자!<span></span></a>
<ul>
<li><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약" id="toc-random-process를-이야기-하기까지의-긴-여정의-요약"><span class="toc-section-number">9.6.1</span> Random Process를 이야기 하기까지의 긴 여정의 요약<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#ft-measurable" id="toc-ft-measurable"><span class="toc-section-number">9.6.2</span> Ft-measurable<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#epilogue" id="toc-epilogue"><span class="toc-section-number">9.6.3</span> EPILOGUE<span></span></a></li>
</ul></li>
<li><a href="concepts.html#concepts" id="toc-concepts"><span class="toc-section-number">9.7</span> Concepts<span></span></a></li>
</ul></li>
<li><a href="#part-22-01" id="toc-part-22-01">(PART) 22-01<span></span></a></li>
<li><a href="scikit.html#scikit" id="toc-scikit"><span class="toc-section-number">10</span> scikit<span></span></a>
<ul>
<li><a href="linear-models.html#linear-models" id="toc-linear-models"><span class="toc-section-number">10.1</span> Linear Models<span></span></a>
<ul>
<li><a href="linear-models.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">10.1.1</span> Ordinary Least Squares<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-00-00" id="toc-appendix-00-00">(APPENDIX) 00-00<span></span></a></li>
<li><a href="concepts-1.html#concepts-1" id="toc-concepts-1"><span class="toc-section-number">11</span> Concepts<span></span></a>
<ul>
<li><a href="autologistic.html#autologistic" id="toc-autologistic"><span class="toc-section-number">11.1</span> Autologistics<span></span></a></li>
<li><a href="orderlogit.html#orderlogit" id="toc-orderlogit"><span class="toc-section-number">11.2</span> Ordered Logit<span></span></a></li>
<li><a href="concepts-questions.html#concepts-questions" id="toc-concepts-questions"><span class="toc-section-number">11.3</span> Concepts Questions<span></span></a>
<ul>
<li><a href="concepts-questions.html#통계-및-수학" id="toc-통계-및-수학"><span class="toc-section-number">11.3.1</span> 통계 및 수학<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="about-cluster-gcn.html#about-cluster-gcn" id="toc-about-cluster-gcn"><span class="toc-section-number">12</span> About Cluster-GCN<span></span></a>
<ul>
<li><a href="about-cluster-gcn.html#ann" id="toc-ann"><span class="toc-section-number">12.0.1</span> ANN<span></span></a></li>
<li><a href="about-cluster-gcn.html#cnn" id="toc-cnn"><span class="toc-section-number">12.0.2</span> CNN<span></span></a></li>
<li><a href="about-cluster-gcn.html#graph-convolution-network" id="toc-graph-convolution-network"><span class="toc-section-number">12.0.3</span> Graph Convolution Network<span></span></a></li>
<li><a href="about-cluster-gcn.html#cluster-gcn" id="toc-cluster-gcn"><span class="toc-section-number">12.0.4</span> Cluster-GCN<span></span></a></li>
</ul></li>
<li><a href="cnn-1.html#cnn-1" id="toc-cnn-1"><span class="toc-section-number">13</span> CNN<span></span></a></li>
<li><a href="cnn-2.html#cnn-2" id="toc-cnn-2"><span class="toc-section-number">14</span> CNN<span></span></a></li>
<li><a href="cnn-3.html#cnn-3" id="toc-cnn-3"><span class="toc-section-number">15</span> CNN<span></span></a></li>
<li><a href="section-8.html#section-8" id="toc-section-8"><span class="toc-section-number">16</span> 01<span></span></a></li>
<li><a href="section-9.html#section-9" id="toc-section-9"><span class="toc-section-number">17</span> 02<span></span></a></li>
<li><a href="서-론.html#서-론" id="toc-서-론"><span class="toc-section-number">18</span> 서 론<span></span></a>
<ul>
<li><a href="연구-배경.html#연구-배경" id="toc-연구-배경"><span class="toc-section-number">18.1</span> 연구 배경<span></span></a></li>
<li><a href="연구-목적.html#연구-목적" id="toc-연구-목적"><span class="toc-section-number">18.2</span> 연구 목적<span></span></a></li>
</ul></li>
<li><a href="method.html#method" id="toc-method"><span class="toc-section-number">19</span> Method<span></span></a>
<ul>
<li><a href="biterm.html#biterm" id="toc-biterm"><span class="toc-section-number">19.1</span> Biterm<span></span></a>
<ul>
<li><a href="biterm.html#section-10" id="toc-section-10"><span class="toc-section-number">19.1.1</span> 2.1.1<span></span></a></li>
<li><a href="biterm.html#줄-요약" id="toc-줄-요약"><span class="toc-section-number">19.1.2</span> 3줄 요약<span></span></a></li>
<li><a href="biterm.html#section-11" id="toc-section-11"><span class="toc-section-number">19.1.3</span> 2.1.2<span></span></a></li>
<li><a href="biterm.html#latent-space-item-response-model" id="toc-latent-space-item-response-model"><span class="toc-section-number">19.1.4</span> 2.1.3 Latent Space Item Response Model<span></span></a></li>
<li><a href="biterm.html#procrustes-matching-and-oblique-roation" id="toc-procrustes-matching-and-oblique-roation"><span class="toc-section-number">19.1.5</span> Procrustes Matching and Oblique Roation<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="latent-space-item-response-model-구현.html#latent-space-item-response-model-구현" id="toc-latent-space-item-response-model-구현"><span class="toc-section-number">20</span> Latent Space Item Response Model 구현<span></span></a>
<ul>
<li><a href="python-네이티브로-모델-설계.html#python-네이티브로-모델-설계" id="toc-python-네이티브로-모델-설계"><span class="toc-section-number">20.1</span> 3.1. python 네이티브로 모델 설계<span></span></a>
<ul>
<li><a href="python-네이티브로-모델-설계.html#python-네이티브에서의-속도-퍼포먼스" id="toc-python-네이티브에서의-속도-퍼포먼스"><span class="toc-section-number">20.1.1</span> 3.1.1. python 네이티브에서의 속도 퍼포먼스<span></span></a></li>
</ul></li>
<li><a href="python에-c-접합한-모델-설계.html#python에-c-접합한-모델-설계" id="toc-python에-c-접합한-모델-설계"><span class="toc-section-number">20.2</span> 3.2. python에 c++ 접합한 모델 설계<span></span></a>
<ul>
<li><a href="python에-c-접합한-모델-설계.html#wrapping-timing" id="toc-wrapping-timing"><span class="toc-section-number">20.2.1</span> Wrapping Timing<span></span></a></li>
<li><a href="python에-c-접합한-모델-설계.html#library-needed-in-c" id="toc-library-needed-in-c"><span class="toc-section-number">20.2.2</span> Library needed in <code>c++</code><span></span></a></li>
<li><a href="python에-c-접합한-모델-설계.html#접합-모델-검증-및-성능-비교" id="toc-접합-모델-검증-및-성능-비교"><span class="toc-section-number">20.2.3</span> 3.2.1. 접합 모델 검증 및 성능 비교<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="implementation-1.html#implementation-1" id="toc-implementation-1"><span class="toc-section-number">21</span> Implementation<span></span></a>
<ul>
<li><a href="preprocess.html#preprocess" id="toc-preprocess"><span class="toc-section-number">21.1</span> <code>preprocess()</code><span></span></a></li>
<li><a href="btmize.html#btmize" id="toc-btmize"><span class="toc-section-number">21.2</span> <code>btmize()</code><span></span></a></li>
<li><a href="lsirmize.html#lsirmize" id="toc-lsirmize"><span class="toc-section-number">21.3</span> <code>lsirmize()</code><span></span></a></li>
</ul></li>
<li><a href="구현-모델-실적용-예시.html#구현-모델-실적용-예시" id="toc-구현-모델-실적용-예시"><span class="toc-section-number">22</span> 구현 모델 실적용 예시<span></span></a>
<ul>
<li><a href="데이터-서술.html#데이터-서술" id="toc-데이터-서술"><span class="toc-section-number">22.1</span> 4.1 데이터 서술<span></span></a></li>
<li><a href="알고리즘-결과.html#알고리즘-결과" id="toc-알고리즘-결과"><span class="toc-section-number">22.2</span> 4.2 알고리즘 결과<span></span></a></li>
</ul></li>
<li><a href="결론.html#결론" id="toc-결론"><span class="toc-section-number">23</span> 결론<span></span></a>
<ul>
<li><a href="section-12.html#section-12" id="toc-section-12"><span class="toc-section-number">23.1</span> 10.<span></span></a>
<ul>
<li><a href="section-12.html#stochastic-block-model-1" id="toc-stochastic-block-model-1"><span class="toc-section-number">23.1.1</span> Stochastic Block Model<span></span></a></li>
<li><a href="section-12.html#likelihood-function-1" id="toc-likelihood-function-1"><span class="toc-section-number">23.1.2</span> Likelihood function<span></span></a></li>
<li><a href="section-12.html#mixed-membership-block-model-mmbm-1" id="toc-mixed-membership-block-model-mmbm-1"><span class="toc-section-number">23.1.3</span> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="concepts-questions" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">A.3</span> Concepts Questions<a href="concepts-questions.html#concepts-questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>통계 및 수학</p>
<ul>
<li>고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?</li>
<li>샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?</li>
<li>확률 모형과 확률 변수는 무엇일까요?</li>
<li>누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요</li>
<li>베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포 / 디리클레 분포에 대해 설명해주세요. 혹시 연관된 분포가 있다면 연관 관계를 설명해주세요</li>
<li>조건부 확률은 무엇일까요?</li>
<li>공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요</li>
<li>신뢰 구간의 정의는 무엇인가요?</li>
<li>p-value를 고객에게는 뭐라고 설명하는게 이해하기 편할까요?</li>
<li>p-value는 요즘 시대에도 여전히 유효할까요? 언제 p-value가 실제를 호도하는 경향이 있을까요?</li>
<li>A/B Test 등 현상 분석 및 실험 설계 상 통계적으로 유의미함의 여부를 결정하기 위한 방법에는 어떤 것이 있을까요?</li>
<li>R square의 의미는 무엇인가요?</li>
<li>평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?</li>
<li>중심극한정리는 왜 유용한걸까요?</li>
<li>엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.</li>
<li>요즘같은 빅데이터(?)시대에는 정규성 테스트가 의미 없다는 주장이 있습니다. 맞을까요?</li>
<li>어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?</li>
<li>“likelihood”와 “probability”의 차이는 무엇일까요?</li>
<li>통계에서 사용되는 bootstrap의 의미는 무엇인가요.</li>
<li>모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?</li>
<li>베이지안과 프리퀀티스트간의 입장차이를 설명해주실 수 있나요?</li>
<li>검정력(statistical power)은 무엇일까요?</li>
<li>missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?</li>
<li>아웃라이어의 판단하는 기준은 무엇인가요?</li>
<li>콜센터 통화 지속 시간에 대한 데이터가 존재합니다. 이 데이터를 코드화하고 분석하는 방법에 대한 계획을 세워주세요. 이 기간의 분포가 어떻게 보일지에 대한 시나리오를 설명해주세요</li>
<li>출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?</li>
<li>필요한 표본의 크기를 어떻게 계산합니까?</li>
<li>Bias를 통제하는 방법은 무엇입니까?</li>
<li>로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요</li>
</ul></li>
<li><p>분석 일반</p>
<ul>
<li>좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요?</li>
<li>“상관관계는 인과관계를 의미하지 않는다”라는 말이 있습니다. 설명해주실 수 있나요?</li>
<li>A/B 테스트의 장점과 단점, 그리고 단점의 경우 이를 해결하기 위한 방안에는 어떤 것이 있나요?</li>
<li>각 고객의 웹 행동에 대하여 실시간으로 상호작용이 가능하다고 할 때에, 이에 적용 가능한 고객 행동 및 모델에 관한 이론을 알아봅시다.</li>
<li>고객이 원하는 예측모형을 두가지 종류로 만들었다. 하나는 예측력이 뛰어나지만 왜 그렇게 예측했는지를 설명하기 어려운 random forest 모형이고, 또다른 하나는 예측력은 다소 떨어지나 명확하게 왜 그런지를 설명할 수 있는 sequential bayesian 모형입니다.고객에게 어떤 모형을 추천하겠습니까?</li>
<li>고객이 내일 어떤 상품을 구매할지 예측하는 모형을 만들어야 한다면 어떤 기법(예: SVM, Random Forest, logistic regression 등)을 사용할 것인지 정하고 이를 통계와 기계학습 지식이 전무한 실무자에게 설명해봅시다.</li>
<li>나만의 feature selection 방식을 설명해봅시다.</li>
<li>데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering을 어떻게 풀어야할까요?</li>
</ul></li>
<li><p>머신러닝</p>
<ul>
<li>Cross Validation은 무엇이고 어떻게 해야하나요?</li>
<li>회귀 / 분류시 알맞은 metric은 무엇일까요?</li>
<li>알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …)</li>
<li>정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?</li>
<li>Local Minima와 Global Minima에 대해 설명해주세요.</li>
<li>차원의 저주에 대해 설명해주세요</li>
<li>dimension reduction기법으로 보통 어떤 것들이 있나요?</li>
<li>PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?</li>
<li>LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?</li>
<li>Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?</li>
<li>텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?</li>
<li>SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? 거기서 어떤 장점이 발생했나요?</li>
<li>다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.</li>
<li>Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.</li>
<li>최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?</li>
<li>머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?</li>
<li>인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?</li>
<li>지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?</li>
<li>ROC 커브에 대해 설명해주실 수 있으신가요?</li>
<li>여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?</li>
<li>K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)</li>
<li>L1, L2 정규화에 대해 설명해주세요</li>
<li>XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?</li>
<li>앙상블 방법엔 어떤 것들이 있나요?</li>
<li>SVM은 왜 좋을까요?</li>
<li>feature vector란 무엇일까요?</li>
<li>좋은 모델의 정의는 무엇일까요?</li>
<li>50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?</li>
<li>스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?</li>
<li>OLS(ordinary least squre) regression의 공식은 무엇인가요?</li>
</ul></li>
<li><p>딥러닝</p>
<ol style="list-style-type: decimal">
<li>딥러닝 일반
<ul>
<li>딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?</li>
<li>왜 갑자기 딥러닝이 부흥했을까요?</li>
<li>마지막으로 읽은 논문은 무엇인가요? 설명해주세요</li>
<li>Cost Function과 Activation Function은 무엇인가요?</li>
<li>Tensorflow, Keras, PyTorch, Caffe, Mxnet 중 선호하는 프레임워크와 그 이유는 무엇인가요?</li>
<li>Data Normalization은 무엇이고 왜 필요한가요?</li>
<li>알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)</li>
<li>오버피팅일 경우 어떻게 대처해야 할까요?</li>
<li>하이퍼 파라미터는 무엇인가요?</li>
<li>Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?</li>
<li>볼츠만 머신은 무엇인가요?</li>
<li>요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?</li>
<li>Non-Linearity라는 말의 의미와 그 필요성은?</li>
<li>ReLU로 어떻게 곡선 함수를 근사하나?</li>
<li>ReLU의 문제점은?</li>
<li>Bias는 왜 있는걸까?</li>
<li>Gradient Descent에 대해서 쉽게 설명한다면?</li>
<li>왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?</li>
<li>GD 중에 때때로 Loss가 증가하는 이유는?</li>
<li>중학생이 이해할 수 있게 더 쉽게 설명 한다면?</li>
<li>Back Propagation에 대해서 쉽게 설명 한다면?</li>
<li>Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?</li>
<li>GD가 Local Minima 문제를 피하는 방법은?</li>
<li>찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?</li>
<li>Training 세트와 Test 세트를 분리하는 이유는?</li>
<li>Validation 세트가 따로 있는 이유는?</li>
<li>Test 세트가 오염되었다는 말의 뜻은?</li>
<li>Regularization이란 무엇인가?</li>
<li>Batch Normalization의 효과는?</li>
<li>Dropout의 효과는?</li>
<li>BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?</li>
<li>GAN에서 Generator 쪽에도 BN을 적용해도 될까?</li>
<li>SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?</li>
<li>SGD에서 Stochastic의 의미는?</li>
<li>미니배치를 작게 할때의 장단점은?</li>
<li>모멘텀의 수식을 적어 본다면?</li>
<li>간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?</li>
<li>어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?</li>
<li>Back Propagation은 몇줄인가?</li>
<li>CNN으로 바꾼다면 얼마나 추가될까?</li>
<li>간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?</li>
<li>CNN이 아닌 MLP로 해도 잘 될까?</li>
<li>마지막 레이어 부분에 대해서 설명 한다면?</li>
<li>학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?</li>
<li>만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까?</li>
<li>딥러닝할 때 GPU를 쓰면 좋은 이유는?</li>
<li>학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?</li>
<li>GPU를 두개 다 쓰고 싶다. 방법은?</li>
<li>학습시 필요한 GPU 메모리는 어떻게 계산하는가?</li>
<li>TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?</li>
<li>뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?</li>
</ul></li>
<li>컴퓨터 비전
<ul>
<li>OpenCV 라이브러리만을 사용해서 이미지 뷰어(Crop, 흑백화, Zoom 등의 기능 포함)를 만들어주세요</li>
<li>딥러닝 발달 이전에 사물을 Detect할 때 자주 사용하던 방법은 무엇인가요?</li>
<li>Fatser R-CNN의 장점과 단점은 무엇인가요?</li>
<li>dlib은 무엇인가요?</li>
<li>YOLO의 장점과 단점은 무엇인가요?</li>
<li>제일 좋아하는 Object Detection 알고리즘에 대해 설명하고 그 알고리즘의 장단점에 대해 알려주세요</li>
<li>그 이후에 나온 더 좋은 알고리즘은 무엇인가요?</li>
<li>Average Pooling과 Max Pooling의 차이점은?</li>
<li>Deep한 네트워크가 좋은 것일까요? 언제까지 좋을까요?</li>
<li>Residual Network는 왜 잘될까요? Ensemble과 관련되어 있을까요?</li>
<li>CAM(Class Activation Map)은 무엇인가요?</li>
<li>Localization은 무엇일까요?</li>
<li>자율주행 자동차의 원리는 무엇일까요?</li>
<li>Semantic Segmentation은 무엇인가요?</li>
<li>Visual Q&amp;A는 무엇인가요?</li>
<li>Image Captioning은 무엇인가요?</li>
<li>Fully Connected Layer의 기능은 무엇인가요?</li>
<li>Neural Style은 어떻게 진행될까요?</li>
<li>CNN에 대해서 아는대로 얘기하라</li>
<li>CNN이 MLP보다 좋은 이유는?</li>
<li>어떤 CNN의 파라미터 개수를 계산해 본다면?</li>
<li>주어진 CNN과 똑같은 MLP를 만들 수 있나?</li>
<li>풀링시에 만약 Max를 사용한다면 그 이유는?</li>
<li>시퀀스 데이터에 CNN을 적용하는 것이 가능할까?</li>
</ul></li>
<li>자연어 처리
<ul>
<li>One Hot 인코딩에 대해 설명해주세요</li>
<li>POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요?</li>
<li>문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?</li>
<li>뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을까요?</li>
<li>음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요?</li>
<li>잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요?</li>
<li>영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?</li>
<li>뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요?</li>
<li>Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요?</li>
<li>영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요?</li>
<li>TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?</li>
<li>한국어에서 많이 사용되는 사전은 무엇인가요?</li>
<li>Regular grammar는 무엇인가요? regular expression과 무슨 차이가 있나요?</li>
<li>RNN에 대해 설명해주세요</li>
<li>LSTM은 왜 유용한가요?</li>
<li>Translate 과정 Flow에 대해 설명해주세요</li>
<li>n-gram은 무엇일까요?</li>
<li>PageRank 알고리즘은 어떻게 작동하나요?</li>
<li>depedency parsing란 무엇인가요?</li>
<li>Word2Vec의 원리는?</li>
<li>그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는?</li>
<li>그 그림에서 오른쪽 파라메터들의 의미는 무엇일까?</li>
<li>남자와 여자가 가까울까? 남자와 자동차가 가까울까?</li>
<li>번역을 Unsupervised로 할 수 있을까?</li>
</ul></li>
<li>강화학습
<ul>
<li>MDP는 무엇일까요?</li>
<li>가치함수는 무엇일까요? 수식으로도 표현해주세요</li>
<li>벨만 방정식은 무엇일까요? 수식으로도 표현해주세요</li>
<li>강화학습에서 다이나믹 프로그래밍은 어떤 의미를 가질까요? 한계점은 무엇이 있을까요?</li>
<li>몬테카를로 근사는 무엇일까요? 가치함수를 추정할 때 어떻게 사용할까요?</li>
<li>Value-based Reinforcement Learning과 Policy based Reinforcement Learning는 무엇이고 어떤 관계를 가질까요?</li>
<li>강화학습이 어려운 이유는 무엇일까요? 그것을 어떤 방식으로 해결할 수 있을까요?</li>
<li>강화학습을 사용해 테트리스에서 고득점을 얻는 프로그램을 만드려고 합니다. 어떻게 만들어야 할까요?</li>
</ul></li>
<li>GAN
<ul>
<li>GAN에 대해 아는대로 설명해주세요</li>
<li>GAN의 단점은 무엇인가요?</li>
<li>LSGAN에 대해 설명해주세요</li>
<li>GAN이 왜 뜨고 있나요?</li>
<li>Auto Encoder에 대해서 아는대로 얘기하라</li>
<li>MNIST AE를 TF나 Keras등으로 만든다면 몇줄일까?</li>
<li>MNIST에 대해서 임베딩 차원을 1로 해도 학습이 될까?</li>
<li>임베딩 차원을 늘렸을 때의 장단점은?</li>
<li>AE 학습시 항상 Loss를 0으로 만들수 있을까?</li>
<li>VAE는 무엇인가?</li>
<li>간단한 MNIST DCGAN을 작성한다면 TF 등으로 몇줄 정도 될까?</li>
<li>GAN의 Loss를 적어보면?</li>
<li>D를 학습할때 G의 Weight을 고정해야 한다. 방법은?</li>
<li>학습이 잘 안될때 시도해 볼 수 있는 방법들은?</li>
</ul></li>
<li>추천 시스템
<ul>
<li>추천 시스템에서 사용할 수 있는 거리는 무엇이 있을까요?</li>
<li>User 베이스 추천 시스템과 Item 베이스 추천 시스템 중 단기간에 빠른 효율을 낼 수 있는 것은 무엇일까요?</li>
<li>성능 평가를 위해 어떤 지표를 사용할까요?</li>
<li>Explicit Feedback과 Implicit Feedback은 무엇일까요? Impicit Feedback을 어떻게 Explicit하게 바꿀 수 있을까요?</li>
<li>Matrix Factorization은 무엇인가요? 해당 알고리즘의 장점과 단점은?</li>
<li>SQL으로 조회 기반 Best, 구매 기반 Best, 카테고리별 Best를 구하는 쿼리를 작성해주세요</li>
<li>추천 시스템에서 KNN 알고리즘을 활용할 수 있을까요?</li>
<li>유저가 10만명, 아이템이 100만개 있습니다. 이 경우 추천 시스템을 어떻게 구성하시겠습니까?</li>
<li>딥러닝을 활용한 추천 시스템의 사례를 알려주세요</li>
<li>두 추천엔진간의 성능 비교는 어떤 지표와 방법으로 할 수 있을까요? 검색엔진에서 쓰던 방법을 그대로 쓰면 될까요? 안될까요?</li>
<li>Collaborative Filtering에 대해 설명한다면?</li>
<li>Cold Start의 경우엔 어떻게 추천해줘야 할까요?</li>
<li>고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요? 위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다.</li>
</ul></li>
</ol></li>
<li><p>데이터베이스</p>
<ul>
<li>PostgreSQL의 장점은 무엇일까요?</li>
<li>인덱스는 크게 Hash 인덱스와 B+Tree 인덱스가 있습니다. 이것은 무엇일까요?</li>
<li>인덱스 Scan 방식은 무엇이 있나요?</li>
<li>인덱스 설계시 NULL값은 고려되야 할까요?</li>
<li>Nested Loop 조인은 무엇일까요?</li>
<li>Windows 함수는 무엇이고 어떻게 작성할까요?</li>
<li>KNN 알고리즘을 쿼리로 구현할 수 있을까요?</li>
<li>MySQL에서 대량의 데이터(500만개 이상)를 Insert해야하는 경우엔 어떻게 해야할까요?</li>
<li>RDB의 char와 varchar의 차이는 무엇일까요?</li>
<li>구글의 BigQuery, AWS의 Redshift는 기존 RDB와 무슨 차이가 있을까요? 왜 빠를까요?</li>
<li>쿼리의 성능을 확인하기 위해 어떤 쿼리문을 작성해야 할까요?</li>
<li>MySQL이 요새 느리다는 신고가 들어왔습니다. 첫번째로 무엇을 확인하시고 조정하시겠나요?</li>
<li>동작하는 MySQL에 Alter table을 하면 안되는 이유를 설명해주세요. 그리고 대안을 설명해주세요</li>
<li>빡세게 동작하고 있는 MySQL을 백업뜨기 위해서는 어떤 방법이 필요할까요?</li>
</ul></li>
<li><p>데이터 시각화</p>
<ul>
<li>네트워크 관계를 시각화해야 할 경우 어떻게 해야할까요?</li>
<li>Tableau같은 BI Tool은 어느 경우 도입하면 좋을까요?</li>
<li>“신규/재방문자별 지역별(혹은 일별) 방문자수와 구매전환율”이나 “고객등급별 최근방문일별 고객수와 평균구매금액”와 같이 4가지 이상의 정보를 시각화하는 가장 좋은 방법을 추천해주세요</li>
<li>구매에 영향을 주는 요소의 발견을 위한 관점에서, 개인에 대한 쇼핑몰 웹 활동의 시계열 데이터를 효과적으로 시각화하기 위한 방법은 무엇일까요? 표현되어야 하는 정보(feature)는 어떤 것일까요? 실제시 어떤 것이 가장 고민될까요?</li>
<li>파이차트는 왜 구릴까요? 언제 구린가요? 안구릴때는 언제인가요?</li>
<li>히스토그램의 가장 큰 문제는 무엇인가요?</li>
<li>워드클라우드는 보기엔 예쁘지만 약점이 있습니다. 어떤 약점일까요?</li>
<li>어떤 1차원값이, 데이터가 몰려있어서 직선상에 표현했을 때 보기가 쉽지 않습니다. 어떻게 해야할까요?</li>
</ul></li>
<li><p>시스템 엔지니어링</p>
<ul>
<li>지속적인 Cron 작업이 필요합니다. (dependency가 있는 작업들도 존재합니다) 어떻게 작업들을 관리할까요?</li>
<li>처음 서버를 샀습니다. 어떤 보안적 조치를 먼저 하시겠습니까?</li>
<li>SSH로의 brute-force attack을 막기 위해서 어떤 조치를 취하고 싶으신가요?</li>
<li>프로세스의 CPU 상태를 보기 위해 top을 했습니다. user,system,iowait중에 뭐를 제일 신경쓰시나요? 이상적인 프로그램이라면 어떻게 저 값들이 나오고 있어야 할까요?</li>
<li>iowait이 높게 나왔다면, 내가 해야하는 조치는 무엇인가요? (돈으로 해결하는 방법과 소프트웨어로 해결하는 방법을 대답해주세요)</li>
<li>동시에 10개의 컴퓨터에 라이브러리를 설치하는 일이 빈번히 발생합니다. 어떤 해결책이 있을까요?</li>
<li>screen과 tmux중에 뭘 더 좋아하시나요?</li>
<li>vim입니까. emacs입니까. 소속을 밝히세요.</li>
<li>가장 좋아하는 리눅스 배포판은 뭡니까. 왜죠?</li>
<li>관리하는 컴퓨터가 10대가 넘었습니다. 중요한 모니터링 지표는 뭐가 있을까요? 뭐로 하실건가요?</li>
<li>GIT의 소스가 있고, 서비스 사용중인 웹서버가 10대 이상 넘게 있습니다. 어떻게 배포할건가요?</li>
</ul></li>
<li><p>분산처리</p>
<ul>
<li>Apache Beam에 대해 아시나요? 기존 하둡과 어떤 차이가 있을까요?</li>
<li>좋게 만들어진 MapReduce는 어떤 프로그램일까요? 데이터의 Size 변화의 관점에서 설명할 수 있을까요?</li>
<li>여러 MR작업의 연쇄로 최종결과물이 나올때, 중간에 작업이 Fail날수 있습니다. 작업의 Fail은 어떻게 모니터링 하시겠습니까? 작업들간의 dependency는 어떻게 해결하시겠습니까?</li>
<li>분산환경의 JOIN은, 보통 디스크, CPU, 네트워크 중 어디에서 병목이 발생할까요? 이를 해결하기 위해 무엇을 해야 할까요?</li>
<li>암달의 법칙에 대해 말해봅시다. 그러므로 왜 shared-nothing 구조로 만들어야 하는지 설명해봅시다.</li>
<li>shared-nothing 구조의 단점도 있습니다. 어떤 것이 해당할까요?</li>
<li>Spark이 Hadoop보다 빠른 이유를 I/O 최적화 관점에서 생각해봅시다.</li>
<li>카산드라는 망한것 같습니다. 왜 망한것 같나요? 그래도 활용처가 있다면 어디인것 같나요.</li>
<li>TB 단위 이상의 기존 데이터와 시간당 GB단위의 신생 로그가 들어오는 서비스에서 모든 가입자에게 개별적으로 계산된 실시간 서비스(웹)를 제공하기 위한 시스템 구조를 구상해봅시다.</li>
<li>대용량 자료를 빠르게 lookup해야 하는 일이 있습니다. (100GB 이상, 100ms언더로 특정자료 찾기). 어떤 백엔드를 사용하시겠나요? 느린 백엔드를 사용한다면 이를 보완할 방법은 뭐가 있을까요?</li>
<li>데이터를 여러 머신으로 부터 모으기 위해 여러 선택지가 있을 수 있습니다. (flume, fluentd등) 아예 소스로부터 kafka등의 메시징 시스템을 바로 쓸 수도 있습니다. 어떤 것을 선호하시나요? 왜죠?</li>
</ul></li>
<li><p>웹 아키텍쳐</p>
<ul>
<li>트래픽이 몰리는 상황입니다. AWS의 ELB 세팅을 위해서 웹서버는 어떤 요건을 가져야 쉽게 autoscale가능할까요?</li>
<li>왜 Apache보다 Nginx가 성능이 좋을까요? node.js가 성능이 좋은 이유와 곁들여 설명할 수 있을까요?</li>
<li>node.js는 일반적으로 빠르지만 어떤 경우에는 쓰면 안될까요?</li>
<li>하나의 IP에서 여러 도메인의 HTTPS 서버를 운영할 수 있을까요? 안된다면 왜인가요? 또 이걸 해결하는 방법이 있는데 그건 뭘까요?</li>
<li>개발이 한창 진행되는 와중에도 서비스는 계속 운영되어야 합니다. 이를 가능하게 하는 상용 deploy 환경은 어떻게 구현가능한가요? WEB/WAS/DB/Cluster 각각의 영역에서 중요한 변화가 수반되는 경우에도 동작 가능한, 가장 Cost가 적은 방식을 구상하고 시나리오를 만들어봅시다.</li>
</ul></li>
<li><p>서비스 구현</p>
<ul>
<li>크롤러를 파이썬으로 구현할 때 BeautifulSoup과 Selenium의 장단점은 무엇일까요?</li>
<li>빈번한 접속으로 우리 IP가 차단되었을 때의 해결책은? (대화로 푼다. 이런거 말구요)</li>
<li>당장 10분안에 사이트의 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? 타 서비스를 써도됩니다.</li>
<li>신규 방문자와 재 방문자를 구별하여 A/B 테스트를 하고 싶다면 어떻게 해야 할까요?</li>
<li>R의 결과물을 python으로 만든 대시보드에 넣고 싶다면 어떤 방법들이 가능할까요?</li>
<li>쇼핑몰의 상품별 노출 횟수와 클릭수를 손쉽게 수집하려면 어떻게 해야 할까요?</li>
<li>여러 웹사이트를 돌아다니는 사용자를 하나로 엮어서 보고자 합니다. 우리가 각 사이트의 웹에 우리 코드를 삽입할 수 있다고 가정할 때, 이것이 가능한가요? 가능하다면, 그 방법에는 어떤 것이 있을까요?</li>
<li>고객사 혹은 외부 서버와의 데이터 전달이 필요한 경우가 있습니다. 데이터 전달 과정에서 보안을 위해 당연히(plain text)로 전송하는 것은 안됩니다. 어떤 방법이 있을까요?</li>
</ul></li>
<li><p>대 고객 사이드</p>
<ul>
<li>고객이 궁금하다고 말하는 요소가 내가 생각하기에는 중요하지 않고 다른 부분이 더 중요해 보입니다. 어떤 식으로 대화를 풀어나가야 할까요?</li>
<li>현업 카운터 파트와 자주 만나며 실패한 분석까지 같이 공유하는 경우와, 시간을 두고 멋진 결과만 공유하는 케이스에서 무엇을 선택하시겠습니까?</li>
<li>고객이 질문지 리스트를 10개를 주었습니다. 어떤 기준으로 우선순위를 정해야 할까요?</li>
<li>오프라인 데이터가 결합이 되어야 해서, 데이터의 피드백 주기가 매우 느리고 정합성도 의심되는 상황입니다. 우리가 할 수 있는 액션이나 방향 수정은 무엇일까요?</li>
<li>동시에 여러개의 A/B테스트를 돌리기엔 모수가 부족한 상황입니다. 어떻게 해야할까요?</li>
<li>고객사가 과도하게 정보성 대시보드만을 요청할 경우, 어떻게 대처해야 할까요?</li>
<li>고객사에게 위클리 리포트를 제공하고 있었는데, 금주에는 별다른 내용이 없었습니다. 어떻게 할까요?</li>
<li>카페24, 메이크샵 같은 서비스에서 데이터를 어떻게 가져오면 좋을까요?</li>
<li>기존에 같은 목적의 업무를 수행하던 조직이 있습니다. 어떻게 관계 형성을 해 나가야 할까요. 혹은 일이 되게 하기 위해서는 어떤 부분이 해소되어야 할까요.</li>
<li>인터뷰나 강의에 활용하기 위한 백데이터는 어느 수준까지 일반화 해서 사용해야 할까요?</li>
<li>고객사가 우리와 일하고 싶은데 현재는 capa가 되지 않습니다. 어떻게 대처해야 할까요?</li>
</ul></li>
<li><p>개인정보</p>
<ul>
<li>어떤 정보들이 개인정보에 해당할까요? ID는 개인정보에 해당할까요? 이를 어기지 않는 합법적 방법으로 식별하고 싶으면 어떻게 해야할까요?</li>
<li>국내 개인 정보 보호 현황에 대한 견해는 어떠한지요? 만약 사업을 진행하는데 장애요소로 작용한다면, 이에 대한 해결 방안은 어떤 것이 있을까요?</li>
<li>제3자 쿠키는 왜 문제가 되나요?</li>
</ul></li>
</ol>
<p><br>
<br>
<br></p>
<hr />
<p><br>
<br>
<br></p>
<div id="통계-및-수학" class="section level3 hasAnchor" number="11.3.1">
<h3><span class="header-section-number">A.3.1</span> 통계 및 수학<a href="concepts-questions.html#통계-및-수학" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요?</li>
</ul>
<p>해당 행렬을 오퍼레이션으로서 가했을 때 오퍼레이션이 가해진 벡터의 상수배가 결과로 출력된다면, 이 배가 된 상수를 ev, 오퍼레이션된 벡터를 evec 이라고 부름. 이는 곧 해당 벡터의 방향은 변하지 않고 위력만 변화했다는 것과 동일함. 본디 행렬은 이동, 반사, 그리고 회전이라는 기능을 모두 보유하고 있음. 그러나 특정 벡터에 대해서는 이러한 기능들이 중화되고 해당 벡터을 늘이고 줄이는 기능만을 하게 되는 것이며, 이는 오퍼레이션으로 사용된 해당 행렬의 성질을 이해하는데 있어 훨씬 쉬운 basis가 됨. 따라서 해당 행렬의 성질을 연구하는데 가장 적합한 basis 가 된다.
고유값과 고유 벡터가 중요한 이유는 그 쓰임세에 있다. 고유 값은 행렬 A를 표현해내는 크기이다. 즉, 고유 값의 크기가 클 수록 행렬 A를 잘 표현한다. 즉, 행렬 A를 고유값과 고유벡터로 분해한 후에 값을 뽑아내는 것이 PCA이다.</p>
<ul>
<li>샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?</li>
<li>확률 모형과 확률 변수는 무엇일까요?</li>
</ul>
<p>확률 변수 : 확률 공간에서 가지게 될 확률, 확률 모형 : 확률 변수를 0~1 사이로 매핑하기 위한 위한 모형 Probability(x) = P(x) = 확률 변수 x가 발생활 확률
확률변수: 실제 발생하는 사건들의 집합 (이벤트) 를 0에서 1 사이로 매핑하는 함수. 확률모형: 이러한 랜덤변수의 개념을 활용하여 대상 사건, 즉 대상 현상이 어떤 확률 형태를 갖는지를 모델링하고나 하는 일련의 과정. 예를 들어 선형회귀모형 또한 이의 연장임.</p>
<ul>
<li>누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요</li>
</ul>
<p>cdf, pdf. pdf의 합.</p>
<ul>
<li>베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포 / 디리클레 분포에 대해 설명해주세요. 혹시 연관된 분포가 있다면 연관 관계를 설명해주세요</li>
</ul>
<p>0혹은 1. 0혹은 1 다회. 선택지 다수 1회. 선택지 다수 다회. normal 분포. 샘플링 상황에서의 노멀분포 모사???????????????? / 노멀분포의 제곱의 분포 / 카이분포의 비의 분포 / ????????????????? / ?????????????????? / ????????????????
베르누이로 복원추출하면 이항 (이항은 곧 성공횟수). 이항에서 비본원추출하면 초기하. 이항에서</p>
<ul>
<li>조건부 확률은 무엇일까요?</li>
</ul>
<p>상황을 제한하고 해당 제한에서의 전체 확률발생을 1로 두고 재계산한 확률</p>
<ul>
<li>공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요</li>
<li>신뢰 구간의 정의는 무엇인가요?</li>
</ul>
<p>p값이 0.05라고 치면, 충분한 횟수의 실험을 100세트 돌렸다고 할 때, 각 실험의 결과값들을 평균내니 그중 95세트는 평균이 신뢰구간 안에 들어가있더라.</p>
<p>같은 방법으로 (샘플링 method 동일, 샘플링 갯수 동일) 100번 표본을 추출했다면, 각 표본 샘플링 과정에서 동반되어 얻어진 신뢰구간을 모으면 100개가 나온다. 이 100개의 신뢰구간 중 모평균을 포함한 신뢰구간의 숫자가 95개 정도 된다.</p>
<ul>
<li>p-value를 고객에게는 뭐라고 설명하는게 이해하기 편할까요?</li>
</ul>
<p>귀무가설이 맞다는 전에 하에 (즉 기존 상식이 사실이라는 가정 하에), 표본에서실제로 관측된 통계치와 “같거나 더 극단적인” 통계치가 관측된 확률. 즉, 가설에서 주어진 데이터가 얼마나 가능한지의 확률.
같은지가 핵심이다. 즉 기존 가능성을 반박하는 데이터가 나왔는데 이게 0.1퍼로 발생하는 거라면? 0.1퍼에 얻어걸렸다고 생각하기보단 기존 상식이 틀렸다고 결론짓는 것이 좋을 것.</p>
<ul>
<li>p-value는 요즘 시대에도 여전히 유효할까요? 언제 p-value가 실제를 호도하는 경향이 있을까요?</li>
</ul>
<p>??????????????????????????????
p값이란 널가설과 대립가설 사이에서의 확률만을 의미함.
??????????????????????????????</p>
<ul>
<li>A/B Test 등 현상 분석 및 실험 설계 상 통계적으로 유의미함의 여부를 결정하기 위한 방법에는 어떤 것이 있을까요?</li>
<li>R square의 의미는 무엇인가요?</li>
</ul>
<p>결정계수. SST(개별 관측값들의 산포정도) 대비 SSR(predict와 실제값의 차이) 의 비율을 구하고 이를 1에서 뺀 비율. 즉 개별 관측값들의 산포 정도에서 predict 를 통해 설명되는 정도를 뺀, 즉 개별 관측값들의 산포 정도에서 predict 를 통해 설명되지 않는 분량을 정규화한 것. 그러니까, 가장 뇌비우고 predict 하는건 그냥 predict 값으로 평균을 던져버리는거란 말이야. 그런데 이러면 뭐 완전히 실패하지는 않겠지만 개개별값들을 추정하는건 그냥 완전히 실패하게됨. 따라서 0. 이제 개별 값들을 완벽하게 트랙하는 predict 펑션이 있다고 하면 이건 1이겠지.</p>
<ul>
<li>평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요?</li>
</ul>
<p>대부분의 경우에 평균이 좋으나 아웃라이어가 많은 데이터라면 메디안 쪽이 나음.</p>
<ul>
<li>중심극한정리는 왜 유용한걸까요?</li>
</ul>
<p>어떤 형태의 데이터든 해당 데이터의 펑션을 노멀 분포를 활용해서 다룰 수 있는 여지를 줌. 해당 데이터 자체에 대한 분포를 추정하는 건 불가능하더라도 평균이라는 통계량에 대해 추정하는 것으로 개별 데이터의 세트에 대해서는 통계적 추정법을 적용하게 해준다.</p>
<p>p값,</p>
<ul>
<li>엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.</li>
</ul>
<p>Entropy : 데이터의 혼합성. 엔트로피가 1이라면 = 동일한 개수의 혼잡성 Information Gain : 지정된 속성이 얼마나 잘 example을 구분하는가에 대한 수치. 즉, Information Gain이 상승하면, Entropy가 감소한다.</p>
<ul>
<li>요즘같은 빅데이터(?)시대에는 정규성 테스트가 의미 없다는 주장이 있습니다. 맞을까요?</li>
</ul>
<p>거의 사실. 데이터 양이 많아지면 아주 작은 residual 에 의해서도 완전한 정규성은 무너지므로. 검정을 실행하는 비용이 비싸고 데이터 크기가 충분히 크다면 생략해도 무방.</p>
<p>Shaprio-Wilks test
표본수(n)가 2000 미만인 데이터셋에 적합한 정규성 검정
Kolmogorove-Smirnov test
표본수(n)가 2000 초과인 데이터셋에 적합한 정규성 검정
Quantile-Quantile plot (Graphic test)
데이터셋이 정규분포를 따르는지 판단하는 시각적 분석 방법
분석할 데이터 종류가 많지 않다면, QQplot을 통해 시각적으로 확인해보는게 가장 간단하며 직관적이다.</p>
<ul>
<li>어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?</li>
</ul>
<p>모수적 방법론은 데이터가 정규성을 가진다는 모수적 특성을 이용하는 통계적 방법론. 데이터가 정규성을 만족한다면 이미 획득되어 있는 분포들을 사용. 표본 크기가 30 이상이면 그냥 써도 되고, 10~30 사이면 노멀리티 한번 거치고.
ㅍ본의 갯수가 10개 이하일 경우 무작위추출한 표본은 정규성을 띄지 않음. 따라서 평균이나 분산 같은 패러미터가 존재하지 않는다. 최소한의 가정만을 사용하여 오류가능성이 적고, 범주형 자료와 같은 순위척도 데이터에 적용 가능. 또한 적합도 검정과 같이 모수적 가정에 대한 검정 방법을 제공하며, 순위 (rank) 나 부호 (sign) 에 기초한 방법 위주이므로 outlier 의 영향을 덜 받음. 다만 모수적 방법에 비해 power 가 낮고, 크기의 차이를 제시할 수 없다.</p>
<ul>
<li>“likelihood”와 “probability”의 차이는 무엇일까요?</li>
</ul>
<p>확률은 0에서 1 사이로 제약된 값. 우도는 아님.</p>
<ul>
<li>통계에서 사용되는 bootstrap의 의미는 무엇인가요.</li>
</ul>
<p>반복 복원추출. 보유하고 있는 데이터가 있다. 이 데이터에서 데이터 크기만큼 복원추출하고, 이 복원추출 결과값의 평균을 저장. 이 과정을 부트스트랩 패러미터만큼 실행. 이렇게 획득한 평균값 다수를 오름차수로 나열하고 이로 신뢰구간 형성하면 이를 통해 보유하고 있는 데이터의 수량이 한정되어 있어도 패러미터 추정의 정확도를 높일 수 있음.</p>
<ul>
<li>모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?</li>
</ul>
<p>비모수적 모델로 Decision Tree나 Random Forest를 구성</p>
<ul>
<li>베이지안과 프리퀀티스트간의 입장차이를 설명해주실 수 있나요?</li>
</ul>
<p>패러미터는 우리의 믿음. 데이터 쌓이면 그 믿음을 수정해나갈 수 있음. 현상에 대한 관찰자의 믿음.
패러미터는 진리. 진리값에 다가갈 뿐. 확률을 객관적으로 발생하는 현상의 빈도수에 대한 기술.</p>
<ul>
<li>검정력(statistical power)은 무엇일까요?</li>
</ul>
<p>대립가설이 사실일 때, 이를 사실로서 결정할 확률. 즉, 1 - 유의수준과 동일</p>
<ul>
<li>missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요?</li>
<li>아웃라이어의 판단하는 기준은 무엇인가요?</li>
</ul>
<p>?????????????????????????????? IQR을 기준으로 판단 / 스튜던트화 잔차로 판단. scipy OLSInfluence로 Linear Model을 만들고 OLS의 residual이 3을 넘기는 data를 이상치로 판단한다. ??????????????????????????????</p>
<ul>
<li>콜센터 통화 지속 시간에 대한 데이터가 존재합니다. 이 데이터를 코드화하고 분석하는 방법에 대한 계획을 세워주세요. 이 기간의 분포가 어떻게 보일지에 대한 시나리오를 설명해주세요</li>
<li>출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?</li>
</ul>
<p>실제로 비가 오고 있다면 셋 모두 진실을 말했으므로 8/27. 안오고 있는거면 1/27. 따라서 8/9.</p>
<ul>
<li>필요한 표본의 크기를 어떻게 계산합니까?</li>
<li>Bias를 통제하는 방법은 무엇입니까?</li>
<li>로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요</li>
</ul>
<p>log함수의 특성상, 작은 숫자는 키워주고, 큰 숫자는 작게 만드는 특징이 있다. 따라서, 데이터의 range가 너무 넓을 경우에 유용하다. 백화점 거래 내역 (양파 한 쪽 vs 명품 가방) /음악 반복 재생 횟수(인기 없는 노래 1, 강남스타일 41억등)</p>
<p>분석 일반
- 좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요?</p>
<p>데이터를 잘 설명할 수 있는 feature가 좋은 feature이며, 그 의미를 잘 파악할 수 있어야 합니다. feature의 성능을 판단하는 방법은 feature importance를 사용해서 확인할 수 있습니다. ex) Decision Tree의 Root Node에 오는 Feature가 더 중요한 Feature일 가능성이 높다</p>
<ul>
<li>“상관관계는 인과관계를 의미하지 않는다”라는 말이 있습니다. 설명해주실 수 있나요?</li>
</ul>
<p>오래 사는 사람 중에는 인중이 긴 사람이 많다. 인중이 길면 오래 산다. 상관관계는 지능활동 없이 쉽게 서술 가능. 그러나 인과관계는 세심해야 한다. 무균실일지 아닐지 알수 없으므로.</p>
<ul>
<li>A/B 테스트의 장점과 단점, 그리고 단점의 경우 이를 해결하기 위한 방안에는 어떤 것이 있나요?</li>
</ul>
<p>일명 버킷테스트, 분할-실행 테스트. 세그멘테이션을 분리한 후, 선택지 1과 2에서 각 세그멘테이셔의 통계량 변화를 체크. 세그멘테이션 내부에선 동질이므로 변화는 차이가 발생한 선택지에서만 발생할 것이므로 목표특정량 차이를 선택지 차이로 인한 것으로 판단 가능. 이때 실험을 너무 주기적으로 실행할 경우 효과가 감소할 수 있으며, 실험 주기가 길 경우 취향 변화 등으로 세그멘테이션 내부가 동질이 아니게 될 수 있음. Multi-Armed Bandit (슬롯머신 다수, 1개는 황금, 수익최적화. 황금발견술). 한번씩 플레이하고 좋은 점수 몰빵 - 동전튕겨서 좋은 슬롯머신하거나 다른 슬롯머신 랜덤으로 - 시간이 지난 정도와 해당 머신이 지난 정도를 살피고, 시간이 오래 지났는데 해당 머신이 플레이된 회수가 부실하면, 얘이거 뭔가 되는거 아닌가 하면서 그쪽 고르는 쪽에 웨이트 줘보기</p>
<ul>
<li>각 고객의 웹 행동에 대하여 실시간으로 상호작용이 가능하다고 할 때에, 이에 적용 가능한 고객 행동 및 모델에 관한 이론을 알아봅시다.</li>
</ul>
<p>??????????????????????????????????????????????????????????????????????????????????????????????????????????????</p>
<ul>
<li>고객이 원하는 예측모형을 두가지 종류로 만들었다. 하나는 예측력이 뛰어나지만 왜 그렇게 예측했는지를 설명하기 어려운 random forest 모형이고, 또다른 하나는 예측력은 다소 떨어지나 명확하게 왜 그런지를 설명할 수 있는 sequential bayesian 모형입니다.고객에게 어떤 모형을 추천하겠습니까?</li>
</ul>
<p>랜덤포레스트. 유저 입장에서 중요한것은 성능이지 알고리즘이 아님. 연구용이면 블랙박스 열어봐야 하긴 하는데 모델을 직접 개발할 입장이 아니라면 단순히 모델을 사용하는 것만으로 충분.</p>
<p>예측력이 필요한 경우 → Random Forest ex) 암 발생 여무
설명력이 필요한 경우 → Sequential Bayesian Model ex) 대출 불가 원인</p>
<ul>
<li>고객이 내일 어떤 상품을 구매할지 예측하는 모형을 만들어야 한다면 어떤 기법(예: SVM, Random Forest, logistic regression 등)을 사용할 것인지 정하고 이를 통계와 기계학습 지식이 전무한 실무자에게 설명해봅시다.</li>
</ul>
<p>Decision Tree. 중요한 피쳐들을 기준으로 분류해가며, 어떤 상품을 구매할 지 예측 ex) 성별, 최근에 구매한 상품, 평균 구매 금액, 평균 매장 재방문 일자 등등. 이를 기반으로 상품군을 구매할 타겟들을 나눔</p>
<ul>
<li>나만의 feature selection 방식을 설명해봅시다.</li>
<li>데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering을 어떻게 풀어야할까요?</li>
</ul>
<p>머신러닝</p>
<ul>
<li>Cross Validation은 무엇이고 어떻게 해야하나요?</li>
</ul>
<p>트레인과 테스트가 있음. 이때 트레인을 트레인과 밸리데이션 용으로 분리하지 않는다면, 우리는 이 상황에서 테스트를 밸리데이션 셋으로 쓰게 됨. 이렇게 고정된 테스트셋으로 밸리데이션을 수행하면 해당 테스트 셋이만 완벽한 오버핏 발생. 즉 데이터 통채로 10개가 있으면 여기서 1개를 테스트로 할당하고 9개는 트레인으로 씀. 여기서 1개를 1번, 2번, 3번 으로 바꾸고 트레인셋도 ^1, ^2, 식으로 바꾸면서 모델 최적화한다는거. 이때 어큐러시를 평가 지표로 쓴다고 하면 이는 평균으로 함. holdout, k-fold, leave-p-out, leave-1-out, stratified k-fold (labeling)</p>
<ul>
<li>회귀 / 분류시 알맞은 metric은 무엇일까요?</li>
</ul>
<p>회귀 → MAE, RSME 등등 / 회귀 → ROC-AUC, Accuracy, F1, Precision, Recall 등등</p>
<ul>
<li>알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …)</li>
</ul>
<p>MAE Mean Absolute Error : 잔차 절대값 평균. 장점:지표 자체가 직관적 / 단점:등락과 같은 지표에 불리, 스케일에 의존적 (1000만 단위 or 1억 단위 등)
RMSE Root Mean Absolute Error : 잔차 제곱 평균에 루트. 장점:지표 자체가 직관적 / 단점:등락과 같은 지표에 불리, 스케일에 의존적
Confusion Matrix를 통해 만들어질 수 있는 지표들: Precision, Recall, F1, Accuracy, ROC-AUC 등</p>
<ul>
<li>정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?</li>
</ul>
<p>Tree 계열의 모델에는 큰 영향이 없다. 하지만 지수평활함수, 선형 함수에는 영향이 크다
- 데이터의 분포가 다르면, 해당 피쳐에 가해지는 가중치가 더욱 커진다
- 방법
- Min Max Scailing → Minimum = 0 , Maximum = 1로 바꾸어, 0~1사이에 데이터를 분포
- Standard Scailing → 데이터를 정규분포상에 바꾼다</p>
<ul>
<li>Local Minima와 Global Minima에 대해 설명해주세요.</li>
</ul>
<p>국지적인 최적해, 전역 최적해. 가령 mode 가 3개인 그래프를 위아래로 뒤집은 그래프가 있다고 하면 이중 2번째 리버스 mode 가 최적해임에도 1번째, 3번째 mode 에서 멈춰버리는 것</p>
<ul>
<li>차원의 저주에 대해 설명해주세요</li>
</ul>
<p>수학적인 의미에서의 차원 (피쳐) 이 증가하면 증가할수록 가지고 있는 데이터의 성능이 구려짐. 가령 각 피쳐가 가질 수 있는 값이 10개라고 한다면 피쳐가 1개일때는 데이터가 존재할 수 있는 포인트가 10개인데 2개면 100, 3개면 1000임. 따라서 피쳐가 늘어날 수록 가지고 있는 데이터는 무조건 sparse 해질수밖에 없음.</p>
<ul>
<li>dimension reduction기법으로 보통 어떤 것들이 있나요?</li>
</ul>
<p>PCA(Principal Component Analysis), LDA(Linear Discriminant Analysis), LLE(Locally Linear Embedding), MDS(Multidimensional Scaling), Isomap, t-SNE(t-Distributed Stochastic Neighbor Embedding)</p>
<ul>
<li>PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?</li>
<li>LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?</li>
<li>Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?</li>
</ul>
<p>짜장면 비유. 점심메뉴 선택때 그제 먹었던 메뉴는 상관없고 어제 먹었던 메뉴만을 바탕으로 결정.</p>
<ul>
<li>텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?</li>
<li>SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? 거기서 어떤 장점이 발생했나요?</li>
<li>다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.</li>
<li>Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.</li>
<li>최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?</li>
<li>머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?</li>
</ul>
<p>제가 질문을 이해한 바가 맞다면 전자는 블랙박스 내부의 알고리즘에 관심이 없고 후자는 있다는 점에서 차이를 가짐.</p>
<ul>
<li>인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?</li>
<li>지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?</li>
<li>ROC 커브에 대해 설명해주실 수 있으신가요?</li>
<li>여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?</li>
<li>K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)</li>
<li>L1, L2 정규화에 대해 설명해주세요</li>
<li>XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?</li>
<li>앙상블 방법엔 어떤 것들이 있나요?</li>
<li>SVM은 왜 좋을까요?</li>
<li>feature vector란 무엇일까요?</li>
<li>좋은 모델의 정의는 무엇일까요?</li>
<li>50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?</li>
<li>스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?</li>
<li>OLS(ordinary least squre) regression의 공식은 무엇인가요?</li>
</ul>
<p>딥러닝
딥러닝 일반
- 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는? 역전파알고리즘 침체기 - 계층이깊어질수록 입력층과 가까이있는 가중치들의 학습이 잘 일어나지 않음 학습이 잘 이루어진다는 것은 가중치의 변화 발생을 의미. 가중치의 변화가 없다 = cost 함수의 미분값이 0. 기울기 소실: 앞층으로 갈수록 오차가 잘 전달되지 않는 현상 (학습이 이루어지지 않음) 시그모이드의 경우 0, 1로 강제 출력하는 영역에서는 학습이 잘 이루어지지 않음, 도함수의 계산 결과가 역방향으로 전달될 때 출력값이 현저하게 감소됨. 입력층으로 갈수록 미분값이 0에 가까워져 학습효과가 발생하지 않음
- 왜 갑자기 딥러닝이 부흥했을까요?
- 마지막으로 읽은 논문은 무엇인가요? 설명해주세요
- Cost Function과 Activation Function은 무엇인가요?
- Tensorflow, Keras, PyTorch, Caffe, Mxnet 중 선호하는 프레임워크와 그 이유는 무엇인가요?</p>
<p>각설하고, 책의 모든 내용이 사이킷런(Scikit-learn)을 이용하는데 문득 텐서플로우(Tensorflow)와의 차이점이 무엇인지 궁금해졌다. 통상적으로 머신러닝이라 하면 텐서플로우를 많이 쓰는데, 굳이 사이킷런을 사용하는 이유가 있을까 궁금해졌다.
왜 라이브러리가 아닌 프레임워크라 부르는지 모르겠지만, 이들은 분류, 회귀, 클러스터링, 비정상행위 탐지, 데이터 준비를 위한 다양한 학습 방법을 다루며 인공 신경망 메서드를 포함할 수도, 포함하지 않을 수도 있다.
차이점
출처에 따르면, 텐서플로우는 상대적으로 로우레벨 라이브러리에 가깝고 사이킷런은 하이레벨 라이브러리에 가깝다. 텐서플로우는 신경망이나 딥러닝을 위해 사용되는 데이터 계산, 연산을 위한 라이브러리며 신경망 네트워크 레이어 정의를 위한 메서드도 제공한다. 하지만 결정 트리, 논리 회귀, K-Means, PCA와 같은 머신러닝 메서드는 제공하지 않는다.
이에 비해, 사이킷런(Scikit-learn)은 데이터 마이닝과 머신러닝을 위한 라이브러리다. 딥러닝이나 강화 학습을 다루지 않지만 지도 학습, 비지도 학습에 관련된 다양한 메서드를 제공하기 때문에 간단하게 학습 알고리즘을 사용하고자 한다면 사이킷런이 사용하기 쉽다는 장점이 있다.</p>
<ul>
<li>Data Normalization은 무엇이고 왜 필요한가요?</li>
<li>알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)</li>
<li>오버피팅일 경우 어떻게 대처해야 할까요?</li>
</ul>
<p>더 다양하고 많은 훈련 데이터를 모은다. 그러면 자연스럽게 많은 데이터의 feature들을 볼 수 있다. 한 종류의 데이터를 다양하게 불린다는 측면에서 Data Augmentation도 비슷한 방법.
모델의 크기를 축소한다. 즉 모델에 있는 학습 parameter(즉, 가중치)의 수를 줄이는 것. 너무 많은 parameter가 있으면 훈련 데이터에 대해 너무 잘 학습되어서 오버피팅이 발생하므로. 이때 parameter 수는 layer의 수와 각 layer의 노드 수에 의해 결정됨.
Dropout을 추가한다. Dropout을 적용하면 훈련하는 동안 무작위로 층의 일부 출력값을 제외시킨다(0으로 만든다). 노이즈를 추가함으로써 훈련 데이터에 대한 너무 좋은 기억을 강제로 삭제시키는 것.
Regularization을 추가한다. Regularization은 가중치의 성장을 제한하는, 즉 가중치를 감소시키는 방향으로 그 효과가 나타난다. 가중치의 성장을 제한한다는 것은 기존 학습에 큰 영향을 끼칠 수 있는 데이터를 ’지양’하겠다는 의미. 따라서, 데이터 셋에서 볼 수 있는 일반적인 패턴이 아닌, 몇몇 독특하면서 희소한 패턴을 가지는 데이터에 대한 영향을 덜 받겠다는 것이다. 그러한 데이터의 대표적인 예시로 노이즈와 이상치가 있다. Regularization의 대표적인 예시로 L1 규제와 L2 규제가 있다. 두 Regularization 방법 모두 loss function에 가중치의 크기를 포함시키는 것이다.
L1 규제: 가중치의 크기에 상관없이 대상 가중치에서 상수값을 뺀다. 대체적으로 불필요한 가중치의 값을 0으로 만들게 됨. 즉, 중요한 가중치만을 취하기 때문에 sparse한 모델을 만드는데에 적합하다.
L2 규제(weight decay): 가중치의 값을 고려해서 대상 가중치에서 값을 뺀다. 즉, 어느 정도 튀는 값에 대응할 수 있지만, 그렇다고 가중치의 값을 0으로 만들지는 않는다. 따라서 이상치나 노이즈가 있는 데이터에 대한 학습을 할 때 좋다. 특히 선형 모델의 일반화에도 좋다.</p>
<ul>
<li>하이퍼 파라미터는 무엇인가요?</li>
<li>Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요?</li>
<li>볼츠만 머신은 무엇인가요?</li>
<li>요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?</li>
</ul>
<p>Vanishing Gradient Problem. sigmoid 의 미분값은 0~0.25 사이이기에 전달되는 weight 가 발산하거나 곡선의 기울기가 0이 되어버림. ReLU의 미분값은 0 혹은 1이라 이 문제에서 자유로움.</p>
<ul>
<li>Non-Linearity라는 말의 의미와 그 필요성은?</li>
<li>ReLU로 어떻게 곡선 함수를 근사하나?</li>
<li>ReLU의 문제점은?</li>
<li>Bias는 왜 있는걸까?</li>
<li>Gradient Descent에 대해서 쉽게 설명한다면?</li>
<li>왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까?</li>
<li>GD 중에 때때로 Loss가 증가하는 이유는?</li>
<li>중학생이 이해할 수 있게 더 쉽게 설명 한다면?</li>
<li>Back Propagation에 대해서 쉽게 설명 한다면?</li>
<li>Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는?</li>
<li>GD가 Local Minima 문제를 피하는 방법은?</li>
<li>찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?</li>
<li>Training 세트와 Test 세트를 분리하는 이유는?</li>
<li>Validation 세트가 따로 있는 이유는?</li>
<li>Test 세트가 오염되었다는 말의 뜻은?</li>
<li>Regularization이란 무엇인가?</li>
<li>Batch Normalization의 효과는?</li>
<li>Dropout의 효과는?</li>
<li>BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는?</li>
<li>GAN에서 Generator 쪽에도 BN을 적용해도 될까?</li>
<li>SGD, RMSprop, Adam에 대해서 아는대로 설명한다면?</li>
<li>SGD에서 Stochastic의 의미는?</li>
<li>미니배치를 작게 할때의 장단점은?</li>
<li>모멘텀의 수식을 적어 본다면?</li>
<li>간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까?</li>
<li>어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까?</li>
<li>Back Propagation은 몇줄인가?</li>
<li>CNN으로 바꾼다면 얼마나 추가될까?</li>
<li>간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가?</li>
<li>CNN이 아닌 MLP로 해도 잘 될까?</li>
<li>마지막 레이어 부분에 대해서 설명 한다면?</li>
<li>학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면?</li>
<li>만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까?</li>
<li>딥러닝할 때 GPU를 쓰면 좋은 이유는?</li>
<li>학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는?</li>
<li>GPU를 두개 다 쓰고 싶다. 방법은?</li>
<li>학습시 필요한 GPU 메모리는 어떻게 계산하는가?</li>
<li>TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?</li>
<li>뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?</li>
</ul>
<p>컴퓨터 비전
- OpenCV 라이브러리만을 사용해서 이미지 뷰어(Crop, 흑백화, Zoom 등의 기능 포함)를 만들어주세요
- 딥러닝 발달 이전에 사물을 Detect할 때 자주 사용하던 방법은 무엇인가요?
- Fatser R-CNN의 장점과 단점은 무엇인가요?
- dlib은 무엇인가요?
- YOLO의 장점과 단점은 무엇인가요?
- 제일 좋아하는 Object Detection 알고리즘에 대해 설명하고 그 알고리즘의 장단점에 대해 알려주세요
- 그 이후에 나온 더 좋은 알고리즘은 무엇인가요?
- Average Pooling과 Max Pooling의 차이점은?
- Deep한 네트워크가 좋은 것일까요? 언제까지 좋을까요?
- Residual Network는 왜 잘될까요? Ensemble과 관련되어 있을까요?
- CAM(Class Activation Map)은 무엇인가요?
- Localization은 무엇일까요?
- 자율주행 자동차의 원리는 무엇일까요?
- Semantic Segmentation은 무엇인가요?
- Visual Q&amp;A는 무엇인가요?
- Image Captioning은 무엇인가요?
- Fully Connected Layer의 기능은 무엇인가요?
- Neural Style은 어떻게 진행될까요?
- CNN에 대해서 아는대로 얘기하라
- CNN이 MLP보다 좋은 이유는?
- 어떤 CNN의 파라미터 개수를 계산해 본다면?
- 주어진 CNN과 똑같은 MLP를 만들 수 있나?
- 풀링시에 만약 Max를 사용한다면 그 이유는?
- 시퀀스 데이터에 CNN을 적용하는 것이 가능할까?</p>
<p>자연어 처리
- One Hot 인코딩에 대해 설명해주세요
- POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요?
- 문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?
- 뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을까요?
- 음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요?
- 잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요?
- 영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?
- 뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요?
- Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요?
- 영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요?
- TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?
- 한국어에서 많이 사용되는 사전은 무엇인가요?
- Regular grammar는 무엇인가요? regular expression과 무슨 차이가 있나요?
- RNN에 대해 설명해주세요
- LSTM은 왜 유용한가요?
- Translate 과정 Flow에 대해 설명해주세요
- n-gram은 무엇일까요?
- PageRank 알고리즘은 어떻게 작동하나요?
- depedency parsing란 무엇인가요?
- Word2Vec의 원리는?
- 그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는?
- 그 그림에서 오른쪽 파라메터들의 의미는 무엇일까?
- 남자와 여자가 가까울까? 남자와 자동차가 가까울까?
- 번역을 Unsupervised로 할 수 있을까?</p>
<p>강화학습</p>
<ul>
<li>MDP는 무엇일까요?</li>
<li>가치함수는 무엇일까요? 수식으로도 표현해주세요</li>
<li>벨만 방정식은 무엇일까요? 수식으로도 표현해주세요</li>
<li>강화학습에서 다이나믹 프로그래밍은 어떤 의미를 가질까요? 한계점은 무엇이 있을까요?</li>
<li>몬테카를로 근사는 무엇일까요? 가치함수를 추정할 때 어떻게 사용할까요?</li>
<li>Value-based Reinforcement Learning과 Policy based Reinforcement Learning는 무엇이고 어떤 관계를 가질까요?</li>
<li>강화학습이 어려운 이유는 무엇일까요? 그것을 어떤 방식으로 해결할 수 있을까요?</li>
<li>강화학습을 사용해 테트리스에서 고득점을 얻는 프로그램을 만드려고 합니다. 어떻게 만들어야 할까요?</li>
</ul>
<p>GAN</p>
<ul>
<li>GAN에 대해 아는대로 설명해주세요</li>
<li>GAN의 단점은 무엇인가요?</li>
<li>LSGAN에 대해 설명해주세요</li>
<li>GAN이 왜 뜨고 있나요?</li>
<li>Auto Encoder에 대해서 아는대로 얘기하라</li>
<li>MNIST AE를 TF나 Keras등으로 만든다면 몇줄일까?</li>
<li>MNIST에 대해서 임베딩 차원을 1로 해도 학습이 될까?</li>
<li>임베딩 차원을 늘렸을 때의 장단점은?</li>
<li>AE 학습시 항상 Loss를 0으로 만들수 있을까?</li>
<li>VAE는 무엇인가?</li>
<li>간단한 MNIST DCGAN을 작성한다면 TF 등으로 몇줄 정도 될까?</li>
<li>GAN의 Loss를 적어보면?</li>
<li>D를 학습할때 G의 Weight을 고정해야 한다. 방법은?</li>
<li>학습이 잘 안될때 시도해 볼 수 있는 방법들은?</li>
</ul>
<p>추천 시스템</p>
<ul>
<li>추천 시스템에서 사용할 수 있는 거리는 무엇이 있을까요?</li>
<li>User 베이스 추천 시스템과 Item 베이스 추천 시스템 중 단기간에 빠른 효율을 낼 수 있는 것은 무엇일까요?</li>
<li>성능 평가를 위해 어떤 지표를 사용할까요?</li>
<li>Explicit Feedback과 Implicit Feedback은 무엇일까요? Impicit Feedback을 어떻게 Explicit하게 바꿀 수 있을까요?</li>
<li>Matrix Factorization은 무엇인가요? 해당 알고리즘의 장점과 단점은?</li>
<li>SQL으로 조회 기반 Best, 구매 기반 Best, 카테고리별 Best를 구하는 쿼리를 작성해주세요</li>
<li>추천 시스템에서 KNN 알고리즘을 활용할 수 있을까요?</li>
<li>유저가 10만명, 아이템이 100만개 있습니다. 이 경우 추천 시스템을 어떻게 구성하시겠습니까?</li>
<li>딥러닝을 활용한 추천 시스템의 사례를 알려주세요</li>
<li>두 추천엔진간의 성능 비교는 어떤 지표와 방법으로 할 수 있을까요? 검색엔진에서 쓰던 방법을 그대로 쓰면 될까요? 안될까요?</li>
<li>Collaborative Filtering에 대해 설명한다면?</li>
<li>Cold Start의 경우엔 어떻게 추천해줘야 할까요?</li>
<li>고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요? 위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다.</li>
</ul>
<p>데이터베이스</p>
<ul>
<li>PostgreSQL의 장점은 무엇일까요?</li>
<li>인덱스는 크게 Hash 인덱스와 B+Tree 인덱스가 있습니다. 이것은 무엇일까요?</li>
<li>인덱스 Scan 방식은 무엇이 있나요?</li>
<li>인덱스 설계시 NULL값은 고려되야 할까요?</li>
<li>Nested Loop 조인은 무엇일까요?</li>
<li>Windows 함수는 무엇이고 어떻게 작성할까요?</li>
<li>KNN 알고리즘을 쿼리로 구현할 수 있을까요?</li>
<li>MySQL에서 대량의 데이터(500만개 이상)를 Insert해야하는 경우엔 어떻게 해야할까요?</li>
<li>RDB의 char와 varchar의 차이는 무엇일까요?</li>
<li>구글의 BigQuery, AWS의 Redshift는 기존 RDB와 무슨 차이가 있을까요? 왜 빠를까요?</li>
<li>쿼리의 성능을 확인하기 위해 어떤 쿼리문을 작성해야 할까요?</li>
<li>MySQL이 요새 느리다는 신고가 들어왔습니다. 첫번째로 무엇을 확인하시고 조정하시겠나요?</li>
<li>동작하는 MySQL에 Alter table을 하면 안되는 이유를 설명해주세요. 그리고 대안을 설명해주세요</li>
<li>빡세게 동작하고 있는 MySQL을 백업뜨기 위해서는 어떤 방법이 필요할까요?</li>
</ul>
<p>데이터 시각화</p>
<ul>
<li>네트워크 관계를 시각화해야 할 경우 어떻게 해야할까요?</li>
<li>Tableau같은 BI Tool은 어느 경우 도입하면 좋을까요?</li>
<li>“신규/재방문자별 지역별(혹은 일별) 방문자수와 구매전환율”이나 “고객등급별 최근방문일별 고객수와 평균구매금액”와 같이 4가지 이상의 정보를 시각화하는 가장 좋은 방법을 추천해주세요</li>
<li>구매에 영향을 주는 요소의 발견을 위한 관점에서, 개인에 대한 쇼핑몰 웹 활동의 시계열 데이터를 효과적으로 시각화하기 위한 방법은 무엇일까요? 표현되어야 하는 정보(feature)는 어떤 것일까요? 실제시 어떤 것이 가장 고민될까요?</li>
<li>파이차트는 왜 구릴까요? 언제 구린가요? 안구릴때는 언제인가요?</li>
<li>히스토그램의 가장 큰 문제는 무엇인가요?</li>
<li>워드클라우드는 보기엔 예쁘지만 약점이 있습니다. 어떤 약점일까요?</li>
<li>어떤 1차원값이, 데이터가 몰려있어서 직선상에 표현했을 때 보기가 쉽지 않습니다. 어떻게 해야할까요?</li>
</ul>
<p>시스템 엔지니어링</p>
<ul>
<li>지속적인 Cron 작업이 필요합니다. (dependency가 있는 작업들도 존재합니다) 어떻게 작업들을 관리할까요?</li>
<li>처음 서버를 샀습니다. 어떤 보안적 조치를 먼저 하시겠습니까?</li>
<li>SSH로의 brute-force attack을 막기 위해서 어떤 조치를 취하고 싶으신가요?</li>
<li>프로세스의 CPU 상태를 보기 위해 top을 했습니다. user,system,iowait중에 뭐를 제일 신경쓰시나요? 이상적인 프로그램이라면 어떻게 저 값들이 나오고 있어야 할까요?</li>
<li>iowait이 높게 나왔다면, 내가 해야하는 조치는 무엇인가요? (돈으로 해결하는 방법과 소프트웨어로 해결하는 방법을 대답해주세요)</li>
<li>동시에 10개의 컴퓨터에 라이브러리를 설치하는 일이 빈번히 발생합니다. 어떤 해결책이 있을까요?</li>
<li>screen과 tmux중에 뭘 더 좋아하시나요?</li>
<li>vim입니까. emacs입니까. 소속을 밝히세요.</li>
<li>가장 좋아하는 리눅스 배포판은 뭡니까. 왜죠?</li>
<li>관리하는 컴퓨터가 10대가 넘었습니다. 중요한 모니터링 지표는 뭐가 있을까요? 뭐로 하실건가요?</li>
<li>GIT의 소스가 있고, 서비스 사용중인 웹서버가 10대 이상 넘게 있습니다. 어떻게 배포할건가요?</li>
</ul>
<p>분산처리</p>
<ul>
<li>Apache Beam에 대해 아시나요? 기존 하둡과 어떤 차이가 있을까요?</li>
<li>좋게 만들어진 MapReduce는 어떤 프로그램일까요? 데이터의 Size 변화의 관점에서 설명할 수 있을까요?</li>
<li>여러 MR작업의 연쇄로 최종결과물이 나올때, 중간에 작업이 Fail날수 있습니다. 작업의 Fail은 어떻게 모니터링 하시겠습니까? 작업들간의 dependency는 어떻게 해결하시겠습니까?</li>
<li>분산환경의 JOIN은, 보통 디스크, CPU, 네트워크 중 어디에서 병목이 발생할까요? 이를 해결하기 위해 무엇을 해야 할까요?</li>
<li>암달의 법칙에 대해 말해봅시다. 그러므로 왜 shared-nothing 구조로 만들어야 하는지 설명해봅시다.</li>
<li>shared-nothing 구조의 단점도 있습니다. 어떤 것이 해당할까요?</li>
<li>Spark이 Hadoop보다 빠른 이유를 I/O 최적화 관점에서 생각해봅시다.</li>
<li>카산드라는 망한것 같습니다. 왜 망한것 같나요? 그래도 활용처가 있다면 어디인것 같나요.</li>
<li>TB 단위 이상의 기존 데이터와 시간당 GB단위의 신생 로그가 들어오는 서비스에서 모든 가입자에게 개별적으로 계산된 실시간 서비스(웹)를 제공하기 위한 시스템 구조를 구상해봅시다.</li>
<li>대용량 자료를 빠르게 lookup해야 하는 일이 있습니다. (100GB 이상, 100ms언더로 특정자료 찾기). 어떤 백엔드를 사용하시겠나요? 느린 백엔드를 사용한다면 이를 보완할 방법은 뭐가 있을까요?</li>
<li>데이터를 여러 머신으로 부터 모으기 위해 여러 선택지가 있을 수 있습니다. (flume, fluentd등) 아예 소스로부터 kafka등의 메시징 시스템을 바로 쓸 수도 있습니다. 어떤 것을 선호하시나요? 왜죠?</li>
</ul>
<p>웹 아키텍쳐</p>
<ul>
<li>트래픽이 몰리는 상황입니다. AWS의 ELB 세팅을 위해서 웹서버는 어떤 요건을 가져야 쉽게 autoscale가능할까요?</li>
<li>왜 Apache보다 Nginx가 성능이 좋을까요? node.js가 성능이 좋은 이유와 곁들여 설명할 수 있을까요?</li>
<li>node.js는 일반적으로 빠르지만 어떤 경우에는 쓰면 안될까요?</li>
<li>하나의 IP에서 여러 도메인의 HTTPS 서버를 운영할 수 있을까요? 안된다면 왜인가요? 또 이걸 해결하는 방법이 있는데 그건 뭘까요?</li>
<li>개발이 한창 진행되는 와중에도 서비스는 계속 운영되어야 합니다. 이를 가능하게 하는 상용 deploy 환경은 어떻게 구현가능한가요? WEB/WAS/DB/Cluster 각각의 영역에서 중요한 변화가 수반되는 경우에도 동작 가능한, 가장 Cost가 적은 방식을 구상하고 시나리오를 만들어봅시다.</li>
</ul>
<p>서비스 구현</p>
<ul>
<li>크롤러를 파이썬으로 구현할 때 BeautifulSoup과 Selenium의 장단점은 무엇일까요?</li>
<li>빈번한 접속으로 우리 IP가 차단되었을 때의 해결책은? (대화로 푼다. 이런거 말구요)</li>
<li>당장 10분안에 사이트의 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? 타 서비스를 써도됩니다.</li>
<li>신규 방문자와 재 방문자를 구별하여 A/B 테스트를 하고 싶다면 어떻게 해야 할까요?</li>
<li>R의 결과물을 python으로 만든 대시보드에 넣고 싶다면 어떤 방법들이 가능할까요?</li>
<li>쇼핑몰의 상품별 노출 횟수와 클릭수를 손쉽게 수집하려면 어떻게 해야 할까요?</li>
<li>여러 웹사이트를 돌아다니는 사용자를 하나로 엮어서 보고자 합니다. 우리가 각 사이트의 웹에 우리 코드를 삽입할 수 있다고 가정할 때, 이것이 가능한가요? 가능하다면, 그 방법에는 어떤 것이 있을까요?</li>
<li>고객사 혹은 외부 서버와의 데이터 전달이 필요한 경우가 있습니다. 데이터 전달 과정에서 보안을 위해 당연히(plain text)로 전송하는 것은 안됩니다. 어떤 방법이 있을까요?</li>
</ul>
<p>대 고객 사이드</p>
<ul>
<li>고객이 궁금하다고 말하는 요소가 내가 생각하기에는 중요하지 않고 다른 부분이 더 중요해 보입니다. 어떤 식으로 대화를 풀어나가야 할까요?</li>
<li>현업 카운터 파트와 자주 만나며 실패한 분석까지 같이 공유하는 경우와, 시간을 두고 멋진 결과만 공유하는 케이스에서 무엇을 선택하시겠습니까?</li>
<li>고객이 질문지 리스트를 10개를 주었습니다. 어떤 기준으로 우선순위를 정해야 할까요?</li>
<li>오프라인 데이터가 결합이 되어야 해서, 데이터의 피드백 주기가 매우 느리고 정합성도 의심되는 상황입니다. 우리가 할 수 있는 액션이나 방향 수정은 무엇일까요?</li>
<li>동시에 여러개의 A/B테스트를 돌리기엔 모수가 부족한 상황입니다. 어떻게 해야할까요?</li>
<li>고객사가 과도하게 정보성 대시보드만을 요청할 경우, 어떻게 대처해야 할까요?</li>
<li>고객사에게 위클리 리포트를 제공하고 있었는데, 금주에는 별다른 내용이 없었습니다. 어떻게 할까요?</li>
<li>카페24, 메이크샵 같은 서비스에서 데이터를 어떻게 가져오면 좋을까요?</li>
<li>기존에 같은 목적의 업무를 수행하던 조직이 있습니다. 어떻게 관계 형성을 해 나가야 할까요. 혹은 일이 되게 하기 위해서는 어떤 부분이 해소되어야 할까요.</li>
<li>인터뷰나 강의에 활용하기 위한 백데이터는 어느 수준까지 일반화 해서 사용해야 할까요?</li>
<li>고객사가 우리와 일하고 싶은데 현재는 capa가 되지 않습니다. 어떻게 대처해야 할까요?</li>
</ul>
<p>개인정보</p>
<ul>
<li>어떤 정보들이 개인정보에 해당할까요? ID는 개인정보에 해당할까요? 이를 어기지 않는 합법적 방법으로 식별하고 싶으면 어떻게 해야할까요?</li>
<li>국내 개인 정보 보호 현황에 대한 견해는 어떠한지요? 만약 사업을 진행하는데 장애요소로 작용한다면, 이에 대한 해결 방안은 어떤 것이 있을까요?</li>
<li>제3자 쿠키는 왜 문제가 되나요?</li>
</ul>
<p>Reference</p>
<p>하용호님 자료
남세동님 자료
Data Science Interview Questions &amp; Detailed Answers
Deep Learning Interview Questions and Answers
Must know questions deeplearning : 객관식 딥러닝 문제
My deep learning job interview experience sharing
Natural Language Processing Engineer Interview Questions</p>
<p>카일스쿨 유튜브 채널을 만들었습니다. 데이터 사이언스, 성장, 리더십, BigQuery 등을 이야기할 예정이니, 관심 있으시면 구독 부탁드립니다 :)</p>
<p>이 글이 도움이 되셨다면 추천 클릭을 부탁드립니다 :)</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="orderlogit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="about-cluster-gcn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/990101_3ConceptsQ.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
