<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.4 Auxiliary Variable MCMC | Self-Study</title>
  <meta name="description" content="4.4 Auxiliary Variable MCMC | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4.4 Auxiliary Variable MCMC | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="https://github.com/lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.4 Auxiliary Variable MCMC | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="advanced-mcmc-wk08.html"/>
<link rel="next" href="approximate-bayesian-computation.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="part"><span><b>I 20-02</b></span></li>
<li class="chapter" data-level="1" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>1</b> Categorical</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Data Type and Statistical Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian</a>
<ul>
<li class="chapter" data-level="2.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>2.1</b> Abstract</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>2.1.1</b> 변수의 독립성</a></li>
<li class="chapter" data-level="2.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>2.1.2</b> 교환가능성</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>2.2</b> Continual Aeassessment Method</a></li>
<li class="chapter" data-level="2.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>2.3</b> Horseshoe Prior</a></li>
</ul></li>
<li class="part"><span><b>II 21-01</b></span></li>
<li class="chapter" data-level="3" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>3</b> Mathematical Stats</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3.1</b> Inference</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>3.1.1</b> Rao-Blackwell thm.</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#completeness"><i class="fa fa-check"></i><b>3.1.2</b> Completeness</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>3.1.3</b> 레만-쉐페 thm.</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#raoblack"><i class="fa fa-check"></i><b>3.1.4</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>3.2</b> Hypothesis Test</a></li>
<li class="chapter" data-level="3.3" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>3.3</b> Power Fucntion</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>3.3.1</b> Significance Probability (p-value)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>3.4</b> Optimal Testing Method</a></li>
<li class="chapter" data-level="3.5" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>3.5</b> Data Reduction</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>3.5.1</b> Sufficiency Principle</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>3.6</b> Borel Paradox</a></li>
<li class="chapter" data-level="3.7" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>3.7</b> Neyman–Pearson lemma</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>3.7.1</b> Overview</a></li>
<li class="chapter" data-level="3.7.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>3.7.2</b> Generalized LRT</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>3.8</b> 개념</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>4</b> MCMC</a>
<ul>
<li class="chapter" data-level="4.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Importance Sampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Independent Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>4.2</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>4.2.1</b> MH Algorithm</a></li>
<li class="chapter" data-level="4.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>4.2.2</b> Random Walk Chains (Most Widely Used)</a></li>
<li class="chapter" data-level="4.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>4.2.3</b> Basic Gibbs Sampler</a></li>
<li class="chapter" data-level="4.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>4.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>4.3</b> Advanced MCMC (wk08)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>4.3.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="4.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>4.3.2</b> Hit-and-Run Algorithm</a></li>
<li class="chapter" data-level="4.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>4.3.3</b> Metropolis-Adjusted Langevin Algorithm</a></li>
<li class="chapter" data-level="4.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>4.3.4</b> Multiple-Try Metropolis Algorithm</a></li>
<li class="chapter" data-level="4.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>4.3.5</b> Reversible Jump MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>4.4</b> Auxiliary Variable MCMC</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Multimodal Target Distribution</a></li>
<li class="chapter" data-level="4.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>4.4.3</b> Doubly-intractable Normalizing Constants</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>4.5.1</b> Simulator-Based Models</a></li>
<li class="chapter" data-level="4.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>4.5.2</b> ABCifying Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>4.5.3</b> ABC-MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>4.6</b> Hamiltonian Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.6.1</b> Introduction to Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>4.7</b> Population Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>4.7.1</b> Adaptive Direction Sampling</a></li>
<li class="chapter" data-level="4.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>4.7.2</b> Conjugate Gradient MC</a></li>
<li class="chapter" data-level="4.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>4.7.3</b> Parallel Tempering</a></li>
<li class="chapter" data-level="4.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>4.7.4</b> Evolutionary MC</a></li>
<li class="chapter" data-level="4.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>4.7.5</b> Sequential Parallel Tempering</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>4.8</b> Stochastic Approximation Monte Carlo</a></li>
<li class="chapter" data-level="4.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>4.9</b> Review</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>4.9.1</b> Wk01</a></li>
<li class="chapter" data-level="4.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>4.9.2</b> wk03</a></li>
<li class="chapter" data-level="4.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>4.9.3</b> wk04, 05</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>4.10</b> Else</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>4.10.1</b> Hw4. Rasch Model</a></li>
<li class="chapter" data-level="4.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>4.10.2</b> DA) Example: MVN</a></li>
<li class="chapter" data-level="4.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>4.10.3</b> Bayesian adaptive clinical trial with delayed outcomes</a></li>
<li class="chapter" data-level="4.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>4.10.4</b> NMAR의 종류</a></li>
<li class="chapter" data-level="4.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>4.10.5</b> wk10) Bayesian Model Selection</a></li>
<li class="chapter" data-level="4.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>4.10.6</b> Autologistic model</a></li>
<li class="chapter" data-level="4.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>4.10.7</b> wk10) Bayesian Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>5</b> MVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>5.1</b> Overview of mva (not ended)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>5.1.1</b> Notation</a></li>
<li class="chapter" data-level="5.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>5.1.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Inference on Correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>5.1.4</b> Standardization</a></li>
<li class="chapter" data-level="5.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>5.1.5</b> Missing Value Treatment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>5.2</b> Multivariate Nomral (wk2)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>5.2.1</b> Overview</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>5.2.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="5.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>5.2.3</b> Properties of MVN</a></li>
<li class="chapter" data-level="5.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>5.2.5</b> Linear Combination of Random Vectors</a></li>
<li class="chapter" data-level="5.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Normal Likelihood</a></li>
<li class="chapter" data-level="5.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>5.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span></a></li>
<li class="chapter" data-level="5.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>5.2.8</b> Assessing Normality</a></li>
<li class="chapter" data-level="5.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>5.2.9</b> Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>5.3</b> Inference about Mean Vector (wk3)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>5.3.2</b> 1. Confidence Region</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>5.3.3</b> 2. Simultaneous CI</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>5.3.4</b> 3. Note: Bonferroni Multiple Comparison</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>5.3.5</b> 4. Large Sample Inferences about a Mean Vector</a></li>
<li class="chapter" data-level="5.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>5.3.6</b> 1. Profile Analysis (wk4, 5)</a></li>
<li class="chapter" data-level="5.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>5.3.7</b> 2. Test for Linear Trend</a></li>
<li class="chapter" data-level="5.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>5.3.8</b> 3. Inferences about a Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>5.4</b> Comparison of Several MV Means (wk5)</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>5.4.1</b> Paired Comparison</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>5.4.2</b> Comparing Mean Vectors from Two Populations</a></li>
<li class="chapter" data-level="5.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>5.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)</a></li>
<li class="chapter" data-level="5.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>5.4.4</b> Comparing Several Multivariate Population Means</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>5.5</b> Multivariate Multiple Regression (wk6)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>5.5.1</b> Overview</a></li>
<li class="chapter" data-level="5.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>5.5.2</b> Multivariate Multiple Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>5.6</b> PCA</a></li>
<li class="chapter" data-level="5.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>5.7</b> Factor</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>5.7.1</b> Method of Estimation</a></li>
<li class="chapter" data-level="5.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>5.7.2</b> Factor Rotation</a></li>
<li class="chapter" data-level="5.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>5.7.3</b> Varimax Criterion</a></li>
<li class="chapter" data-level="5.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>5.7.4</b> Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>5.8</b> Discrimination and Classification</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>5.8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="5.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>5.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations</a></li>
<li class="chapter" data-level="5.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>5.8.3</b> Evaluating Classification Functions</a></li>
<li class="chapter" data-level="5.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>5.8.4</b> Classification with several Populations (wk13)</a></li>
<li class="chapter" data-level="5.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>5.8.5</b> Other Discriminant Analysis Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>5.9</b> Clustering, Distance Methods, and Ordination</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>5.9.1</b> Overview</a></li>
<li class="chapter" data-level="5.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.9.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>5.9.3</b> K-means Clustering</a></li>
<li class="chapter" data-level="5.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>5.9.4</b> 군집의 평가방법</a></li>
<li class="chapter" data-level="5.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>5.9.5</b> Clustering using Density Estimation (wk14)</a></li>
<li class="chapter" data-level="5.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>5.9.6</b> Multidimensional Scaling (MDS)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>6.1</b> SVD</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="svd.html"><a href="svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>6.1.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="6.1.2" data-path="svd.html"><a href="svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>6.1.2</b> Singular value Decomposition: General-version</a></li>
<li class="chapter" data-level="6.1.3" data-path="svd.html"><a href="svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>6.1.3</b> Singular value Decomposition: Another-version</a></li>
<li class="chapter" data-level="6.1.4" data-path="svd.html"><a href="svd.html#quadratic-forms"><i class="fa fa-check"></i><b>6.1.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="6.1.5" data-path="svd.html"><a href="svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>6.1.5</b> Partitioned Matrices</a></li>
<li class="chapter" data-level="6.1.6" data-path="svd.html"><a href="svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>6.1.6</b> Geometrical Aspects</a></li>
<li class="chapter" data-level="6.1.7" data-path="svd.html"><a href="svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>6.1.7</b> Column, Row and Null Space</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>6.2</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-1.html"><a href="introduction-1.html#what"><i class="fa fa-check"></i><b>6.2.1</b> What</a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-1.html"><a href="introduction-1.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>6.2.2</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="6.2.3" data-path="introduction-1.html"><a href="introduction-1.html#multivariate-normal-distributions"><i class="fa fa-check"></i><b>6.2.3</b> Multivariate Normal Distributions</a></li>
<li class="chapter" data-level="6.2.4" data-path="introduction-1.html"><a href="introduction-1.html#distributions-of-quadratic-forms"><i class="fa fa-check"></i><b>6.2.4</b> Distributions of Quadratic Forms</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>6.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>6.3.1</b> Identifiability and Estimability</a></li>
<li class="chapter" data-level="6.3.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>6.3.2</b> Estimation: Least Squares</a></li>
<li class="chapter" data-level="6.3.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>6.3.3</b> Estimation: Best Linear Unbiased</a></li>
<li class="chapter" data-level="6.3.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>6.3.4</b> Estimation: Maximum Likelihood</a></li>
<li class="chapter" data-level="6.3.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>6.3.5</b> Estimation: Minimum Variance Unbiased</a></li>
<li class="chapter" data-level="6.3.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>6.3.6</b> Sampling Distributions of Estimates</a></li>
<li class="chapter" data-level="6.3.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>6.3.7</b> Generalized Least Squares(GLS)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>6.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>6.4.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="6.4.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>6.4.2</b> More About Models</a></li>
<li class="chapter" data-level="6.4.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> Estimating and Testing Contrasts</a></li>
<li class="chapter" data-level="6.4.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>6.4.4</b> Cochran’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>6.5</b> Testing</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>6.5.1</b> More About Models: Two approaches for linear model</a></li>
<li class="chapter" data-level="6.5.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>6.5.2</b> Testing Models</a></li>
<li class="chapter" data-level="6.5.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>6.5.3</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>6.5.4</b> Testing Linear Parametric Functions</a></li>
<li class="chapter" data-level="6.5.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>6.5.5</b> Theoretical Complements</a></li>
<li class="chapter" data-level="6.5.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>6.5.6</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>6.5.7</b> Testing Single Degrees of Freedom in a Given Subspace</a></li>
<li class="chapter" data-level="6.5.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>6.5.8</b> Breaking SS into Independent Components</a></li>
<li class="chapter" data-level="6.5.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>6.5.9</b> General Theory</a></li>
<li class="chapter" data-level="6.5.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>6.5.10</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="6.5.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>6.5.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="6.5.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>6.5.12</b> Tests for Generalized Least Squares Models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>6.6</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>6.6.1</b> A direct solution via inner products</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>6.7</b> Flat</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>6.7.1</b> 1.Flat</a></li>
<li class="chapter" data-level="6.7.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>6.7.2</b> 2. Solutions to systems of linear equations</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>6.8</b> Unified Approach to Balanced ANOVA Models</a></li>
</ul></li>
<li class="part"><span><b>III 21-02</b></span></li>
<li class="chapter" data-level="7" data-path="network-stats.html"><a href="network-stats.html"><i class="fa fa-check"></i><b>7</b> Network Stats</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="introduction-2.html"><a href="introduction-2.html#types-of-network-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Types of Network Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="introduction-2.html"><a href="introduction-2.html#network-modeling-and-inference"><i class="fa fa-check"></i><b>7.1.2</b> Network Modeling and Inference</a></li>
<li class="chapter" data-level="7.1.3" data-path="introduction-2.html"><a href="introduction-2.html#network-processes"><i class="fa fa-check"></i><b>7.1.3</b> Network Processes</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html"><i class="fa fa-check"></i><b>7.2</b> Descriptive Statistics of Networks</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics"><i class="fa fa-check"></i><b>7.2.1</b> Vertex and Edge Characteristics</a></li>
<li class="chapter" data-level="7.2.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion"><i class="fa fa-check"></i><b>7.2.2</b> Characterizing Network Cohesion</a></li>
<li class="chapter" data-level="7.2.3" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#graph-partitioning"><i class="fa fa-check"></i><b>7.2.3</b> Graph Partitioning</a></li>
<li class="chapter" data-level="7.2.4" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing"><i class="fa fa-check"></i><b>7.2.4</b> Assortativity and Mixing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html"><i class="fa fa-check"></i><b>7.3</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#sampling-designs"><i class="fa fa-check"></i><b>7.3.1</b> Sampling Designs</a></li>
<li class="chapter" data-level="7.3.2" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#coping-strategies"><i class="fa fa-check"></i><b>7.3.2</b> Coping Strategies</a></li>
<li class="chapter" data-level="7.3.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#big-data-solves-nothing"><i class="fa fa-check"></i><b>7.3.3</b> Big Data Solves Nothing</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html"><i class="fa fa-check"></i><b>7.4</b> Mathematical Models for Network Graphs</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models"><i class="fa fa-check"></i><b>7.4.1</b> Classical Random Graph Models</a></li>
<li class="chapter" data-level="7.4.2" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models"><i class="fa fa-check"></i><b>7.4.2</b> Generalized Random Graph Models</a></li>
<li class="chapter" data-level="7.4.3" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms"><i class="fa fa-check"></i><b>7.4.3</b> Network Graph Models Based on Mechanisms</a></li>
<li class="chapter" data-level="7.4.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics"><i class="fa fa-check"></i><b>7.4.4</b> Assessing Significance of Network Graph Characteristics</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html"><i class="fa fa-check"></i><b>7.5</b> Introduction to ERGM</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#exponential-random-graph-models"><i class="fa fa-check"></i><b>7.5.1</b> Exponential Random Graph Models</a></li>
<li class="chapter" data-level="7.5.2" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation"><i class="fa fa-check"></i><b>7.5.2</b> Difficulty in Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html"><i class="fa fa-check"></i><b>7.6</b> Parameter Estimation of ERGM</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm"><i class="fa fa-check"></i><b>7.6.1</b> Current Methods for ERGM</a></li>
<li class="chapter" data-level="7.6.2" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm"><i class="fa fa-check"></i><b>7.6.2</b> Approximation-based Algorithm</a></li>
<li class="chapter" data-level="7.6.3" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches"><i class="fa fa-check"></i><b>7.6.3</b> Auxiliary Variable MCMC-based Approaches</a></li>
<li class="chapter" data-level="7.6.4" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc"><i class="fa fa-check"></i><b>7.6.4</b> Varying Trunction Stochastic Approximation MCMC</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>7.7</b> Conclusion</a></li>
<li class="chapter" data-level="7.8" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html"><i class="fa fa-check"></i><b>7.8</b> ERGM for Dynamic Networks</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#temporal-ergm"><i class="fa fa-check"></i><b>7.8.1</b> Temporal ERGM</a></li>
<li class="chapter" data-level="7.8.2" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm"><i class="fa fa-check"></i><b>7.8.2</b> Separable Temporal ERGM</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="latent-network-models.html"><a href="latent-network-models.html"><i class="fa fa-check"></i><b>7.9</b> Latent Network Models</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-model"><i class="fa fa-check"></i><b>7.9.1</b> Latent Position Model</a></li>
<li class="chapter" data-level="7.9.2" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-cluster-model"><i class="fa fa-check"></i><b>7.9.2</b> Latent Position Cluster Model</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html"><i class="fa fa-check"></i><b>7.10</b> Additive and Multiplicative Effects Network Models</a>
<ul>
<li class="chapter" data-level="7.10.1" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#introduction-3"><i class="fa fa-check"></i><b>7.10.1</b> Introduction</a></li>
<li class="chapter" data-level="7.10.2" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression"><i class="fa fa-check"></i><b>7.10.2</b> Social Relations Regression</a></li>
<li class="chapter" data-level="7.10.3" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models"><i class="fa fa-check"></i><b>7.10.3</b> Multiplicative Effects Models</a></li>
<li class="chapter" data-level="7.10.4" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation"><i class="fa fa-check"></i><b>7.10.4</b> Inference via Posterior Approximation</a></li>
<li class="chapter" data-level="7.10.5" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r"><i class="fa fa-check"></i><b>7.10.5</b> Discussion and Example with R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="high-dimension.html"><a href="high-dimension.html"><i class="fa fa-check"></i><b>8</b> High Dimension</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-4.html"><a href="introduction-4.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html"><i class="fa fa-check"></i><b>8.2</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#motivation"><i class="fa fa-check"></i><b>8.2.1</b> Motivation</a></li>
<li class="chapter" data-level="8.2.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#from-markov-to-chernoff"><i class="fa fa-check"></i><b>8.2.2</b> From Markov to Chernoff</a></li>
<li class="chapter" data-level="8.2.3" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-variables"><i class="fa fa-check"></i><b>8.2.3</b> sub-Gaussian random variables</a></li>
<li class="chapter" data-level="8.2.4" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables"><i class="fa fa-check"></i><b>8.2.4</b> Properties of sub-Gaussian random variables</a></li>
<li class="chapter" data-level="8.2.5" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#equivalent-definitions"><i class="fa fa-check"></i><b>8.2.5</b> Equivalent definitions</a></li>
<li class="chapter" data-level="8.2.6" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-vectors"><i class="fa fa-check"></i><b>8.2.6</b> Sub-Gaussian random vectors</a></li>
<li class="chapter" data-level="8.2.7" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#hoeffdings-inequality"><i class="fa fa-check"></i><b>8.2.7</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="8.2.8" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#maximal-inequalities"><i class="fa fa-check"></i><b>8.2.8</b> Maximal inequalities</a></li>
<li class="chapter" data-level="8.2.9" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#section"><i class="fa fa-check"></i><b>8.2.9</b> </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html"><i class="fa fa-check"></i><b>8.3</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#sub-exponential-random-variables"><i class="fa fa-check"></i><b>8.3.1</b> Sub-exponential random variables</a></li>
<li class="chapter" data-level="8.3.2" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#bernsteins-condition"><i class="fa fa-check"></i><b>8.3.2</b> Bernstein’s condition</a></li>
<li class="chapter" data-level="8.3.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#mcdiarmids-inequality"><i class="fa fa-check"></i><b>8.3.3</b> McDiarmid’s inequality</a></li>
<li class="chapter" data-level="8.3.4" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#levys-inequality"><i class="fa fa-check"></i><b>8.3.4</b> Levy’s inequality</a></li>
<li class="chapter" data-level="8.3.5" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#quadratic-form"><i class="fa fa-check"></i><b>8.3.5</b> Quadratic form</a></li>
<li class="chapter" data-level="8.3.6" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma"><i class="fa fa-check"></i><b>8.3.6</b> The Johnson–Lindenstrauss Lemma</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html"><i class="fa fa-check"></i><b>8.4</b> Metric entropy and its uses</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#metric-space"><i class="fa fa-check"></i><b>8.4.1</b> Metric space</a></li>
<li class="chapter" data-level="8.4.2" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy"><i class="fa fa-check"></i><b>8.4.2</b> Covering numbers and metric entropy</a></li>
<li class="chapter" data-level="8.4.3" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#packing-numbers"><i class="fa fa-check"></i><b>8.4.3</b> Packing numbers</a></li>
<li class="chapter" data-level="8.4.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-1"><i class="fa fa-check"></i><b>8.4.4</b> </a></li>
<li class="chapter" data-level="8.4.5" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-2"><i class="fa fa-check"></i><b>8.4.5</b> </a></li>
<li class="chapter" data-level="8.4.6" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-3"><i class="fa fa-check"></i><b>8.4.6</b> </a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="covariance-estimation.html"><a href="covariance-estimation.html"><i class="fa fa-check"></i><b>8.5</b> Covariance estimation</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="covariance-estimation.html"><a href="covariance-estimation.html#matrix-algebra-review"><i class="fa fa-check"></i><b>8.5.1</b> Matrix algebra review</a></li>
<li class="chapter" data-level="8.5.2" data-path="covariance-estimation.html"><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm"><i class="fa fa-check"></i><b>8.5.2</b> Covariance matrix estimation in the operator norm</a></li>
<li class="chapter" data-level="8.5.3" data-path="covariance-estimation.html"><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices"><i class="fa fa-check"></i><b>8.5.3</b> Bounds for structured covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html"><i class="fa fa-check"></i><b>8.6</b> Matrix concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-calculus"><i class="fa fa-check"></i><b>8.6.1</b> Matrix calculus</a></li>
<li class="chapter" data-level="8.6.2" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-chernoff"><i class="fa fa-check"></i><b>8.6.2</b> Matrix Chernoff</a></li>
<li class="chapter" data-level="8.6.3" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices"><i class="fa fa-check"></i><b>8.6.3</b> Sub-Gaussian and sub-exponential matrices</a></li>
<li class="chapter" data-level="8.6.4" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><i class="fa fa-check"></i><b>8.6.4</b> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>8.7</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-1"><i class="fa fa-check"></i><b>8.7.1</b> PCA</a></li>
<li class="chapter" data-level="8.7.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#matrix-perturbation"><i class="fa fa-check"></i><b>8.7.2</b> Matrix Perturbation</a></li>
<li class="chapter" data-level="8.7.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#spiked-cov-model"><i class="fa fa-check"></i><b>8.7.3</b> Spiked Cov Model</a></li>
<li class="chapter" data-level="8.7.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#sparse-pca"><i class="fa fa-check"></i><b>8.7.4</b> sparse PCA</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>8.8</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="linear-regression.html"><a href="linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>8.8.1</b> Problem formulation</a></li>
<li class="chapter" data-level="8.8.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimator-in-high-dimensions"><i class="fa fa-check"></i><b>8.8.2</b> Least Squares Estimator in high dimensions</a></li>
<li class="chapter" data-level="8.8.3" data-path="linear-regression.html"><a href="linear-regression.html#sparse-linear-regression"><i class="fa fa-check"></i><b>8.8.3</b> Sparse linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>9</b> Survival Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>9.2</b> </a></li>
<li class="chapter" data-level="9.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html"><i class="fa fa-check"></i><b>9.3</b> Counting Processes and Martingales</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#conditional-expectation"><i class="fa fa-check"></i><b>9.3.1</b> Conditional Expectation</a></li>
<li class="chapter" data-level="9.3.2" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#martingale"><i class="fa fa-check"></i><b>9.3.2</b> Martingale</a></li>
<li class="chapter" data-level="9.3.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#key-martingales-properties"><i class="fa fa-check"></i><b>9.3.3</b> Key Martingales Properties</a></li>
<li class="chapter" data-level="9.3.4" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#section-5"><i class="fa fa-check"></i><b>9.3.4</b> </a></li>
<li class="chapter" data-level="9.3.5" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#section-6"><i class="fa fa-check"></i><b>9.3.5</b> </a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>9.4</b> </a></li>
<li class="chapter" data-level="9.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>9.5</b> Cox Regression</a></li>
<li class="chapter" data-level="9.6" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html"><i class="fa fa-check"></i><b>9.6</b> Filtration의 개념을 정복하자!</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약"><i class="fa fa-check"></i><b>9.6.1</b> Random Process를 이야기 하기까지의 긴 여정의 요약</a></li>
<li class="chapter" data-level="9.6.2" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#ft-measurable"><i class="fa fa-check"></i><b>9.6.2</b> Ft-measurable</a></li>
<li class="chapter" data-level="9.6.3" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#epilogue"><i class="fa fa-check"></i><b>9.6.3</b> EPILOGUE</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>9.7</b> Concepts</a></li>
</ul></li>
<li class="appendix"><span><b>00-00</b></span></li>
<li class="chapter" data-level="A" data-path="concepts-1.html"><a href="concepts-1.html"><i class="fa fa-check"></i><b>A</b> Concepts</a>
<ul>
<li class="chapter" data-level="A.1" data-path="autologistic.html"><a href="autologistic.html"><i class="fa fa-check"></i><b>A.1</b> Autologistics</a></li>
<li class="chapter" data-level="A.2" data-path="orderlogit.html"><a href="orderlogit.html"><i class="fa fa-check"></i><b>A.2</b> Ordered Logit</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="abstract-1.html"><a href="abstract-1.html"><i class="fa fa-check"></i><b>B</b> ABSTRACT</a></li>
<li class="chapter" data-level="C" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>C</b> CNN</a></li>
<li class="chapter" data-level="D" data-path="cnn-1.html"><a href="cnn-1.html"><i class="fa fa-check"></i><b>D</b> CNN</a></li>
<li class="chapter" data-level="E" data-path="cnn-2.html"><a href="cnn-2.html"><i class="fa fa-check"></i><b>E</b> CNN</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="auxiliary-variable-mcmc" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Auxiliary Variable MCMC</h2>
<p>실전에서 마주치는 대부분의 상황에서 ABC나 HMC 문제를 제외하고는 대부분의 경우 MCMC 문제를 완벽하게 풀어내는 건 불가능. 이때 주어진 variable 말고 보조변수 (Auxiliary Variable)을 추가함으로써 시뮬레이션 품질을 좀 더 높일 수 있지 않을까 하는 것이 논하고자 하는 바.</p>
<div id="introduction" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Introduction</h3>
<ul>
<li>Difficulties with MH Algorithm. 일반적인 MH 알고리즘으로 풀어낼 수 없는 2가지 상황이 존재:
<ol style="list-style-type: decimal">
<li>Local-trap problem: 에너지 계가 울퉁불퉁한 complex system에서 시뮬레이션을 진행했을 때 끝없이 로컬 최적값에서 빠져나오지 못함. 시뮬레이션을 비효율적으로 만듬.
<ul>
<li>density가 높다는 것은 해당 파트의 에너지가 낮다는 것이며, density가 낮은 에너지가 많은 파트에서 high density로 가는 것은 쉽고 자주 일어나도 역은 드뭄. 조밀하면 움직일 여력이 없으니까. 이것이 local trap의 원인</li>
<li>에너지는 이하로 표시 가능: energy function <span class="math inline">\(= -log \pi(\theta \vert x)\)</span>, 즉 negative log posterior, 혹은 negative log density.</li>
</ul></li>
<li>Doubly-intractable normalizing constants problem:
<ul>
<li>Inability to sample from distributions with intractable integrals
<ul>
<li>보통이라면, <span class="math inline">\(pi(\theta \vert x) \propto \kappa(x) f(x\vert\theta)\pi(\theta)\)</span>. <span class="math inline">\(r= \dfrac{pi(\theta &#39; \vert x)}{pi(\theta^{(t)} \vert x)} = \dfrac{\kappa(x) f(x\vert\theta &#39; )\pi(\theta &#39; )}{\kappa(x) f(x\vert\theta^{(t)})\pi(\theta^{(t)})}\)</span> 과정에서 normarlizing constant <span class="math inline">\(\kappa\)</span>가 알아서 캔슬되어 MH 돌리는데 문제가 없음.</li>
</ul></li>
<li>let <span class="math inline">\(f(x) \propto \kappa(x;\theta) \psi(x)\)</span> 는 알고자 하는 분포. 여기서 <span class="math inline">\(\kappa(x)\)</span>는 unnormalized density의 함수. 이때 <span class="math inline">\(\kappa(x)\)</span>는 패러미터의 함수이며 각 이터레이션의 다른 패러미터 추정값마다 변화해버려서 캔슬되지 않음. 그러면 계산하면 되는거 아님? 계산 불가능한 상황 존재 - nearly infinite summation or integration 포함하는 경우. (ex:) 이는 곧 intractable integral. acceptance <span class="math inline">\(Pr\)</span>이 알 수 없는 비 <span class="math inline">\(\frac{\kappa(x&#39;)}{\kappa(x)}\)</span>를 포함하므로 MH 알고리즘은 사용불가. <br> 이러한 문제는 bayesian 추론에서 spatial statistical models, random effects models, 그리고 exponential random graph models 등 다양한 통계적 모형에서 부딪히게 된다.
<ul>
<li>ex: Lattice system of areal model (Lattice의 승만큼 연산 필요)</li>
<li>e.g., <strong>Random Effect Model</strong>. 이때는 각 individual별로 Random Effect를 integration 해줘야 하므로 문제터짐</li>
<li>ex: Exponential Random Graph model: 네트워크에 사용되는 모델. 얘도 power임.</li>
<li>이러한 상황에서는 대부분의 optimization 알고리즘도 다 먹통됨</li>
</ul></li>
</ul></li>
</ol></li>
</ul>
<p><img src = "4-1.png"></p>
<ul>
<li>이러한 2개의 문제점을 극복하기 위해 다양한 진보된 MCMC 방법론이 제시되었음.
<ol style="list-style-type: decimal">
<li>Auxiliary variable-based methods</li>
<li>Population-based methods</li>
<li><del>Importance weight-based methods</del></li>
<li>Stochastic approximation-based methods</li>
</ol></li>
</ul>
<p><br>
<br>
<br></p>
<div id="auxiliary-variable-mcmc-methods" class="section level4" number="4.4.1.1">
<h4><span class="header-section-number">4.4.1.1</span> Auxiliary Variable MCMC Methods</h4>
<p><span class="math inline">\(f(x)\)</span>를 가지는 mv 분포에서의 샘플링을 생각해보자. <strong>Rao-Blackwellization</strong>(<a href="inference.html#raoblack">#</a>)이 MC 시뮬레이션에의 최우선원칙임은 알려져 있다. 시뮬레이션의 수렴을 좀 더 강화하기 위해 우리는 가능한한 많은 <span class="math inline">\(x\)</span>의 구성물을 integrate하는 것을 시도해보아야 한다. 하지만 이하의 두가지 경우(이외에도 존재)에 시뮬레이션을 양질로 만들기 위해 우리는 1개 이상의 변수를 추가하는 상황을 고려할 수 있다.</p>
<ol style="list-style-type: decimal">
<li>타겟분포 <span class="math inline">\(f(x)\)</span>가 multimodal. 온도 혹은 아직 관측되지 않은 측정값과 같은 auxiliary variable이 계가 <strong>로컬 트랩</strong>에서 빠져나올 수 있도록 도움을 줌. multimodal 상황.</li>
<li>타겟분포 <span class="math inline">\(f(x)\)</span>가 intractable normalizing constant 포함. <span class="math inline">\(X\)</span>의 auxiliary 실현값이 시뮬레이션에 포함됨으로써 시뮬레이션에서 normalizing constant 를 무력화시킴.</li>
</ol>
<p>MH 알고리즘 <span class="math inline">\(\dfrac{ f(\theta &#39; \vert x )}{f(\theta^{(t)} \vert x )} \dfrac{ g(\theta &#39; \rightarrow \theta^{(t)} )}{ g(\theta^{(t)} \rightarrow \theta &#39;)}\)</span>은 이하의 2가지 기본적인 부품을 가지고 있다.</p>
<ol style="list-style-type: decimal">
<li>타겟분포 (左)</li>
<li>proposal 분포 (右)</li>
</ol>
<p>이에 더해서 auxiliary variable 방법론은 이하의 2가지 방법으로 행해질 수 있다. 타겟과 제안 어느쪽에 변수를 추가하는지에 대한 이야기이다.</p>
<ol style="list-style-type: decimal">
<li>타겟분포 augmentation 방법론: Augmenting auxiliary variables to the <strong>target</strong> distribution
<ul>
<li>auxiliary variable <span class="math inline">\(u\)</span>와 조건부 분포 <span class="math inline">\(f(u \rvert x )\)</span>를 정의한다. joint 분포 <span class="math inline">\(f(x,u) = f(u \rvert x) f(x)\)</span>를 만들기 위해. 이후 MH 알고리즘이나 GS를 사용해 <span class="math inline">\((x,u)\)</span>를 업데이트. <span class="math inline">\(f(x)\)</span>의 샘플은 <span class="math inline">\((X, U)\)</span>의 실현값 <span class="math inline">\((x_1, u_1), \cdots, (x_N, u_N)\)</span>를 이용해 marginalization이나 프로젝션 등을 이용해 획득될 수 있다.</li>
</ul></li>
<li>Method of Proposal Distribution Augmentation: Augmenting auxiliary variables to the <strong>proposal</strong> distribution.
<ul>
<li>proposal 분포 <span class="math inline">\(T(x&#39;, u \rvert x)\)</span>를 특정하고, 이의 reversible version <span class="math inline">\(T(x, u \rvert x&#39;)\)</span>도 특정한다. 즉슨 <span class="math inline">\(\int T(x&#39;, u \vert x)du = T(x&#39; \vert x)\)</span>, <span class="math inline">\(\int T(x, u \vert x&#39;)du = T(x \vert x&#39;)\)</span>의 관계가 성립한다.<br> 이제 proposal <span class="math inline">\(T(x&#39;, u \vert x)\)</span> 로부터 후보 (candidate) 샘플 <span class="math inline">\(x&#39;\)</span>를 생산하고, 이를 with probability <span class="math inline">\(\min \left\{ 1, r(x, x&#39;, u) \right \}\)</span>. 이때 <span class="math inline">\(r(x, x&#39;, u) = \dfrac {f(x&#39;)} {f(x)} \dfrac {T(x,u \vert x&#39;)} {T(x&#39;,u \vert x)}\)</span>.</li>
</ul></li>
</ol>
<p>실현값 (realizations) <span class="math inline">\(x_1 , \cdots, x_N\)</span>을 생산할 때까지 이를 반복한다. 이제 <span class="math inline">\(N\)</span>이 충분히 크다면, 이 실현값들은 근사적으로 <span class="math inline">\(f(x)\)</span>에 의해 분포되어 있다.</p>
<p>이러한 방법론의 타당성은 이하를 통해 보일 수 있다.</p>
<p><span class="math display">\[
K(x&#39; \vert x) = \int_{\mathcal{u}} s(x, x&#39;, u) du + \mathbf{1}(x=x&#39;) \left[ 1-\int_{\mathcal{X}}\int_{\mathcal{u}} s(x, x&#39;, u) du dx&#39; \right]
\]</span></p>
<p>이는 <span class="math inline">\(x\)</span>로부터 <span class="math inline">\(x&#39;\)</span>로의 <strong>integrated transition kernel</strong> 을 의미하며, 이때 <span class="math inline">\(s(x, x&#39;, u) = T(x&#39;, u \rvert x) \ast r(x, x&#39;, u)\)</span>. Then,</p>
<p><span class="math display">\[
f(x) \int_{\mathcal{u}} s(x, x&#39;, u) du = \int_{\mathcal{u}} \min \left[ f(x&#39;) T(x, u \vert x&#39;), \; \; f(x)T(x&#39;, u \vert x) \right] du
\]</span></p>
<p>이는 <span class="math inline">\(x\)</span>와 <span class="math inline">\(x&#39;\)</span> 에 대해 symmetric. 이는 곧 <span class="math inline">\(f(x)K(x&#39; \vert x) = f(x&#39;)K(x \vert x&#39;)\)</span> 임을 의미한다.</p>
<p>original density?</p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="multimodal-target-distribution" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Multimodal Target Distribution</h3>
<p><br>
<br>
<br></p>
<div id="simulated-tempering" class="section level4" number="4.4.2.1">
<h4><span class="header-section-number">4.4.2.1</span> Simulated Tempering</h4>
<p>분포 <span class="math inline">\(f(x) \propto \exp \left(-H(x) \right), x \in X\)</span> 에서 샘플링하는데에 관심이 있다고 하자. simulated annealing에서 그러했던 것처럼, simulated tempering <span class="math inline">\(f(x, T) \propto \exp \left( -\dfrac {H(x)} {T} \right)\)</span>로 타겟 분포를 확장시켰다. 이는 auxiliary variable인 temperture <span class="math inline">\(T\)</span>를 포함함으로서 이루어진다. <span class="math inline">\(T\)</span>는 <strong>사용자가 미리 지정한 값들의 finite set</strong>이 된다. <span class="math inline">\(H(x)\)</span>는 사실상 energy function.</p>
<p>Temperature Transition Matrix <span class="math inline">\(T = \begin{bmatrix} q_{11} &amp; q_{12} &amp; \cdots &amp; q_{1n} \\ q_{21} &amp; \ddots &amp; &amp; \\ \vdots &amp; &amp; \ddots &amp; \\ q_{n1} &amp; \cdots &amp; &amp; q_{nn} \end{bmatrix}\)</span>. 이때 row는 current 온도 <span class="math inline">\(T_1, \cdots, T_n\)</span>, column은 행선지 온도.</p>
<hr />
<p><strong>Parallel Tempering</strong>은 인접한 온도로만 이동 가능 (가장 높은 온도에서 가장 낮은 온도로 한단계 한단계씩). 온도 자체를 시뮬레이션한게 아니라 온도의 chain이 주어져 있어 각 온도 간의 움직임을 만드는 것에 그친다. 따라서 이는 multiple chain을 이용하는 population MC 방법론 쪽에 소속됨.</p>
<p>Simulated Tempering과는 이 점에서 차이를 보임. 후자는 어느 온도로든 다 이동. 온도 매트릭스 만들어놓고, <span class="math inline">\(U(0,1)\)</span> 분포에서 온도 하나 생산하고 이 온도로 이동할 것인지의 여부를 MH 알고리즘으로 결정.</p>
<hr />
<p><span class="math inline">\(U(0,1)\)</span>에서 랜덤하게 숫자를 뽑고, <span class="math inline">\(j\)</span>의 값을 proposal transition matrix <span class="math inline">\((q_{ij})\)</span>에 따라서 정한다. <span class="math inline">\(u&lt;q_{11}\)</span>이면 <span class="math inline">\(T_1 \rightarrow T_1\)</span>, <span class="math inline">\(q_{11}&lt;u&lt;q_{11} + q_{12}\)</span>이면 <span class="math inline">\(T_1 \rightarrow T_2\)</span>, ….
- if <span class="math inline">\(j=i_t\)</span>, let <span class="math inline">\(i_{t+1}=i_t\)</span>, and let <span class="math inline">\(x_{t+1}\)</span>을 MH kernal <span class="math inline">\(K_{i_t}(x,y)\)</span>에서 뽑는다. 이때 <span class="math inline">\(K_{i_t}(x,y)\)</span>는 <span class="math inline">\(f(x, T_{i_t})\)</span>을 invariant distribution로 허용하는 아이이다. 즉 새로운 <span class="math inline">\(x\)</span>를 생산하면 된다.
- if <span class="math inline">\(j \not= i_t\)</span>, let <span class="math inline">\(x_{t+1}=x_t\)</span>하고 proposal을 이하의 <span class="math inline">\(Pr\)</span>에 따라 채택한다. 이때 <span class="math inline">\(Z\)</span>는 <span class="math inline">\(Z_i\)</span>의 측정값이다. 채택된다면 <span class="math inline">\(i_{t+1} = j\)</span>이고, 그외의 경우에는 <span class="math inline">\(i_{t+1} = i_t\)</span>로 한다. 새로운 <span class="math inline">\(x\)</span>를 생산하는 것이 아니라 들고 있던 <span class="math inline">\(x\)</span>를 쓰되, 이걸 accept 할건지 안할건지를 체크한다.</p>
<p><span class="math display">\[
\min \left[ 
1, \; \; 
\dfrac {\hat Z_j} {\hat Z_{i_t}} \ast \exp \right \{
-H(x) \left( \dfrac{1}{T_j}-\dfrac{1}{T_{i_t}} \right)
\left \}
\cdot
\dfrac {q_{j,i_t}} {q_{i_t , j}} 
\right]
\]</span></p>
<p>이때 <span class="math inline">\(\dfrac {q_{j,i_t}} {q_{i_t , j}}\)</span> 는 proposal distribution이라고 생각할 수 있다. 나머지는 Likelihood part이며, 이때 <span class="math inline">\(\dfrac {\hat Z_j} {\hat Z_{i_t}}\)</span> 가 normalizing constant의 ratio이다. 온도가 변화하였으므로 두 식의 normalizing constant가 같지 않기 때문이다.</p>
<p><br>
<br></p>
<hr />
<p><del><strong>Issues on Simulated Tempering:</strong></del></p>
<ol style="list-style-type: decimal">
<li><strong>Temperature Ladder를 어떻게 고를 것인가.</strong> → 각 chain별로 이동이 원활하게 잡는 것이 핵심.
<ul>
<li>가장 높은 온도 <span class="math inline">\(T_1\)</span>은 대부분의 uphill move가 해당 레벨에서 accept 될 수 있도록 설정되어야 한다.</li>
<li>사이의(intermediate) 온도들은 sequential manner로 설정될 수 있다. <span class="math inline">\(T_1\)</span>에서 시작해서, 점차적으로 다음으로 낮은 온도를 <span class="math inline">\(Var_i \left\{ H(x) \right\} \ast \delta^2 = O(1)\)</span>을 만족하도록 설정하는 것이다. 이때 <span class="math inline">\(\delta = \dfrac {1}{T_{i+1}} - \dfrac{1}{T_i}\)</span>이며, <span class="math inline">\(Var_i(\cdot)\)</span>은 <span class="math inline">\(H(x)\)</span> (taken with respect to <span class="math inline">\(f(x, T_i)\)</span>) 의 분산을 의미한다.
<ul>
<li>이러한 조건들은 <span class="math inline">\(f(x,T_i), f(x,T_{i+1})\)</span> 사이에 상당히 겹치는 점이 많아야 한다는 것을 의미하기도 한다. 실전에선 <span class="math inline">\(Var_i \left( H(x) \right)\)</span>는 샘플러를 레벨 <span class="math inline">\(T_i\)</span>에서 예비적으로(preliminary) 돌려보았던 결과에서 러프하게나마 예측될 수 있다.<br />
</li>
</ul></li>
</ul></li>
<li><strong><span class="math inline">\(Z_i\)</span>를 어떻게 estimate 할 것인가.</strong> → accept 여부가 normalizing constant에도 의존해서 이거 이상하게 고르면 효율 떨어짐. 엄청난 단점이라서 요즘은 이 알고리즘 자체를 잘 안씀
<ul>
<li>이는 simulated tempering의 효율에 직결되는 부분이다. <span class="math inline">\(Z_i\)</span>들이 잘 estimate 되었다면, simulated tempering은 temperature ladder을 따라 <strong>symmetric RW</strong>처럼 동작한다. (<span class="math inline">\(x\)</span>-updating step을 제하고 볼 경우) 그렇지 않다면 이는 특정 temperature 레벨에서 멈춰버린다. 시뮬레이션이 실패함은 물론이다(rendering). <br> 실전에서 <span class="math inline">\(Z_i\)</span>들은 stochastic approximation MC 방법론을 사용해서 estimate 가능하다. 혹은 reverse logistic regression 방법론을 사용해서도 <span class="math inline">\(Z_i\)</span>를 estimate 할 수 있다.</li>
</ul></li>
</ol>
<p><br>
<br>
<br></p>
</div>
<div id="slice-sampler" class="section level4" number="4.4.2.2">
<h4><span class="header-section-number">4.4.2.2</span> Slice Sampler</h4>
<p>density <span class="math inline">\(f(x), \; \; \; x \in \mathcal X\)</span>에서 샘플링하고자 한다. <span class="math inline">\(x \sim f(x)\)</span>에서 샘플링하는 것은, <span class="math inline">\(f(x)\)</span> 그래프 이하의 영역에서 uniform하게 샘플링하는 것과 동등하다. 해당 영역은 <span class="math inline">\(A = \{ (x,u): 0 \le u \le f(x) \}\)</span>이며, 이것이 acceptance-rejection 알고리즘의 기초(basis)였다. 이 목적을 달성하기 위해 우리는 타겟분포 <span class="math inline">\(f\)</span>를 auxiliary variable <span class="math inline">\(U\)</span>를 사용하여 확장해볼 수 있다. 이 <span class="math inline">\(U\)</span>는, <span class="math inline">\(x\)</span>에 대해서 조건부일 때, 구간 <span class="math inline">\([0, f(x)]\)</span>에서 uniform하게 분포되어 있다.</p>
<p><img src="4-2.png"></p>
<p>따라서, <span class="math inline">\((X, U)\)</span>의 joint density function은 <span class="math inline">\(f(x,u)=f(x)f(u \rvert x) \propto I_{(x,u)\in \textit A}\)</span>. 후자의 인디케이터는 언급되었던 영역 안에 속한다는 의미. 이는 GS에 의해 이하와 같이 샘플링 가능하다.</p>
<ol style="list-style-type: decimal">
<li>draw <span class="math inline">\(u_{t+1} \sim U[0, f(x_t)]\)</span>.</li>
<li>draw <span class="math inline">\(x_{t+1}\)</span> uniformly from the region <span class="math inline">\(\{ x: f(x) \ge u_{t+1} \}\)</span>.</li>
</ol>
<p>위의 샘플러는 <strong>slice sampler</strong>라고 불림. 이는 multimodal 분포들에 대해 단순 MH 알고리즘보다 더 나을 가능성이 있음. slice 때문에 b/w-mode-transition에 자유롭기 때문. 현재도 핫한 샘플러중 하나임. horseshoe prior 에서의 패러미터 estimate에 대표적으로 이녀석이 쓰인다.</p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="doubly-intractable-normalizing-constants" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Doubly-intractable Normalizing Constants</h3>
<p>Spatial models, e.g., the autologistic model, the Potts model, and the autonormal model (Besag, 1974)는 많은 과학적 문제들을 위한 모델링에 쓰이고 있음. 이러한 모델들에 해당하는 주요한 문제는 normalizing constant가 doubly-intractable하다는데 있음.</p>
<p>for dataset <span class="math inline">\(X\)</span>, 패러미터 <span class="math inline">\(\theta\)</span>, normalizing constant <span class="math inline">\(\kappa (\theta)\)</span>. 이때 <span class="math inline">\(\kappa (\theta)\)</span>는 <span class="math inline">\(\theta\)</span>에 의존하나 closed form으로는 만들 수 없음. 이하는 dataset을 생산한 likelihood function.</p>
<p><span class="math display">\[
\begin{align*}
X \sim f(x \vert \theta) = \dfrac{1}{\kappa (\theta)} exp \{ -U(x, \theta) \}, &amp;x \in \mathcal{X}, &amp;\theta \in \Theta
\end{align*}
\]</span></p>
<p><span class="math inline">\(\pi(\theta)\)</span>는 <span class="math inline">\(\theta\)</span>의 prior. 이 경우 post는 <span class="math inline">\(f(\theta \vert x) \propto \dfrac{1}{\kappa (\theta)} exp \{ -U(x, \theta) \} \ast \pi(\theta)\)</span>.</p>
<p><br>
<br>
<br></p>
<div id="ising" class="section level4" number="4.4.3.1">
<h4><span class="header-section-number">4.4.3.1</span> Boltzmann Density</h4>
<p>known as <strong>Ising Model</strong>, 그리고 ~로 확장될 경우 <strong><a href="autologistic.html#autologistic">autologistic model</a></strong>.</p>
<p>Consider a 2-D Ising model with the Boltzmann density</p>
<p><span class="math display">\[
f(\pmb x) \propto \exp \left\{ K \sum_{i\sim j} x_i x_j \right\}
\]</span></p>
<ul>
<li>spins <span class="math inline">\(x_i = \pm 1\)</span> (S극이 -1)</li>
<li><span class="math inline">\(K\)</span>는 inverse temperature (measure for interaction : <span class="math inline">\(x_i\)</span>가 주변에 있는 값과 얼마나 많은 같은 값을 가지는지, 다른 값을 가지는지에 대해 측정해주는 패러미터) 온도가 낮을수록 interaction가 강해지며, 이에 의해 동일값 확률이 높아짐.</li>
<li><span class="math inline">\(i\sim j\)</span>는 lattice 상의 가장 가까운 neighbors.</li>
</ul>
<p>온도가 높다면, 이 모델은 GS를 사용해 쉽게 시뮬레이션 가능하다. 조건부 분포에 따라 각 spin의 값을 iteratively 초기화한다. 아래의 식에서 <span class="math inline">\(n(i)\)</span>는 spin <span class="math inline">\(i\)</span>의 neighbors의 집합 (set). 이하의 수식은 autologistic 과 그 과정이 유사하다.</p>
<p>$$
<span class="math display">\[\begin{align*}

P(x_i =1 \vert x_j, \; \; j \in n(i)) &amp;= \dfrac {1}{1+ \exp \left \{ -2K \sum_{j \in n(i)} \right\}} \\
P(x_i =-1 \vert x_j, \; \; j \in n(i)) &amp;= \dfrac {\exp \left \{ -2K \sum_{j \in n(i)} \right\}}{1+ \exp \left \{ -2K \sum_{j \in n(i)} \right\}} &amp;= 1- P(x_i =1 \vert x_j, \; \; j \in n(i))

\end{align*}\]</span>
$$</p>
<p>하지만, GS는 temperature가 critical temperature로 근접하거나 이하로 내려갈 경우 GS가 빠르게 느려진다. 온도가 낮으면 interaction이 강해져, 주변값과 비슷한 값을 generate 해야만 하기 때문이다. 이렇게 샘플링이 어려워지는 지점, 온도를 <strong>critical point</strong>라고 부른다. 이는 대략 <span class="math inline">\(\theta \approx 0.43\)</span>. 이것이 소위 <strong>critical slowing down</strong> 이라고 불리는 현상이다.</p>
<p><br></p>
<div id="perfect-sampler" class="section level5" number="4.4.3.1.1">
<h5><span class="header-section-number">4.4.3.1.1</span> Perfect Sampler</h5>
<p>과거 샘플들의 굉장히 많은 조합을 커플링해서 샘플을 생산. previous realization 전체에 대해 (이는 그 이전의 샘플, 아니면 그 이전의 샘플, 혹은 original 데이터에 대해서조차도) independent한 샘플을 생산해내는 sampler. 즉 그 어떤 것에서도 independent한 sample을 생산해낸다. 문제는 이 샘플러는 <span class="math inline">\(\theta&gt;0.43\)</span>인 순간 바로 작동을 안함. <span class="math inline">\(\theta&gt;0.32, 0.35\)</span> 정도로 엔간 크기만 해도 드럽게 느림.</p>
<p><br></p>
</div>
<div id="swendsen-wang-algorithm" class="section level5" number="4.4.3.1.2">
<h5><span class="header-section-number">4.4.3.1.2</span> Swendsen-Wang Algorithm</h5>
<p>slice sampling에서 Boltzmann 덴시티는 이하의 형으로 다시 쓰여진다. 이때 <span class="math inline">\(\beta = 2K\)</span>. indicator function으로 변형했을 때 저 둘이 어떻게 equation이 성립하는지 유의.</p>
<p><span class="math display">\[
f(\pmb x) 
\; \propto 
\; \prod_{i\sim j} \exp \left\{ K(1+x_i x_j) \right\} 
\;  = 
\; \prod_{i\sim j} \exp \left\{ \beta \ast 
\mathbf{1}(x_i = x_j)
\right\}
\]</span></p>
<p>이때 우리가 auxiliary variable <span class="math inline">\(\pmb u = (u_{i \sim j})\)</span>, where each component <span class="math inline">\(u_{i \sim j}\)</span>, conditional on <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>, is uniformly distributed on <span class="math inline">\(\left[ 0, \; \exp \{\beta \ast \mathbf{1}(x_i = x_j)\} \right]\)</span>, then</p>
<p><span class="math display">\[
f(\pmb x, \pmb u) 
\; \propto  \; 
\prod_{i \sim j} \mathbf{1} \left( 0 \le u_{i \sim j } \le \exp\left\{ \beta \ast \mathbf{1} (x_i = x_j) \right\} \right)
\]</span></p>
<p>이때 <span class="math inline">\(u_{i \sim j}\)</span> 자체는 <strong>bond variable</strong>이라고 명명된다. 이는 spin <span class="math inline">\(i\)</span>와 spin <span class="math inline">\(j\)</span> 사이의 가장자리에 물리적으로 앉아 있는 변수로서 생각될 수 있다. (i와 j가 묶여져 있는지, 같은 group 안에 존재하는 것인지 아닌지에 대한 indicator가 되는 variable)</p>
<ul>
<li>if <span class="math inline">\(u_{i \sim j}&gt;1\)</span>, then <span class="math inline">\(\exp \left\{ \beta \ast \mathbf{1}(x_i = x_j) \right \}&gt;1\)</span>, 따라서 반드시 <span class="math inline">\(x_i = x_j\)</span>.</li>
<li>if <span class="math inline">\(u_{i \sim j}&lt;1\)</span>, 이 경우 <span class="math inline">\(x_i, x_j\)</span>에 제약 (constraint) 이 없다.</li>
</ul>
<p><span class="math inline">\(b_{i \sim j}\)</span>가 제약에 대한 indicator variable이라고 정의하자. 즉, <span class="math inline">\(x_i, x_j\)</span>가 같도록 제약되었다면, <span class="math inline">\(b_{i \sim j}=1\)</span>이며 이외엔 0이다. for any 2개의 “like-spin” (i.e. 2개의 spin이 같은 값을 가진다) neighbors에 대해서, 이 둘은 with probability <span class="math inline">\(1-\exp (-\beta)\)</span>를 따라 bonded 될 수 있는 가능성이 있음을 기억하라. <span class="math inline">\(\pmb u\)</span>의 설정 (configuration)에 따라, “mutual bond” (i.e., <span class="math inline">\(b_{i \sim j}=1\)</span>) 을 통하여 연결될 수 있는지 없는지 여부에 따라 spin들을 군집 (cluster) 할 수 있다. (위에서 i와 j가 같다고 indicator가 판별했을 경우에만 이런 cluster 로 묶는 것이 가능하다) Then 동일 클러스터 내의 모든 spin은 같은 값을 가질 것이다. 또한 군집 내부의 모든 spin을 동시에 뒤집는 (flip) 것은 <span class="math inline">\(f(\pmb x , \pmb u)\)</span>의 평형 (equilibrium)을 해치지 않을 것이다.</p>
<p><br></p>
<p>Proceeds:</p>
<ol style="list-style-type: decimal">
<li>Update the bond values: check all “like-spin” neighbors, and set <span class="math inline">\(b_{i \sim j}=1\)</span> with probability <span class="math inline">\(1-\exp (-\beta)\)</span>.</li>
<li>Update the spin values: Cluster spins by connecting neighboring sites with a mutual bond, and then flip each cluster with probability <span class="math inline">\(0.5\)</span>.</li>
</ol>
<p>For the Ising model, the introduction of the auxiliary variable <span class="math inline">\(\pmb u\)</span> has the dependence between neighboring spins partially decoupled, and the resulting sampler can thus converge substantially faster than the single site updating algorithm. As demonstrated by Swendsen and Wang (1987), this algorithm can eliminate much of the <strong>critical slowing down</strong>.</p>
<p><img src = "4-3.png"></p>
<p>같은 값들이 모여있는 cluster를 판별하여 각각을 grouping. grouping을 랜덤으로 하므로 인접해 있는 동일값임에도 그룹에 포함되지 못하는 경우가 존재함. Swendsen-Wang에서는 이렇게 그룹을 만든 후, 해당 그룹을 통채로 toggling. group을 통채로 토글링하기 때문에 dependency가 있는 것들이 통채로 toggling되어서 dependency가 있는 것들은 나머지 것들과 인제 이렇게 independent한 것도 있지만 dependent한 것을 통채로 묶어서 하는 것이므로 좀더 한꺼번에 뒤집으니까 실제로 우리가 업데이트하는 것은 group 내부 말고 group 외부 간들에는 independent하다고 가정될 수 있는 몇몇개의 group들만이 남음. 이 덩어리들을 한꺼번에 업데이트하므로 따라서 샘플러 generate가 상대적으로 쉬움. 하지만 이 만든 덩어리는 매 이터레이션마다 덩어리를 새로 만들어야 함. 매 이터레이션마다 클러스터를 새로 만들고 flip하여 이를 accept할지 말지를 결정하는 이런 형태의 구조를 가짐.</p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="møollers-algorithm" class="section level4" number="4.4.3.2">
<h4><span class="header-section-number">4.4.3.2</span> <del>Møoller’s Algorithm</del></h4>
<p>auxiliary variable <span class="math inline">\(y\)</span>, 이는 <span class="math inline">\(x\)</span>와 같은 state space를 공유한다고 정의. 그 경우 이하의 joint pdf <span class="math inline">\(f\)</span> 를 생각해볼 수 있다. <span class="math inline">\(f(y \vert \theta , x)\)</span>는 <span class="math inline">\(y\)</span>의 분포.</p>
<p><span class="math display">\[
f(\theta, y \vert x) = f(x \vert \theta) \ast f(\theta) \ast f(y \vert \theta , x)
\]</span></p>
<p><span class="math inline">\(f(\theta, y \vert x)\)</span> 에서 MH 알고리즘을 통해 시뮬레이트하기 위해서는 이하와 같은 제안분포 <span class="math inline">\(q\)</span> 를 사용해볼 수 있다. 이는 패러미터 벡터 <span class="math inline">\(\theta \rightarrow \theta&#39;\)</span>의 usual change에 상응하며, 이 후에는 <span class="math inline">\(q(\cdot \vert \theta &#39; )\)</span>에서 <span class="math inline">\(y&#39;\)</span>를 추출하는 exact sampling step이 따른다.</p>
<p>$$
q(’ , y’ , y) = q(‘, y) q(y’ ’)
$ $</p>
<p><span class="math inline">\(q(y&#39; \vert \theta &#39; )\)</span>가 <span class="math inline">\(f(y&#39; \vert \theta)\)</span>로 설정되었다면, MH ratio <span class="math inline">\(r\)</span>은 이하와 같이 쓰일 수 있다. 이때 unknown normalizing constant <span class="math inline">\(\kappa(\theta)\)</span>가 상쇄 (cancel) 되었음에 주목하라.</p>
<p>$$
<span class="math display">\[\begin{align*}

r(\theta, y, \theta&#39;, y&#39; \vert x)

&amp;= 


\dfrac 
{f(x \vert \theta&#39;) f(\theta&#39;) f(y&#39; \vert \theta&#39; , x) \ast q(\theta\vert \theta&#39; , y&#39;) q(y \vert \theta)} 
{f(x \vert \theta) f(\theta) f(y \vert \theta , x) \ast q(\theta&#39;\vert \theta , y) q(y&#39; \vert \theta&#39;)}

 \\

&amp;= 

\dfrac {f(\theta&#39;, y&#39; \vert x)}{f(\theta, y \vert x)} \ast 
\dfrac {q(\theta , y \vert \theta&#39; , y&#39;))}{q(\theta&#39; , y&#39; \vert \theta , y)}


 \\
&amp;=

\dfrac
{
\dfrac {f(\theta&#39;, y&#39; \vert x)}{q(\theta&#39; , y&#39; \vert \theta , y)} 
}
{
\dfrac{f(\theta, y \vert x) }{q(\theta , y \vert \theta&#39; , y&#39;))}
}



\end{align*}\]</span>
$$</p>
<p>여기서 계산을 간단하게 하기 위해 제안분포 <span class="math inline">\(q\)</span>와 auxiliary distribution을 이하와 같이 정리하는 것을 생각해볼 수 있다. 이때 <span class="math inline">\(\hat \theta\)</span>는 <span class="math inline">\(\theta\)</span>의 estimate로써, 예를 들어 pseudo-likelihood function을 극대화하는 것으로 얻어진 값이다.</p>
<p>$$
<span class="math display">\[\begin{align*}

q(\theta&#39; \vert \theta , y) &amp;= q(\theta&#39; \vert \theta ) , q(\theta \vert \theta &#39;, y&#39;) &amp;= q(\theta \vert \theta &#39;) \\

f(y \vert \theta , x) &amp;= f(y \vert \hat \theta ), f(y&#39; \vert \theta&#39; , x) &amp;= f(y&#39; \vert \hat \theta )

\end{align*}\]</span>
$$</p>
<p>분포 <span class="math inline">\(f(x \vert \theta)\)</span>를 auxiliary variable, 가령 normalizing constant ratio <span class="math inline">\(\dfrac {\kappa(\theta)} {\kappa(\theta&#39;)}\)</span> 등으로 살찌워놓은 것은, 시뮬레이션 진행 과정에서 상쇄시키는 것이 가능하다.</p>
<ol style="list-style-type: decimal">
<li>generate <span class="math inline">\(\theta \sim q(\theta&#39; \vert \theta_t)\)</span></li>
<li>generate exact sample <span class="math inline">\(y&#39; \sim f(y \vert \theta&#39;)\)</span></li>
<li>accept <span class="math inline">\((\theta&#39;, y&#39;)\)</span> with probability <span class="math inline">\(\min (1, r)\)</span>, <span class="math inline">\(r=\dfrac {f(x \vert \theta&#39;) f(\theta&#39;) f(y&#39; \vert \hat \theta&#39;) \ast q(\theta_t \vert \theta&#39;) q(y \vert \theta_t)} {f(x \vert \theta_t) f(\theta_t) f(y \vert \hat \theta) \ast q(\theta&#39;\vert \theta_t) q(y&#39; \vert \theta&#39;)}\)</span>.
<ul>
<li>채택된다면, set <span class="math inline">\((\theta_{t+1}, y_{t+1}) = (\theta&#39;, y&#39; )\)</span>.</li>
<li>o.w., <span class="math inline">\((\theta_{t+1}, y_{t+1}) = (\theta_t, y&#39;_t)\)</span>.</li>
</ul></li>
</ol>
<p><br>
<br>
<br></p>
</div>
<div id="exchange-algorithm" class="section level4" number="4.4.3.3">
<h4><span class="header-section-number">4.4.3.3</span> Exchange Algorithm</h4>
<p>Møller’s 알고리즘을 <strong>parallel tempering</strong> 개념을 도입하여 개선.</p>
<ol style="list-style-type: decimal">
<li>propose candidate point $’ $ proposal distribution <span class="math inline">\(q(\theta&#39; \vert \theta, x)\)</span>.</li>
<li>propose auxiliary variable <span class="math inline">\(y \sim\)</span> perfect sampler <span class="math inline">\(f(y \vert \theta &#39; )\)</span>.</li>
<li>accept $’ $ with probability <span class="math inline">\(\min \{ 1, r(\theta, \theta&#39; \vert x) \}\)</span>.
<ul>
<li><span class="math inline">\(r(\theta, \theta&#39; \vert x) = \dfrac{\pi(\theta&#39;) }{\pi(\theta) } \ast \dfrac{f(x \vert \theta &#39;) }{f(x \vert \theta)} \ast \dfrac{f(y \vert \theta) }{f(y \vert \theta&#39;)} \ast \dfrac{f(\theta \; \vert \theta&#39;, x) }{f(\theta&#39; \vert \theta, x)}\)</span></li>
</ul></li>
</ol>
<p>The exchange algorithm can be viewed as an auxiliary variable MCMC algorithm with the proposal distribution being augmented, for which the proposal distribution can be written as</p>
<p>$</p>
<p><span class="math display">\[\begin{align}

T \left( \theta \rightarrow (\theta&#39; , y) \right) &amp;= q(\theta &#39; \vert \theta) f(y \vert \theta &#39;) \\
T \left( \theta&#39; \rightarrow (\theta , y) \right) &amp;= q(\theta \vert \theta &#39; ) f(y \vert \theta)


\end{align}\]</span>
$</p>
<p>This simply validates the algorithm, following the arguments for auxiliary variable Markov chains.</p>
<p>The exchange algorithm generally improves the performance of the Møller algorithm, as it avoids an initial estimation step (for <span class="math inline">\(\theta\)</span>) that required by the Møller.</p>
<p>Although the Møller’s and exchange algorithms work well for some discrete models, such as the Ising and autologistic models, they cannot be applied to many other models for which perfect sampling is not available.</p>
<p>Even for the Ising and autologistic models, perfect sampling may be very expensive when the temperature is near or below the critical point.</p>
<hr />
</div>
<div id="adaptive-exchange-algorithm" class="section level4" number="4.4.3.4">
<h4><span class="header-section-number">4.4.3.4</span> Adaptive Exchange Algorithm</h4>
<p>Object: An adaptive exchange algorithm (AEX) is an adaptive Monte Carlo version of the exchange algorithm, where the auxiliary variables are generated via an importance sampling procedure from a Markov chain running in parallel.</p>
<ul>
<li>Advantage
<ul>
<li>Removes the requirement of perfect sampling</li>
<li>Overcomes its theoretical difficulty caused by inconvergence of finite MCMC runs</li>
</ul></li>
</ul>
<p>AEX consists of two chains running in parallel.</p>
<p>The first chain is <strong>auxiliary</strong>, which is run in the space <span class="math inline">\({\mathcal{x}}\)</span> with an aim to draw samples from a family of distributions <span class="math inline">\(f(X \vert \theta^{(1)}), \; \; \cdots, \; \; f(X \vert \theta^{(m)})\)</span> for a set of pre-specified parameter values <span class="math inline">\(\theta^{(1)}, \; \cdots, \; \theta^{(m)}\)</span>.</p>
<p>The second chain is the <strong>target</strong> chain, which is run in the space <span class="math inline">\(\theta\)</span> with an aim to draw samples from the target posterior <span class="math inline">\(\pi(\theta \vert y)\)</span>. For a candidate point <span class="math inline">\(\theta&#39;\)</span>, the auxiliary variable <span class="math inline">\(x\)</span> is resampled from the past samples of the auxiliary chain via an importance sampling procedure.</p>
<p>Assume that the neighboring distributions <span class="math inline">\(f(X \vert \theta^{(i)})\)</span>’s have a reasonable overlap and the set <span class="math inline">\(\left \{ \theta^{(1)}, \; \cdots, \; \theta^{(m)} \right \}\)</span> has covered the major part of the support of <span class="math inline">\(\pi (\theta \vert y)\)</span>.</p>
<ul>
<li>ALGORITHM: PART 1 - (Auxiliary Chain) Auxiliary Sample Collection via SAMC</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>(Sampling) Choose to update <span class="math inline">\(\vartheta\)</span> or <span class="math inline">\(\pmb z_t \vert \vartheta\)</span> with pre-specified probabilities, e.g., <span class="math inline">\(0.75\)</span> for updating <span class="math inline">\(\vartheta\)</span> and <span class="math inline">\(0.25\)</span> for updating <span class="math inline">\(z_t\)</span>.</p>
<p>1.-a Update <span class="math inline">\(\vartheta_{t}\)</span> : Draw <span class="math inline">\(\vartheta &#39;\)</span> from the set <span class="math inline">\(\left \{ \theta^{(1)}, \; \cdots, \; \theta^{(m)} \right \}\)</span> according to a proposal distribution <span class="math inline">\(T_1 ( \; \cdot \; \vert \vartheta_{t})\)</span>, set <span class="math inline">\((\vartheta_{t+1}, \pmb z_{t+1}) = (\vartheta &#39; , \pmb z_{t+1} )\)</span> with probability <span class="math inline">\(\min \left\{ 1, \; \; \dfrac{\omega_t^{J(\vartheta_t)}}{\omega_t^{J(\vartheta &#39;)}} \ast \dfrac {\varphi (\pmb z_{t} \vert \vartheta &#39;)} {\varphi (\pmb z_{t} \vert \vartheta_{t})} \ast \dfrac{T_1 (\vartheta_{t} \vert \vartheta &#39; )}{T_1 (\vartheta &#39; \vert \vartheta_{t} )} \right\}\)</span>, and set <span class="math inline">\((\vartheta_{t+1}, \pmb z_{t+1}) = (\vartheta_{t}, \pmb z_t)\)</span> with remaining probability, where <span class="math inline">\(J(\vartheta_t)\)</span> denotes the index of <span class="math inline">\(\vartheta_t\)</span>, i.e., <span class="math inline">\(J(\vartheta_t) = j\)</span> if <span class="math inline">\(\vartheta_t = \theta_i^{(k)}\)</span> and <span class="math inline">\(\varphi(\pmb z \vert \vartheta)\)</span> is an unnormalized density of <span class="math inline">\(f(\pmb z \vert \vartheta)\)</span>.</p>
<p>1.-b. Update <span class="math inline">\(\pmb z_t\)</span> : Draw <span class="math inline">\(\pmb z &#39;\)</span> according to a proposal distribution <span class="math inline">\(T_2 ( \; \cdot \; \vert \pmb z_t)\)</span>, set <span class="math inline">\((\pmb z_{t+1} , \vartheta_{t+1}) = (\pmb z &#39; , \vartheta_{t})\)</span> with probability <span class="math inline">\(\min \left\{ 1, \; \; \dfrac {\varphi (\pmb z &#39; \vert \vartheta_{t})} {\varphi (\pmb z_{t} \vert \vartheta_{t})} \ast \dfrac{T_2 (\pmb z_{t} \vert \pmb z &#39; )}{T_2 (\pmb z &#39; \vert \pmb z_{t} )} \right\}\)</span>, and set <span class="math inline">\((\pmb z_{t+1} , \vartheta_{t+1}) = (\pmb z_t , \vartheta_{t})\)</span></p></li>
<li><p>(Abundance Factor Updating) Set</p></li>
</ol>
<p><span class="math display">\[
\log (\omega_{t+0.5}^{(j)}) =\log (\omega_{t}^{(j)}) + a_{t+1} (e_{t+1, \; j} - p_j), \; \; \; \; \; j=1, \cdots, m
\]</span></p>
<p>where <span class="math inline">\(e_{t+1, \; j} = \begin{cases} 1 &amp; &amp;&amp; \vartheta^{t+1} = \theta^{(j)} \\ 0 &amp; &amp;&amp; o.w. \end{cases}\)</span>.</p>
<p>If <span class="math inline">\(\omega_{t+0.5}^{(j)} \in \mathcal{K}_{\varsigma_t}\)</span>, set <span class="math inline">\((\omega_{t+1}, \pmb z_{t+1}) = (\omega_{t+0.5}, \pmb z_{t+1})\)</span> and <span class="math inline">\(\varsigma_{t+1} = \varsigma_t\)</span>.</p>
<p>o.w., set <span class="math inline">\((\omega_{t+1}, \pmb z_{t+1}) = \mathbb{T}(\omega_{t}, \pmb z_{t})\)</span> and <span class="math inline">\(\varsigma_{t+1} = \varsigma_t + 1\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>(Auxiliary Sample Collection) Append the sample <span class="math inline">\(\left(\pmb z_{t+1} , \vartheta_{t+1}, \omega_{t+1}^{J(\vartheta_{t+1}} \right)\)</span> to the collection <span class="math inline">\(S_t\)</span>. Denote the new collection by <span class="math inline">\(S_{t+1}\)</span> which is set by <span class="math inline">\(S_{t+1} = S_t \cup \left\{ \left(\pmb z_{t+1} , \vartheta_{t+1}, \omega_{t+1}^{J(\vartheta_{t+1}} \right) \right\}\)</span>.</li>
</ol>
<ul>
<li>ALGORITHM: PART 2 - (Target Chain) Adaptive Exchange Sampler</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>(Proposal) Propose a candidate point <span class="math inline">\(\theta &#39;\)</span> from a proposal distribution <span class="math inline">\(q(\theta &#39; \vert \theta)\)</span></p></li>
<li><p>(Resampling for Auxiliary Variables) Resample an auxiliary variable <span class="math inline">\(\pmb x\)</span> from the collection <span class="math inline">\(S_{t+1}\)</span> via a dynamic importance sampling procedure;</p></li>
</ol>
<p>that is, setting <span class="math inline">\(\pmb x = \pmb z_k\)</span> with probability</p>
<p><span class="math display">\[
P(\pmb x = \pmb z_k) 
\dfrac
{\sum_{j=1}^{\vert S_{t+1} \vert} \omega_t^{\left( J(\vartheta_j) \right)} \tfrac{\varphi(\pmb z_k \vert \theta &#39; )} {\varphi(\pmb z_k \vert \vartheta_j &#39; )} \ast I(\pmb z_j = \pmb z_k )}
{\sum_{j=1}^{\vert S_{t+1} \vert} \omega_t^{\left( J(\vartheta_j) \right)} \tfrac{\varphi(\pmb z_k \vert \theta &#39; )} {\varphi(\pmb z_k \vert \vartheta_j &#39; )}}
\]</span></p>
<ul>
<li><span class="math inline">\(\left(\pmb z_{j} , \vartheta_{j}, \omega_{t}^{J(\vartheta_{j}} \right)\)</span> denotes the <span class="math inline">\(j\)</span>-th element of the set <span class="math inline">\(S_{t+1}\)</span>.</li>
<li><span class="math inline">\(\vert S_{t+1} \vert\)</span>는 <span class="math inline">\(S_{t+1}\)</span>의 size.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>(Exchange Algorithm) Set <span class="math inline">\(\theta_{t+1} = \theta &#39;\)</span> with the probability <span class="math inline">\(\alpha(\theta_t , \pmb x, \theta&#39; )\)</span>, and <span class="math inline">\(\theta_{t+1} = \theta_{t}\)</span> with probability <span class="math inline">\(1-\alpha(\theta_t , \pmb x, \theta&#39; )\)</span>.</li>
</ol>
<p><span class="math display">\[
\alpha(\theta_t , \pmb x, \theta&#39; ) = 
\dfrac {\pi(\theta &#39; )} {\pi(\theta_t )}
\dfrac {\varphi(\pmb y \vert \theta &#39; )} {\varphi(\pmb y \vert \theta_t )}
\dfrac {q(\theta_t \vert \theta &#39; )} {q(\theta&#39; \vert \theta_t )}
\dfrac {\varphi(\pmb x \vert \theta_t )} {\varphi(\pmb x \vert \theta &#39; )}
\]</span></p>
<ul>
<li>Why this algorithm is adaptive?</li>
</ul>
<p>Since the underlying true proposal distribution for generating auxiliary variables in part II is changing from iteration to iteration, the new algorithm falls into the class of adaptive MCMC algorithms (for which the proposal distribution is changing from iteration to iteration).</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="advanced-mcmc-wk08.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="approximate-bayesian-computation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/211204_AuxiliaryVariableMCMC.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
