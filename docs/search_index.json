[["index.html", "Self-Study Intro", " Self-Study Intro [https://yamalab.tistory.com/98] [https://parkgeonyeong.github.io/1.VC-generalization-bound%EC%9D%98-%EC%9D%B4%ED%95%B4/] [https://sanghyukchun.github.io/66/] [https://sanghyukchun.github.io/home/] https://www.google.com/search?q=vapnik+svm&amp;oq=vapnik+svm&amp;aqs=edge..69i57j0i30l2j0i8i30j0i5i30j0i8i30l2.990j0j4&amp;sourceid=chrome&amp;ie=UTF-8 [http://www.aistudy.co.kr/pattern/support_vector_machine.htm] [http://www.aistudy.co.kr/] [VC dimension] [https://stats.hohoweiya.xyz/2019/03/08/Bernstein/] [https://stats.hohoweiya.xyz/] [https://math.stackexchange.com/questions/533146/dantzigs-unsolved-homework-problems] [https://high-dimensional-statistics.github.io/2021/03/17/exercise-6.10.html] [https://metamath1.github.io/cnn/index.html] [https://slcladal.github.io/net.html] [http://web.humoruniv.com/board/humor/read.html?table=pds&amp;number=1113163] [https://blog.naver.com/PostView.nhn?blogId=dnjswns2280&amp;logNo=221925748769&amp;categoryNo=10&amp;parentCategoryNo=0&amp;viewDate=&amp;currentPage=1&amp;postListTopCurrentPage=1&amp;from=postView] [http://taewan.kim/post/cnn/] [https://every-day-life.tistory.com/29] [https://everyday-deeplearning.tistory.com/entry/SGD-Stochastic-Gradient-Descent-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95] [https://kkokkilkon.tistory.com/67] [https://kkokkilkon.tistory.com/115] [https://mongxmongx2.tistory.com/25] [https://mons1220.tistory.com/93] [https://soh9646.tistory.com/68] [https://yamalab.tistory.com/category/Recommender%20System/%EC%B6%94%EC%B2%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C] [https://velog.io/@sennys2/JavaLog4j] [https://www.youtube.com/watch?v=ezYLdnfo_wk] [https://blog.zarathu.com/posts/2020-10-29-survivalpractice/] networklabhp 네트워크00! "],["categorical.html", "Chapter 1 Categorical ", " Chapter 1 Categorical "],["overview.html", "1.1 Overview", " 1.1 Overview 1.1.1 Data Type and Statistical Analysis "],["bayesian.html", "Chapter 2 Bayesian ", " Chapter 2 Bayesian "],["abstract.html", "2.1 Abstract", " 2.1 Abstract 조건부 확률은 sample sapce가 \\(S\\)에서 \\(B\\)로 축소되었다는 것을 의미한다. Bayesian의 Multiplication Rule은 사건이 시간순서대로 발생할 때 유용하게 사용될 수 있다. set of events become partition of sample space \\(S\\): 1. mutually exclusive(disjoint) 2. Pr of union \\(=1\\) event \\(H\\)와 event \\(A\\), \\(B\\)가 주어져 있다. \\(A\\)와 \\(B\\) 가 서로 독립이라면, \\(H\\)가 주어졌을 때 \\(A\\)가 추가되는 것이 \\(B\\)에 대한 정보를 아는데 영향을 미치지 않는다. 수식으로 증명가능. \\(Dirichlet\\), \\(Wishart\\) \\(posterior odds\\) 2.1.1 변수의 독립성 \\(X_1 , \\cdots, X_n\\)이 공통 sample space \\(S\\)를 갖는 변수이고 \\(\\theta\\) is unknown parameter. if \\(S\\), with for any subset(events) \\(A_1 , \\cdots, A_n\\), $Pr(X_1 A_1 , , X_n A_n ) = Pr(X_1 A_1 ) * Pr(X_n A_n ), then \\(X_1 , \\cdots, X_n\\) 는 \\(\\theta\\)가 주어졌을 때 조건부 독립이다. 이는 앞서 말한 event의 독립성에 대응된다. 위의 독립성은 event의 독립성과 마찬가지로 $Pr(X_i A_i , X_j A_j) = Pr(X_i A_i ) 가 성립. 이는 \\(\\theta\\)가 주어졌을 때 \\(X_j\\)의 정보가 \\(X_i\\)에 대하여 아무런 추가정보를 주지 못함을 의미한다. 만약 세타가 주어진 상태에서 X1~Xn이 조건부 독립이라면 조건부 joint pdf는 각 조건부 margianl pdf의 곱과 같다. 만약 X-i가 모두 같은 분포를 따르면~. 이때 X_i들은 세타가 주어졌을 때 conditionally iid. 이는 marginal iid와는 구변된다. marginal iid는 X_i들의 marginal iid가 모두 같고 또한 독립이라는 소리. 2.1.2 교환가능성 독립성은 엄격한 조건. 만족안되는 경우 많음. 이것보다는 약조건이 교환가능성. 독립성 \\(\\rightarrow\\) 교환가능성이지만 교환가능성 \\(\\not \\rightarrow\\) 독립성. 교환가능성까지만 만족되면 De Finetti thm은 성립함. "],["continual-aeassessment-method.html", "2.2 Continual Aeassessment Method", " 2.2 Continual Aeassessment Method "],["horseshoe-prior.html", "2.3 Horseshoe Prior", " 2.3 Horseshoe Prior "],["mathematical-stats.html", "Chapter 3 Mathematical Stats ", " Chapter 3 Mathematical Stats "],["inference.html", "3.1 Inference", " 3.1 Inference \\(T(X)\\)가 \\(\\theta\\)의 추정량. * bias $ = E - $ * if bias\\(=0\\), \\(T(X)\\)는 \\(\\theta\\)의 UE. 이때, \\(\\theta\\) can be \\(g(\\theta)\\). 즉슨, \\(\\theta\\)는 패러미터 그 자체만이 아니라 패러미터의 함수를 패러미터 삼아 이를 추정하려고 들 수도 있다. 이하의 전개에서는 \\(\\theta = g(\\theta)\\) 로 이해하자. \\(\\theta\\)의 추정량 \\(T(X)\\)의 MSE는 $MSE = Var +(bias)^2 $. \\(T_1(X)\\), \\(T_2(X)\\)는 \\(\\theta\\)의 UE. \\(T_1(X)\\)의 \\(T_2(X)\\)에 대한 Relative Efficiency \\(RE= \\dfrac {Var \\left[ T_2 (X) \\right]} {Var \\left[ T_1 (X) \\right]}\\) rv $X_1 , , X_n f(x_1 , , x_n ) $. 이하의 조건 하에서 추정량 \\(T^\\ast (X)\\)는 \\(\\theta\\)의 MVUE. 1. \\(E \\left [ T^\\ast (X) \\right] = \\theta\\). 즉 \\(T^\\ast (X)\\)는 \\(\\theta\\)의 UE. 2. $T(X):Var $. Fisher’s Information $I() = E { ^2 } $ regularity condition: 1. The partial derivative of \\(f(X; \\theta)\\) with respect to \\(\\theta\\) exists almost everywhere. (It can fail to exist on a null set, as long as this set does not depend on \\(\\theta\\).) 2. The integral of \\(f(X; \\theta)\\) can be differentiated under the integral sign with respect to \\(\\theta\\). 3. The support of \\(f(X; \\theta)\\) does not depend on \\(\\theta\\). e.g., 패러미터 다르면 pdf 다름. 즉, \\(\\theta \\not = \\theta&#39;: f(x;\\theta) \\not = f(x;\\theta&#39;)\\) set \\(A = \\{ x: f(x;\\theta)&gt;0 \\}\\)은 패러미터 \\(\\theta\\)에 의존하지 않고, \\(\\forall x \\in A, \\theta \\in \\Omega : \\log f(x;\\theta)\\)는 \\(\\theta\\)에 대해 두 번 미분 가능하고 도함수가 연속이다. 통계량 \\(T(X)\\)가 \\(\\forall \\theta \\in \\Omega: E \\left [ T (X) \\right] &lt; \\infty\\) 라면, $ {} E $에 있어 미분과 적분의 순서를 바꿀 수 있다. Information inequality: under regularity condition, \\(\\forall g^{-1}(\\theta) \\in \\Omega, Var \\left [ T^\\ast (X) \\right] &lt; \\infty, E \\left [ T^\\ast (X) \\right] = \\theta, 0&lt;I(\\theta)&lt; \\infty:\\) \\(\\theta\\) is differentiable, and \\(Var \\left [ T^\\ast (X) \\right] \\ge \\dfrac {1}{n} \\dfrac {\\left[ g&#39;(\\theta) \\right]^2}{I (\\theta)}\\). rv $X_1 , , X_n f(x_1 , , x_n ) $. \\(l\\)개 stats(통계량)의 벡터 \\(\\pmb {S(X)} = \\left[ S_1(X), \\cdots, S_l(X) \\right]\\). 이때 rv $X_1 , , X_n $의 분포가 패러미터 $ = (_1 , , _k )$에 의존하지 않으면 stats \\(\\pmb {S(X)}\\)는 joint SS. rv $X_1 , , X_n f(x_1 , , x_n ) $. 1개 stats(통계량) \\(S(X)\\). 이때 rv \\(X_1 , \\cdots, X_n \\rvert S(X)\\) 의 분포가 패러미터 \\(\\theta = (\\theta_1 , \\cdots, \\theta_k )\\)에 의존하지 않으면 stats \\(S(X)\\)는 SS. Decomposition thm.: rv $X_1 , , X_n f(x_1 , , x_n ) $. \\(k\\)개 stats(통계량) \\(\\pmb {S(X)} = \\left[ S_1(X), \\cdots, S_k(X) \\right]\\). stats \\(\\pmb {S(X)}\\)는 joint SS \\(\\iff\\) $f(x_1 , , x_n ; ) = g h(x_1 , , x_n) $ 3.1.1 Rao-Blackwell thm. 패러미터의 함수 \\(\\theta\\), \\(S\\)는 SS, \\(T(X)\\)는 UE. let \\(\\delta (S) = E \\left [ T(X) \\rvert S \\right]\\). 이때 \\(\\delta (S)\\)는 \\(\\theta\\)의 UE. 따라서 $$ \\[\\begin{align*} Var \\left[ \\delta (S) \\right ] &amp;= E \\left\\{ \\left[ \\delta (S) - \\theta \\right]^2 \\right\\} \\\\ &amp;\\le E \\left\\{ \\left[ T(X) - \\theta \\right]^2 \\right\\} = Var \\left [ T(X) \\right] \\end{align*}\\] $$ 3.1.2 Completeness rs \\(X_1 , \\cdots, X_n\\)의 stats \\(T (X_1 , \\cdots, X_n )\\)에 대해, let \\[ \\forall \\theta \\in \\Omega: \\; \\; E \\left[ g(T) \\right]=0 \\] 이때 이를 만족하는 \\(\\theta\\)에 무관한 함수 \\(g\\)가 \\(g(\\cdot) \\equiv 0\\) 뿐이라면, \\(T\\)는 CS. \\(T\\)가 \\(\\theta\\)에 대한 SS라면, 이는 CSS. stats \\(Y\\)가 분포모임 \\(\\{g(y;\\theta);\\theta \\in \\Theta \\}\\)의 한 원소를 pdf로 가진다고 하자. \\[ \\forall \\theta \\in \\Theta: \\; \\; E_{\\theta} \\left[ \\varphi(Y) \\right] \\overset{\\theta}{=}{0} \\; \\; \\; \\rightarrow \\; \\; \\; \\varphi(y) \\overset{y}{=} 0 \\] 위의 명제가 성립할 때 위 분포족은 completeness를 지닌다. 여기서 \\(\\varphi\\)는 \\(\\theta\\)에 무관한 함수이다. 피명제는 보다 엄밀히는 \\(\\forall \\theta: P_{\\theta} \\{ \\varphi (Y)=0 \\}=1\\). \\(\\overset{\\theta}{=}\\)는 모든 \\(\\theta \\in \\Omega\\)에 대해 등호가 성립함을 나타낸다. Remarks: completeness는 본질적으로 확률분포의 패러미터 \\(\\theta\\)가 통계량 \\(Y\\)를 통해 추정될 수 있음을 보장하는 조건으로 이해될 수 있다. 즉, completeness는 서로 다른 패러미터값을 지니는 두 분포는 서로 구분(distinct)됨을 보장해주는 조건이다. 통계량 \\(Y\\)의 분포족이 completeness를 만족하면, \\(Y\\)를 완비통계량 CS라고 부른다. 완비성은 CS의 함수로 이루어지는 UE는 unique하다는 사실을 보이는 도구로 이용된다. 레만-쉐페 thm 참조. 3.1.3 레만-쉐페 thm. 패러미터 \\(\\theta\\)에 대해 \\(T\\)가 CSS, \\(S(X)\\)는 \\(\\theta\\)의 UE. 이때 \\(\\delta(T)=E \\left [ S(X) \\rvert T \\right]\\) 는 \\(\\theta\\) 의 UMVUE. rs \\(X_1 , \\cdots, X_n \\overset{iid}{\\sim} f(x;\\theta)\\). \\(\\theta\\)에 대한 CSS \\(Y=u(X_1 , \\cdots, X_n)\\). 이때 임의의 UE \\(\\hat \\theta\\)에 대해 \\[ \\varphi (Y) = E(\\hat \\theta \\rvert Y) \\] 는 \\(\\theta\\)에 대한 UMVUE. 이는 unique. 3.1.4 Rao-Blackwell thm. rs \\(X_1 , \\cdots, X_n \\overset{iid}{\\sim} f(x;\\theta), \\theta \\in \\Theta\\). \\(Y= u(X_1 , \\cdots, X_n)\\)는 \\(\\theta\\)의 CSS. \\(Z= v(X_1 , \\cdots, X_n)\\)의 분포는 \\(\\theta\\)에 의존하지 않는다. 이상의 조건이 만족되면 \\(Y \\perp Z\\). exponentail family: pdf가 적절한 함수 \\(a, b, c_i, t_i (i=1,\\cdots, k)\\)에 대해 \\(f(x;\\theta) = a(\\theta) b(x) \\exp \\left[ \\sum_{i=1}^k c_i (\\theta) t_i (x) \\right], -\\infty\\) 지수족에 속하는 pdf로부터 rs \\(X_1 , \\cdots, X_n\\) 를 얻었다면, 통계량 \\(S_1 = \\sum_{i=1}^n t_1 (X_i), \\cdots, S_k = \\sum_{i=1}^n t_k (X_i)\\) 는 패러미터 \\(\\theta_1 , \\cdots, \\theta_k\\) 에 대한 joint (C) SS이다. \\(g(\\theta)\\)에 대한 est \\(\\tau(\\pmb X)\\)가 \\(\\forall \\epsilon &gt;0: \\lim_{n \\rightarrow \\infty} P \\left( \\vert \\tau(\\pmb X) - g(\\theta) \\vert \\le \\epsilon \\right) =1\\) 을 만족하면 est \\(\\tau(\\pmb X)\\)는 consistency를 가진다. 이는 표본의 크기가 커짐에 따라 est \\(\\tau(\\pmb X)\\)가 \\(g(\\theta)\\) 에 확률적으로 수렴한다는 것. 표본의 크기가 매우 클 때, est \\(\\tau(\\pmb X)\\) 로부터 계산된 추정값 estimates는 높은 확률로 참모수값에 매우 가까이 있다는 뜻. est \\(\\tau(\\pmb X)\\)를 \\(g(\\theta)\\) 의 것일 때, \\(\\forall \\theta \\in \\Theta: \\lim_{n \\rightarrow \\infty} P E \\left\\{ \\tau(\\pmb X)-g(\\theta)\\right\\}^2 = 0\\) 이 성립하면 est \\(\\tau(\\pmb X)\\) 는 consistent. est \\(\\tau(\\pmb X)\\) 가 \\(\\theta\\) 의 consistent이고, \\(g(x)\\) 가 \\(\\theta\\) 에서 연속인 함수라면, \\(g\\tau(\\pmb X)\\). "],["hypothesis-test.html", "3.2 Hypothesis Test", " 3.2 Hypothesis Test 통계적 가설 | Statistical Hypothesis | 관심있는 population의 성질에 대한 단정이나 추측 등의 표현 (statement) 이러한 가설은 흔히 모집단의 성질을 나타내는 rv의 분포에 대한 표현으로 나타난다. | 단순가설 | Simple Hypothesis | 어떤 가설이 확률분포 (pd) 를 완전히 결정한다 | 복합가설 | Composite Hypothesis | 그렇지 않다 | 다양한 검정법에서 우선순위를 정하는 것은 옳은 결론을 내리는 빈도가 높은, 즉 잘못된 결정을 내릴 확률이 낮은 검정법이 좋은 검정법이라는 것. 검정통계량(Test Statistics): 주어진 rs에 근거하여 통계적 가설에 대한 증거를 살펴볼 때 사용되는 통계량 기각영역(Rejection Region, Critical Region): \\(H_0\\)를 기각하게 되는 검정통계량의 값을 가지는 sample space의 부분집합 (event) | \\(H_0\\) True | \\(H_0\\) False | reject \\(H_0\\) | | Type 2 Error (\\(\\beta\\)) 유죄인데 석방 | accept \\(H_0\\) | Type 1 Error (\\(\\alpha\\)) 무죄인데 사형 | | 제1종 오류를 범활 확률 \\(\\alpha\\)는 유의확률(Significance Level) 라고 따로 칭함. \\(H_1\\)은 기존으로부터의 변화이므로 채택에 있어 훨씬 엄격해야 함. 따라서 \\(\\alpha\\)가 \\(\\beta\\)보다 훨씬 더 중시됨. let Rejection Region \\(C\\). then $$ \\[\\begin{alignat*}{2} \\alpha &amp;= P(\\text{Type 1 Error}) \\\\ &amp;= P(\\text{accept }H_1 \\vert H_0) \\\\ &amp;= P(\\pmb X_n \\in C \\vert H_0) \\begin{aligned}[t] &amp; = \\int_C f(\\pmb x \\vert H_0) d \\pmb x\\\\ &amp;= \\sum_C f(\\pmb x \\vert H_0) \\end{aligned} \\end{alignat*}\\] $$ This can also be written as Loss Function. $$ \\[\\begin{align*} L(H_i ; H_j ) = \\begin{cases} 0, &amp; \\text{if } i = j \\\\ 1, &amp; \\text{for } i \\not = j, \\; \\; (i,j = 0, 1) \\end{cases} \\end{align*}\\] $$ $$ \\[\\begin{align*} E \\left [ L(H_1 ; H_0 ) \\right] &amp;= P(\\text{Type 1 Error}) \\\\ E \\left [ L(H_0 ; H_1 ) \\right] &amp;= P(\\text{Type 2 Error}) \\end{align*}\\] $$ "],["power-fucntion.html", "3.3 Power Fucntion", " 3.3 Power Fucntion 여기서, \\(H_0\\)에 대한 기각영역이 \\(C\\)인 test의 검정력함수 (power function)은 이하와 같다. 즉, 이는 \\(H_0\\)를 기각하는 확률로 정의된다. \\[ \\pi(\\theta) = P (\\pmb X_n \\in C \\vert \\theta) \\] 이는 패러미터 \\(\\theta\\)의 참값이 무엇이냐에 따라 다른 값을 가지므로 \\(\\theta\\)의 함수이다. 주어진 \\(\\theta\\)에서의 power function의 값 \\(\\pi(\\theta)\\)은 이 \\(\\theta\\)에서의 검정력 (power). power는 \\(H_0\\)를 기각할 확률. * if \\(\\theta \\in H_0\\), power는 작을수록 좋다. * \\(\\theta = \\theta_0 \\in H_1\\), 이 경우 power \\(\\pi(\\theta) = \\pi(\\theta_0) = \\alpha\\). * if \\(\\theta \\in H_1\\), power는 클수록 좋다. * \\(\\theta \\in H_1\\), and \\(H_1\\)이 simple hypothesis, 이 경우 power \\(\\pi(\\theta) = 1- \\beta\\). 이와 같이 power function은, 마치 MSE가 점추정의 기준이 되었던 것처럼, \\(\\alpha\\) (유의수준)이 고정되었을 때 test 방법의 성능을 결정하는 기준이 된다. 3.3.1 Significance Probability (p-value) 앞에서 언급했던 것과 같이, 좋은 검정법을 찾기 위해 sample space를 \\(C\\)와 채택영역 \\(C^c\\)로 나누고 \\(\\alpha\\)와 \\(\\beta\\)를 계산하여 오류의 확률을 작게 만드는 검정법을 고르게 된다. 사용할 검정법을 결정하고 나면, 자료에서 관측된 값이 \\(C\\)에 속할 경우 \\(H_0\\)를 기각하고, 이외에는 \\(H_0\\)를 기각하지 않는다고 결론을 내리게 된다. 그런데 관찰된 test stat의 값이 \\(C\\)에 속한다 하더라도 값의 크기 등에 따라 통계적 유의성에 대한 의미가 다를 수 있다. 따라서 기각할 것인지, 하지 않을 것인지 이분법적인 결론만을 제시하기보다, 관측한 자료가 \\(H_0\\)에 대하여 어느 정도의 반증이 되는지를 수치적으로 나타낼 수 있는 \\(\\alpha\\) (유의확률)을 이용하여 test의 결론에 이르는 경우가 많이 있다. p값 (p-value), 즉 관측된 유의수준 (observed significance level), 혹은 유의확률 (Significance Probability), 는 \\(H_0\\)가 참이라는 가정 하에, 우리가 관측한 값과 같거나 더 극단적인 값을 얻을 확률 (ex. \\(P(T \\ge t \\vert H_0 )\\)) 로 정의된다. 여기서 더 극단적이라는 것은, 관측한 값보다 \\(H_1\\)에 더 가까운 것을 의미한다. 만약 어떤 관측값에 대하여 p값을 계산하였더니 아주 작은 값이 나왔다면, 우리가 관측한 값 자체가 이미 매우 극단적이라서 이보다 더 강한 \\(H_1\\)에 대한 증거를 관측할 확률이 작다는 것이다. 즉, 관측값이 \\(H_0\\) 하에서 나오기 어려운 값이라는 뜻이므로 \\(H_0\\)를 기각할 근거가 된다고 할 수 있다. 만약 어떤 관측값에 대하여 p값을 계산하였더니 작지 않은 값이 나왔다면, 우리가 관측한 값이 \\(H_0\\) 하에서 흔히 나올 수 있는 값이라는 것이고, 즉 \\(H_0\\)를 기각할 근거가 되지 않는다고 할 수 있다. p값이 \\(H_0\\)를 기각할만큼 작은지를 결정하는 것은 보통 결과를 해석하는 사람에게 달려있다. 그러나 가설검정을 할 때는 흔히 적당한 유의수준 \\(\\alpha\\)의 값을 생각하고 있기 마련이므로, p값이 \\(\\alpha\\)보다 작으면 관측된 자료가 대립 가설에 대한 충분한 증거가 된다고 판단하여 \\(H_0\\)를 기각하게 된다. 정리하자면, p값은 \\(H_0\\) 하에서 test stat의 관찰값 (test stats) 이 \\(H_0\\)를 기각하는 방향으로 나타나는 확률을 의미한다. 주어진 유의수준 \\(\\alpha\\)보다 p값이 작으면 \\(H_0\\)를 기각하며, 그렇지 않은 경우에는 \\(H_0\\)를 받아들이게 된다. "],["optimal-testing-method.html", "3.4 Optimal Testing Method", " 3.4 Optimal Testing Method 항상 옳은 결과를 가져다주는 검정법을 사용할 수 있다면 가장 좋겠지만, 샘플에서 주어지는 정보만을 가지고 모집단의 특성에 대한 결론을 내려야 하는 상황에서 언제나 옳은 결과를 가져다주는 test 방법을 찾을 수는 없다. 그렇기에 이 장의 목표는 옳은 결과를 가져다주는 빈도가 높은 test 방법을 찾는 것이 된다. 잘못된 결론을 내릴 확률은 두 가지 오류로 표현되므로, 제 1종 오류와 제 2종 오류의 발생확률을 낮게 하는 test 방법을 찾아야 한다. 불행히도, 샘플의 크게가 정해져 있는 경우 둘 다를 최소로 하는 test 방법을 찾는 거은 불가능하다. 예를 들면, \\(\\alpha\\)를 최소로 하는 가장 간단한 방법은 언제나 \\(H_0\\)를 채택하는 것이지만 (\\(\\alpha = 0\\)), 이는 \\(H_1\\)에서의 power를 0으로 최소화시키고, 즉, \\(\\beta\\)를 극대화시킨다. let \\(\\pmb X_{25} \\overset {\\text{iid}} {\\sim} N(\\mu, 10^2 )\\). \\[ H_0 : \\mu = 100, \\; \\; \\; \\; \\; H_1 : \\mu &gt; 100 \\] 이때. \\(\\mu=100\\)에서의 power는 유의수준 \\(\\alpha\\)와 같고, \\(\\mu&gt;100\\)일 경우에는 \\(\\pi(\\mu) = 1-\\beta(\\mu)\\). 이인즉 $$ \\[\\begin{align*} \\lim_{\\mu \\downarrow 100} \\beta(\\mu) &amp;= 1- \\pi(100) \\\\ &amp;= 1- \\alpha \\end{align*}\\] $$ 따라서 \\(H_0\\)와 \\(H_1\\)의 경계점에서 \\(\\alpha + \\beta = 1\\)이 된다. 즉, 샘플의 크기가 일정할 때 \\(\\alpha\\)를 줄이고자 하면 경계점에서 \\(\\beta\\)의 값이 커지며, 이 역 또한 성립한다. 이를 power로 표현하면, \\(H_0\\) 하에서 power는 큰 것이 바람직하나 power \\(\\pi (\\mu)\\)를 늘이고자 하면 \\(\\alpha\\)의 값이 같이 커지게 되므로 제1종 오류의 확률 (\\(\\alpha\\))의 확률을 최소화하면서 power를 최대화하는 일은 sample의 크기가 정해져 있는 경우 불가능하다. 만약 sample의 크기를 늘인다면, \\(\\alpha\\)의 값을 고정시킨 상태에서 주어진 \\(H_1\\) 하에서의 \\(\\mu\\) 값에서의 power를 크게 할 수 있다. 이 절에서는 power function \\(\\pi(\\cdot)\\)을 기준으로 하는 Optimal Testing Method (최량검정법)에 대해 살펴볼 것이다. 우선, \\(H_0\\)와 \\(H_1\\)이 모두 simple인 경우를 생각해보자. 위에서 이야기하였듯 \\(\\alpha\\)를 최소화하면서 \\(H_0\\) 하에서의 power를 최대화하는 것은 불가능하므로, 이에 대한 합리적 대안으로 \\(\\alpha\\) (제1종 오류를 범할 확률)을 주어진 작은 값으로 제한한 상태에서, power를 최대화하는 의미에서의 OTM을 다음과 같이 정의한다. $$ H_0: = _0, ; ; ; ; ; H_1: = _1 $$ 에 대한 rejection region \\(C^\\ast\\) 가 다음 조건을 만족할 때 이를 유의수준 \\(\\alpha\\) 에서의 MPT의 RR, 또는 MPRR이라고 한다. \\(\\pi^\\ast\\)가 \\(C^\\ast\\)에 해당하는 power function이라 하면, 1. \\(\\pi^\\ast (\\theta_0) = \\alpha\\), 2. \\(\\forall \\text{ RR } C, \\; \\text{whose 유의수준과 power function } \\alpha, \\pi: \\pi^\\ast(\\theta_1) \\ge \\pi(\\theta_1)\\). "],["data-reduction.html", "3.5 Data Reduction", " 3.5 Data Reduction 3.5.1 Sufficiency Principle \\(X \\vert T(X)\\)의 분포가 \\(\\theta\\)에 의존하지 않는다면, \\(T(X)\\)는 \\(\\theta\\)의 SS. - \\(T(X)\\)가 \\(\\theta\\)의 SS라면, \\(\\theta\\)에 대한 모든 추론은 \\(T(X)\\)를 거쳐서만이 \\(X\\)에 의존함. 즉 \\(T(X)\\) 값만 알 수 있다면 모든 \\(X\\)에 대해 알지 못해도 무관. 비율 \\(\\dfrac{f_X(x \\vert \\theta)}{f_T(X) \\left( T(x) \\vert \\theta \\right)}\\)가 \\(\\forall x \\in \\Omega\\)에 대해 \\(\\theta\\)의 함수로서 constant 하다면, \\(T(X)\\)는 \\(\\theta\\)의 SS. 이인즉 \\(f(x \\vert T(x))\\) 는 \\(\\theta\\)에 의존하지 않는다. - rs itself와 rs의 order statistics는 SS이다. Factorization thm.: sample point \\(x\\), parameter points \\(\\theta\\) $$ T(X) x, : g , h(x) : f(x ) = g h(x) $$ SS를 찾기 위해 factorization thm.을 쓰려면, 우리는 샘플의 joint pdf를 두 부분으로 나눠야 한다. 이는 \\(\\theta\\)를 포함하지 않는 (의존하지 않는) \\(h(x)\\)와 \\(\\theta\\) 를 포함하는 \\(g \\left[ T(x) \\vert \\theta \\right]\\) 이다. \\(\\theta\\)를 포함하는 \\(g\\) 쪽의 식이 \\(T(x)\\)로 표시될 수 있으면, 즉 \\(x\\)에 의존하는 바가 \\(T(x)\\)를 통해서만 의존한다면, \\(T(x)\\)는 \\(\\theta\\)의 SS이다. proof) \\(X_1 , \\cdots, x_n \\overset {iid} {\\sim} f(x \\vert \\pmb \\theta) = h(x)c(\\pmb \\theta) \\exp \\left( \\sum_{i=1}^k w_i (\\pmb \\theta)t_i(x) \\right)\\), s.t. exponential family, where \\(\\pmb \\theta = (\\theta_1 , \\cdots, \\theta_d), d \\le k\\). then \\[ T(X) = \\left( \\sum_{j=1}^n t_1 (X_j) , \\cdots, \\sum_{j=1}^n t_k (X_j) \\right) \\] is SS for \\(\\theta\\). "],["borel-paradox.html", "3.6 Borel Paradox", " 3.6 Borel Paradox Throughout this chapter, for continuous rv \\(X, Y\\), we have been writing expressions such as \\(E(Y \\rvert X=x)\\) and \\(P(Y \\le y \\rvert X=x)\\). Thus far, we have not gotten into trouble. However, we might have. Formally, the conditioning in a conditional expectation is done with respect to a sub sigma-algebra(1.2.1), and the conditional E $E(Y G) $ is defined as a rv whose integral, over any set in the sub sigma-algebra \\(G\\), agrees with that of \\(X\\). This is quite an advanced concept in probatbility theory (see Billingsley 1995, Section 34). Since the conditional E is only defined in terms of its integral, it may not be unique even if the conditioning is well-defined. However, when we condition on sets of probatbility 0 (such as $ { X=x }$), conditioning may not be well defined, so different conditional expectations are more likely to appear. To see how this could affect us, it is easiest to look at conditional distributions, which amounts to calculating \\(E \\left[ I(Y \\le y) \\rvert X=x \\right]\\). Proschan and Presnell (1998) tell the story of a statistics exam that had the question “If \\(X\\) and \\(Y\\) are independent standard normals, what is the conditional distributions of \\(Y\\) given that \\(Y=X\\)?” Different students interpreted the condition \\(Y=X\\) in the following ways: 1. \\(Z_1 = 0\\), where \\(Z_1 = Y-X\\); 2. \\(Z_2 = 1\\), where \\(Z_2 = Y/X\\); 3. \\(Z_3 = 1\\), where \\(Z_3 = I(Y=X)\\). Each condtion is a correct interpretation of the conditon \\(Y=X\\), and each leads to a different conditional distribution (see Excercise 4.60.). This is the Borel Paradox and arises b/c different (Correct) interpretations of the probatbility 0 conditioning sets result in different conditional E. How can we avoid the paradox? One way is to avoid conditioning on sets probatbility 0. That is, compute only \\(E(Y \\rvert X \\in B )\\), where \\(B\\) is a set with \\(P (X \\in B)&gt;0\\). So to compute something like \\(E(Y \\rvert X =x )\\), take a sequence \\(B_n \\downarrow x\\), and define \\(E(Y \\rvert X =x )= \\lim_{n \\rightarrow \\infty} E(Y \\rvert X \\in B_n )\\). We now avoid the paradox, as the different answers for \\(E(Y \\rvert X =x )\\) will arose from different sequences, so there should be no surprises (Exercise 4.61). "],["neymanpearson-lemma.html", "3.7 Neyman–Pearson lemma", " 3.7 Neyman–Pearson lemma rs \\(X_1 , \\cdots, X_n \\overset {iid}{\\sim} f(x_1 , \\cdots, x_n ; \\theta)\\)이고, $H_0 : =_0, ; ; ; H_1 : =_1 $. 이때 이하를 만족하면 rejection region \\(R\\)은 MP test의 기각역. \\(\\exists k \\ge 0\\): \\(\\pmb x \\in R\\) if \\(f(\\pmb x \\vert \\theta_1) &gt; k f(\\pmb x \\vert \\theta_0)\\). \\(\\pmb x \\in R^c\\) if \\(f(\\pmb x \\vert \\theta_1) &lt; k f(\\pmb x \\vert \\theta_0)\\). \\(\\mathbb{P}_{\\theta_0} \\left( \\pmb X \\in R \\right) = \\alpha\\) for the prefiexed significance level \\(\\alpha\\). Proof: \\[\\begin{align} P(\\pmb X \\in A \\vert \\theta) &amp;= \\int_A L(\\theta ; \\pmb x) d \\pmb x \\\\ &amp;= \\int_A f(\\pmb x ; \\theta) d \\pmb x \\end{align}\\] 이므로, \\(A \\subset C^\\ast\\)라면 \\[\\begin{alignat}{4} \\int_A f(\\pmb x ; \\theta) d \\pmb x &amp;\\le \\int_A &amp;&amp; k \\ast f(\\pmb x ; \\theta) d \\pmb x \\\\ \\\\ P(\\pmb X \\in A \\vert \\theta_0) &amp;\\le &amp;&amp; k \\ast P(\\pmb X \\in A \\vert \\theta_1) \\end{alignat}\\] 마찬가지 방법으로 \\(A \\subset \\left( C^\\ast \\right)^c\\)라면 \\(P(\\pmb X \\in A \\vert \\theta_0) \\ge k \\ast P(\\pmb X \\in A \\vert \\theta_1)\\). \\(C^\\ast\\)의 유의수준이 \\(\\alpha\\)라 하고, 유의수준이 동일한 임의의 RR \\(C\\)를 가정하자. 이때 두 RR은 각각 \\[\\begin{align} C^\\ast &amp;= (C^\\ast \\cap C) \\cup (C^\\ast \\cap C^c) \\\\ C &amp;= (C^\\ast \\cap C) \\cup ({C^\\ast}^c \\cap C) \\end{align}\\] 로 표현할 수 있으며, 두 RR에 대한 power function은 각각 \\[\\begin{alignat}{4} \\pi^\\ast(\\theta) &amp;= P(\\pmb X \\in C^\\ast \\vert \\theta) &amp;&amp;= P(\\pmb X \\in C^\\ast \\cap C \\vert \\theta) &amp;&amp;+ P(\\pmb X \\in C^\\ast \\cap C^c \\vert \\theta) \\\\ \\pi(\\theta) &amp;= P(\\pmb X \\in C \\vert \\theta) &amp;&amp;= P(\\pmb X \\in C^\\ast \\cap C \\vert \\theta) &amp;&amp;+ P(\\pmb X \\in {C^\\ast}^c \\cap C \\vert \\theta) \\\\ \\end{alignat}\\] 이때 \\(H_0\\)에서 두 power의 차이는 \\[\\begin{alignat}{4} \\pi^\\ast(\\theta_1) -\\pi(\\theta_1) &amp;= &amp;&amp; P(\\pmb X \\in C^\\ast \\cap C^c \\vert \\theta_1) - P(\\pmb X \\in {C^\\ast}^c \\cap C \\vert \\theta_1) \\\\ &amp;\\ge \\dfrac{1}{k} &amp;&amp; \\left\\{ P(\\pmb X \\in C^\\ast \\cap C^c \\vert \\theta_0) - P(\\pmb X \\in {C^\\ast}^c \\cap C \\vert \\theta_0) \\right\\} \\\\ &amp;= \\dfrac{1}{k} &amp;&amp; \\left\\{ P(\\pmb X \\in C^\\ast \\cap C^c \\vert \\theta_0) - P(\\pmb X \\in {C^\\ast}^c \\cap C \\vert \\theta_0) \\\\ + P(\\pmb X \\in C^\\ast \\cap C \\vert \\theta_0) - P(\\pmb X \\in C^\\ast \\cap C \\vert \\theta_0) \\right\\} \\\\ &amp;= \\dfrac{1}{k} &amp;&amp; \\left\\{ \\pi^\\ast (\\theta_0) - \\pi (\\theta_0) \\right \\} \\\\ &amp;=0 &amp;&amp; \\end{alignat}\\] 이에 의해 MP test의 정의를 만족한다. 이때 \\(C\\)의 유의수준이 \\(&lt; \\alpha\\)인 경우, \\(\\pi^\\ast (\\theta_1) &gt; \\pi (\\theta_1)\\)이 되므로, \\(C^\\ast\\)의 \\(H_1\\)에서의 power인 \\(\\pi^\\ast(\\theta_1)\\)은 유의수준이 \\(\\le \\alpha\\)인 모든 RR의 power보다 크거나 같음을 알 수 있다. 3.7.1 Overview Example \\(x\\) 1 2 3 4 5 6 \\(f(x \\vert \\theta_0)\\) .01 .02 .02 .05 .10 .80 \\(f(x \\vert \\theta_1)\\) .03 .05 .15 .10 0 .67 $ $ .33 .4 .13 .5 \\(\\infty\\) 1.19 유의수준이란 기본적으로 \\(H_0\\)이 사실인데 \\(H_1\\)을 선택할 확률. 선택한 RR에 해당하는 \\(H_0\\)와 \\(H_1\\)에서의 density가 각각 있다면, \\(H_0\\)에서의 density의 합이 된다. 기각을 해버렸는데 \\(H_0\\)가 발생해버렸다는 소리니까. power란 RR에서의 \\(H_1\\)이 발생할 확률. test 자체가 \\(H_1\\)에 마음을 두고 시작하는 거임. power는 무조건 \\(H_1\\)에만 직결. 실패하면 어쩌지? 무지성으로 \\(H_1\\) 골라버리자. 이랬다가 \\(H_0\\) 발생해버리면? 난 망하는거잖아. 이 망함의 risk를 고정해두자. 이게 \\(\\alpha\\). power function은 \\(H_0\\)와 \\(H_1\\) 각각에 대해서 존재한다. 이는 각각에서의 pdf이다. 즉, 표본을 통한 $ $의 값이 크면 \\(H_0\\)를 기각할 이유가 없고, 작으면 기각할 근거를 갖는다. 이 값이 얼마나 작아야 기각할 수 있는가는 유의수준에 의해 결정. 이와 같이 rs의 LR을 통해 MP test의 RR을 찾을 수 있다. 이때 RR과 검정법은 실제로 동일한 것이므로 혼돈이 없다는 전제 하에 test라는 단어를 주로 사용한다. \\(LR(\\theta_0, \\theta_1 ; \\pmb x) = \\dfrac{L(\\theta_0 ; \\pmb x)} {L(\\theta_1 ; \\pmb x)}\\) 는 표본의 \\(\\theta_0\\)에 대한 지지 (그리고 \\(\\theta_1\\)에 대한 반증)의 정도를 표현한다고 볼 수 있다. 3.7.2 Generalized LRT rs \\(\\pmb X_n \\overset {iid}{\\sim} f(\\pmb x ; \\theta)\\), \\(H_0: \\theta \\in \\Omega_0\\), \\(H_0: \\theta \\in \\Omega_1 (=\\Omega - \\Omega_0)\\). $ (x) = = $ rs \\(X_1, \\cdots, X_n\\)의 pdf가 \\(f(x ; \\theta), \\; \\; \\; \\theta \\in \\Omega\\)라고 하자. 확률구간 \\(\\left[ L(\\pmb X_n ), U(\\pmb X_n ) \\right]\\)가 \\[ P \\left[ L(\\pmb X_n ) \\le \\theta \\le U(\\pmb X_n ) \\right] = 1- \\alpha \\] 를 만족하면 이를 패러미터 \\(\\alpha\\)의 \\(100(1-\\alpha) \\%\\) CI라 부른다. rs \\(\\pmb X_n\\) 의 분포가 pdf \\(f(x ; \\theta), \\; \\; \\; \\theta \\in \\Omega\\)를 따른다 하자. 이때 샘플과 패러미터 \\(\\theta\\)의 함수인 random quantity \\(T(\\pmb X_n ; \\theta)\\)의 분포가 패러미터 \\(\\theta\\)에 의존하지 않으면 이는 pivotal quantity. \\(H_0: \\theta \\in \\Omega_0, H_1: \\theta \\in \\Omega - \\Omega_0\\) 에 대한 RR \\(C^\\ast\\)가 이하를 만족하면 이는 UMP test. \\(\\pi^\\ast\\)가 이 test의 power function이라면 $$ { ^() _0 } =, $$ 모든 다른 power function에 대해 위의 기각역에서의 power 가 최대. rs $X_n $ 의 joint pdf가 \\(f(\\pmb X_n ; \\theta)\\)일 때, \\(LR( \\theta_1 ,\\theta_2 ; \\pmb X_n) = \\dfrac{L(\\theta_1 ; \\pmb X_n)}{L(\\theta_1 ; \\pmb X_n)}\\)가 \\(\\theta_1 &lt; \\theta_2\\)에 대해 \\(T(\\pmb X_n)\\)의 non-dec 혹은 non-inc라면 \\(L(\\theta)\\)는 \\(T(\\pmb X_n)\\)에 대해 monotone LR를 갖는다. Karlin-Rubin \\(H_0: \\theta \\le \\theta_0, H_1: \\theta \\ge \\theta_0\\). \\(T\\)가 \\(\\theta\\)에 대한 SS임을 가정하고, \\(T\\)의 pdf의 family는 MLR을 가짐. then \\(\\forall t_0\\), reject \\(H_0 \\; \\; \\; \\iff \\; \\; \\; T&gt;t_0\\) 하는 test는 level \\(\\alpha\\)의 UMP test이다. 이때, \\(\\alpha = P_{\\theta_0} (T&gt;t_0)\\). \\(L(\\theta ; \\pmb X_n)\\)이 \\(T(\\pmb X_n)\\)에 대해 non-inc인 MLR. 이때 \\(H_0: \\theta \\le \\theta_0, H_1: \\theta \\ge \\theta_0\\)에 대한 level \\(\\alpha\\)의 UMP test는 \\(C = \\left\\{ \\pmb X_n : T(\\pmb X_n) \\ge k \\right\\}\\) 이며, 상수 \\(k\\)는 \\(P[T(\\pmb X_n) \\ge k \\vert H_0 ] = \\alpha\\)에 의해 결정. \\(H_0: \\theta \\ge \\theta_0, H_1: \\theta \\le \\theta_0\\)는 \\(C = \\left\\{ \\pmb X_n : T(\\pmb X_n) \\le k \\right\\}\\). MLE의 불변성 MLE의 함수는 MLE 서로 독립인 rv X Y의 공통된 성공 확률 p의 MLE. f(X)와 f(Y)를 곱해서 쓴다. \\(\\pmb X_n \\sim U(\\theta - \\tfrac{1}{2}, \\theta + \\tfrac{1}{2})\\). 이때 LF로 MLE 구하는 건 굳이 log 안 거쳐도 가능함. 안 거쳐야 증명이 깔끔한 부분이 있음. $$ = f(x;) $$ 에 의해 $$ E { f(X;) }^2 E { f(X;) } =0 $$ \\(X \\sim U(0, \\theta)\\)일 때, \\(\\theta^2\\)의 UE는? \\(E(X^2) = \\dfrac{\\theta^2}{3}\\)이므로 \\(T(X)=3X^2\\)는 \\(\\theta\\)의 UE. \\(\\pmb X_n \\sim U(-\\theta, \\theta)\\)일 때, \\(c(X_{(n)}-X_{(1)}\\)가 \\(\\theta\\)의 UE가 되기 위한 c의 값은? \\(\\pmb X_n \\sim N(\\mu, \\sigma^2 )\\). 이때 \\(cS = c \\sqrt{\\dfrac{\\sum (X_i - \\bar X)^2}{n-1}}\\)가 \\(\\sigma\\)의 UE가 되도록 하는 c의 값은? \\(Y=(n-1)\\dfrac{S^2}{sigma^2}\\)이 카이제곱을 따르는 rv임을 이용하여 \\(E(\\sqrt{Y})\\)를 구하라. \\(Var \\left( \\sum a_i \\hat \\theta_i \\right)\\)는 \\(a_i = \\dfrac{\\tfrac{1}{\\sigma^2_i}}{\\sum \\tfrac{1}{\\sigma^2_i}}\\)일 때 최소화. 통계량 \\(S(X)\\)의 분포가 패러미터 \\(\\theta\\)에 의존하지 않는다면 이는 ancillary statistic. 최소 SS가 존재한다면, 모든 CSS는 MSS이다. "],["개념.html", "3.8 개념", " 3.8 개념 충분통계량 분해정리 Minimum 충분통계량 Completeness 6.3. ancillary 통계량 (분포가 모수에 의존 안함) 바수정리 complete고 minimum 충분통계량이면 모든 ancillary랑 독립 지수족 만족하면 뭐의 묶음은 complete 충분통계량 (추가조건, 6.6 minimum 충분통계량 존재하면 모든 Complete 통계량은 동시에 minimum 충분통계량 모먼트, MLE (2차까지 확인) MLE 불변성 MSE를 통해 통계량 성능 비교 가능함 bias MSE = precision + accuracy UMVUE 7.5 크래머-라오 부등식 : 최저 분산 뽑아내는 수단 피셔 정보 2차원 피셔 정보 라오-블랙웰 : uniform better UE 뽑아내는 수단 unique best UE best UE는 오직 하나뿐 (레만쉐페) CSS에 기반한 UE는 오직 유일함 W가 best UE면 W는 다른 모든 0에 대한 추정자들과 무연관 7.7 consistent (점근성) 충분통계량에 기반한 가설검정은 원본데이터 가설검정과 결과 동일 test으 unbaised 8.8 네이만 피어슨 카를린 루빈 8.3 빅 샘플 추정자들과 8.5 스코어 스탯 8.12 왈드 테스트 8.13 1-a confidence iterval = acceptance region of level 알파 test 뒤집은 테스트의 성질은 컨피던스 인터벌에도 전이됨 pivotal 주어진 X랑 모수로 다른 변량 만들었을 때 이것이 오리지널 모수와 무관한 분포 따름. CLT. MLE는 asymptotic 성질 갖음. MLE를 asymptotic 했을 때 이는 정규분포 따름. 따라서 MLE의 함수는 추축변량. cdf는 출신과 무관하게 U(0,1)을 따르므로 이를 추축변량으로 삼는게 가능. 이떄 자주 쓰이는건 알파/2. 감마와 포아송간 변환 유니모달 cdf가 이하의 조건을 지키면 shortest. 9.5. dog-tired Bubble Plot 3D Scatter Plot Star Plot Chernoff Faces Parallel Coordinate Plot 1.Q-Q Plot Shapiro-Wilks Test Kolmogorov-Smirnov Test Skewness Test ( ) Kurtosis Test: ( ) Lin and Mudholkar Scatter Plot Squared Generalized Distances Chi-Square Plot (Gamma Plot) nqplot contour plot cqplot (Python – assumption check) "],["mcmc.html", "Chapter 4 MCMC ", " Chapter 4 MCMC "],["importance-sampling.html", "4.1 Importance Sampling", " 4.1 Importance Sampling 4.1.1 Independent Monte Carlo 타겟분포 \\(f\\)로부터의 시뮬레이션의 랜덤 draws $ X_1 , , X_n $. 적분 범위에 걸쳐 (over) support가 펼쳐져 있는 분포로부터 무작위로 포인트를 추출해서 해당 포인트들의 적분값을 종합하여 만들어내는, 적분값에 대한 통계적 측정. let \\(f\\)는 \\(X\\)의 density, \\(\\mu = E_f \\left[ h(X) \\right]\\). 이때 $ {MC} = {i=1}^n h(X_i ) h(x)f(X) dx =; ; ; ; ; n $ let $ v(x) = { h(x)-}^2$, $ E_f { ^2 } &lt; $. Then, sampling \\(Var\\) of $ _{MC} $ is $ = E { }. This can be written as $ ({MC}) = {n-1} {i=1}^n ^2 $ when ^2 exists, by CLT, \\(\\hat \\mu_MC \\overset {\\cdot} {\\sim} N\\), for large \\(n\\). 수치해석은 다차원 문제에는 적용하기 어렵다. 하지만, MC integration은 \\(p\\)차원의 \\(f\\)의 support에 걸쳐서 \\(f\\)에서 랜덤하게 샘플링 한 후 이 영역에 대한 그 어떤 체계적인 탐색도 시도하지 않는다. 샘플링 후에는 그냥 냅둬버림. 따라서 MC는 고차원에서도 덜 피곤함. 4.1.1.0.1 Inverse-cdf \\(\\forall F, X=F^{-1}(U) = \\text{inf}\\{ x:F(x) \\ge U \\}\\)는 \\(F\\)와 같은 cdf를 가짐. 이때 \\(F\\)는 continuous distribution function, \\(U \\sim U(0, 1)\\). 이때, linear interpolation을 활용해, \\(F^{-1}\\) 계산 없이 \\(F\\)만으로 난수 샘플링 가능. 1. \\(f\\)의 supoprt를 span하는 grid \\(x_1 , \\cdots, x_m\\) 정의 2. 각 grid point에서 \\(u_i = F(x_i)\\) 계산하거나 approximate 3. 가장 가까운 grid points \\(u_i , u_j\\)에 대해, \\(u_i \\le U \\le u_j\\)에 해당하는 영역을 이하에 따라 linearly interpotate. \\(X = \\dfrac{u_j-U}{u_j - u_i}x_i + \\dfrac{U-u_i}{u_j - u_i}x_j\\). 이때 \\(U \\sim U(0, 1)\\). - illustration of Rejection Sampling for a target distribution \\(f\\) using a Rejection Sampling envelope \\(e\\). 4.1.1.0.2 Rejection Sampling \\(f(x)\\)의 상수배 (proportionality constant) 만이라도 계산될 수 있다면, 정확한 타겟분포 \\(f(x)\\)로부터의 샘플링을 위하여 Rejection Sampling 사용 가능. 이는 더 간단한 후보 (candidate) 분포로부터 샘플링한 후 이렇게 샘플링한 것 중 일부를 확률에 기반하여 랜덤하게 쳐냄으로써 샘플링 확률을 보정하는 것. * \\(g\\)는 우리가 분포의 형태를 정확히 알고 있고 \\(g(x)\\) 계산도 쉬운 덴시티라고 정의. * \\(e\\)는 envelope, 이하의 성질을 갖는다. \\(\\forall x \\; \\; \\; \\text{s.t. for a given constant } \\alpha \\le 1, f(x)&gt;0 \\; \\; \\; : \\; \\; \\; e(x) = \\dfrac {g(x)}{\\alpha} \\ge f(x)\\). 방법은 이하와 같다. 1. \\(Y \\sim g\\)에서 샘플링. 2. \\(U&gt;\\dfrac {f(y)}{e(Y)}\\)일 경우 \\(Y\\)를 기각. 기각된다면 \\(Y\\)값을 target random sample의 요소로 기록하지 않음. step 1으로. 3. \\(U \\le \\dfrac {f(y)}{e(Y)}\\)일 경우 set \\(X=Y\\)로 한 후 \\(X\\)를 타겟 랜덤샘플의 요소로 넣음. step 1으로. 여기서 \\(\\alpha\\)는 채택될 후보들의 expected 비율로 해석될 수 있다. good RS envelope의 요건: * 간단하게 제작되거나, 모든 값에서 타겟분포를 넘김이 간단하게 확인되어야 한다. * 샘플링이 쉬어야 한다. * rejected draws가 적어야 한다. Example: Normal From Double Exponential, Sampling a Bayesian Posterior 4.1.1.0.3 Variants of the RS: Squeeze RS \\(f\\) 계산해내는 게 비용이 많이 들고 RS가 매력적인 상황이면 Squeeze RS에 의해 더 빠른 연산속도를 획득할 수 있음. nonnegative squeezing function \\(s(x)\\)를 정의하고 이를 사용함. 이때 \\(s\\)가 적합한 squeezing function이기 위해선 \\(f\\)의 모든 support에서 \\(s&lt;f\\). * illustration of squeezed Rs for a target distribution \\(f\\), using envelope \\(e\\) and squeezing function \\(s\\). Keep First and Keep Later correspond to steps 3 and 4 of the algorithm, respectively. proceeds: 1. \\(Y \\sim g\\)에서 샘플링. 2. if \\(U \\le \\dfrac {s(Y)}{e(Y)}\\), keep \\(Y\\). 3. if not, whether if \\(U \\le \\dfrac {f(Y)}{e(Y)}\\), keep \\(Y\\). 4. both are not, reject \\(Y\\). 2번에선 \\(s\\), 3번에선 \\(f\\)임에 주목. 샘플링 쉬운 \\(s\\)에서 먼저 비교해서 우선권 시드 주고, 그 후에 \\(f\\)로 본선 해보는거. Example: Lower Bound for Normal Generation 4.1.1.0.3.1 Variants of the RS: Adaptive RS 적절한 envelope \\(e\\)를 어떻게 만들 것인가? squeezed RS를 위해, support의 connected region에 대해 continuous, differentiable, log-concave인 덴시티를 만드는 자동화된 envelope 생산 전략에 해당함. 패키지로 실행. * envelopes \\(e\\) and squeezing function \\(s\\) for adaptive RS. The target density \\(f\\) is smooth, nearly bell-shaped curve. The first method discussed in the text, using the derivative of \\(l\\), produces the envelope \\(e\\) shown as upper boundary of the lighter shaded region. This correponds to Equation (6.9) and Figure 6.4. Later in the text, a derivative-free method is presented. That envelope is the upper bound of the darker shaed region and corresponds to (6.11) and Figure 6.6. The squeezing function \\(s\\) for both approaches is given by the dotted curve. 4.1.1.0.4 Importance Sampling Importance Sampling 접근법은 \\(E\\{h(x)\\}\\) w.r.t. its density는 이하처럼 alternative form으로 쓰일 수 있다는 것에 기반한다. 이때 \\(g\\)는 envelope의 importance sampling function. $ \\[\\begin{align*} \\mu &amp;= \\int h(x)f(x)dx &amp;= \\int \\left( h(x) \\dfrac {f(x)}{g(x)} \\right)g(x)dx \\tag{1} \\\\ \\\\ \\\\ &amp;= \\dfrac {\\int h(x)f(x) dx}{\\int f(x) dx} &amp;= \\dfrac {\\int \\left( h(x) \\dfrac {f(x)}{g(x)} \\right) g(x) dx}{\\int \\left( \\dfrac {f(x)}{g(x)} \\right) g(x) dx} \\tag{2} \\end{align*}\\] $ (1)은 \\(E \\{ h(X) \\}\\)를 측정하기 위한 MC 접근법이 이하임을 제시한다. \\(X_1 , \\cdots, X_n \\overset {\\text{iid}}{\\sim} g\\)처럼 \\(g\\)에서 랜덤샘플을 뽑고, 이의 (이를 활용한) estimator는 이하. 이때 \\(w^{\\ast} (X_i)\\)는 unstandardized weights, i.e., importance ratios. $ {IS}^{} = {i=1}^n h(X_i) w^{}(X_i) = _{i=1}^n h(X_i) $ (2)는 \\(g\\)에서 \\(X_1 , \\cdots, X_n \\overset {\\text{iid}}{\\sim} g\\)의 랜덤샘플을 뽑고 이하를 계산. 이때 \\(w(X_i)\\)는 standardized weight. 이 (2)는 \\(f\\)의 상수배 (proportionality constant) 까지만 알 수 있더라도 적용할 수 있다는 점에서 매우 중요함. \\(f\\)의 상수배까지만 알 수 있는 상황은 베이지안 분석의 post에서 빈번하게 발생함. Both estimators converge by the same argument applied to the simple Monte Carlo estimator. $ {IS} = {i=1}^n h(X_i) w(X_i) = _{i=1}^n h(X_i) $ Proceeds: 1. Sample \\(X_j \\sim g(\\cdot)\\). 2. Calculate \\(w(X_j) = \\dfrac {f(X_j)}{g(X_j)}\\) 3. 지정 샘플 갯수까지 반복 then, $ \\[\\begin{align*} E\\{\\hat h(x)\\} &amp;= \\dfrac {1}{n} \\sum_{j=1}^n w(X_j)h(X_j) \\\\ \\hat \\sigma^2 &amp;= \\dfrac {1}{n-1} \\sum_{j=1}^n \\left\\{ h(X_j) - E\\left[ \\hat h(x) \\right] \\right\\}^2 \\end{align*}\\] $ 과도한 변동성을 회피하기 위해, \\(\\dfrac {f(x)} {g(x)}\\)는 bounded여야 하며 또한 \\(g\\)는 \\(f\\)보다 heavier tail을 가져야 한다. 이것이 만족되지 않는다면 standardized importance weight는 제법 커질 수 있음. 함수 \\(g\\)는, \\(h(x)\\)가 매우매우 작을 경우에만 \\(\\dfrac {f(x)} {g(x)}\\)가 커지게 만드는 녀석으로 잘 골라야 한다. 가령 \\(h\\)가 아주 드문 상황에서만 1을 반환하는 indicator function이라면, 우리는 \\(g\\)로 하여금 샘플링의 편의성을 위해 해당 사건을 좀 더 빈번하게 발생시키도록 하는 녀석을 고를 수도 있을 것이다. 이를 택한다면 우리는 우리의 관심사가 아닌 사건, 가령 \\(h(x)=0\\)에 대한 적절한 샘플링 power을 어느정도 희생하게 된다. 이는 낮은 확률에 해당하는 case의 측정에 특히 잘 들어맞는 방법론이다. $_{IS}^$ 자체는 unbiased지만, 이를 importance weight로 standardize 하는 과정에서 \\(\\hat \\mu_{IS}\\)에 다소 bias가 생겨버린다. standardized weight를 쓰는 건 \\(w^\\ast(X)\\)와 \\(h(X)w^\\ast(X)\\)가 서로 강하게 상관관계가 있는 상황에서 더욱 우수한 estimator를 반환한다. standardized weight는 \\(f\\)의 비례상수를 요구하지 않는다. (우리가 갖고 있는 덴시티가 \\(f\\)의 얼마만큼의 상수배인지를 알지 않아도 된다) IS 방법론의 매력은 시뮬레이션의 reusability이다. 같은 sample points들과 weight들이 다양한 다른 quantity의 MC 적분 estimates를 구하는데 사용될 수 있다. (컴퓨팅 파워가 증가한 오늘날에 와서는 유의미한 장점은 아니다.) Example: Small Tail Probabilities 4.1.1.0.5 Antithetic Sampling let \\(\\hat \\mu_1, \\hat \\mu_2\\). 이 둘은 identically distributed, UE, and \\(Corr(\\hat \\mu_1, \\hat \\mu_2)&lt;0\\). 이 estimator 둘을 평균한 \\(\\hat \\mu_{AS} = \\dfrac{\\hat \\mu_1 + \\hat \\mu_2}{2}\\)는 각 estimator들의 샘플을 2배 한 것보다 우월함. \\(Corr(\\hat \\mu_1, \\hat \\mu_2)&lt;0\\)이기 때문에 성립한다는 것을 유의. $ Var(_{AS}) = ( Var(_1) + Var(_2) ) + Cov(_1, _2) = {n} (1+) $ \\(\\hat \\mu_1 (X)\\)를 MC integral estimate로 잡는다면, 이는 $ 1 (X) = {i=1}^n h_1 { F_1^{-1}(U_{i1}), , F_m^{-1}(U_{im}) } $ 이때 \\(h_1\\)은 그의 arguments에 monotone이며, \\(F_j\\)는 각 \\(X_{ij}, \\; j=1,\\cdots,m\\)의 cdf이며 \\(U_{ij} \\sim U(0,1)\\). 이에 의해 \\(1-U_{ij} \\sim U(0,1)\\)이기도 하며, 이에 의해 이하도 성립. $ 2 (X) = {i=1}^n h_1 { F_1^{-1}(1-U_{i1}), , F_m^{-1}(1-U_{im}) } $ 이는 \\(\\mu\\)의 2번째 estimator이며, 이는 \\(\\hat \\mu_1 (X)\\)와 같은 분포를 가짐. 따라서 \\(\\hat \\mu_{AS} = \\dfrac{\\hat \\mu_1 + \\hat \\mu_2}{2}\\)는 \\(\\hat \\mu_1\\)의 샘플을 2배 한 것(2n)보다 더 작은 \\(Var\\)을 가지며, 따라서 더 우월함. 4.1.1.0.6 Control Variates 우리는 알지 못하는 quantity \\(\\mu = E \\{ h(X) \\}\\)를 알고자 하며, 이에 연관된 quantity \\(\\theta = E[c(Y)]\\)에 대해서는 알고 있음. 후자는 수치적으로 획득 가능. \\((X_1 , Y_1 ) ,\\cdots, (X_n , Y_n )\\)은 simulation outcom에서 독립적으로 관측된 pairs of rv. 이때 MC estimator는 이하와 같다. \\(\\hat \\mu_{MC}, \\hat \\theta_{MC}\\) 간에 상호연관이 있음을 유의. $ \\[\\begin{align*} \\hat \\mu_{MC} = \\dfrac {1}{n} \\sum_{i=1}^n h(X_i), &amp; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\hat \\theta_{MC} = \\dfrac {1}{n} \\sum_{i=1}^n c(Y_i) \\end{align*}\\] $ 즉 우리는 여기서 \\(\\mu = E[h(x)]\\) \\(\\theta = E[c(Y)]\\) MC (ex. \\(\\theta_{MC}\\)) able able itself able 즉 \\(\\theta\\)와 \\(\\theta_{MC}\\) 간의 차이를 알아내고, 이 차이를 적당히 스케일링해서 \\(\\mu\\)에 적용한다는 것이 기본 메커니즘. 여기서 Control Variate Estimator는 \\(\\hat \\mu_{CV} = \\hat \\mu_{MC} + \\lambda(\\hat \\theta_{MC} - \\theta)\\). \\(\\lambda\\)는 사용자에 의해 정해지는 임의의 parameter. 이에 의해 $ Var({CV} ) = Var ({MC}) + ^2 Var ({MC}) + 2 Cov({MC}, _{MC}) $ 이며 이가 최소가 된 경우의 분산은 아래와 같으며, 이를 최소로 하는 \\(\\lambda\\)는 아래와 같다. when \\(\\lambda = - \\dfrac {Cov(\\hat \\mu_{MC}, \\hat \\theta_{MC})}{Var(\\hat \\theta_{MC})}\\), \\(\\min_\\lambda \\left( Var(\\hat \\mu_{CV} ) \\right) = Var(\\hat \\mu_{MC}) - \\dfrac{\\left[ Cov(\\hat \\mu_{MC}, \\hat \\theta_{MC}) \\right]^2} {Var(\\hat \\theta_{MC})}\\). 4.1.1.0.7 Rao-Blackwellizaiton rs \\(X_1 , \\cdots, X_n \\overset {\\text{iid}}{\\sim} f\\)를 활용해 \\(\\mu = E \\{ h(X) \\}\\)를 estimation. 각각의 \\(X_i = (X_{i1}, X_{i2})\\)라고 가정하고, 조건부 기댓값 \\(E\\{ h(X_i) \\rvert x_{i2} \\}\\)가 수치적으로 풀릴 수 있다고 가정하자. $ E{h(X_i)} = E_{X_{i2}}{ E }$라는 사실을 활용하여 $ _{MC} $ 에 대한 다른 estimator를 구축해보자. Rao-Blackwellized estimator \\(\\hat \\mu_{RB} = \\dfrac 1 n \\sum_{i=1}^n E \\{ h(X_i) \\rvert X_{i2} \\}\\). 이는 ordinary MC estimator \\(\\hat \\mu_{MC}\\)와 같은 mean을 갖는다. Note that $ Var({MC}) = {n^2} Var { E} + {n^2} E { Var } Var ({RB}) $ 따라서 Mean Squared Error, MSE 관점에서 \\(\\hat \\mu_{RB}\\)는 \\(\\hat \\mu_{MC}\\) 보다 우수하다. 4.1.1.0.8 Sampling Importance Resampling SIR 알고리즘은 몇 타겟분포에서 실현값을 모사적으로 시뮬레이트한다. SIR은 Importance Sampling의 개념에 기초하고 있다. IS에서 우리는 IS function \\(g\\)에서 샘플링하는 식으로 진행했었다. 샘플의 각 point는 샘플링 확률을 보정 (correct)하기 위해 weighted 되었었으며, 이에 의해 weighted 샘플들은 타겟분포 \\(f\\)와 연결지어질 수 있었다. 타겟분포 \\(f\\)를 획득하기 위해 샘플링 확률 보정 목적으로 가해지는 weight는 standardized importance weight \\(w(x_i)\\)로 불렸으며, $ w(x_i) = 이 만으로 난수 샘플링 가능. {} {_{i=1}^m } $ 이렇게 획득했던 standardized weight는 이후에 출신 density가 아닌 다른 타겟 \\(f\\)에서 다른 샘플을 생산할 때 재사용되는 것이 가능하다. for a collection of values, \\(x_i , \\cdots, x_m \\overset {\\text{iid}} {\\sim} g\\), 이때 \\(g\\)는 Importance Sampling Function. proceeds: 1. sample candidates \\(Y_1 , \\cdots, Y_m \\overset {\\text{iid}} {\\sim}\\) 타겟분포 \\(g\\). \\(g\\)가 타겟분포라고?????? 수업발언 2. caculate the standardized importance weights, \\(w(Y_1) , \\cdots, w(Y_m)\\). 3. resample \\(X_1 , \\cdots, X_m\\) from \\(Y_1 , \\cdots, Y_m\\) with probabilities, \\(w(Y_1) , \\cdots, w(Y_m)\\). for \\(n\\) samples Rejection Sampling SIR perfect not perfect distribution of generation draw is exactly \\(f\\) random degree of approxiamtion to \\(f\\) required number of draws random determined It is important to consider the relative sizes of the initial sample and the resample. In principle, we require \\(\\dfrac n m \\rightarrow 0\\) for distributional convergence of the sample. 1만개를 생산해놓고 이 안에서 추가적으로 공정을 진행해서 목표했던 랜덤한 샘플을 뽑아내는 것이 SIR. 그러나 전 영역에서 체크하는것과 생산해놓은 1만개에 randomness를 첨가하여 만들어낸 샘플은 퍼포먼스 차이가 당연히 존재. 그러나 전 영역 대비 1만개라는 한정된 영역에서 추가공정을 진행하므로 cost down. 기존에 만들어두었던 weight를 재사용하므로 시뮬레이션을 다시 할 필요가 없음. 시간 down. Rejection Sampling | envelope \\(e\\)를 만들고 이 안에서 뽑음. 이는 continuous point. | perfect, exact | SIR | n개의 candidate point를 이미 선택해놓고 이 안에서 뽑음. discrete. | approximate sampling | candidate \\(m\\)개, 샘플 \\(n\\)개. 당연하지만 candidate \\(m\\)의 숫자가 커질수록 효율성 (approximate 성능) 은 높아짐. The maximum tolerable ratio \\(\\dfrac n m\\) depends on the quality of the envelope, bsed on \\(m\\) candidate samples and their weights. 이상적으로는 \\(m\\)이 무한해지면 SIR 조차도 exact sampling일 수 있다. The SIR algorithm can be sensitive to the choice of \\(g\\). * The support of \\(g\\) must include the entire support of \\(f\\), for a reweighted sample from \\(g\\) is to approximate a sample from \\(f\\). * \\(g\\) should have heavier tails than \\(f\\), or more generally \\(g\\) should be chosen to ensure that $ $ never grows to o large. * If \\(g(x)\\) is nearly zero anywhere where \\(g(x)\\) is positive, then a draw from this region will happen only extremely rarely, but when it does it will receive a huge weight. * weight-degeneracy problem Example: Slash Distribution Example: Sampling a Bayesian Posterior 4.1.1.0.9 Sequential Monte Carlo When the target density \\(f\\) becomes high dimensional, SIR is increasingly inefficient and can be difficult to implement. Specifying a very good high-dimensional envelope that closely approximates the target with sufficiently heavy tails but little waste can be challenging. Sequential Monte Carlo methods address the problem by splitting the high-dimensional task into a sequence of simpler steps, each of which updates the previous one. \\(\\pmb X_{1:t} = (X_1 , \\cdots, X_t )\\) represents a discrete time stochastic process with \\(X_t\\) being the observation at time \\(t\\). \\(\\pmb X_{1:t}\\) represents the entire history of the sequence. Suppose the density of \\(\\pmb X_{1:t}\\) is \\(f_t\\) and we wish to estimate the expected value of \\(h(\\)X_{1:t}\\()\\) w.r.t. \\(f_t\\). A direct application of the SIR approach would be to draw a sample \\(\\pmb x_{1:t}\\) sequences from an envelope gt and then calculate the importance weighted average of this sample of \\(h(\\pmb X_{1:t})\\) values. This SIR approach overlooks a key aspect of the problem. * As t increases, \\(\\pmb X_{1:t}\\) and the expected value of \\(h(\\pmb x_{1:t})\\) evlove. * At time \\(t\\) it would be better to update previous inferences than to act as if we had no previous information. Inefficient !!! Need to develop a strategy that will simulate \\(X_t\\) from previously simulated \\(\\pmb X_{1:t-1}\\) and adjust the previous importance weights in order to estimate the expected value of \\(h(\\pmb X_{1:t})\\) . Sequential Importance Sampling. 4.1.1.0.10 SIS for Markov Process Simplify assumption that \\(\\pmb X_{1:t}\\) is a Markov process. The target density \\(f_t (\\pmb x_{1:t})\\) may be expressed as $ \\[\\begin{align*} f_t (\\pmb x_{1:t}) &amp;= f_1 (x_1) &amp;\\ast f_2 (x_2 \\rvert \\pmb x_{1:1}) &amp;\\ast f_3 (x_3 \\rvert \\pmb x_{1:2}) &amp;\\cdots &amp;\\ast f_t (x_t \\rvert \\pmb x_{1:t-1}) \\\\ &amp;= f_1 (x_1) &amp;\\ast f_2 (x_2 \\rvert x_1) &amp;\\ast f_3 (x_3 \\rvert x_2) &amp;\\cdots &amp;\\ast f_t (x_t \\rvert x_{t-1}) \\end{align*}\\] $ Suppose that we adopt the same Markov form for the envelope, namely $ g_t (x_{1:t})= g_1 (x_1) g_2 (x_2 x_1) g_3 (x_3 x_2) g_t (x_t x_{t-1}) $ Sample from \\(g_t (\\pmb x_{1:t})\\) and reweight each \\(\\pmb x_{1:t}\\) value by \\(w_t = \\dfrac {f_t (\\pmb x_{1:t})}{g_t (\\pmb x_{1:t})}\\). The weight at time \\(t\\) is \\(w_t = \\dfrac {f_1 (x_1) \\ast f_2 (x_2 \\rvert x_1) \\ast \\cdots} {g_1 (x_1) \\ast g_2 (x_2 \\rvert x_1) \\ast \\cdots}\\). A sample of \\(n\\) such points and their weights can be used to approximate \\(f_t (\\pmb x_{1:t} )\\) and calculate the expected value of \\(h(\\pmb x_{1:t} )\\). The sequential Monte Carlo algorithm for generating one sample is 1. Sample $X_1 g_1 $. Let \\(w_1 = u_1 = \\dfrac {f_1(x_1)}{g_1(x_1)}\\). Set \\(t = 2\\). 2. Sample \\(X_t \\rvert x_{t-1} \\sim g_t (x_t \\rvert x_{t-1})\\). 3. Append \\(x_t\\) to \\(\\pmb x_{1:t-1}\\), obtaining \\(\\pmb x_{1:t}\\). 4. \\(u_t = \\dfrac{f_t (x_t \\rvert x_{t-1})}{g_t (x_t \\rvert x_{t-1})}\\). 5. let \\(w_t = w_{t-1}u_t\\). \\(w_t\\) is the importance weight for \\(\\pmb x_{1:t}\\) . 6. Increment t and return to step 2. The weighted average \\(\\sum_{i=1}^n \\left( \\dfrac {w_t^{(i)}}{\\sum_{i=1}^n w_t^{(i)}} \\right) \\ast h(\\pmb X_{1:t}^{(i)})\\) serves as the estimate of \\(E_{f_T} h(\\pmb X_{1:t})\\). 4.1.1.0.11 Generalized Sequential Importance Sampling Assume that \\(\\pmb X_{1:t}\\) is not a Markov process. target density \\(f_t (\\pmb x_{1:t})\\) and envelope \\(g_t (\\pmb x_{1:t})\\) may be expressed as $ \\[\\begin{align*} f_t (\\pmb x_{1:t}) &amp;= f_1 (x_1) \\ast f_2 (x_2 \\rvert \\pmb x_{1:1}) \\ast f_3 (x_3 \\rvert \\pmb x_{1:2}) &amp;\\cdots &amp;\\ast f_t (x_t \\rvert \\pmb x_{1:t-1}) \\\\ g_t (\\pmb x_{1:t}) &amp;= g_1 (x_1) \\ast g_2 (x_2 \\rvert \\pmb x_{1:1}) \\ast g_3 (x_3 \\rvert \\pmb x_{1:2}) &amp;\\cdots &amp;\\ast g_t (x_t \\rvert \\pmb x_{1:t-1}) \\end{align*}\\] $ the importance weight at time \\(t\\) is $ w_t (x_{1:t}) = {g_1 (x_1) g_2 (x_2 x_{1:1}) g_3 (x_3 x_{1:2}) g_t (x_t x_{1:t-1})} $ and the recursive updating expression for the importance weights is \\(w_t(\\pmb x_{1:t}) = w_t(\\pmb x_{1:t}) \\dfrac {f_t (x_t \\rvert \\pmb x_{1:t-1})}{g_t (x_t \\rvert \\pmb x_{1:t-1})}, \\; \\; \\; \\; \\; \\; \\; \\; \\text{for }t&gt;1\\) "],["markov-chain-monte-carlo.html", "4.2 Markov Chain Monte Carlo", " 4.2 Markov Chain Monte Carlo \\(f\\) 가 측정은 되는데 샘플화가 안되면, MC를 통해 유사한 샘플을 만들어낼 수 있었다. 이를 넘어서 MCMC는 $ f $ 의 모사함수에서 샘플링하는 게 가능하지만, 이 이상으로 이는 임의의 함수 \\(p\\)에 대해 \\(E[p(X)]\\)가 신뢰도 높게 측정되는 경우에만 샘플링 가능한 별개의 방법론으로 보는 게 정확하다. MC MCMC numerical integration approach iterative nature can be customized to very diverse &amp; difficult problem 무관하며 implementation이 complex하지도 않음 문제가 고차원이면 수렴이 느려짐 시퀀스 \\(\\{\\textbf X^{(t)}\\}\\)는 MC, \\(t = 0, 1, 2, ….\\). \\(\\textbf X^{(t)} = (X_1^{t} , \\cdots, X_p^{(t)})\\) 와 state space 는 양쪽 모두 연속이거나 discrete. For the types of Markov chains, \\(\\{ \\textbf X^{(t)} \\}\\)의 분포는 체인의 limiting stationary distribution으로 수렴한다. 체인이 irreducible, aperiodic 할 때. MCMC의 샘플링 전략은 irreducible, aperiodic MC를 만드는 것. stationary distribution이 목표분포 \\(f\\) 와 일치하는. t가 충분히 크다면 이 체인으로부터의 \\(\\textbf X^{(t)}\\)의 실현값은 근사적으로 마지널 분포 \\(f\\) 를 갖는다. 이런 MCMC의 특성은 베이지안 추론에 크게 도움이 되며 자주 쓰인다. Markov Chain 자체는 어떤 상태에서 다른 상태로 넘어갈 때, 바로 전 단계의 상태에만 영향을 받는 (Markov Property) 확률 과정을 의미한다. 보통 사람들은 전날 먹은 식사와 유사한 식사를 하지 않으려는 경향이 있다. 가령, 어제 짜장면을 먹었다면 오늘은 면종류의 음식을 먹으려고 하지 않는다. 4.2.1 MH Algorithm MCMC 중 가장 유명한 적용법은 MH 알고리즘. \\(t=0\\)에서 시작. 시작 distribution \\(g\\)에서 추출한, \\(f(\\textbf x^{(0)} )&gt; 0\\) 을 만족하는 \\(\\textbf x^{(0)}\\)를 \\(\\textbf X^{(0)} = \\textbf x^{(0)}\\) 로 잡고 개시한다. 이때 제안분포 \\(g\\) 에서 후보 \\(\\textbf X^{( \\ast )}\\) 를 하나 만들고, MH ratio \\(R (\\textbf {x}^{(t)}, \\textbf X^{\\ast} )\\) 는 \\[ R (\\textbf {x}^{(t)}, \\textbf X^{\\ast} ) = \\dfrac {f(\\textbf X^{( \\ast )}) g(\\textbf x^{( t )} | \\textbf X^{( \\ast )})} {f(\\textbf x^{( t )}) g(\\textbf X^{( \\ast )} | \\textbf x^{( t )}) } =\\dfrac {\\dfrac {f(\\textbf X^{( \\ast )})} {g(\\textbf X^{( \\ast )} | \\textbf x^{( t )})} } {\\dfrac {f(\\textbf x^{( t )})} {g(\\textbf x^{( t )} | \\textbf X^{( \\ast )})} } =\\dfrac {\\dfrac {f(\\textbf X^{( t+1 )})} {g(\\textbf X^{( t+1 )} | \\textbf x^{( t )})} } {\\dfrac {f(\\textbf x^{( t )})} {g(\\textbf x^{( t )} | \\textbf X^{( t+1 )})} } \\] warning 여기서 단순 Metropolis 알고리즘은 단순히 $ {f(x_0)}$ &gt; $1 $ 이기만 하면 새로운 샘플을 수용한다. 이인즉 \\(g\\)로 표준화해주는 것의 가장 주요한 요점은 둘의 시작 높이, 즉 쥐고 나온 수저가 다를 수 있으므로 이를 표준화해준다는 것이다. 아웃풋이 높은 \\(x_i\\)를 선택하는 것은 MLE 관점에 기반한다. 단 언제나 그렇듯 이렇게 샘플을 다쳐내면 오히려 음질의 결과가 나온다. 따라서 샘플의 풀을 넓히기 위해 탈락할 녀석들도 확률적으로 살려서 합류시킨다. 이게 고정된 기준점으로 샘플을 쳐내는 것이 아닌, \\(\\dfrac {f(x_1)} {f(x_0)}\\) &gt; \\(u \\sim U {(0,1)}\\) 을 기준으로 삼아 샘플을 생존시키는 것이다. 이 조건을 실패하면 생산해두었던 샘플 \\(\\textbf X^{(t)}\\) 는 버려지고 새로운 샘플을 \\(t+1\\)으로 설정해 재진행한다. 이후 \\[ \\textbf {X}^{(t+1)} = \\left\\{\\begin{array}{@{}lr@{}} \\textbf {X}^{\\ast}, &amp; \\text{with probability } min \\left\\{ R \\left( \\textbf {x}^{(t+1)}, \\textbf {X}^{\\ast} \\right), 1 \\right\\} \\\\ \\textbf {x}^{(t)}, &amp; o.w. \\end{array}\\right\\} \\] 이러한 MH 알고리즘에 의해 생성된 MC가 aperiodic &amp; irreducible 이라면, 해당 체인은 정적분포로 수렴. 우리는 이러한 MH 체인에 의해 생성된 정적분포의 실현값들을 평균함으로써 rv의 함수의 기댓값을 구할 수 있다. \\(E \\left[ h \\left( \\textbf {X} \\right) \\right] \\approx \\dfrac {1} {n} \\sum_{i=1}^n {h \\left( \\textbf {x}^{(i)} \\right)},\\) \\(E { \\left\\{ h \\left( \\textbf {X} \\right) - E \\left[ h \\left( \\textbf {X} \\right) \\right] \\right\\} }^2\\), \\(E \\{ I_{h ( \\textbf {X} \\le q )} \\}\\) 시퀀스 \\(\\{ \\textbf x^{(\\inf)} \\}\\) 는 state space의 몇몇 포인트들의 multiple copies를 가질 수 있다는 것을 명심. 이는 \\(\\textbf {X}^{(t+1)}\\)가 제안값 \\(\\textbf {x}^{(\\ast )}\\)가 아니라 \\(\\textbf {x}^{(t)}\\)를 채택했을 때 발생. Burn-in Period: 체인의 초기값에 대한 persistent dependence는 이의 성능을 심각하게 낮출 수 있다. 이는 샘플 평균을 계산할 때 체인의 초기 실현값 일부를 제하는 것으로 보정될 수 있다. consistent 결과들을 관측하기 위해 MCMC를 여러 시작점에서 각각 돌려본다. 잘 골라진 제안분포 $ g $가 생산하는 후보값들은 stationary 분포의 서포트를 합리적인 반복 안에서 전부 커버하고, 내놓는 후보값들이 지나치게 여러번 accepted되거나 rejected 되지 않는다. proposal 분포 $ g $ 가 지나치게 퍼져있으면, 후보값들은 자주 reject되고 체인은 타겟분포 $ f $ 의 space를 탐색하기 위해 많은 반복을 요구하게 된다. proposal 분포 $ g $ 가 지나치게 모여있으면, 체인은 다회의 반복동안 타겟분포 $ f $의 한 작은 구역에 모여있게 된다. 따라서 타겟분포의 다른 영역은 적절하게 탐색되지 못한다. 4.2.1.1 Independent Chains acceptance 여부 결정시에 ratio 자체는 MH ratio를 사용한다. 이 MC ratio에는 과거 실현값(\\(x^{(t)}\\))이 들어있다. 따라서 이는 MCMC 방법론에 해당한다. 하지만 새로운 value \\(g(x&#39;)\\)을 생산할 때, 이 자체는 \\(g(x&#39;\\rvert x^{(t)})=g(x&#39; \\rvert \\cdot )\\)을 따르게, 즉 \\(g(x&#39;\\rvert x^{(t)})=g(x&#39; )\\) 마냥 과거의 실현값에 dependent 하지 않게 새로운 값을 생산해내는 방법론. 즉 새로이 제시되는 candidate value가 과거의 실현값들과 independent 하므로 명칭이 저러한 것이다. 즉 MH ratio 자체는 과거의 샘플을 이용해서 MCMC 범주에 들어가나 샘플 자체는 과거의 샘플과 independent하게 생산. MH 알고리즘의 제안분포는 고정된 덴시티 \\(g\\)에 대해서 $ g ( ^{} ^{(t)} )$ 따위로 생성. 이는 independent chain 이라고 불리며, 이에 사용되는 각 후보값들은 과거에서 독립적으로 추출되었다. MH ratio는 \\[ R (\\textbf {x}^{(t)}, \\textbf X^{\\ast} ) = \\dfrac {f(\\textbf X^{( \\ast )}) g(\\textbf x^{( t )} | \\textbf X^{( \\ast )})} {f(\\textbf x^{( t )}) g(\\textbf X^{( \\ast )} | \\textbf x^{( t )}) } = \\dfrac {f(\\textbf X^{( \\ast )}) g(\\textbf x^{( t )})} {f(\\textbf x^{( t )}) g(\\textbf X^{( \\ast )}) } =\\dfrac {\\dfrac {f(\\textbf X^{( \\ast )})} {g(\\textbf x^{( t )})} } {\\dfrac {f(\\textbf x^{( t )})} {g(\\textbf X^{( \\ast )})} } =\\dfrac {\\dfrac {f(\\textbf X^{( \\ast )})} {g(\\textbf X^{( \\ast )})} } {\\dfrac {f(\\textbf x^{( t )})} {g(\\textbf x^{( t )})} } = \\dfrac {w^{\\ast}} {w^{(t)}} \\tag{1} \\] importance ratio of \\(X&#39;\\), importance ratio of \\(X^{(t)}\\). This can be seen as weight also. 이때 가장 우측의 등식은 importance ratio (\\(r\\))의 등식으로 재표현된 것이며, 이때 \\(f\\) 는 타겟분포, \\(g\\) 는 그것의 envelope로 본 것이다. 4.2.1.1.1 Example: Bayesian Inference, Mixture Distribution MCMC 알고리즘은 \\(p(\\theta \\rvert y ) = c \\cdot p(\\theta) L(\\theta \\rvert y)\\) 로 표현될 수 있는 베이지안 추론에서 특히 강력하다. 베이지안 추론에서 \\(c\\)의 계산이 드럽게 어렵다는 것이 이것 이상의 다른 추론전략을 방해하기 때문이다. 보유하는 stationary 분포가 타겟 post인 MCMC에서 생산된 샘플들은 post 모먼트, tail 확률, 그리고 다른 유용한 quantity 계산에 쓰일 수 있다. independent 체인에서 prior를 proposal 분포(\\(g\\))로 쓰자. 즉 \\(f\\)가 post, \\(g\\)가 prior다. 이런다면 \\[ R ({\\theta}^{(t)}, \\theta^{\\ast} ) =\\dfrac {\\dfrac {f(\\theta^{( \\ast )})} {g(\\theta^{( \\ast )})} } {\\dfrac {f(\\theta^{( t )})} {g(\\theta^{( t )})} } =\\dfrac {c \\; \\cdot \\; \\pi( \\theta^{\\ast} ) L( \\theta^{\\ast} \\rvert y )} {c \\; \\cdot \\; \\pi( \\theta^{(t)} ) L( \\theta^{(t)} \\rvert y )} \\cdot \\dfrac { \\pi (\\theta^{(t)} )}{ \\pi (\\theta^{\\ast})} =\\dfrac {\\dfrac {\\pi (\\theta^{\\ast} \\rvert y)} {\\pi (\\theta^{\\ast})} } {\\dfrac {\\pi (\\theta^(t) \\rvert y)} {\\pi (\\theta^{(t)})} } = \\dfrac {L \\left( \\theta^{\\ast} \\rvert y \\right)} {L \\left( \\theta^{(t)} \\rvert y \\right)} \\] Mixing Properties: Good Mixing: 첫번째 그림의 MC는 시작점에서 빠르게 멀어지며 \\(\\delta\\)에 대한 post에서의 모든 서포트에 해당하는 패러미터 space의 모든 부분을 훑으면서 샘플을 뽑아내는게 쉬워보인다. Bad Mixing: 두번째 그림은 starting value에서 멀어지는 것도 느리고, posterior support의 영역을 탐색하는 게 시원찮아보인다. Burn-in 이후의 실현값들을 히스토그램으로 만들어서 살펴보면 \\(BETA(1,1)\\) proposal 덴시티를 쓴 MCMC만이 \\(\\delta\\)의 참값을 잘 모사하는듯. 4.2.2 Random Walk Chains (Most Widely Used) MH method 에 해당. let \\(\\textbf {X}^{\\ast} = \\textbf {X}^{(t)} + \\epsilon , \\epsilon \\sim h(\\epsilon )\\). 이때 \\(h\\)는 임의의 덴시티. \\(h\\) 로 자주 선택되는건 \\(U, \\textbf{N}, \\text {Student&#39;s } t\\). 이러면 proposal 덴시티 \\(g\\)는 어떻게 되는가? \\(x^{&#39;} g \\sim N( \\cdot \\rvert x^{(t)}, \\sigma^2 )\\). 가장 빈번하게 쓰이는게 \\(N\\)이므로 \\(N\\)으로 설명. 여기서 \\(x^{(t)}\\)는 평균으로 사용되었고, \\(\\sigma^2\\)은 Jumping Rule에 해당한다. proposal can be too diffused: Jumping rule is too big too focused: Jumping rule is too small Random Walk generate \\(x^{&#39;} g \\sim N( \\cdot \\rvert x^{(t)}, \\sigma^2 )\\) \\(u \\sim U(0,1)\\). calculate MH ratio: \\(r = \\dfrac {f(x&#39;)}{f(x^{(t)}} \\dfrac {g(x&#39; \\rightarrow x^{(t)}} {g(x^{(t)} \\rightarrow x&#39;} = \\dfrac {f(x&#39;)}{f(x^{(t)}}\\), cause it’s \\(N\\). if \\(u&lt;r\\), \\(x^{(t+1)} = x&#39;\\). o.w., \\(x^{(t+1)} = x^(t)\\). 4.2.2.1 Example: Mixture Distribution let proposal \\(\\delta^{(t+1)} = \\delta^{(t)} + U(-a, a)\\)1. 이인즉 몇몇 proposal들은 \\([0, 1]\\) 이외에서 생산된다. note that \\(\\forall \\delta \\notin [0,1]\\), post is zero, Reparameterize the problem by letting \\(U = \\log{\\dfrac {\\delta}{1-\\delta}}\\)(logit). 왜? Probabilty Space is bounded: \\(0 \\le P(\\cdot) \\le 1\\). Run a random walk chain on \\(U\\) by adding \\(U(-b,b)\\). Two ways of Reparamaterization: Run MCMC in \\(\\delta\\)-space. \\(u\\) 값을 다시 T해와서 \\(\\delta\\)-space에서 돌림. Run MCMC in \\(u\\)-space. \\(u\\)-space에서 돌리고 마지막에 모델 샘플들을 전부 \\(\\delta\\)-space로 환원. 4.2.2.1.1 \\(\\delta\\)-space에서 돌리는 방법 개략적으로는 \\(T^{-1}: \\delta&#39; \\leftarrow u&#39; \\sim g( \\cdot \\rvert u^{(t)})\\) 와 같은 형을 띤다. 조건부 proposal \\(g\\)에서 생산된 u를 하나하나마다 \\(\\delta\\)로 역변환해서 그 하나하나의 역변환 값으로 MH 알고리즘을 돌린다. proposal 덴시티 \\(g( \\cdot \\rvert u^{(t)} )\\) 는 \\(\\delta\\)-space에서의 proposal 덴시티로 변환되어야 한다. 이경우 MH ratio는 \\[ R (\\textbf {x}^{(t)}, \\textbf X^{\\ast} ) =\\dfrac {\\dfrac {f(\\delta^{\\ast})} {g \\left( logit(\\delta^{\\ast}) \\rvert logit(\\delta^{(t)}) \\right) \\cdot \\left| J(\\delta^{\\ast})\\right|} } {\\dfrac {f(\\delta^{(t)})} {g \\left( logit(\\delta^{(t)})|logit(\\delta^{\\ast}) \\right) \\cdot \\left| J(\\delta^{(t)})\\right|} } \\] $J(^{(t)}) $ 는 $T: u $에 대한 \\(J\\)를 \\(\\delta^{(t)}\\)에서 측정한 값. 주의해야 할 것이 해당 방법론에서는 \\(T\\)한 value를 사용하였으므로 \\(g\\)에 대한 \\(J\\)를 구해야 한다. 4.2.2.1.2 \\(u\\)-space에서 돌리는 방법 이 상황에서 쓰이는 proposal은 \\(\\dfrac {g(u^{(t)} \\rightarrow u&#39;)} {g(u&#39; \\rightarrow u^{(t)})}\\). \\(\\delta\\)에 대한 타겟 덴시티는 \\(u\\)에 대한 덴시티로 변형되어야 한다. 이때 \\(\\delta = logit^{-1}(U) = \\dfrac{\\exp(U)}{1+\\exp(U)}\\) 였으므로, \\(U^{\\ast} = u^{\\ast}\\) 로 두었을 때 생산되는 MH ratio는 $$ R (^{(t)}, ^{} ) = { {f(logit^{-1} {(u^{})})} {g (u{}|u{(t)}) ) * | J(u^{(t)})|} } { {f(logit^{-1} {(u^{(t)})})} {g (u{(t)}|u{()}) ) * | J(u^{})|} } $$ 우리가 transform value를 사용한데가 \\(f\\) 덴시티이므로 \\(f\\) 덴시티에 대한 야코비안을 붙여줘야 하는데 그 덴시티에 대한 야코비안은 \\(u\\)에 대한 야코비안이므로 쓰인 야코비안은 위와 같다. 이때 \\(\\lvert J(u^{\\ast})\\ \\rvert = \\dfrac {1} {\\lvert J(\\delta^{\\ast}) \\rvert}\\) 이므로 위와 아래에서 만들어지는 MH ratio는 같다. 따라서 두 관점은 equivalent한 체인을 생산한다. Sample paths for \\(\\delta\\) from RW chains in Ex. 7.3, run in \\(u\\)-space iwth b=1 (top) and b=0.01 (bottom). 4.2.2.2 Example: Autocorrelation Plot (ACF) 배우지 않은 MCMC 방법론 중 하나. reminder. thinning을 하더라도 거의 줄지 않아서 MCMC 샘플로서 거의 가치가 없는 케이스가 존재한다. no 4.2.3 Basic Gibbs Sampler need to derive the conditional density for all, 모든 joint density에 대해 coefficient를 1개씩 제한 상황의 모든 conditional density를 구한 후 GS 제작이 가능 if conditional densities are not available, we can use MH algorithm when updating \\(x_i\\) -&gt; 이런식으로 접근할 경우 이를 MH-within-Gibbs라고 부른다. 1PL IRT HW let \\(\\textbf {X} = (X_1 , \\cdots, X_p )^{&#39;}\\) , \\(\\textbf {X}_{-i} = (X_1 , \\cdots, X_{i-1}, X_{i+1}, \\cdots, X_p )^{&#39;}\\). 시작값 $^{(0)} $를 잡고, \\(t=0\\)으로 설정한다. 이후 각각을 \\(t+1\\) 단계의 시퀀스의 구성요소 각각을 \\(X_i^{(t+1)} \\vert \\; \\; \\cdot \\sim f \\left( x_1 \\rvert x_2^{(t)}, \\cdots, x_p^{(t)} \\right)\\) 에 따라서 생산한다. Gibbs Sampler의 stationary 분포는 \\(f\\). \\(X_i^{(t)}\\)의 limiting 마지널 분포는 \\(i\\)번째 coordinate에 따른 타겟분포의 단변량 마지널化와 같다. MH 알고리즘과 마찬가지로, 우리는 \\(X\\)의 임의의 함수 \\(g(X)\\)에 대해 \\(E \\left[ g(X) \\right]\\) 를 추정하기 위해 체인에서의 실현값을 사용할 수 있다. 4.2.3.1 Example: Fur Seal Pup Capture-Recapture Model 1800년대 후반 (by the late 1800s) 뉴질랜드 물범은 거의 전멸했다가 요즘 들어 폭증함 (abundance). 물범의 고름 숫자를 capture-recapture 사용해서 해보자. 사이즈 불명인 모집단의 크기 파악 위에 반복 연구 실행. 각 연구마다 포획었던 개체는 표식 새기고 풀어줌. 후속 연구에서 또 포획되면 재포획으로 표기. 높은 재포획 비율은 참 모집단 사이즈값이 포획되었던 개체들의 총량을 크게 넘지 않을 것임을 암시. \\(N\\): 불명인 모집단 사이즈. \\(l\\)회의 조사 통해 얻어진 각 회의 총 포획 숫자는 각각 \\(c=(c_1, \\cdots, c_l)\\)로 저장. 모집단 사이즈는 샘플링 동안에는 변동 없다(죽음, 출생, 이주 없음 inconsequential)고 가정한다. \\(r\\): 연구 동안에 포획되었던 이질 동물들의 총 숫자. 각 연구 시도에서 상응하는 구분되고 알려지지 않은 포획 확률은 \\(\\alpha = (\\alpha_1 , \\cdots, \\alpha_l )\\). 이 모델은 모든 동물들이 각 1회의 포획 발생에서 잡힐 가능성 자체는 각각의 동물에 대해서 동일하나, 이 被포획 확률은 시간이 지남에 따라 변할 수 있다는 것을 말함. 이 모델의 likelihood는 \\[ L \\left( N, \\alpha \\rvert c, r \\right) \\propto \\dfrac {N!}{(n-r)!} \\prod_{=1}^{l} \\alpha_i^{c_i} \\] Fur Seal Data for Seven Studies in One Season on the Otago Peninsula가 주어졌으며, prior은 \\(\\pi(N) \\propto 1\\), \\(\\pi (\\alpha_i ) = BETA(\\theta_1 , \\theta_2)\\) 이다. 계산하라. 해당 모델의 conditional posterior distribution에서 시뮬레이트하는 것으로 Gibbs Sampler를 제작할 수 있다. \\[ N^{(t+1)}-r \\rvert \\cdot \\sim Negative Binomial \\left( r+1, 1- \\prod_{i=1}^7 \\left( 1- \\alpha_i^{(t)} \\right) \\right) \\] \\[ \\alpha_i^{(t+1)} \\rvert \\cdot \\sim BETA \\left( c_i + \\dfrac{1}{2}, N^{(t+1)} - c_i + \\dfrac{1}{2} \\right) \\] for \\(i= 1, \\cdots, 7\\), \\(r = \\sum_{i=1}^7 {m_i} =84\\). 이는 unique fur seals were observed during the sampling period. Split boxplots of \\(\\bar {\\alpha}^{(t)}\\) against $N^{(t)} $ for the seal pup example. Estimated marginal posterior probabilities for \\(N\\) for the seal pup example. 4.2.3.2 MH-within-Gibbs Sampler 실제 implementation에 무지막지 유용하다. 이는 각각의 사이클을 GS의 사이클로 만들어놓고, conditional density는 MH 알고리즘으로 획득하는 것이다. Gibbs Sampler는 MH Sampler의 특별한 경우라고 볼 수 있다. MH 알고리즘의 proposal 분포를 시간에 따라 변화하도록 함으로써, GS와 MH 알고리즘 사이에 연결고리가 생긴다. 각 Gibbs 사이클은 \\(p\\) 개의 MH 스텝으로 구성되어 있다. 사이클 내에서의 \\(i\\)번째 Gibbs 스텝은, 체인의 현 상태 \\((x_{1}^{(t+1)}, \\cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\\underline{t})}, \\cdots, x_{p}^{(\\underline{t})})\\) 가 주어졌을 때, 효과적으로 후보 벡터 \\((x_{1}^{(t+1)}, \\cdots, x_{i-1}^{(t+1)}, {\\underline{X_i^{*}}}, x_{i+1}^{(\\underline{t})}, \\cdots, x_{p}^{(\\underline{t})})\\) 를 생산한다. 밑줄 차이점에 주목. \\(i\\)번째 단변량 Gibbs 업데이트는 이하와 같이 MH 스텝 drawing으로 볼 수 있다. \\[ {\\underline{X_i^{*}}} \\rvert (x_{1}^{(t+1)}, \\cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\\underline{t})}, \\cdots, x_{p}^{(\\underline{t})}) \\sim g_i \\left( \\cdot \\rvert (x_{1}^{(t+1)}, \\cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\\underline{t})}, \\cdots, x_{p}^{(\\underline{t})}) \\right) \\] where \\[ g_i \\left( \\cdot \\rvert (x_{1}^{(t+1)}, \\cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\\underline{t})}, \\cdots, x_{p}^{(\\underline{t})})\\right) = \\left\\{ \\begin{array}{@{}lr@{}} f(x_i^{*} \\rvert x_i^{(t)} ), &amp; \\text{if } \\; \\; \\; X_i^{*} = x_i^{(t)} \\\\ 0 &amp; o.w. \\end{array}\\right\\} \\] 이 경우, MH ratio는 1과 같아진다. 즉슨 모든 후보들은 언제나 accept 된다. 즉슨 GS는 MH 알고리즘에서 acceptance ratio가 항상 1인 경우에 해당한다. 따라서 conditional density을 구할 수만 있으면 GS를 사용하는게 좋다. 샘플을 버릴 필요가 없고, 버려지는 샘플이 없기 때문. 4.2.3.3 Update Ordering Random Scan Gibbs Sampling: 기본 GS에서 $ $ 에 가해지는 업데이트의 순서는 한 사이클에서 다음 사이클로 넘어갈 때마다 바뀔 수 있다. 패러미터들이 높은 수준에서 상호연관되어있을 경우, 각 사이클을 랜덤하게 순서배치하는 것은 효과적일 수 있다.2 특정 모델에 대한 전문화된 지식이 없다면, 한 이터레이션에서 다음으로 넘어갈 때 패러미터끼리 높이 상호연관되어있다면 deterministic 한 방법과 RSGS 양쪽 모두를 시도해보는 것이 권장된다. 4.2.3.4 Blocking with \\(p=4\\), e.g., 각 사이클을 다음 절차를 따르면서 업데이트: \\(X_1^{(t+1)}\\rvert \\cdot \\sim f \\left(x_1 \\rvert x_2^{(t)}, x_3^{(t)}, x_4^{(t)} \\right)\\). \\(X_2^{(t+1)}, X_3^{(t+1)}\\rvert \\cdot \\sim f \\left(x_2, x_3 \\rvert x_1^{(t+1)}, x_4^{(t)} \\right)\\). \\(X_4^{(t+1)}\\rvert \\cdot \\sim f \\left(x_4 \\rvert x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)} \\right)\\). Blocing은 \\(X\\)의 구성요소들이 서로 상관관계가 있을 때 \\(X_i\\) 내부가 상관이라는 건가, 아니면 \\(X_t, X_{t+1}\\) 이 상관이라는 건가? 유용함. 해당 알고리즘을 통해 더욱 상관된 구성요소끼리는 한 블럭 안에서 샘플링됨. 4.2.3.5 Hybrid Gibbs Sampling 하나 이상의 \\(X\\)에 대한 조건부 분포는 대부분 closed form으로 만들 수 없음. 깁스 샘플러의 주어진 스텝에서, 적절한 조건부 분포에서 샘플링하기 위해 MH 알고리즘이 쓰인다면 Hybrid MCMC 알고리즘이 완성됨. with \\(p=5\\), e.g., 하이브리드 MCMC 알고리즘은 다음 절차를 따르면서 업데이트: Update \\(X_1^{(t+1)} \\rvert \\left( x_2^{(t)}, x_3^{(t)}, x_4^{(t)}, x_5^{(t)} \\right)\\) with 깁스 스텝. Update ( x_2^{(t+1)}, x_3^{(t+1)} ) $ ( x_1^{(t+1)}, x_4^{(t)}, x_5^{(t)} )$ with MH 스텝. Update \\(X_4^{(t+1)} \\rvert \\left( x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)}, x_5^{(t)} \\right)\\) with 랜덤워크. Update \\(X_5^{(t+1)} \\rvert \\left( x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)}, x_4^{(t+1)} \\right)\\) with 깁스 스텝. 4.2.3.5.1 Example: Fur Seal Pup Capture-Recapture Study 4.2.4 Implementation MCMC의 목적은 타겟분포 \\(f\\)의 특징들을 알아내는 것. 모든 MCMC는 정답인 limiting 정적분포를 가지고 있음. 실전에는 체인을 얼마나 충분히 오래 돌릴지를 결정하는 게 중요함. \\(X\\)의 dimensionality(차원)이 높다면 수렴이 엄청 느려서 엄청 긴 run을 요할 수도 있음. 이하의 요건들을 생각해서 long run을 결정해야 함. Has the chain run long enough? Is the first portion of the chain highly influenced by the starting value? Should the chain be run from several different starting values? Has the chain traversed all portions of the region of support of \\(g\\)? Are the sampled values approximate draws from \\(f\\)? How shall the chain output be used to produce estimates and assess their precision? 4.2.4.1 Ensuring Good Mixing and Convergence MCMC 알고리즘이 대상 문제에 얼마나 쓸만한 정보를 주는지 고민해야 함. 이는 곧 체인이 얼마나 빠르게 체인의 starting value를 까먹는가 얼마나 빠르게 체인이 타겟분포 \\(f\\)의 모든 서포트를 훑는가 체인이 그것의 정적분포에 근사적으로나마 닿는가를 고민. There is substantial overlap between the goals of diagnosing convergence to the stationary distribution and investigating the mixing properties of the chain. 4.2.4.2 Simple Graphical Diagnostics 트레이스 플롯 (sample path)은 이터레이션 횟수 \\(t\\)와 \\(X^{(t)}\\)의 실현값 간의 플롯이다. 체인의 mixing이 구리면 이는 장기간의 이터레이션동안 동일값 근처에서 머무르게 됨. 체인의 mixing이 좋으면 시작값에서 빠르게 떠나서 \\(f\\)의 서포트에 해당하는 영역을 열심히 훑음. autocorrelation 플롯은 \\(X^{(t)}\\)의 시퀀스에서 다른 이터레이션 래그에서의 상관관계를 서술한다. 래그 \\(i\\)에서의 autocorrelation은 \\(i\\) 이터레이션만큼 떨어진 이터레이트 간의 상관관계이다. 구린 mixing 프로퍼티를 가지는 체인은 이터레이션 간의 래그가 증가하더라도 autocorrelation의 부식(decay)이 느림. MCMC 체인에서의 첫 \\(D\\) 개의 값은 보통 burn-in period라고 해서 버려짐. 체인의 시작점에 대한 의존이 강하게 남아있을지도 모르기 때문. 어느정도가 적절한 번인 피리어드인가는 Gelman-Rubin diagnostics에 의해 결정된다. 결정된 번인 피리어드가 제대로 된 값을 못내면 \\(D\\)가 늘어나던가 \\(L\\)이 늘어나던가 둘다 늘리던가 해야함. - Motivated by an Analysis of Variance - 사슬간 분산 (between-chain variance)이 사슬내 분산 (within-chain variance)보다 유의하게 크면 번인 피리어드나 MCMC 길이가 늘어나야 함 - Difficulties - multimodal \\(f\\)에서 적절한 스타팅 밸류를 찾는건 어렵고, 체인이 로컬 region이나 mode에서 갇힐수 있음 - 이의 단일차원성 (uni-dimensionality) 때문에 타겟분포 \\(f\\)가 멀티디멘션이면 타겟분포의 수렴에 대한 정보에 대한 잘못된 직관을 줘버릴 수 있음 proposal \\(g\\) 선택할 때 고려해야할 요소는? mixing은 proposal 분포 \\(g\\)의 특질에 큰 영향을 받으며 특히 이의 스프레드(spread)에 큰 영향을 받음. 타겟분포 \\(f\\)와 \\(g\\)간의 닮음에 있어서, 프로포절 \\(g\\)의 tail behavior의 닮음은 high density의 닮음보다 훨씬 중요함. \\(f/g\\)가 bounded 라면, MC의 정적분포로의 수렴은 대체로(overall) 빠름. 실전에선 정보를 줄 수 있는 이터레이티브 프로세스를 통해 proposal 분포의 분산이 택해질 수 있음. (In practice, the variance of the proposal distribution can be selected through an informal iterative process.) 20%~50% 사이의 acceptance rate가 선호되어야 함. Reparameterization 모델의 reparameterization은 MCMC 알고리즘의 mixing behavior에 상당한 기능향상을 가져올 수 있음. reparameterization은 dependence를 낮추기 위해 가장 우선되어야 할 전략 중 하나임. 서로 다른 모델들은 서로 다른 reparameterization 전략을 적용해야 함. reparameterization 접근법은 보통 특정 모델에 대한 원오프로서 채택되므로 일반화된 조언을 하기에는 어려운 부분이 있음. Comparing Chains MCMC 실현값이 크게 상관관계있다면, MCMC의 각 이터레이션에서 주어지는 정보는 run length에서 주어지는 정보 대비 보잘것 없다(will be less than suggested by the run length). 여기서 reduced information은 effective sample size라고 칭해지는 더 작은 iid 샘플에 담겨있는 정보와 동등하다. 여기서 샘플의 총 숫자와 effective sample size 사이의 차이는 잃게 된 효율을 의미한다. 관심있는 변량을 확인하기 위해 우리가 MC 체인에서의 correlated 샘플들을 사용했을 때 잃게된 효율. Number of Chains 모델 진단에 있어서 가장 진단을 어렵게 하는 부분은 체인이 타겟분포 \\(f\\)의 1개 이상의 모드에 걸리냐 안걸리냐 하는 부분. 이 경우 모든 수렴진단은 체인이 수렴하긴 한다는 결론을 내리게 된다. 정작 체인이 타겟분포 \\(f\\)의 모든 특성을 나타내지 못하는데도. 그래서 여러번 돌려보게 되는 것. 최소한 그 여러번의 run들 중 체인 1개에서라도 타겟분포 \\(f\\)의 모든 흥미로운 특질들이 드러났으면 하니까. 이러한 특질을을 찾아내는데 실패한 개별 체인들의 경우에는 mixing을 더 좋게 만들기 위해 체인 길이가 늘어나거나 문제가 reparameterize 되어야 함. random increment↩︎ The ordering of updates made to the components of X in the basic Gibbs sampler can change from one cycle to the next. Random ordering each cycle can be effective when parameters are highly correlated. In practice without specialized knowledge for a particular model, we recommend trying both deterministic and random scan Gibbs sampling when parameters are highly correlated from one iterations to the next.↩︎ "],["advanced-mcmc-wk08.html", "4.3 Advanced MCMC (wk08)", " 4.3 Advanced MCMC (wk08) 1, 3, 5번이 자주 사용됨 2, 3, 4의 목적은 proposal density 개선 1번은 missing data handling 5번은 variable selection 기본적으로 \\(f(x) = c \\psi (x) = c \\exp \\left( -\\frac{U(x)}{t} \\right)\\)의 개형을 따름. 4.3.1 Data Augmentation Missing Pattern MCAR MAR NMAR in missingness in missing on a variable No patterns can be predicted by other variables Missing values related to variable? not to any other variables the variable itself complete data considered as a random subsample from the original target sample whether data is NMAR is a theoretical and conceptual considerate Assumption power 3 2 1 missing을 의식할 필요가 있는 상황인가? No.missing 자체를 무시 (ignore). Yes.predict missing by other variable Yes.impute with seperate model bayesian 분석에서 Handling Missing Data에 자주 사용된다. Data Augmentation(DA) 알고리즘은, 불완전 데이터에 대한 bayesian 분석으로 설명될 수 있다. \\(X_{obs}\\) observed data \\(X_{mis}\\) missing data \\(X_{com}= \\left(X_{obs},X_{mis} \\right)\\) complete data assume complete 데이터 모델 \\(X_{com} = (X_{obs}, X_{mis}) \\sim g \\left( X_{obs}, X_{mis} \\rvert \\theta \\right)\\), where 패러미터 \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^d, d \\in \\mathbb{Z}^+\\). 여기서 목적은 패러미터 \\(\\theta\\)에 대한 prior 분포 \\(\\pi (\\theta)\\)와 함께 bayesian 추론을 만드는 것. 이는 곧 \\(g \\left( X_{obs}, X_{mis} \\rvert \\theta \\right) \\ast \\pi(\\theta)\\) 를 말한다. Multiple Imputation 데이터 impute, 그 impute한걸로 패러미터 업데이트, 다시 impute, …. 여기선 impute 셋을 여러개를 만들어놓고, 그걸 패러미터를 계산을 해서 패러미터 분포를 갖고 inference. Data Augmentation interatively하게 패러미터 impute, 업데이트, impute, 업데이트, …. proceeds: MCMC로 DA let observed-data model \\(f(X_{obs} \\rvert \\theta)\\). 이는 아래와 같이 joint pdf에서 마지널을 뽑아냄으로써 (integrate) 단독 pdf를 획득하는 것이 가능함. MCMC 방법론을 사용해 \\(\\theta\\)에 대한 Bayesian 추론을 진행하면 true or observed-data post를 샘플링하거나, 혹은 더욱 일반적으로는 \\(\\theta\\)와 \\(X_{mis}\\)의 joint 분포를 샘플링할 것을 요구한다. 이의 각각을 수식으로 나타내면 다음과 같다. $$ \\[\\begin{alignat}{4} &amp; &amp;&amp;f(X_{obs} \\rvert \\theta) &amp;&amp; &amp;&amp;= \\int_{\\mathbb{X}_{mis}} g(X_{obs}, X_{mis} \\rvert \\theta) \\; dX_{mis}, \\; \\; \\; &amp;&amp; \\theta \\in \\Theta \\\\ \\\\ \\pi (\\theta \\lvert X_{obs}) &amp;\\propto \\; &amp;&amp;f (X_{obs} \\lvert \\theta) &amp;&amp; \\pi(\\theta), &amp;&amp; \\; \\; \\; \\; \\; \\; \\; &amp;&amp; \\theta \\in \\Theta \\tag{1}\\\\ \\\\ \\pi (\\theta, X_{mis} \\lvert X_{obs}) &amp;\\propto \\; &amp;&amp;g (X_{obs}, X_{mis} \\lvert \\theta) &amp;&amp; \\pi(\\theta), &amp;&amp; \\; \\; \\; \\; \\; \\; \\; &amp;&amp; \\theta \\in \\Theta \\tag{2}\\\\ \\end{alignat}\\] $$ integrate하여 획득한 위의 likelihood \\(f(X_{obs} \\rvert \\theta)\\)에 post를 곱하여 direct하게 업데이트하는 1번 방법 missing을 하나의 unknown 패러미터로 두고, 이 missing 업데이트 한 다음에 missing까지 포함하여 \\(\\theta\\)를 inference하고, 그 후에 다시 missing을 업데이트 하는 방법. 1번에서는 마지널化한 pdf를 사용했고, 2번에서는 complete의 pdf를 통채로 사용했다. 1번에서는 마지널 시켜서 missing의 영향력을 삭제했다는 것을 발견할 수 있어야 한다. 의 2가지 방법을 취하는 것이 가능하다. let \\(h( X_{mis} \\lvert \\theta, X_{obs})\\)는 \\(X_{mis}, X_{obs}\\)의 conditional 분포. 이때, 이 분포와 \\[ \\pi (\\theta \\lvert X_{obs}, X_{mis}) \\propto g (X_{mis}, X_{obs} \\rvert \\theta) \\pi(\\theta) \\tag{Full Posterior} \\] 상기의 분포 2개가 양쪽 모두 샘플링이 간단하다고 가정하자. 이 두 조건부 분포 2개에 기반한 2단계 GS로 이를 해결하고자 하는 것은 당연한 귀결이다. 이를 Data Augmentation 알고리즘 (DA)라 칭한다. 이는 이하와 같이 설명될 수 있다. take \\(\\theta^{(0)} \\in \\Theta\\), and iterate for \\(t=1,2, \\cdots\\). Imputation-step: generate \\(X_{mis}^{(t)} \\sim f_{mis}(X_{mis} \\rvert \\theta^{(t-1)}, X_{obs})\\). (이는 정식적인 likelihood가 아니라 missing variable과 observed variable 간의 관계를 의미하는 것). 해당 스텝에서 missing을 대체. 다른 조건들 (패러미터, obs) 들이 주어졌을 때의 mis의 조건부분포를 구하여 이를 기반으로 mis를 imputation. Parameter update-step: generate \\(\\theta^{(t)} \\sim \\pi (\\theta \\lvert X_{obs}, X_{mis} )\\). 해당 스텝에서 패러미터를 갱신. 위에서 채운 mis와 obs를 합쳐 com 데이터로 삼고 이를 토대로 패러미터 추정한다. As a two-step GS, DA creates two interleaving Markov Chain: $$ \\[\\begin{align*} \\{ \\theta(t) &amp;: t=1,2,\\cdots \\} \\\\ \\{ X_{mis}^{(t)} &amp;: t=1,2,\\cdots \\} \\end{align*}\\] $$ Example: Multivariate Normal Distribution 4.3.2 Hit-and-Run Algorithm for improving the inefficiency of the RW 이를 위해 proposal 분포에 대한 수정이 요구된다. 이는 RW에 추가적인 요소를 넣어 변형하는 것인데, RW의 거리와 방향을 거리에 대응하는 pdf, 방향에 대응하는 pdf를 각각 만들어 거기서 랜덤하게 생산하는 것이다. draw below two parameters, and compute an MH acceptance probability \\(\\alpha(x,y)\\), where \\(x = x^{(t)} = X^{(t)}\\). direction \\(d \\sim g(d) \\; \\; \\;(d \\in \\mathbb{O})\\). distance \\(\\lambda \\sim I(\\lambda \\rvert d,x )\\) over \\(\\mathcal{X}_{x,d}\\). \\(X^{(t+1)} = \\begin{cases} x^{(t)}+ \\lambda d, &amp; \\text{if } \\; U \\le \\alpha(x,y) = \\frac{f(x&#39;)}{f(x^{(t)})}\\frac{g(x&#39; -&gt; x^{(t)})}{g(x^{(t)}-&gt; x&#39; ))}\\\\ x^{(t)}, &amp; o.w.\\\\ \\end{cases}\\). 이때, \\(g(d)\\)에 대한 가장 흔한 choice는 \\(\\mathbb{O}\\)에 대한 \\(U\\). 이외에 \\(g(\\cdot \\rvert x, d)\\), \\(\\alpha(x,y)\\)에 대한 가장 흔한 choice도 논해졌던 바가 있다. 이는 특히 sharply constrained 패러미터 space (\\(\\Theta\\)) 와 함께하는 문제에 효과적이다. - Wrapped Normal Distribution 4.3.3 Metropolis-Adjusted Langevin Algorithm how do we propose a new value? proposal improvement에 자주 사용된다. 또한, Hamiltonian MC와 아주 밀접한 관련이 있다. Langevin 방정식에 기반하여 생성된 알고리즘. 해당 알고리즘은 기본적으로 \\(f\\)를 정적 (stationary) 분포로서 내버려두게 된다. gradient flow에 의해 발생하면 얘도 로컬트랩 가능성 있는거 아님? \\[ dX_t = dB_t + \\frac {1}{2} \\bigtriangledown \\log f(X_t ) \\] 이때 \\(B_t\\)는 표준 브라운 운동. 이의 실적용은 Langevin diffusion process를 RW-like Transition으로 대체한 discretion step을 포함하는 아래의 식으로 이루어진다. \\[ x^{(t+1)} = x^{(t)} + \\frac {\\sigma^2}{2} \\bigtriangledown \\log f(X^{(t)} ) + \\sigma \\epsilon_t, \\; \\; \\; \\; \\; \\; \\epsilon_t \\sim N_d (0, I_d) \\] 이때 \\(\\sigma\\)는 step size of discretization. 하지만 discretized 된 프로세스는 transient (일시적, 해당 성질이 이후에도 이어질 것이라 장담 불가) 할 우려가 있으며 \\(f\\)에 대해 더이상 reversible 하지 않음. 이 negative behavior (악영향)을 보정 (correct)하기 위해서 discretization step을 MH acceptance-rejection rule에 기반하여 완화하는 것이 제기됨. 이인즉슨 실적용되는 수식을 conventional MH 제안 (proposal) 분포로서 취급하자는 것. 새로운 state: Langevin Dynamics 사용해서 제안 새로운 state의 accept 여부: MH 알고리즘 사용해서 평가 따라서 Langevin 알고리즘의 1회의 이터레이션은: 새로운 state \\(x^\\ast = x^{(t)} + \\frac {\\sigma^2} {2} \\bigtriangledown \\log f(x^{(t)}) + \\sigma \\epsilon_t\\). 이때 \\(\\sigma\\)는 user-specified 패러미터. For limited classes of target distributions, the optimal acceptance rate for this algorithm can be shown to be 0.574. MH ra tio \\(r = \\frac {f(x^\\ast)}{f(x^{(t)})} \\ast \\frac {\\exp \\left\\{ - \\frac {1} {2\\sigma^2} \\left[ x^{(t)} - x^\\ast - \\frac {\\sigma^2} {2} \\bigtriangledown \\log f(x^\\ast) \\right]^2 \\right\\}}{\\exp \\left\\{ - \\frac {1} {2\\sigma^2} \\left[ x^{\\ast} - x^{(t)} - \\frac {\\sigma^2} {2} \\bigtriangledown \\log f(x^{(t)}) \\right]^2 \\right\\}}\\). set \\(x^{(t+1)} = x^\\ast\\) with probability \\(\\min (1, r)\\), 남는 확률로는 \\(x^{(t+1)} = x^{(t)}\\) Advantages gradient flow 방법론에 따라 높은 density region으로 향하도록 RW를 충동질함. RW 대비 mixing이나 convergence 관점에서 효과적. 고차원 분포에서 유리 Disadvantages gradient 계산에 자원 다쳐먹는 경우 있음 multi-mode 관장 불가 이때 with probability \\(\\min (1, r)\\)라는 것은 \\(u \\sim U(0,1): u &lt; min(1,r)\\)과 동치라는 것을 알아두자. 앞으로는 모두 이렇게 서술할 것. 4.3.4 Multiple-Try Metropolis Algorithm H&amp;R, MALA, MTMA + HMC는 모두 proposal의 성능을 높이기 위함 MH Transition rule에 기반한 MC에서, 이의 효율성은 제안 (proposal) 분포에 크게 의존한다. 여러번의 이동을 거치면 과거의 실값과는 멀어지기 때문에 independent하기 때문에 어떤 측면에선 효율적이지만 acceptance 측면에선 떨어짐. let \\(\\lambda(x,y)\\) non-negative symmetric function. \\(q(y \\rvert x) &gt;0\\) 이면 항상 \\(\\lambda(x,y)&gt;0\\)임을 가정. define \\(w(x,y)=f(x) \\ast q(y \\rvert x) \\ast \\lambda(x,y)\\). At here, when \\(q(y \\rvert x)\\) is symmetric, for example, one can choose \\(\\lambda(x,y) = \\frac {1} {q(y \\rvert x)}\\), and then \\(w(x, y) = f(x)\\). 이 경우, MTM 알고리즘은 orientational bias MC 알고리즘으로 축소되는데, 이는 molecular simulation에서 사용되는 방법론 중 하나다. See Liu, Liang and Wong (2000) for the details. 위의 수식에서 proposal인 \\(q(y \\vert x)\\)에 symmetric이라는 조건을 붙이자. 그러면 \\(q(y \\vert x) = {1}{\\lambda(x,y)}\\)라는 상황을 가정하는 것이 가능하다. Current state is at \\(\\pmb x\\). Proceeds: draw \\(y_1 , \\cdots, y_k \\sim T(\\pmb x \\rightarrow \\pmb y)\\), \\(T\\) is proposal. select \\(\\pmb y = y_j\\) with probability \\(\\propto w(y_j , \\pmb x)\\). draw \\(x_1^\\ast, \\cdots, x_{k-1}^\\ast\\) from \\(T(y \\rightarrow x)\\). let \\(x_k^\\ast = \\pmb x\\). 이때 3번에서 원본 데이터로 돌아가는 프로세스가 포함된 이유는 MHMCMC 알고리즘에서는 얼마만큼 original proposal로 잘 돌아갈 수 있는지를 항상 고려해주어야 함. for reversability. accept proposed \\(\\pmb y\\) with probability \\(p = \\min \\left\\{ 1, \\frac{w(y_1, \\pmb x) + \\cdots + w(y_k, \\pmb x)} {w(x_1^\\ast, \\pmb y) + \\cdots + w(x_k^\\ast, \\pmb y)} \\right \\} \\tag{1}\\). (1)의 확률 자체가 결정된 메커니즘은 고급확률론이 필요하므로 이해 불가능함 current state \\(\\pmb x\\)에서, \\(\\pmb x \\rightarrow y_1\\rightarrow y_2 \\rightarrow \\cdots \\rightarrow y_k\\)로 가도록 한다. 이때 각 y에 대해 weight값이 존재. 4.3.5 Reversible Jump MCMC Algorithm number of variable이 작을 때, 베이지안 Variable Selection에 자주 사용됨 4.3.5.1 Bayesian Variable Dimension Model A Bayesian variable dimension model is defined as a collection of models \\[ \\mathcal{M}_k = \\left\\{ f(\\cdot \\lvert \\theta_k ) ; \\; \\; \\theta_k \\in \\Theta_k \\right\\}, \\; \\; \\; \\; \\; k=1:K \\] with a collection of priors \\(\\pi_k (\\theta_k)\\) on the 패러미터 of these models. 이는 곧 model의 변화에 따라 각각의 model들 또한 다른 prior를 갖는다는 이야기이다. and a prior distribution \\(\\rho_k, \\; \\; \\; k=1, \\cdots, K\\) on the indices of these models. ※ Note: 당연히 각각의 model space \\(\\Theta_k\\)들은 may have different dimensions. may라고 했지만 대부분의 경우 다름. In this setting one can compute the posterior probability of models, i.e. \\[ p \\left( \\mathcal{M}_k \\rvert \\pmb y \\right) = \\frac {\\rho_k \\ast \\int f_k (y \\lvert \\theta_k) \\ast \\pi_k (\\theta_k) d\\theta_k } {\\sum_j \\rho_j \\ast \\int f_j (y \\lvert \\theta_j) \\ast \\pi_j (\\theta_j) d\\theta_j} \\] RJMCMC 알고리즘이란, 패러미터 space의 dim이 정해지지 않은 상황에서, 이 dim과 상관없이 모델 space 자체를 이동할 수 있게 만들어준 MCMC 알고리즘. 단, 패러미터 space의 dim은 모르지만, 여기서는 모델의 총 갯수인 \\(k\\)로 fix가 되어 있다. 모델 간을 이동시키면서 모델 셀렉션 이와 동시에 패러미터 estimation까지 동시에 한다 는 것이 RJMCMC 알고리즘. 즉 RJMCMC 중에는 2가지 공정이 동시에 돌아간다 A variable dimension model is a “model where one among things you do not know is the number of things you do now know,” 연구자 모르는 것들 중 하나에, 니가 지금 알고 있는 것들의 갯수가 있는 모델. 즉, 내가 지금 알고 있는 것이 총 몇개인지조차도 모른다는 이야기이다. 그럼 대체 아는게 뭐야; e.g., 패러미터 space의 dimension이 고정되어 있지 않음. model selection, checking, improvement, 등등 다양한 상황에서 발생 가능. 모델 \\(\\mathcal{M}_k\\) 사이에서의 움직임을 설계하는데 있어 적절한 framework를 구축하고 싶다. 즉슨 모델에 대한 다양한 예상들이 있고, 이러한 모델의 예상도를 확률적으로 옮겨다니면서 이게 맞나? 이게 맞나? 를 체크한다는 이야기. 이를 위한 Green의 원칙: 모델 한 쌍 간의 움직임(채택 모델의 변경)만을 고려. “dimension matching” moves를 설계. MH 알고리즘과 유사하게 움직임을 with probability로 수용. 여기서의 probability 노테이션은 \\(q\\). let \\(x_t = (k^{(t)}, \\theta_k^{(t)} )\\)가 현 상태를 나타내고, \\(x^{(t+1)}\\)에 대한 proposed state \\(x&#39; = (k&#39;, \\theta_k&#39;)\\). if \\(k&#39;=k\\), proposed move 가 같은 subspace \\(\\mathcal{X}_k\\)에서 다른 위치를 탐색한다는 것이다. 따라서 dimension-matching problem 자체가 발생하지 않는다. if \\(k&#39; \\not= k\\), 분포 \\(\\psi_{k^{(t)} \\rightarrow k&#39;} (u)\\)로부터의 \\(s\\)개의 rv \\(\\pmb u = (u_1 , \\cdots, u_s)\\) 를 생산한다. 그리고 bijection \\((\\theta_k &#39; , u&#39; ) = T(\\theta_k^{(t)}, u)\\)를 생각하자. 이때 \\(s&#39;\\) 차원의 rvec\\(u&#39; = (u_1 , \\cdots, u_{s&#39;})\\)이며, \\(s\\)와 \\(s&#39;\\)는 dimension-matching condition \\(s+d_k = s&#39; + d_{k&#39;}\\)를 만족한다. Proceeds: 모델 \\(\\mathcal{M}_k\\) with probability \\(q(k^{(t)}, k&#39;)\\)에 의해 선택. generate \\(u_1 , \\cdots, u_s \\sim \\psi_{k^{(t)} \\rightarrow k&#39;} (u)\\) \\((\\theta_{k&#39;}&#39;, u&#39;) = T(\\theta_k^{(t)}, u)\\). Compute MH ratio \\(r = \\frac {f(k&#39;, \\theta&#39;_{k&#39;} \\rvert Y) } {f(k^{(t)}, \\theta^{(t)}_{k} \\rvert Y) } \\frac {g(k^{(t)} \\rvert k&#39;)} {g(k&#39; \\rvert k^{(t)})} \\frac {\\psi_{k&#39; \\rightarrow k^{(t)}} (u&#39;)} {\\psi_{k^{(t)} \\rightarrow k&#39;} (u)} \\left\\lvert {\\frac{\\partial (\\theta&#39;_{k&#39;}, u&#39;)}{\\partial (\\theta^{(t)}_{k}, u)}} \\right\\rvert\\)where \\(\\frac{\\partial (\\theta&#39;_{k&#39;}, u&#39;)}{\\partial (\\theta^{(t)}_{k} , u ) }\\) is Jacobian of Transformation. set \\(X^{(t+1)} = (k&#39;, \\theta&#39;_{k})\\) with probability \\(\\min (1,r)\\). 그러나 이렇게 무제한 모델을 만들면 너무 어려움. 따라서 보통 제약식을 추가해서 간단한 모델을 사용함. 사용되는 제약식은 각 이터레이션에서 이전 것과 이후 것 간의 dim 차이가 ±1 이라는 것. Example: Reversible Jump MCMC Algorithm "],["auxiliary-variable-mcmc.html", "4.4 Auxiliary Variable MCMC", " 4.4 Auxiliary Variable MCMC 실전에서 마주치는 대부분의 상황에서 ABC나 HMC 문제를 제외하고는 대부분의 경우 MCMC 문제를 완벽하게 풀어내는 건 불가능. 이때 주어진 variable 말고 보조변수 (Auxiliary Variable)을 추가함으로써 시뮬레이션 품질을 좀 더 높일 수 있지 않을까 하는 것이 논하고자 하는 바. 4.4.1 Introduction Difficulties with MH Algorithm. 일반적인 MH 알고리즘으로 풀어낼 수 없는 2가지 상황이 존재: Local-trap problem: 에너지 계가 울퉁불퉁한 complex system에서 시뮬레이션을 진행했을 때 끝없이 로컬 최적값에서 빠져나오지 못함. 시뮬레이션을 비효율적으로 만듬. density가 높다는 것은 해당 파트의 에너지가 낮다는 것이며, density가 낮은 에너지가 많은 파트에서 high density로 가는 것은 쉽고 자주 일어나도 역은 드뭄. 조밀하면 움직일 여력이 없으니까. 이것이 local trap의 원인 에너지는 이하로 표시 가능: energy function \\(= -log \\pi(\\theta \\vert x)\\), 즉 negative log posterior, 혹은 negative log density. Doubly-intractable normalizing constants problem: Inability to sample from distributions with intractable integrals 보통이라면, \\(pi(\\theta \\vert x) \\propto \\kappa(x) f(x\\vert\\theta)\\pi(\\theta)\\). \\(r= \\dfrac{pi(\\theta &#39; \\vert x)}{pi(\\theta^{(t)} \\vert x)} = \\dfrac{\\kappa(x) f(x\\vert\\theta &#39; )\\pi(\\theta &#39; )}{\\kappa(x) f(x\\vert\\theta^{(t)})\\pi(\\theta^{(t)})}\\) 과정에서 normarlizing constant \\(\\kappa\\)가 알아서 캔슬되어 MH 돌리는데 문제가 없음. let \\(f(x) \\propto \\kappa(x;\\theta) \\psi(x)\\) 는 알고자 하는 분포. 여기서 \\(\\kappa(x)\\)는 unnormalized density의 함수. 이때 \\(\\kappa(x)\\)는 패러미터의 함수이며 각 이터레이션의 다른 패러미터 추정값마다 변화해버려서 캔슬되지 않음. 그러면 계산하면 되는거 아님? 계산 불가능한 상황 존재 - nearly infinite summation or integration 포함하는 경우. (ex:) 이는 곧 intractable integral. acceptance \\(Pr\\)이 알 수 없는 비 \\(\\frac{\\kappa(x&#39;)}{\\kappa(x)}\\)를 포함하므로 MH 알고리즘은 사용불가. 이러한 문제는 bayesian 추론에서 spatial statistical models, random effects models, 그리고 exponential random graph models 등 다양한 통계적 모형에서 부딪히게 된다. ex: Lattice system of areal model (Lattice의 승만큼 연산 필요) e.g., Random Effect Model. 이때는 각 individual별로 Random Effect를 integration 해줘야 하므로 문제터짐 ex: Exponential Random Graph model: 네트워크에 사용되는 모델. 얘도 power임. 이러한 상황에서는 대부분의 optimization 알고리즘도 다 먹통됨 이러한 2개의 문제점을 극복하기 위해 다양한 진보된 MCMC 방법론이 제시되었음. Auxiliary variable-based methods Population-based methods Importance weight-based methods Stochastic approximation-based methods 4.4.1.1 Auxiliary Variable MCMC Methods \\(f(x)\\)를 가지는 mv 분포에서의 샘플링을 생각해보자. Rao-Blackwellization(#)이 MC 시뮬레이션에의 최우선원칙임은 알려져 있다. 시뮬레이션의 수렴을 좀 더 강화하기 위해 우리는 가능한한 많은 \\(x\\)의 구성물을 integrate하는 것을 시도해보아야 한다. 하지만 이하의 두가지 경우(이외에도 존재)에 시뮬레이션을 양질로 만들기 위해 우리는 1개 이상의 변수를 추가하는 상황을 고려할 수 있다. 타겟분포 \\(f(x)\\)가 multimodal. 온도 혹은 아직 관측되지 않은 측정값과 같은 auxiliary variable이 계가 로컬 트랩에서 빠져나올 수 있도록 도움을 줌. multimodal 상황. 타겟분포 \\(f(x)\\)가 intractable normalizing constant 포함. \\(X\\)의 auxiliary 실현값이 시뮬레이션에 포함됨으로써 시뮬레이션에서 normalizing constant 를 무력화시킴. MH 알고리즘 \\(\\dfrac{ f(\\theta &#39; \\vert x )}{f(\\theta^{(t)} \\vert x )} \\dfrac{ g(\\theta &#39; \\rightarrow \\theta^{(t)} )}{ g(\\theta^{(t)} \\rightarrow \\theta &#39;)}\\)은 이하의 2가지 기본적인 부품을 가지고 있다. 타겟분포 (左) proposal 분포 (右) 이에 더해서 auxiliary variable 방법론은 이하의 2가지 방법으로 행해질 수 있다. 타겟과 제안 어느쪽에 변수를 추가하는지에 대한 이야기이다. 타겟분포 augmentation 방법론: Augmenting auxiliary variables to the target distribution auxiliary variable \\(u\\)와 조건부 분포 \\(f(u \\rvert x )\\)를 정의한다. joint 분포 \\(f(x,u) = f(u \\rvert x) f(x)\\)를 만들기 위해. 이후 MH 알고리즘이나 GS를 사용해 \\((x,u)\\)를 업데이트. \\(f(x)\\)의 샘플은 \\((X, U)\\)의 실현값 \\((x_1, u_1), \\cdots, (x_N, u_N)\\)를 이용해 marginalization이나 프로젝션 등을 이용해 획득될 수 있다. Method of Proposal Distribution Augmentation: Augmenting auxiliary variables to the proposal distribution. proposal 분포 \\(T(x&#39;, u \\rvert x)\\)를 특정하고, 이의 reversible version \\(T(x, u \\rvert x&#39;)\\)도 특정한다. 즉슨 \\(\\int T(x&#39;, u \\vert x)du = T(x&#39; \\vert x)\\), \\(\\int T(x, u \\vert x&#39;)du = T(x \\vert x&#39;)\\)의 관계가 성립한다. 이제 proposal \\(T(x&#39;, u \\vert x)\\) 로부터 후보 (candidate) 샘플 \\(x&#39;\\)를 생산하고, 이를 with probability \\(\\min \\left\\{ 1, r(x, x&#39;, u) \\right \\}\\). 이때 \\(r(x, x&#39;, u) = \\dfrac {f(x&#39;)} {f(x)} \\dfrac {T(x,u \\vert x&#39;)} {T(x&#39;,u \\vert x)}\\). 실현값 (realizations) \\(x_1 , \\cdots, x_N\\)을 생산할 때까지 이를 반복한다. 이제 \\(N\\)이 충분히 크다면, 이 실현값들은 근사적으로 \\(f(x)\\)에 의해 분포되어 있다. 이러한 방법론의 타당성은 이하를 통해 보일 수 있다. \\[ K(x&#39; \\vert x) = \\int_{\\mathcal{u}} s(x, x&#39;, u) du + \\mathbf{1}(x=x&#39;) \\left[ 1-\\int_{\\mathcal{X}}\\int_{\\mathcal{u}} s(x, x&#39;, u) du dx&#39; \\right] \\] 이는 \\(x\\)로부터 \\(x&#39;\\)로의 integrated transition kernel 을 의미하며, 이때 \\(s(x, x&#39;, u) = T(x&#39;, u \\rvert x) \\ast r(x, x&#39;, u)\\). Then, \\[ f(x) \\int_{\\mathcal{u}} s(x, x&#39;, u) du = \\int_{\\mathcal{u}} \\min \\left[ f(x&#39;) T(x, u \\vert x&#39;), \\; \\; f(x)T(x&#39;, u \\vert x) \\right] du \\] 이는 \\(x\\)와 \\(x&#39;\\) 에 대해 symmetric. 이는 곧 \\(f(x)K(x&#39; \\vert x) = f(x&#39;)K(x \\vert x&#39;)\\) 임을 의미한다. original density? 4.4.2 Multimodal Target Distribution 4.4.2.1 Simulated Tempering 분포 \\(f(x) \\propto \\exp \\left(-H(x) \\right), x \\in X\\) 에서 샘플링하는데에 관심이 있다고 하자. simulated annealing에서 그러했던 것처럼, simulated tempering \\(f(x, T) \\propto \\exp \\left( -\\dfrac {H(x)} {T} \\right)\\)로 타겟 분포를 확장시켰다. 이는 auxiliary variable인 temperture \\(T\\)를 포함함으로서 이루어진다. \\(T\\)는 사용자가 미리 지정한 값들의 finite set이 된다. \\(H(x)\\)는 사실상 energy function. Temperature Transition Matrix \\(T = \\begin{bmatrix} q_{11} &amp; q_{12} &amp; \\cdots &amp; q_{1n} \\\\ q_{21} &amp; \\ddots &amp; &amp; \\\\ \\vdots &amp; &amp; \\ddots &amp; \\\\ q_{n1} &amp; \\cdots &amp; &amp; q_{nn} \\end{bmatrix}\\). 이때 row는 current 온도 \\(T_1, \\cdots, T_n\\), column은 행선지 온도. Parallel Tempering은 인접한 온도로만 이동 가능 (가장 높은 온도에서 가장 낮은 온도로 한단계 한단계씩). 온도 자체를 시뮬레이션한게 아니라 온도의 chain이 주어져 있어 각 온도 간의 움직임을 만드는 것에 그친다. 따라서 이는 multiple chain을 이용하는 population MC 방법론 쪽에 소속됨. Simulated Tempering과는 이 점에서 차이를 보임. 후자는 어느 온도로든 다 이동. 온도 매트릭스 만들어놓고, \\(U(0,1)\\) 분포에서 온도 하나 생산하고 이 온도로 이동할 것인지의 여부를 MH 알고리즘으로 결정. \\(U(0,1)\\)에서 랜덤하게 숫자를 뽑고, \\(j\\)의 값을 proposal transition matrix \\((q_{ij})\\)에 따라서 정한다. \\(u&lt;q_{11}\\)이면 \\(T_1 \\rightarrow T_1\\), \\(q_{11}&lt;u&lt;q_{11} + q_{12}\\)이면 \\(T_1 \\rightarrow T_2\\), …. - if \\(j=i_t\\), let \\(i_{t+1}=i_t\\), and let \\(x_{t+1}\\)을 MH kernal \\(K_{i_t}(x,y)\\)에서 뽑는다. 이때 \\(K_{i_t}(x,y)\\)는 \\(f(x, T_{i_t})\\)을 invariant distribution로 허용하는 아이이다. 즉 새로운 \\(x\\)를 생산하면 된다. - if \\(j \\not= i_t\\), let \\(x_{t+1}=x_t\\)하고 proposal을 이하의 \\(Pr\\)에 따라 채택한다. 이때 \\(Z\\)는 \\(Z_i\\)의 측정값이다. 채택된다면 \\(i_{t+1} = j\\)이고, 그외의 경우에는 \\(i_{t+1} = i_t\\)로 한다. 새로운 \\(x\\)를 생산하는 것이 아니라 들고 있던 \\(x\\)를 쓰되, 이걸 accept 할건지 안할건지를 체크한다. \\[ \\min \\left[ 1, \\; \\; \\dfrac {\\hat Z_j} {\\hat Z_{i_t}} \\ast \\exp \\right \\{ -H(x) \\left( \\dfrac{1}{T_j}-\\dfrac{1}{T_{i_t}} \\right) \\left \\} \\cdot \\dfrac {q_{j,i_t}} {q_{i_t , j}} \\right] \\] 이때 \\(\\dfrac {q_{j,i_t}} {q_{i_t , j}}\\) 는 proposal distribution이라고 생각할 수 있다. 나머지는 Likelihood part이며, 이때 \\(\\dfrac {\\hat Z_j} {\\hat Z_{i_t}}\\) 가 normalizing constant의 ratio이다. 온도가 변화하였으므로 두 식의 normalizing constant가 같지 않기 때문이다. Issues on Simulated Tempering: Temperature Ladder를 어떻게 고를 것인가. → 각 chain별로 이동이 원활하게 잡는 것이 핵심. 가장 높은 온도 \\(T_1\\)은 대부분의 uphill move가 해당 레벨에서 accept 될 수 있도록 설정되어야 한다. 사이의(intermediate) 온도들은 sequential manner로 설정될 수 있다. \\(T_1\\)에서 시작해서, 점차적으로 다음으로 낮은 온도를 \\(Var_i \\left\\{ H(x) \\right\\} \\ast \\delta^2 = O(1)\\)을 만족하도록 설정하는 것이다. 이때 \\(\\delta = \\dfrac {1}{T_{i+1}} - \\dfrac{1}{T_i}\\)이며, \\(Var_i(\\cdot)\\)은 \\(H(x)\\) (taken with respect to \\(f(x, T_i)\\)) 의 분산을 의미한다. 이러한 조건들은 \\(f(x,T_i), f(x,T_{i+1})\\) 사이에 상당히 겹치는 점이 많아야 한다는 것을 의미하기도 한다. 실전에선 \\(Var_i \\left( H(x) \\right)\\)는 샘플러를 레벨 \\(T_i\\)에서 예비적으로(preliminary) 돌려보았던 결과에서 러프하게나마 예측될 수 있다. \\(Z_i\\)를 어떻게 estimate 할 것인가. → accept 여부가 normalizing constant에도 의존해서 이거 이상하게 고르면 효율 떨어짐. 엄청난 단점이라서 요즘은 이 알고리즘 자체를 잘 안씀 이는 simulated tempering의 효율에 직결되는 부분이다. \\(Z_i\\)들이 잘 estimate 되었다면, simulated tempering은 temperature ladder을 따라 symmetric RW처럼 동작한다. (\\(x\\)-updating step을 제하고 볼 경우) 그렇지 않다면 이는 특정 temperature 레벨에서 멈춰버린다. 시뮬레이션이 실패함은 물론이다(rendering). 실전에서 \\(Z_i\\)들은 stochastic approximation MC 방법론을 사용해서 estimate 가능하다. 혹은 reverse logistic regression 방법론을 사용해서도 \\(Z_i\\)를 estimate 할 수 있다. 4.4.2.2 Slice Sampler density \\(f(x), \\; \\; \\; x \\in \\mathcal X\\)에서 샘플링하고자 한다. \\(x \\sim f(x)\\)에서 샘플링하는 것은, \\(f(x)\\) 그래프 이하의 영역에서 uniform하게 샘플링하는 것과 동등하다. 해당 영역은 \\(A = \\{ (x,u): 0 \\le u \\le f(x) \\}\\)이며, 이것이 acceptance-rejection 알고리즘의 기초(basis)였다. 이 목적을 달성하기 위해 우리는 타겟분포 \\(f\\)를 auxiliary variable \\(U\\)를 사용하여 확장해볼 수 있다. 이 \\(U\\)는, \\(x\\)에 대해서 조건부일 때, 구간 \\([0, f(x)]\\)에서 uniform하게 분포되어 있다. 따라서, \\((X, U)\\)의 joint density function은 \\(f(x,u)=f(x)f(u \\rvert x) \\propto I_{(x,u)\\in \\textit A}\\). 후자의 인디케이터는 언급되었던 영역 안에 속한다는 의미. 이는 GS에 의해 이하와 같이 샘플링 가능하다. draw \\(u_{t+1} \\sim U[0, f(x_t)]\\). draw \\(x_{t+1}\\) uniformly from the region \\(\\{ x: f(x) \\ge u_{t+1} \\}\\). 위의 샘플러는 slice sampler라고 불림. 이는 multimodal 분포들에 대해 단순 MH 알고리즘보다 더 나을 가능성이 있음. slice 때문에 b/w-mode-transition에 자유롭기 때문. 현재도 핫한 샘플러중 하나임. horseshoe prior 에서의 패러미터 estimate에 대표적으로 이녀석이 쓰인다. 4.4.3 Doubly-intractable Normalizing Constants Spatial models, e.g., the autologistic model, the Potts model, and the autonormal model (Besag, 1974)는 많은 과학적 문제들을 위한 모델링에 쓰이고 있음. 이러한 모델들에 해당하는 주요한 문제는 normalizing constant가 doubly-intractable하다는데 있음. for dataset \\(X\\), 패러미터 \\(\\theta\\), normalizing constant \\(\\kappa (\\theta)\\). 이때 \\(\\kappa (\\theta)\\)는 \\(\\theta\\)에 의존하나 closed form으로는 만들 수 없음. 이하는 dataset을 생산한 likelihood function. \\[ \\begin{align*} X \\sim f(x \\vert \\theta) = \\dfrac{1}{\\kappa (\\theta)} exp \\{ -U(x, \\theta) \\}, &amp;x \\in \\mathcal{X}, &amp;\\theta \\in \\Theta \\end{align*} \\] \\(\\pi(\\theta)\\)는 \\(\\theta\\)의 prior. 이 경우 post는 \\(f(\\theta \\vert x) \\propto \\dfrac{1}{\\kappa (\\theta)} exp \\{ -U(x, \\theta) \\} \\ast \\pi(\\theta)\\). 4.4.3.1 Boltzmann Density known as Ising Model, 그리고 ~로 확장될 경우 autologistic model. Consider a 2-D Ising model with the Boltzmann density \\[ f(\\pmb x) \\propto \\exp \\left\\{ K \\sum_{i\\sim j} x_i x_j \\right\\} \\] spins \\(x_i = \\pm 1\\) (S극이 -1) \\(K\\)는 inverse temperature (measure for interaction : \\(x_i\\)가 주변에 있는 값과 얼마나 많은 같은 값을 가지는지, 다른 값을 가지는지에 대해 측정해주는 패러미터) 온도가 낮을수록 interaction가 강해지며, 이에 의해 동일값 확률이 높아짐. \\(i\\sim j\\)는 lattice 상의 가장 가까운 neighbors. 온도가 높다면, 이 모델은 GS를 사용해 쉽게 시뮬레이션 가능하다. 조건부 분포에 따라 각 spin의 값을 iteratively 초기화한다. 아래의 식에서 \\(n(i)\\)는 spin \\(i\\)의 neighbors의 집합 (set). 이하의 수식은 autologistic 과 그 과정이 유사하다. $$ \\[\\begin{align*} P(x_i =1 \\vert x_j, \\; \\; j \\in n(i)) &amp;= \\dfrac {1}{1+ \\exp \\left \\{ -2K \\sum_{j \\in n(i)} \\right\\}} \\\\ P(x_i =-1 \\vert x_j, \\; \\; j \\in n(i)) &amp;= \\dfrac {\\exp \\left \\{ -2K \\sum_{j \\in n(i)} \\right\\}}{1+ \\exp \\left \\{ -2K \\sum_{j \\in n(i)} \\right\\}} &amp;= 1- P(x_i =1 \\vert x_j, \\; \\; j \\in n(i)) \\end{align*}\\] $$ 하지만, GS는 temperature가 critical temperature로 근접하거나 이하로 내려갈 경우 GS가 빠르게 느려진다. 온도가 낮으면 interaction이 강해져, 주변값과 비슷한 값을 generate 해야만 하기 때문이다. 이렇게 샘플링이 어려워지는 지점, 온도를 critical point라고 부른다. 이는 대략 \\(\\theta \\approx 0.43\\). 이것이 소위 critical slowing down 이라고 불리는 현상이다. 4.4.3.1.1 Perfect Sampler 과거 샘플들의 굉장히 많은 조합을 커플링해서 샘플을 생산. previous realization 전체에 대해 (이는 그 이전의 샘플, 아니면 그 이전의 샘플, 혹은 original 데이터에 대해서조차도) independent한 샘플을 생산해내는 sampler. 즉 그 어떤 것에서도 independent한 sample을 생산해낸다. 문제는 이 샘플러는 \\(\\theta&gt;0.43\\)인 순간 바로 작동을 안함. \\(\\theta&gt;0.32, 0.35\\) 정도로 엔간 크기만 해도 드럽게 느림. 4.4.3.1.2 Swendsen-Wang Algorithm slice sampling에서 Boltzmann 덴시티는 이하의 형으로 다시 쓰여진다. 이때 \\(\\beta = 2K\\). indicator function으로 변형했을 때 저 둘이 어떻게 equation이 성립하는지 유의. \\[ f(\\pmb x) \\; \\propto \\; \\prod_{i\\sim j} \\exp \\left\\{ K(1+x_i x_j) \\right\\} \\; = \\; \\prod_{i\\sim j} \\exp \\left\\{ \\beta \\ast \\mathbf{1}(x_i = x_j) \\right\\} \\] 이때 우리가 auxiliary variable \\(\\pmb u = (u_{i \\sim j})\\), where each component \\(u_{i \\sim j}\\), conditional on \\(x_i\\) and \\(x_j\\), is uniformly distributed on \\(\\left[ 0, \\; \\exp \\{\\beta \\ast \\mathbf{1}(x_i = x_j)\\} \\right]\\), then \\[ f(\\pmb x, \\pmb u) \\; \\propto \\; \\prod_{i \\sim j} \\mathbf{1} \\left( 0 \\le u_{i \\sim j } \\le \\exp\\left\\{ \\beta \\ast \\mathbf{1} (x_i = x_j) \\right\\} \\right) \\] 이때 \\(u_{i \\sim j}\\) 자체는 bond variable이라고 명명된다. 이는 spin \\(i\\)와 spin \\(j\\) 사이의 가장자리에 물리적으로 앉아 있는 변수로서 생각될 수 있다. (i와 j가 묶여져 있는지, 같은 group 안에 존재하는 것인지 아닌지에 대한 indicator가 되는 variable) if \\(u_{i \\sim j}&gt;1\\), then \\(\\exp \\left\\{ \\beta \\ast \\mathbf{1}(x_i = x_j) \\right \\}&gt;1\\), 따라서 반드시 \\(x_i = x_j\\). if \\(u_{i \\sim j}&lt;1\\), 이 경우 \\(x_i, x_j\\)에 제약 (constraint) 이 없다. \\(b_{i \\sim j}\\)가 제약에 대한 indicator variable이라고 정의하자. 즉, \\(x_i, x_j\\)가 같도록 제약되었다면, \\(b_{i \\sim j}=1\\)이며 이외엔 0이다. for any 2개의 “like-spin” (i.e. 2개의 spin이 같은 값을 가진다) neighbors에 대해서, 이 둘은 with probability \\(1-\\exp (-\\beta)\\)를 따라 bonded 될 수 있는 가능성이 있음을 기억하라. \\(\\pmb u\\)의 설정 (configuration)에 따라, “mutual bond” (i.e., \\(b_{i \\sim j}=1\\)) 을 통하여 연결될 수 있는지 없는지 여부에 따라 spin들을 군집 (cluster) 할 수 있다. (위에서 i와 j가 같다고 indicator가 판별했을 경우에만 이런 cluster 로 묶는 것이 가능하다) Then 동일 클러스터 내의 모든 spin은 같은 값을 가질 것이다. 또한 군집 내부의 모든 spin을 동시에 뒤집는 (flip) 것은 \\(f(\\pmb x , \\pmb u)\\)의 평형 (equilibrium)을 해치지 않을 것이다. Proceeds: Update the bond values: check all “like-spin” neighbors, and set \\(b_{i \\sim j}=1\\) with probability \\(1-\\exp (-\\beta)\\). Update the spin values: Cluster spins by connecting neighboring sites with a mutual bond, and then flip each cluster with probability \\(0.5\\). For the Ising model, the introduction of the auxiliary variable \\(\\pmb u\\) has the dependence between neighboring spins partially decoupled, and the resulting sampler can thus converge substantially faster than the single site updating algorithm. As demonstrated by Swendsen and Wang (1987), this algorithm can eliminate much of the critical slowing down. 같은 값들이 모여있는 cluster를 판별하여 각각을 grouping. grouping을 랜덤으로 하므로 인접해 있는 동일값임에도 그룹에 포함되지 못하는 경우가 존재함. Swendsen-Wang에서는 이렇게 그룹을 만든 후, 해당 그룹을 통채로 toggling. group을 통채로 토글링하기 때문에 dependency가 있는 것들이 통채로 toggling되어서 dependency가 있는 것들은 나머지 것들과 인제 이렇게 independent한 것도 있지만 dependent한 것을 통채로 묶어서 하는 것이므로 좀더 한꺼번에 뒤집으니까 실제로 우리가 업데이트하는 것은 group 내부 말고 group 외부 간들에는 independent하다고 가정될 수 있는 몇몇개의 group들만이 남음. 이 덩어리들을 한꺼번에 업데이트하므로 따라서 샘플러 generate가 상대적으로 쉬움. 하지만 이 만든 덩어리는 매 이터레이션마다 덩어리를 새로 만들어야 함. 매 이터레이션마다 클러스터를 새로 만들고 flip하여 이를 accept할지 말지를 결정하는 이런 형태의 구조를 가짐. 4.4.3.2 Møoller’s Algorithm auxiliary variable \\(y\\), 이는 \\(x\\)와 같은 state space를 공유한다고 정의. 그 경우 이하의 joint pdf \\(f\\) 를 생각해볼 수 있다. \\(f(y \\vert \\theta , x)\\)는 \\(y\\)의 분포. \\[ f(\\theta, y \\vert x) = f(x \\vert \\theta) \\ast f(\\theta) \\ast f(y \\vert \\theta , x) \\] \\(f(\\theta, y \\vert x)\\) 에서 MH 알고리즘을 통해 시뮬레이트하기 위해서는 이하와 같은 제안분포 \\(q\\) 를 사용해볼 수 있다. 이는 패러미터 벡터 \\(\\theta \\rightarrow \\theta&#39;\\)의 usual change에 상응하며, 이 후에는 \\(q(\\cdot \\vert \\theta &#39; )\\)에서 \\(y&#39;\\)를 추출하는 exact sampling step이 따른다. $$ q(’ , y’ , y) = q(‘, y) q(y’ ’) $ $ \\(q(y&#39; \\vert \\theta &#39; )\\)가 \\(f(y&#39; \\vert \\theta)\\)로 설정되었다면, MH ratio \\(r\\)은 이하와 같이 쓰일 수 있다. 이때 unknown normalizing constant \\(\\kappa(\\theta)\\)가 상쇄 (cancel) 되었음에 주목하라. $$ \\[\\begin{align*} r(\\theta, y, \\theta&#39;, y&#39; \\vert x) &amp;= \\dfrac {f(x \\vert \\theta&#39;) f(\\theta&#39;) f(y&#39; \\vert \\theta&#39; , x) \\ast q(\\theta\\vert \\theta&#39; , y&#39;) q(y \\vert \\theta)} {f(x \\vert \\theta) f(\\theta) f(y \\vert \\theta , x) \\ast q(\\theta&#39;\\vert \\theta , y) q(y&#39; \\vert \\theta&#39;)} \\\\ &amp;= \\dfrac {f(\\theta&#39;, y&#39; \\vert x)}{f(\\theta, y \\vert x)} \\ast \\dfrac {q(\\theta , y \\vert \\theta&#39; , y&#39;))}{q(\\theta&#39; , y&#39; \\vert \\theta , y)} \\\\ &amp;= \\dfrac { \\dfrac {f(\\theta&#39;, y&#39; \\vert x)}{q(\\theta&#39; , y&#39; \\vert \\theta , y)} } { \\dfrac{f(\\theta, y \\vert x) }{q(\\theta , y \\vert \\theta&#39; , y&#39;))} } \\end{align*}\\] $$ 여기서 계산을 간단하게 하기 위해 제안분포 \\(q\\)와 auxiliary distribution을 이하와 같이 정리하는 것을 생각해볼 수 있다. 이때 \\(\\hat \\theta\\)는 \\(\\theta\\)의 estimate로써, 예를 들어 pseudo-likelihood function을 극대화하는 것으로 얻어진 값이다. $$ \\[\\begin{align*} q(\\theta&#39; \\vert \\theta , y) &amp;= q(\\theta&#39; \\vert \\theta ) , q(\\theta \\vert \\theta &#39;, y&#39;) &amp;= q(\\theta \\vert \\theta &#39;) \\\\ f(y \\vert \\theta , x) &amp;= f(y \\vert \\hat \\theta ), f(y&#39; \\vert \\theta&#39; , x) &amp;= f(y&#39; \\vert \\hat \\theta ) \\end{align*}\\] $$ 분포 \\(f(x \\vert \\theta)\\)를 auxiliary variable, 가령 normalizing constant ratio \\(\\dfrac {\\kappa(\\theta)} {\\kappa(\\theta&#39;)}\\) 등으로 살찌워놓은 것은, 시뮬레이션 진행 과정에서 상쇄시키는 것이 가능하다. generate \\(\\theta \\sim q(\\theta&#39; \\vert \\theta_t)\\) generate exact sample \\(y&#39; \\sim f(y \\vert \\theta&#39;)\\) accept \\((\\theta&#39;, y&#39;)\\) with probability \\(\\min (1, r)\\), \\(r=\\dfrac {f(x \\vert \\theta&#39;) f(\\theta&#39;) f(y&#39; \\vert \\hat \\theta&#39;) \\ast q(\\theta_t \\vert \\theta&#39;) q(y \\vert \\theta_t)} {f(x \\vert \\theta_t) f(\\theta_t) f(y \\vert \\hat \\theta) \\ast q(\\theta&#39;\\vert \\theta_t) q(y&#39; \\vert \\theta&#39;)}\\). 채택된다면, set \\((\\theta_{t+1}, y_{t+1}) = (\\theta&#39;, y&#39; )\\). o.w., \\((\\theta_{t+1}, y_{t+1}) = (\\theta_t, y&#39;_t)\\). 4.4.3.3 Exchange Algorithm Møller’s 알고리즘을 parallel tempering 개념을 도입하여 개선. propose candidate point $’ $ proposal distribution \\(q(\\theta&#39; \\vert \\theta, x)\\). propose auxiliary variable \\(y \\sim\\) perfect sampler \\(f(y \\vert \\theta &#39; )\\). accept $’ $ with probability \\(\\min \\{ 1, r(\\theta, \\theta&#39; \\vert x) \\}\\). \\(r(\\theta, \\theta&#39; \\vert x) = \\dfrac{\\pi(\\theta&#39;) }{\\pi(\\theta) } \\ast \\dfrac{f(x \\vert \\theta &#39;) }{f(x \\vert \\theta)} \\ast \\dfrac{f(y \\vert \\theta) }{f(y \\vert \\theta&#39;)} \\ast \\dfrac{f(\\theta \\; \\vert \\theta&#39;, x) }{f(\\theta&#39; \\vert \\theta, x)}\\) The exchange algorithm can be viewed as an auxiliary variable MCMC algorithm with the proposal distribution being augmented, for which the proposal distribution can be written as $ \\[\\begin{align} T \\left( \\theta \\rightarrow (\\theta&#39; , y) \\right) &amp;= q(\\theta &#39; \\vert \\theta) f(y \\vert \\theta &#39;) \\\\ T \\left( \\theta&#39; \\rightarrow (\\theta , y) \\right) &amp;= q(\\theta \\vert \\theta &#39; ) f(y \\vert \\theta) \\end{align}\\] $ This simply validates the algorithm, following the arguments for auxiliary variable Markov chains. The exchange algorithm generally improves the performance of the Møller algorithm, as it avoids an initial estimation step (for \\(\\theta\\)) that required by the Møller. Although the Møller’s and exchange algorithms work well for some discrete models, such as the Ising and autologistic models, they cannot be applied to many other models for which perfect sampling is not available. Even for the Ising and autologistic models, perfect sampling may be very expensive when the temperature is near or below the critical point. 4.4.3.4 Adaptive Exchange Algorithm Object: An adaptive exchange algorithm (AEX) is an adaptive Monte Carlo version of the exchange algorithm, where the auxiliary variables are generated via an importance sampling procedure from a Markov chain running in parallel. Advantage Removes the requirement of perfect sampling Overcomes its theoretical difficulty caused by inconvergence of finite MCMC runs AEX consists of two chains running in parallel. The first chain is auxiliary, which is run in the space \\({\\mathcal{x}}\\) with an aim to draw samples from a family of distributions \\(f(X \\vert \\theta^{(1)}), \\; \\; \\cdots, \\; \\; f(X \\vert \\theta^{(m)})\\) for a set of pre-specified parameter values \\(\\theta^{(1)}, \\; \\cdots, \\; \\theta^{(m)}\\). The second chain is the target chain, which is run in the space \\(\\theta\\) with an aim to draw samples from the target posterior \\(\\pi(\\theta \\vert y)\\). For a candidate point \\(\\theta&#39;\\), the auxiliary variable \\(x\\) is resampled from the past samples of the auxiliary chain via an importance sampling procedure. Assume that the neighboring distributions \\(f(X \\vert \\theta^{(i)})\\)’s have a reasonable overlap and the set \\(\\left \\{ \\theta^{(1)}, \\; \\cdots, \\; \\theta^{(m)} \\right \\}\\) has covered the major part of the support of \\(\\pi (\\theta \\vert y)\\). ALGORITHM: PART 1 - (Auxiliary Chain) Auxiliary Sample Collection via SAMC (Sampling) Choose to update \\(\\vartheta\\) or \\(\\pmb z_t \\vert \\vartheta\\) with pre-specified probabilities, e.g., \\(0.75\\) for updating \\(\\vartheta\\) and \\(0.25\\) for updating \\(z_t\\). 1.-a Update \\(\\vartheta_{t}\\) : Draw \\(\\vartheta &#39;\\) from the set \\(\\left \\{ \\theta^{(1)}, \\; \\cdots, \\; \\theta^{(m)} \\right \\}\\) according to a proposal distribution \\(T_1 ( \\; \\cdot \\; \\vert \\vartheta_{t})\\), set \\((\\vartheta_{t+1}, \\pmb z_{t+1}) = (\\vartheta &#39; , \\pmb z_{t+1} )\\) with probability \\(\\min \\left\\{ 1, \\; \\; \\dfrac{\\omega_t^{J(\\vartheta_t)}}{\\omega_t^{J(\\vartheta &#39;)}} \\ast \\dfrac {\\varphi (\\pmb z_{t} \\vert \\vartheta &#39;)} {\\varphi (\\pmb z_{t} \\vert \\vartheta_{t})} \\ast \\dfrac{T_1 (\\vartheta_{t} \\vert \\vartheta &#39; )}{T_1 (\\vartheta &#39; \\vert \\vartheta_{t} )} \\right\\}\\), and set \\((\\vartheta_{t+1}, \\pmb z_{t+1}) = (\\vartheta_{t}, \\pmb z_t)\\) with remaining probability, where \\(J(\\vartheta_t)\\) denotes the index of \\(\\vartheta_t\\), i.e., \\(J(\\vartheta_t) = j\\) if \\(\\vartheta_t = \\theta_i^{(k)}\\) and \\(\\varphi(\\pmb z \\vert \\vartheta)\\) is an unnormalized density of \\(f(\\pmb z \\vert \\vartheta)\\). 1.-b. Update \\(\\pmb z_t\\) : Draw \\(\\pmb z &#39;\\) according to a proposal distribution \\(T_2 ( \\; \\cdot \\; \\vert \\pmb z_t)\\), set \\((\\pmb z_{t+1} , \\vartheta_{t+1}) = (\\pmb z &#39; , \\vartheta_{t})\\) with probability \\(\\min \\left\\{ 1, \\; \\; \\dfrac {\\varphi (\\pmb z &#39; \\vert \\vartheta_{t})} {\\varphi (\\pmb z_{t} \\vert \\vartheta_{t})} \\ast \\dfrac{T_2 (\\pmb z_{t} \\vert \\pmb z &#39; )}{T_2 (\\pmb z &#39; \\vert \\pmb z_{t} )} \\right\\}\\), and set \\((\\pmb z_{t+1} , \\vartheta_{t+1}) = (\\pmb z_t , \\vartheta_{t})\\) (Abundance Factor Updating) Set \\[ \\log (\\omega_{t+0.5}^{(j)}) =\\log (\\omega_{t}^{(j)}) + a_{t+1} (e_{t+1, \\; j} - p_j), \\; \\; \\; \\; \\; j=1, \\cdots, m \\] where \\(e_{t+1, \\; j} = \\begin{cases} 1 &amp; &amp;&amp; \\vartheta^{t+1} = \\theta^{(j)} \\\\ 0 &amp; &amp;&amp; o.w. \\end{cases}\\). If \\(\\omega_{t+0.5}^{(j)} \\in \\mathcal{K}_{\\varsigma_t}\\), set \\((\\omega_{t+1}, \\pmb z_{t+1}) = (\\omega_{t+0.5}, \\pmb z_{t+1})\\) and \\(\\varsigma_{t+1} = \\varsigma_t\\). o.w., set \\((\\omega_{t+1}, \\pmb z_{t+1}) = \\mathbb{T}(\\omega_{t}, \\pmb z_{t})\\) and \\(\\varsigma_{t+1} = \\varsigma_t + 1\\). (Auxiliary Sample Collection) Append the sample \\(\\left(\\pmb z_{t+1} , \\vartheta_{t+1}, \\omega_{t+1}^{J(\\vartheta_{t+1}} \\right)\\) to the collection \\(S_t\\). Denote the new collection by \\(S_{t+1}\\) which is set by \\(S_{t+1} = S_t \\cup \\left\\{ \\left(\\pmb z_{t+1} , \\vartheta_{t+1}, \\omega_{t+1}^{J(\\vartheta_{t+1}} \\right) \\right\\}\\). ALGORITHM: PART 2 - (Target Chain) Adaptive Exchange Sampler (Proposal) Propose a candidate point \\(\\theta &#39;\\) from a proposal distribution \\(q(\\theta &#39; \\vert \\theta)\\) (Resampling for Auxiliary Variables) Resample an auxiliary variable \\(\\pmb x\\) from the collection \\(S_{t+1}\\) via a dynamic importance sampling procedure; that is, setting \\(\\pmb x = \\pmb z_k\\) with probability \\[ P(\\pmb x = \\pmb z_k) \\dfrac {\\sum_{j=1}^{\\vert S_{t+1} \\vert} \\omega_t^{\\left( J(\\vartheta_j) \\right)} \\tfrac{\\varphi(\\pmb z_k \\vert \\theta &#39; )} {\\varphi(\\pmb z_k \\vert \\vartheta_j &#39; )} \\ast I(\\pmb z_j = \\pmb z_k )} {\\sum_{j=1}^{\\vert S_{t+1} \\vert} \\omega_t^{\\left( J(\\vartheta_j) \\right)} \\tfrac{\\varphi(\\pmb z_k \\vert \\theta &#39; )} {\\varphi(\\pmb z_k \\vert \\vartheta_j &#39; )}} \\] \\(\\left(\\pmb z_{j} , \\vartheta_{j}, \\omega_{t}^{J(\\vartheta_{j}} \\right)\\) denotes the \\(j\\)-th element of the set \\(S_{t+1}\\). \\(\\vert S_{t+1} \\vert\\)는 \\(S_{t+1}\\)의 size. (Exchange Algorithm) Set \\(\\theta_{t+1} = \\theta &#39;\\) with the probability \\(\\alpha(\\theta_t , \\pmb x, \\theta&#39; )\\), and \\(\\theta_{t+1} = \\theta_{t}\\) with probability \\(1-\\alpha(\\theta_t , \\pmb x, \\theta&#39; )\\). \\[ \\alpha(\\theta_t , \\pmb x, \\theta&#39; ) = \\dfrac {\\pi(\\theta &#39; )} {\\pi(\\theta_t )} \\dfrac {\\varphi(\\pmb y \\vert \\theta &#39; )} {\\varphi(\\pmb y \\vert \\theta_t )} \\dfrac {q(\\theta_t \\vert \\theta &#39; )} {q(\\theta&#39; \\vert \\theta_t )} \\dfrac {\\varphi(\\pmb x \\vert \\theta_t )} {\\varphi(\\pmb x \\vert \\theta &#39; )} \\] Why this algorithm is adaptive? Since the underlying true proposal distribution for generating auxiliary variables in part II is changing from iteration to iteration, the new algorithm falls into the class of adaptive MCMC algorithms (for which the proposal distribution is changing from iteration to iteration). "],["approximate-bayesian-computation.html", "4.5 Approximate Bayesian Computation", " 4.5 Approximate Bayesian Computation Likelihood를 사용하지 않고 Summary Stat을 사용함. 이와 같은 성질로 인해 Likelihood-free inference라고도 불리며, 이의 근거는 simulator-based model (모델 자체를 data generation process라고 보는 것. 모델을 통해 data를 생산해내는 것이 가능하다는 소리). 모델이 있으면, 이 모델과 대응하는 패러미터가 존재할 것. 이 모델 자체를 하나의 data generating process라고 본다면, 당연히 우리는 Auxiliary Data를 생산하는 것 또한 가능. 이 AD를 생산하는 것이 가능하기 때문에, 이렇게 생산한 AD 데이터가 기존 데이터 (original data) 와 얼마만큼 가까운지, 아니면 얼마만큼 떨어져 있는지를 체크하는 것이 ABC 방법론. 이것이 충분히 가깝다면 우리가 가지고 있는 패러미터를 패러미터의 샘플로 인정하는 것이 가능하다는 것. 이때 직접 AD와 OD를 비교하는 것인 대단히 어려우므로 대체재로 Summary Stat을 사용함. 이것이 ABC 방법론의 핵심. 이는 genetics에서 개발됨. 4.5.1 Simulator-Based Models deterministic model은 뭐임? 모든 모델이 pdf \\(p(y \\vert \\theta)\\)의 family로 특정되는 것은 아님. Simulator-based Models: Models which are specified via a mechanism (rule) for generating data. Models specified via a data generating mechanism occur in multiple and diverse scientific fields. Different communities use different names for simulator-based models: Generative Models: 데이터를 생성해주는 메커니즘을 연구하는 모델 Implicit Models Stochastic Simulation Models Probabilistic Programs Examples Astrophysics: Simulating the formation of galaxies, stars, or planets. Evolutionary Biology: Simulating Evolution Neuroscience: Simulating Neural Circuits Ecology: Simulating Species Migration Health Science: Simulating the spread of an infectious disease Advantages of Simulator-Based Models Direct implementation of hypotheses of how the observed data were generated. obs가 어떻게 생산되었는지에 대한 가설을 입증할 수 있는 하나의 방법이 됨. Neat interface with physical or biological models of data. 갖고 있는 데이터는 한둘에서 끝나고 이의 variation을 고려하는 것이 중요한데 이 variation을 연구하는건 데이터 한둘로는 어려움. 이때 가지고 있는 데이터를 replicate하고 simulator-based model을 이용해 비슷한 데이터를 만들어내서 variation을 연구해 좀더 정확한 inference를 가능케 하고, 그러면서 uncertainty quantification도 가능하게 함. Modeling by replicating the mechanisms of nature which produced the observed/measured data. (“Analysis by synthesis”) Possibility to perform experiments. Disadvantages of Simulator-Based Models Generally elude analytical treatment. analytic한 solution이 없어 수학적 증명이 어려움. Approximate BC인 이유가 여기 있음 Can be easily made more complicated than necessary. Statistical inference is difficult but possible. Family of pdfs Induced by the Simulator: For any fixed \\(\\theta\\) (패러미터 \\(\\theta\\) 는 주어져 있다는 소리), the output of the simulator \\(y_\\theta = g( \\cdot \\; , \\theta)\\) is a random variable. 즉 \\(\\theta \\longrightarrow y_\\theta\\). No closed-form available for \\(p(y \\vert \\theta)\\), and Simulator defines the model pdfs \\(p(y \\vert \\theta)\\) implicitly. ### Approximate Bayesian Computation (ABC) ##### Intractability $ (D ) = $ - Usual intractability in Bayesian inference is not knowing \\({\\pi (D)}\\), which is marginal Likelihood of Data. - \\(\\pi (D \\vert \\theta) = \\kappa(\\theta)\\pi (D \\vert \\theta)\\) 인 경우라면, MH 알고리즘을 사용하면 됨. 계산 과정에서 \\(\\pi (D \\vert \\theta)\\)가 캔슬되니까. 그러나 \\(\\pi (D \\vert \\theta) = \\kappa(\\theta)f (D \\vert \\theta)\\), \\(f (D \\vert \\theta)\\) 가 unnormalized density인 경우라면 이는 doubly-intractable 케이스. 캔슬도 안되고 고려해줄수밖에 없음. (with \\(\\kappa(\\theta)\\) unknown.) - ABC가 해법이 된다. 만능은 아님. 적용에 약간 회의적. - A problem is completely-intractable if \\(\\pi (D \\vert \\theta)\\) is unknown and cannot be evaluated (unknown is subjective). That is, if the analytic distribution of the simulator, \\(f(\\theta)\\) run at \\(\\theta\\) is unknown. 모델이 지나치게 복잡해서 명시적으로 Likelihood 자체가 주어지지 않는 경우가 존재함. Completely intractable models are where we need to resort to ABC methods. (Likelihood가 intractable한 모델에서 ABC가 많이 사용된다. 전 챕터에서 언급된 Doubly Intractable 모델이 대표적인 예. 단 ABC가 스켈레톤 키는 아님. ABC도 패러미터 튜닝이 요구되고, 이 패러미터 튜닝은 상당히 어려움. 따라서 ABC도 상당히 약점이 많음.) - Genetic Background of ABC - ABC is a recent computational technique that only requires being able to sample from the likelihood \\(f(\\cdot \\; \\theta)\\) - This technique stemmed from population genetics models, about 15 years ago, and population geneticists still contribute significantly to methodological developments of ABC. - Population Genetics - Describe the genotypes (AA, AO, etc ⇔ penotypes A) , estimate the alleles frequencies, determine their distribution among individuals, populations and between populations. - Predict and understand the evolution of gene frequencies in populations as a result of various factors. - Analyses the effect of various evolutive forces (mutation, drift, migration, selection) on the evolution of gene frequencies in time and space. If the likelihood function is intractable, then ABC (approximate Bayesian computation) is one of the few approaches we can use to do inference. ABC algorithms are a collection of Monte Carlo methods used for calibrating simulators. ABC 자체는 simulator-based model을 calibration한 모델이기 때문에 우리가 Likelihood function에 대해 명시적으로 알 필요가 없다. - they do not require explicit knowledge of the likelihood function. - inference is done using simulation from the model (they are ‘likelihood-free’). ABC methods are popular in biological disciplines, particularly genetics. They are - Simple to implement - Intuitive - Embrassingly parallelizable (MCMC 모델이 아니기 때문에 생기는 성질. multiple chain으로 돌려도 괜찮다는 이야기이다.) MCMC의 사용 상황은 아래의 그림과 같이 도식화된다. - Can usually be applied - Proceeds: Target is \\(\\pi(\\theta)f(x \\vert \\theta)\\). When likelihood \\(f(x \\vert \\theta)\\) not in closed form, likelihood-free rejection technique of ABC Algorithm is: \\(y \\sim f(y \\vert \\theta)\\), under the prior \\(\\pi(\\theta)\\) (population genetics에서는 prior information이 대단히 강력하다. 따라서 prior에서 샘플을 생산해도 무관하며, ABC의 이러한 성질은 이의 연장선이다.), keep jointly simulating $ ’ () \\ \\ z f(z ’ ) $ until the \\(z\\) is equal to the observed value \\(y\\). \\(z = y\\)라는 결과를 얻을 때까지 위의 과정을 반복. \\(z = y\\)가 된다면 $’ $를 accept. - Proof $ \\begin{alignat}{4} f(i) &amp;{z } (_i) f(z _i) l_y(z) &amp;&amp; \\ &amp;(_i) f(y _i) &amp;&amp;= (_i y) \\end{alignat} $ - Tolerance Condition When \\(y\\) is a continuous rv, \\(z = y\\) is replaced with a tolerance condition, \\(\\rho(y,z) \\le \\epsilon\\), where \\(\\rho\\) is a distance. Output distributed from $ () P_{ (y,z) &gt; } ; ; ; ; ( (y,z) &gt; ) $ 하지만 이 방법은 - \\(\\epsilon\\) 결정이 어려움 - distance \\(d(z,y)\\) 계산이 어려움 The idea behind ABC is that the summary statistics coupled with a small tolerance should provide a good approximation of the posterior: 위의 난점을 해결하기 위해 summary statistics \\(\\eta(z), \\eta(y)\\)를 사용하여 \\(dist\\left\\{ \\eta(z), \\eta(y) \\right\\}\\)를 구하여 이를 기준점으로 사용. $ \\[\\begin{align} \\pi_\\epsilon (\\theta \\vert y) &amp;= \\int \\pi_\\epsilon (\\theta , z \\vert y)dz \\\\ &amp;\\approx \\pi \\left( \\theta \\vert \\eta(y) \\right) \\end{align}\\] $ where \\(\\eta(y)\\) defines a (not necessarily sufficient) statistic. 하지만 대부분의 경우에는 SS를 이용은 함. - Proceeds: For each iteration, 1. Repeat followings until \\(\\rho \\left( \\eta(z), \\eta(y) \\right) \\le \\epsilon\\). - Generate \\(\\theta &#39;\\) from the prior distribution \\(\\pi(\\cdot)\\). - Generate \\(z\\) from the likelihood \\(f(\\cdot \\; \\vert \\theta &#39; )\\). 2 Set $_i = ’ $. However, Simulating from the prior is often poor in efficiency. (population genetics에서는 strong prior를 사용했기 때문에 이러한 문제가 표면화되지 않았었음. 그러나 통계적 상황에서는 Bayesian prior를 사용하는 경우도 많으므로 이러한 문제가 유의해짐. 특히 non-informative prior를 쓸 때 poor performance 가능성 높아짐) - By modifying the proposal distribution on \\(\\theta\\) to increase the density of \\(x\\)’s within the vicinity of \\(y\\). \\(\\theta\\)의 proposal 제안할 때 그 과정을 개선한다는 소리. - By viewing the problem as a conditional density estimation and by developing techniques to allow for larger \\(\\epsilon\\). \\(\\epsilon\\) 잡기가 너무 어려워서 ABC 자체가 애매해짐. summary statistics 따라 \\(\\epsilon\\)이 항상 바뀌고 이거에 대한 규칙성도 현재로서는 없음. Summary Statistics가 모든 정보를 담고 있지 않아서, summary statistics로 model selection을 하려는 시도는 있었지만 실패했음. summary statistics가 담고 있는 정보가 데이터의 성질을 온전히 드러내고 있는지조차도 불명확함. - By including \\(\\epsilon\\) in the inferential framework. \\(\\epsilon\\) reflects the tension between computability and accuracy. - As \\(\\epsilon \\rightarrow \\infty\\), we get observations from the prior, \\(\\pi(\\theta)\\). - If \\(\\epsilon=0\\), we generate observations from \\(\\pi(\\theta \\vert D)\\). posterior에서 뽑는 것이라고 생각할 수 있다. \\(\\epsilon=0\\)에 가까우면 (최소한 summary statistics로는) \\(y=z\\) 조건을 만족하는 것. ABC가 사용 불가한 상황은? - if the data are too high dimensional, we never observe simulations that are ‘close’ to the field data. - 데이터가 고차원이라면, 데이터의 차원들간의 interaction이 반드시 존재할 수밖에 없으므로, 기존 데이터와 같은 데이터를 생산해내는 것은 사실상 불가능 -Reduce the dimension using summary statistics. - 1개의 실현값 \\(\\theta&#39;\\) 에서 summary statistics를 10000개 \\(\\eta(z^{(1)}), \\cdots, \\eta(z^{(10000)})\\) 를 가령 생산한 상황이라면, 이 summary statistics의 distribution이 multimodal이면 절대로 사용불가. 4.5.1.0.1 Potential Risks and Remedies in ABC Non-zero tolerance \\(\\epsilon\\) - The inexactness introduces a bias in the computed posterior distribution. - Practical studies of the sensitivity of the posterior distribution to the tolerance. - sensitivity analysis, specification difficult - summary statistics를 쓰지 sufficient statistics를 쓰는게 아니기 때문에 필연적으로 information loss가 있고 CI가 넓어짐 Non-sufficient summary statistics - The information loss causes inflated credible intervals. - Automatic selection/semi-automatic identification of sufficient statistics. Model validation check. Small number of models/Mis-specified models - The investigated models are not representative/lack predictive power. - Careful selection of models. Evaluation of the predictive power. Priors and parameter ranges - Conclusions may be sensitive to the choice of priors. Model choice may be meaningless. - Check sensitivity of Bayes factors to the choice of priors. Use alternative methods for model validation. Curse-of-dimensionality - Low parameter acceptance rates. Model errors cannot be distinguished from an insufficient exploration of the parameter space. Risk of overfitting. - Methods for model reduction if applicable. Methods to speed up the parameter exploration. Quality controls to detect overfitting. Model ranking with summary statistics - The computation of Bayes factors on summary statistics may not be related to the Bayes factors on the original data, which may therefore render the results meaningless. - Only use summary statistics that fulfill the necessary and sufficient conditions to produce a consistent Bayesian model choice. Use alternative methods for model validation. Implementation - Low protection to common assumptions in the simulation and the inference process. - Sanity checks of results. 4.5.2 ABCifying Monte Carlo Methods Rejection ABC is the basic ABC algorithm: rejection 알고리즘임 - Inefficient as it repeatedly samples from prior More efficient sampling algorithms allow us to make better use of the available computational resource: spend more time in regions of parameter space likely to lead to accepted values. - allows us to use smaller values of \\(\\epsilon\\), and hence finding better approximations. Most Monte Carlo algorithms now have ABC versions for when we don’t know the likelihood 이를 개선하기 위해 제안되는 모델이 아래의 ABC-MCMC 알고리즘 4.5.3 ABC-MCMC Algorithm 이 알고리즘에서는 Embrassingly parallelizable 성질을 잃어버리게 된다. We are targeting the joint distribution $ {ABC} (, x D) {} (D x) (x ) () $ To explore the \\((\\theta, x)\\) space, proposals of the form $ Q ( (, x), (‘, x’) ) = q(, ‘) (x’ ’ ) $ seem to be inevitable. The MH acceptance probability is then $ \\[\\begin{align} r &amp;= \\dfrac{\\pi_{ABC} (\\theta&#39; , x&#39; \\vert D) Q \\left( (\\theta&#39;, x&#39;), (\\theta, x) \\right)}{\\pi_{ABC} (\\theta , x \\vert D) Q \\left( (\\theta, x), (\\theta&#39;, x&#39;) \\right)} \\\\ &amp;= \\dfrac{\\pi_{\\epsilon} (D \\vert x&#39;) \\pi (x&#39; \\vert \\theta&#39;) \\pi(\\theta&#39;)}{\\pi_{\\epsilon} (D \\vert x) \\pi (x \\vert \\theta) \\pi(\\theta)} \\dfrac{q(\\theta&#39;, \\theta) \\pi (x&#39; \\vert \\theta )}{q(\\theta, \\theta&#39;) \\pi (x&#39; \\vert \\theta &#39; )} &amp;= \\dfrac{\\pi_{\\epsilon} (D \\vert x&#39;) \\pi(\\theta&#39;)}{\\pi_{\\epsilon} (D \\vert x) \\pi(\\theta)} \\dfrac{q(\\theta&#39;, \\theta) }{q(\\theta, \\theta&#39;) } \\end{align}\\] $ For each iteration, Repeat followings until \\(\\rho \\left( \\eta(z), \\eta(y) \\right) \\le \\epsilon\\). Propose \\(\\theta &#39;\\) from a transition kernel \\(g(\\theta &#39; \\vert \\theta^{(t)})\\) Generate \\(z\\) from the likelihood \\(f(\\cdot \\vert \\theta &#39;)\\) accept or stay $ = \\[\\begin{cases} \\theta &#39; &amp; \\text{with probability MH ratio } \\alpha = \\min \\left( 1, \\; \\; \\dfrac{\\pi(\\theta &#39;)}{\\pi(\\theta^{(t)})} \\dfrac{g(\\theta^{(t)} \\vert \\theta &#39;)}{g(\\theta &#39; \\vert \\theta^{(t)} )} \\right) \\\\ \\theta &amp; o.w. \\end{cases}\\] $ 즉 \\(\\theta &#39;\\)를 MH 알고리즘에서 생산하며, MH 알고리즘에서 생산하였으므로 \\(z\\)와 \\(\\theta &#39;\\) 양쪽 모두가 수용할지 여부의 평가 대상. \\(z\\)가 수용되지 않았다면 \\(\\theta &#39;\\)도 수용되지 않고 \\(\\theta&#39;\\)를 새로 생산한다. 4.5.3.0.1 Sequential ABC Algorithms The most popular efficient ABC algorithms are those based on sequential methods. We aim to sample N particles successively from a sequence of distributions $ _1 () , , _T () = $ For ABC, we decide upon a sequence of tolerance \\(\\epsilon_1 &gt;\\epsilon_2 &gt; \\cdots &gt; \\epsilon_T\\), and let \\(\\pi_t\\) be the ABC distribution found by the ABC algorithm when we use tolerance \\(\\epsilon_t\\). 4.5.3.0.2 Synthetic Likelihood The synthetic likelihood approach of Wood (2010) is an ABC algorithm which uses a Gaussian likelihood. However, instead of using $ \\[\\begin{align} \\pi_\\epsilon(D \\vert X) &amp;= N(D; X, \\epsilon) \\\\ \\pi_{ABC}(D \\vert \\theta) &amp;= \\int N(D; X, \\epsilon)\\pi(X \\vert \\theta) dX \\end{align}\\] $ they repeatedly run the simulator at \\(\\theta\\), generating \\(X_1, \\cdots, X_n\\), and then use $ (D ) = N ( D ; , ) $ where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) is the sample mean and covariance of the (summary of the) simulator output. "],["hamiltonian-monte-carlo.html", "4.6 Hamiltonian Monte Carlo", " 4.6 Hamiltonian Monte Carlo RW의 단점 (randomness에서 오는 inefficiency를 줄이기 위해) 을 보완하기 위해 나온 개념. Hamiltonian Monte Carlo borrows an idea from physics to suppress the local random walk behavior in the Metropolis algorithm, thus allowing it to move much more rapidly through the target distribution. Version of Metropolis where you take many “steps” per “iteration” Use “Hamiltonian dynamics” and a latent “momentum (\\(\\phi_j\\))” vector so the steps within an iteration move along a “trajectory.” Requires computation of gradient of log target density. 한번의 스텝에서 어느정도 움직이는지를 알기 위해선 이것이 요구되기 때문. Take \\(L\\) leapfrog steps (leapfrog를 몇번을 할 것인지), each of distance \\(\\epsilon\\) (1번의 leapfrog에서 얼마만큼을 움직일 것인지), then accept/reject. In a leapfrog step, both \\(\\theta\\) and \\(\\phi\\) are changed, each in relation to the other. Effecitive Sample Size \\(=ESS/S\\): correlated되지 않았을 경우에 샘플 사이즈로 볼 수 있는 크기. MCMC는 과거 체인에 dependent하므로 당연히 correlated되어 있으며, 따라서 10000개를 생산했다고 치면 10000개의 샘플 사이즈에서 independent한 샘플의 사이즈가 얼마만큼인지를 체크하는 것. 따라서 ESS가 크면 클 수록 좋음. 마지막에 computation time \\(S\\)로 나누어준 이유는 단위시간 당 샘플로 비교해야 하니까. 해당 개념은 알고리즘 comparison에서 대단히 많이 사용됨. Rstan은 GS가 아니라 HMC를 사용함. 그래서 샘플 크기가 상대적으로 작아도 OK. How far to jump in each step? - If \\(\\epsilon\\) is too small, you waste time shuffling along. - If \\(\\epsilon\\) is too large, the physical approximation breaks and you find yourself rejecting. How many steps? (No U-turn Sampler) - If \\(L\\) is too small, you might not go far enough in each iteration. - If \\(L\\) is too large, you’ll waste time circling around and around. Still HMC can be much better than Gibbs or Metropolis in high dimensions. Energy Barrier. Multimodal에 취약. Proceeds: 우선 momentum부터 만들어야 함. 운동에너지를 위치에너지로 바꾸는 것이 Hamiltonian의 기본적인 메커니즘. 이때 운동에너지 + 위치에너지는 항상 일정한 상수로 일정하다. 이를 위해 momentum을 생산해야 하는데, 이때 가장 손쉽게 momentum을 생산할 수 있는 방법이 \\(M=I\\) 로 잡는 것. 단, 어떤 측면에서는 \\(I\\)가 inefficient하기도 함. 다른 제안으로 Inverse Fisher’s Information 사용이 제안된 적도 있음. 하지만 중간중간에 fixed point iteration으로 패러미터를 정해줘야 한다고? 경사가 완만할 때 더 많은 거리 이동 가능. A. The iteration begins by updating \\(\\phi\\) with a random draw from its posterior distribution - which is the same as its prior distribution \\(\\phi \\sim N(0, M)\\). B. A simultaneous update of \\((\\theta, \\phi)\\) conducted in an elaborate but effective fashion via a discrete mimicking of physical dynamics. B step의 1회 이터레이션이 leapfrog Step 1회에 해당한다. Use the gradient of the log-posterior density of \\(\\theta\\) to make a half-step of \\(\\phi\\). $ \\[\\begin{align} \\phi_{half.new} \\; \\; \\; &amp;\\leftarrow \\; \\; \\; \\phi + \\dfrac {1}{2} \\epsilon \\ast \\dfrac{d \\log \\pi(\\theta \\vert y)}{d \\theta} \\\\ &amp;= \\; \\; \\; \\dfrac{1}{2} \\epsilon \\bigtriangledown_{\\pmb \\theta} \\log f(\\theta^\\ast) \\\\ &amp;= \\; \\; \\; \\phi + \\dfrac {1}{2} \\epsilon \\ast \\dfrac{d \\log \\pi(\\theta \\vert y)}{d \\theta} \\Bigg \\vert_{\\theta^\\ast} \\end{align}\\] $ {:start=“2”} Use the ‘momentum’ vector \\(\\phi\\) to update the ‘position’ vector \\(\\theta\\): $ ; ; ; ; ; ; + M^{-1} $ {:start=“3”} Use the gradient of \\(\\theta\\) to half-update \\(\\phi\\): $ {new} ; ; ; ; ; ; {half.new} + $ C. label \\(\\theta^{t-1}, \\phi^{t-1}\\) as the value of the parameter and momentum vectors at the start of the leapfrog process and \\(\\theta^{\\ast}, \\phi^{\\ast}\\) as the value after the \\(L\\) steps. In the accept-reject step, we compute $ r = $ D. Set \\(\\theta^t = \\begin{cases} \\theta^\\ast &amp; \\text{with probability } \\min(1,r) \\\\ \\theta^{t-1} &amp; o.w. \\end{cases}\\) 4.6.0.0.1 HMC Example Trajectory Blue ellipse is contour of target distribution Initial position at black solid circle. Arrows indicate a U-turn in momentum “One practical impediment to the use of Hamiltonian Monte Carlo is the need to select suitable values for the leapfrog stepsize, \\(\\epsilon\\), and the number of leapfrog steps \\(L\\). Tuning HMC will usually require preliminary runs with trial values for \\(\\epsilon\\) and \\(L\\). Unfortunately, preliminary runs can be misleading….” 4.6.1 Introduction to Hamiltonian Monte Carlo Hamiltonian Monte Carlo (HMC) was originally developed in the late 1980s as Hybrid Monte Carlo to tackle calculations in Lattice Quantum Chromodynamics, a field focused on understanding the structure of the protons and neutrons. Neal (1995, 2011) introduced the Hamiltonian Monte Carlo into the mainstream of statistical computing. HMC is built upon a rich theoretical foundation, which is formulated in terms of differential geometry, that makes it uniquely suited to the high-dimensional problems of applied interest. 4.6.1.0.1 Foundations of Hamiltonian Monte Carlo Most Markov Transitions are diffusive, concentrating around the initial point such that the corresponding MArkov chains linger in small neighborhoods of the typical set for long periods of time. In order to maximize the utility of our computational resources, we need coherent Markov Transitions that are able to glide across the typical set towards new, unexplored neighborhoods. In order to make large jumps away from the initial point, and into new, unexplored regions of the typical set, we need to exploit information about the geometry of the typical set. HMC is the unique procedure for automatically generating this coherent exploration for sufficiently well-behaved target distributions. Introduce some intuition to motivate how we can generate the desired exploration by carefully exploiting the differential structure of the target probability density. Discuss the procedure with the complete construction of the Hamiltonian Markov transition. A vector field is the assignment of a direction at every point in parameter space. When those directions are aligned with the typical set, we can follow them like guide posts, generating coherent exploration of the target distribution. When the sample space is continuous, a natural way of encoding this direction information is with a vector field aligned with the typical set. A vector field is the assignment of a direction at every point in parameter space, and if those directions are aligned with the typical set then they act as a guide through this neighborhood of largest target probability. By construction this, we follow the direction assigned to each at point for a small distance. Continuing this process traces out a coherent trajectory through the typical set that efficiently moves us far away from the initial point to new, unexplored regions of the typical set as quickly as possible. The gradient of the target pdf encodes information about the geometry of the typical set, but not enough to guide us through the typical set by itself. Follwing along the gradient instead pulls us away from the typical set and towards the mode of the target density. In order to generate motion through the typical set we need to introduce additional structure that carefully twists the gradient into alignment with the typical set. Need to construct a vector field aligned with the typical set using only information that we can extract from the target distribution. The natural information is the differential structure of the target distribution which we can query through the gradient of the target probability density. The gradient defines a vector field in parameter space sensitive to the structure of the target distribution. Unfortunately, that sensitivity is not sufficient as the gradient will never be aligned with the typical set. Following the guidance of the gradient pulls us away from the typical set and towards the mode of the target density. Without enough transverse momentum to balance againts the gravitational attraction of the planet, a satellte will still crash into the planet. On the other hand, if the satelite is given too much momentum then the gravitational attraction will be too weak to capture the satelite in a stable orbit, which will instead abandon the planet for the depths of space. When we introduce exactly the right amount of momentum to the physical system, the equations describing the evolution of the satelite define a vector field aligned with the orbit. The subsequent evolution of the system will then trace out orbital trajectories. The gradient can direct us towards only parameterization sensitive neighborhoods like that around the mode, and not the parameterization-invariant neighborhoods within the typical set. To utilize the information in the gradient we need to complement it with additional geometric constraints, carefully removing the dependence on any particular parameterization while twisting the directions to align with the typical set. If we add the right amount of momentum, then the momentum will exactly balance against the gradient information, and the corresponding dynamics of the system will be conservative. The key is twisting the gradient vector field into a vector field aligned with the typical set, and hence once capable of generating efficient exploration, is to expand our original probabilistic system with the introduction of auxiliary momentum parameters. There is only one procedure for introducing auxiliary momentum with such a probabilistic structure: Hamiltonian Monte Carlo. 4.6.1.0.2 Phase Space and Hamilton’s Equations A defining feature of conservative dynamics is the preservation of volume in position-momentum phase space. For example, althou dynamics might compress volumes in position space, the corresponding volume in momentum space expands to compensate and ensure that the total volume is invariant. Such volume-preserving mapping are also known as shear Transformations. Conservative dynamics in physical systems requires that volumes are exactly preserved. As the system evolves, any compression or expansion in position space must be compensated with a respective expansion or compression in momentum space to ensure that the volume of any neighborhood in position-momentum phase space is unchanged. In order to mimic this behavior in our probabilistic system we need to introduce auxiliary momentum parameter, \\(p_n\\), to complement each dimension of our target parameter space, \\(q_n\\), expanding the D-dimensional into a 2D-dimensional phase space. $ q_n ( q_n, p_n ) $ Moreover, these auxiliary momentum have to be dual to the target parameters, transforming in the opposite way under any reparameterization so that phase space volumes are invariant. Having expanded the target parameter space to phase space, we can lift the target distribution onto a joint probability distribution on phase space called the canonical distribution. Then, the choice of a conditional probability distribution over the auxiliary momentum, $ (q, p) = (p q) (q) $, which ensures that if we marginalize out the momentum we immediately recover our target distribution. By constructing a probability distribution on phase space that marginalizes to the target distribution, we ensure that the typical set on phase space projects to the typical set of the target distribution. In particular, if we can construct trajectories that efficiently explore the joint distribution (black) they will project to trajectories that efficientyl explore the target distribution (green). The canonical density \\(\\pi(q, p)\\) does not depend on a particular choice of parameterization, and we can write it in terms of an invariant Hamiltonian function, \\(H(q, p)\\), $ (q,p) = ( - H(q, p) ) $ Because \\(H(q, p)\\) is independent of the details of any parameterization, it captures the invariant probabilistic structure of the phase space distribution and, the geometry of its typical set. The value of the Hamiltonian at any point in phase space is called the energy at that point. Hamiltonian decomposes into two terms, Density over the auxiliary momentum, \\(K(p,q)\\) is called the kinetic energy (unconstrained and specified by the implementation), while the term corresponding to the density of the target distribution, \\(V(q)\\) is known as the potential energy (determined by the target distribution). $ H(q,p) (q,p) = - (p q) - (q) K(p,q) + V(q) $ Because the Hamiltonian captures the geometry of the typical set, it should be able to use it to generate a vector field oriented with the typical set of the canonical distribution. The desired vector field can be generated from a given Hamiltonian with $ = + {p} = {p}, ; ; ; ; ; = - {q} = - {q} - {q} $ By channeling the gradient through the momentum instead of the target parameter directly, Hamilton’s equations twist differential information to align with the typical set of canonical distribution. Following the Hamiltonian vector field for some time, \\(t\\), generates trajectories \\(\\phi_t (q, p)\\), that rapidly move through phase space while being constrained to the typical set. Projecting these trajectories back down onto the target parameter space finally yields the efficient exploration of the target typical set for which we are searching. Every Hamiltonian Markov Transition is comprised of a random lift from the target parameter space onto phase space (light red), a deterministic Hamiltonian trajectory through phase space (dark red), and a projection back down to the target parameter space (light red). Need a mechanism for introducing momentum to a given point in thetarget parameter space. To lift an initial point in parameter space into one on phase space, we simply sample from the conditional distribution over the momentum, \\(p \\sim \\pi(p \\vert q)\\). Once on phase space we can explore the joint typical set by integrating Hamilton’s equations for some time, \\((q,p) \\rightarrow \\phi_t (q,p)\\). By construction these trajectories coherently push the Markov transition away from the initial point, and neighborhoods that we have already explored, while staying confined to the joint typical set. After integrating Hamilton’s equations, we can return to the target parameter space by simply projecting away the momentum, \\((q,p) \\rightarrow q\\). "],["population-monte-carlo.html", "4.7 Population Monte Carlo", " 4.7 Population Monte Carlo Population-Based MCMC. population이란? 즉, multiple 체인을 돌린다는 것. 로컬 트랩 문제를 회피하기 위해서. 이는 이하의 조건을 탄다. 각 체인이 가지는 분포는 서로 달라야 하지만, 아예 그 분포들끼리 아예 무관해서는 안된다. (2번을 위해) 체인들 간에 정보의 교환이 이루어져야 한다. 서로가 가지고 있는 정보를 공유하는 것으로 체인들의 타겟 분포로의 수렴을 가속시키는 것이 이 여러개의 체인의 존재 목적이기 때문이다. 이는 패러렐 컴퓨팅을 가능하게 한다. Embarrassingly Parallel MCMC 언급하였듯 위의 다중체인은 서로간에 정보의 교환이 이루어져 속도가 다소 느려지는 측면이 분명히 생긴다. 이러한 발목잡힘을 피하기 위해 체인간의 정보교환이 전혀 없는 MCMC. 타겟분포 \\(f(x)\\)에서 샘플 생산. f(x_1 , , x_N)=_{i=1}^N f_i (x_i) 이때 f(x)와 f_i(x_i) 사이에는 반드시 연관이 존재해야 하며, 적어도 하나의 i값에 대해 f(x) = f_i(x_i)를 만족해야 함. N은 체인의 갯수, 혹은 population의 size. 4.7.1 Adaptive Direction Sampling 가장 기본적인 형태. 여러개의 체인을 돌린 후 특정 방향으로 다른 샘플을 이동시키는 방법. 4.7.2 Conjugate Gradient MC 4.7.3 Parallel Tempering temperature 개념 사용. 따라서 T_1&gt;T_2&gt;&gt;T_n=1 이어야 하며, T_n은 타겟분포에 상응한다. 이때 각 temperature에 대응하는 pdf f_i(x) ( - ) simulated tempering은 temperature를 생산해서 해당 temperature를 accept한 후 해당 temperature로 이동. Parallel Tempering은 n개의 온도에 대응하는 체인 n개를 만들어 각각의 온도에 대응시킨 후 n개의 체인을 병렬적으로 돌림. 이때 높은 온도인 T_1은 넓은 space를 탐색하고 낮은 온도인 T_n은 좁은 space를 탐색. 이때 태생의 온도에 따라 탐색시키는 것은 탐색 효율이 떨어지므로 체인이 일정 경과할 때마다 swapping (Exhange) opertation을 하여 출신 이외의 다른 체인과 서로 바꿈. 이 바꿈은 인접한 체인과만 발생. 이를 통해 상황이 맞아떨어지면 T_1 출신의 탐색자가 T_n까지 가서 탐색하는 상황도 나올 수 있음. 이러한 swapping opertation을 Auxiliary Variable Generation까지 확장시킨 것이 Exhange Algorithm. Proceeds: 1. local MH. MH 알고리즘을 통해 X_i^{(t)} X_i^{(t+1)} 로 업데이트. 2. Exhange 스텝. 인접한 체인이 2개라면 각각으로 이동할 확률을 0.5로 잡고, 1개라면 서로밖에 교환 못하니 이는 1로 설정. 이후 해당 교환을 accept할지 여부는 확률 { 1, ; ; ; }에 따라 결정. 이외면 교환 안 한 채로 남긴다. 이를 고차원으로 확장시키면 Sequential Parallel Tempering. 4.7.3.0.1 Exchange Algorithm 4.7.4 Evolutionary MC Combinatorial Optimization Problem, 무수히 많은 조합 중에 local optimal 찾는 문제에서 가장 자주 사용되는 방법론인 genetic Algorithm, 의 MC 버전이라고 볼 수 있다. 다른 모델, 크로스오버 (스위치), 뮤테이션. 다만 Evolutionary MC 자체는 Combinatorial Optimization을 푼다기보단 Multimodal 문제를 푸는 용도에 가까움. genetic Algorithm과 Evolutionary MC 둘 모두 local mode를 찾는 알고리즘, local Trap 문제를 해결하는 알고리즘. 또한 Evolutionary MC는 local trap 해결 이외에 Variable Selection에도 아주 유용하며 자주 사용됨. 이 Variable Selection 자체가 일종의 Combinatorial Optimization 문제로 생각될 수 있음. 가용 변수의 조합 중 어떤 조합을 써야 효율이 잘 나올 것인가를 고민하는 거니까. Evolutionary MC 또한 temperature 개념을 사용하므로 parallel Tempering과 유사. 온도 설정과 온도에 따른 체인 설정 후 탐색은 완전히 똑같지만, parallel tempering의 swapping operator가 바로 인접한 체인과의 교환만 발생했던 반면 evolutionary MC는 crossover operator를 사용하여 인접한 것만이 아닌 모든 체인 중에 교환할 체인을 선정. 이렇게 교환 범위가 넓기 때문에 parallel tempering보다 성능이 훨씬 좋음. Proceeds: 1. Mutation은 자주 일어나는 일이 아니므로 확률로 판정하여 Mutation과 Crossover 둘 중 하나만 일어나도록 함. Mutation의 확률은 q_m, Crossover의 확률은 1-q_m. 이 확률 q_m은 uniform에서 획득. 잘 모르면 0.5. 1. Local MH. 이는 genetic Algorithm에서의 Mutation에 대응. x_1 , , x_N 중 1개를 균일확률로 선정해서 뽑고 뽑은 그 x_k 1개에 대응하는 y_k = x_k + e_k 를 만들어 이로 대체할 것을 제안. 이는 (1, r_m)의 확률로 accept되고, 이때 accpetance ratio r_m = { - } . 이때 뒤쪽의 fraction은 proposal density에 해당. 2. Crossover Operator: (1) 현 population x에서 x_i를 고른 후, (2) x / x_i 에서 x_j 를 고른다. 이때 x_j를 선정할 때 ( - )의 확률에 따라 선정한다. k는 앞서 선정되었던 i를 제외한 모든 수. (3) e=x_i - x_j, y_i = x_j + r e. 이때 r은 direction에 해당하며 모든 실수일 수 있고, r의 선정은 density f(r) r ^{d-1} f(x_i + re)에 따른다. (4) 여기서 획득한 y_i로 x_i를 교체한 후 이렇게 교체한 집합을 새로운 population으로 삼는다. 3. Exchange 스텝. 이를 통해 X 내용물을 구성하는 \\(x_i\\), 즉 각 온도 H(x_i) 에 대응하는 chromosome들의 교환이 이루어짐. Exchange Operator: parallel tempering과 마찬가지로 양옆의 것들과 확률 판정해서 교환. 4.7.5 Sequential Parallel Tempering "],["stochastic-approximation-monte-carlo.html", "4.8 Stochastic Approximation Monte Carlo", " 4.8 Stochastic Approximation Monte Carlo Neural Network의 Multimodality issue. 패러미터는 p개인데 equation은 q개. q가 p보다 훨씬 작아서 관계성이 식으로밖에 나오지 않음. 따라서 identifiability 이슈가 발생하기 때문. gradient descent 방법으로 최적해 찾아내다 뭐 이렇게 말을 하는데 사실 이걸 최적해로 말할 수 없음. combinatorial optimization에도 로컬 최적해가 다수 존재하고 글로벌 최적해 파악이 어려우므로 유사하게 multimodal issue 존재. SAMC는 과거의 샘플 전체를 이용하므로 Monte Carlo긴 하나 MCMC는 아님. 에너지 펑션 \\(-\\log \\psi (x)\\), $ (x)$는 unnormalized density. 이 에너지 펑션을 U(X)로 둔다면 이는 결국 y축에 대응되는 값. 이 U(X)를 파티션한다. 이렇게 나눈 영역 E_i들은 각각 고유한 weight를 보유하고 있음. 난수 1개를 샘플링했을 때 이 난수가 영역 E_i에서 나왔다고 한다면, 이 영역 E_i에 배정되었던 weight를 줄이고 다른 모든 구간의 weight를 높인다. 이를 통해서 모든 영역에서의 고른 샘플링을 기대할 수 있음. 이 weight의 증감량을 얼마만큼 시킬 것인지가 stochastic Approximation을 통해서 얻어짐. 이를 모두 반영한 식은 c (x) _{i=1}^m I(X E_i) 이때 이의 denominator는 E_i에 대한 weight이며, 이것이 결국 c에 대응하는 부분인데 영역이 partition되었으므로 이를 mixture distribution의 형태로 나타내줌. 여기서 ^{(i)}=log g_i이고, g_i = _{E_i} (x). 따라서 이를 exp의 승으로 만드는 것은 log를 취했던 것을 벗기는 작업임. _i : 구간 i에서 얼마만큼의 샘플을 생산할 것인가. 보통은 구간별 생산 갯수를 동일하게 하는 것을 목적으로 함. 이에 의해서 보통 이 알고리즘을 flat-histrogram 알고리즘이라고 부름. gain factor sequence { k }{k=0}^. 이는 Stochastic Approximation에 필요함. gain factor _k = . 이때 t_0는 prospected value. 따라서 첫 이터레이션때는 1로 유지되고, 이후에 빠르게 감소함. 따라서 weight도 이에 영향받아 첫번째 이터레이션때는 일정하게 유지되다가 이후에 떨어진다. t_0가 크면 수렴이 빠르지만 그렇기 때문에 지나치게 커서는 안됨. 이렇게 생산한 샘플을 그냥 써서는 안되고, 획득한 샘플 ^{(t)}와 weight (theta^{(t)} )= g_i를 사용하여 Importance Sampling을 한번 해서 그 결과물을 사용해야 함. ???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????? 3시 18분 "],["review.html", "4.9 Review", " 4.9 Review 4.9.1 Wk01 Write the inverse-CDF method and state how we can generate random numbers from \\(W(\\alpha, \\beta)\\). inverse-CDF method는, 우리가 알다싶이 \\(0 \\le F(x) \\le 1\\). 즉 \\(F(x) \\sim U(0,1)\\)나 다름없다. \\(u \\sim U(0,1)\\)을 하나 샘플링. 이는 \\(F(x)\\)의 range와 일치한다. 따라서 \\(F(x)=u \\iff x=F^{-1} (u)\\). Weibull Dist: shape parameter \\(k\\), scale parameter \\(\\lambda\\)에 대해 $ f(x) = \\[\\begin{cases} \\left( \\dfrac {k}{\\lambda} \\right) \\left( \\dfrac {x}{\\lambda} \\right)^{k-1} \\exp{- \\left( \\dfrac{x}{\\lambda} \\right) } &amp; x\\ge 0 \\\\ \\\\ 0 &amp; o.w. \\end{cases}\\] $ let quantity \\(X\\) is “time-to-failure.” $ \\[\\begin{align} F(x) = 1- \\exp{- \\left( \\dfrac{x}{\\lambda} \\right) } &amp;= u \\\\ \\Longrightarrow x &amp;= \\lambda \\left[ -\\log (1-u) \\right]^{\\tfrac{1}{k}} \\end{align}\\] $ State the RS algorithm. target density \\(f(x)\\) proposal density \\(g(x)\\) envelope density \\(e(x) = c \\ast g(x)\\) evaluate easy easy generate difficult easy cover all areas of \\(f(x)\\), in all parameter supports, \\(f(x) \\le e(x)\\) State how we can generate random numbers using RS. generate sample \\(x\\) from \\(g(x)\\) generate \\(u \\sim U(0,1)\\) 위에서 언급하였듯, envelope는 proposal의 상수배이며, envelope는 target보다 항상 크므로 \\(\\dfrac{f(x)}{e(x)}\\)는 항상 0 이상이며 1 이하. 이는 곧 \\(U(0,1)\\)에서 생산되는 값과 동일한 분포를 지니며, \\(e(x)\\) 아래의 값들 중 \\(f(x)\\) 아래에도 해당하는 값들은 곧 \\(f(x)\\)에서 생산된 난수라고 볼 수 있었다. 따라서 if \\(u \\le \\dfrac{f(x)}{e(x)}\\), sample \\(x\\)를 accept, 이외엔 reject. 4.9.2 wk03 State one iteration of squeezing RS. \\(f(x)\\)가 evaluate 자체는 가능한데 그거조차도 비용이 expensive 한 경우를 가정. \\(f(x)\\)가 샘플 generate가 어려우니 RS를 쓰는건데 평가 비용조차 높으니 샘플링 과정이 비효율적일 수밖에 없음. 따라서 squeeze function \\(s(x)\\)를 설정하여 확실하게 \\(f(x)\\)에 속하는 샘플들은 먼저 우선선발 시켜서 패스시키고, 우선선발이 아닌 샘플들만 \\(f(x)\\)를 직접 사용해서 조건을 통과하였는지 여부를 체크. 당연하지만 \\(s(x)\\)는 모든 support에서 \\(f(x)\\)보다 작아야 하며, evaluate 비용이 cheap해야 함. proceeds: 1. \\(Y \\sim g\\)에서 샘플링. 2. sample \\(u \\sim U(0,1)\\). 3. if \\(U \\le \\dfrac {s(Y)}{e(Y) = c \\ast g(y)}\\), keep \\(Y\\). 4. if not, whether if \\(U \\le \\dfrac {f(Y)}{e(Y)}\\), keep \\(Y\\). 5. both are not, reject \\(Y\\). 이때 \\(s(x)\\)를 생산하기 위해 Talyor Series Expansion을 사용하는 경우 잦다. State the adaptive RS. Make envelope function \\(e(x)\\) adaptively to the shape of \\(f(x)\\). adaptive RS 자체에는 제약이 있다. 이는 log concave function인 density에만 적용이 가능하다는 것. 즉슨 multimodal인 density에는 적용이 불가하다. 이 제약을 해소하기 위해 Adaptive Rejection Metropolis Sampling이 존재. mode가 필수라는 게 RS 자체가 mode가 필수라는 소리인가? State the Importance Sampling \\(\\mu = E[h(x)]\\). 이때 \\(h(x)\\)는 \\(x\\)의 함수이며, \\(x\\)는 \\(f(x)\\)를 따르므로 \\(h(x)\\)의 기댓값 계산 또한 이를 따르지만, 이는 기댓값 계산에서 density를 \\(g\\)로 바꾸고 이의 각 확률에 발생하는 값들을 \\(h \\ast \\dfrac{f}{g}\\)로 바꾸는 것과 다르지 않음. 이는 \\(f\\)에서 샘플 생산이 힘들때 \\(f\\)를 거치지 않고도 샘플을 생산하여 기댓값을 계산할 수 있다는 점에서 빛을 발함. 즉 확률은 \\(g\\)를 참조하고, 이 확률에서 발생하는 값들이 있을 것이고, 이 값들을 다시 한번 함수에 넣어서 역변환하면 \\(f\\)의 확률에서 발생했었을 각 값들을 획득하는 것이 가능하다는 소리. 이러한 역변환 함수에서 \\(f, g\\)가 차지하는 부분을 weight라고 부르는 것이고, 이를 weight의 총합으로 표준화하면 standardized weight. \\(g\\)의 support가 \\(f\\)의 그것을 다 덮을 필요는 없음. 하지만 1. \\(\\dfrac{f}{g}\\)는 bounded여야 하고, 가장 중요하게, \\(g\\)는 \\(f\\)보다 꼬리가 두꺼워야 함. 이는 극단적인 \\(x\\)값이 나왔을 때 \\(g\\)의 확률이 \\(f\\)보다 지나치게 작으면 해당 부분에서의 weight가 너무너무 커져서 다른 샘플 실값들의 영향력을 다 잡아먹어버리는 weight-degeneracy가 발생해버리기 때문. State the polar methods for generating normal random variable. $ X, Y {} N(0,1)$ f(x,y) = {2} ( - (x^2 + y^2 )) \\(\\theta \\sim U(0, 2\\pi)\\), \\(R^2 \\sim \\EXP (\\tfrac{1}{2})\\). \\(X\\)와 \\(Y\\)를 모은 만큼의 샘플이 \\(N\\)을 따른다. 4.9.3 wk04, 05 State the effect of proposaldensity \\(g\\) in IS. 과도한 variability를 피하기 위해, \\(\\dfrac{f}{g}\\)로 설정하고 \\(g\\)가 \\(f\\)보다 두꺼운 꼬리를 지니도록 설정해야 함. \\(g\\)가 너무 작으면 weight-degeneracy. \\(h\\)가 너무 작다면, \\(\\dfrac{f}{g}\\)를 크게 할 수 있는 \\(g\\)를 선정한다. State the antithetic sampling, Control Variate, and Rao-Balckwellization. antithetic: use two id UE, whose \\(Corr(\\hat \\mu_1 , \\hat \\mu_2)&lt;0\\). State one iteration of sampling Importance Resampling. 왜 옛날에는 variance reduction 하고 요즘엔 안함? "],["else.html", "4.10 Else", " 4.10 Else 4.10.1 Hw4. Rasch Model $$ \\[\\begin{align} L(\\theta, \\beta) &amp;= \\prod_{k=1}^n \\prod_{i=1}^p \\left\\{ \\dfrac{\\exp(\\theta_k + \\beta_i)}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{y_{ki}} \\left\\{ \\dfrac{1}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{1-y_{ki}} \\\\ \\pi (\\theta, \\beta \\vert y) &amp;= \\pi(\\theta) \\pi(\\beta) \\ast \\prod_{k=1}^n \\prod_{i=1}^p \\left\\{ \\dfrac{\\exp(\\theta_k + \\beta_i)}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{y_{ki}} \\left\\{ \\dfrac{1}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{1-y_{ki}} \\end{align}\\] $$ \\[ 0&lt;\\left\\{ \\dfrac{\\exp(\\theta_k + \\beta_i)}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{y_{ki}} &lt;1, \\; \\; \\; \\; \\; 0&lt;\\left\\{ \\dfrac{1}{1 + \\exp(\\theta_k + \\beta_i)}\\right\\}^{1-y_{ki}}&lt;1 \\] underflow problem. log 취하면 해결. \\[ \\pi(\\theta_k) \\sim N(0, \\sigma^2), \\; \\; \\; \\; \\; \\pi(\\sigma^2 ) \\sim IG(0.001, 0.001) \\] update \\(\\theta_k , k=1, \\cdots, n\\)\" \\[ \\log (r) = \\log \\pi (\\theta_k &#39; \\vert y, \\beta^{(t)}. \\theta_{-k}^{(t)} - \\log \\pi (\\theta^{(t)} \\vert y, \\beta^{(t)}. \\theta_{-k}^{(t)} \\] if \\(\\log U &lt; min(\\log(r), 0))\\), accept. else, reject. 4.10.2 DA) Example: MVN for DA 알고리즘, I-step과 P-step이 존재. 4.10.2.1 1. I-Step $$ \\[\\begin{alignat}{4} Y_2 \\vert Y_1 &amp;\\sim N &amp;&amp; \\Big( \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (Y_1 - \\mu_1) , &amp;&amp; \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1} \\Sigma_{12} \\Big) \\\\ Y_{i, mis} \\vert Y_{i, obs}, \\mu, \\Sigma &amp;\\sim N_{dim(Y_{mis}^{(i)})} &amp;&amp; \\Big( \\mu_{mis}^{(i)} + \\Sigma_{mis, obs}^{(i)} \\Sigma_{obs, obs}^{-1} (Y_{i, obs} - \\mu_{i, mis}^{(i)}) , &amp;&amp; \\Sigma_{mis,mis}^{(i)} - \\Sigma_{mis,obs}^{(i)}[\\Sigma_{obs,obs}^{(i)}]^{-1} \\Sigma_{obs,mis}^{(i)} \\Big) \\end{alignat}\\] $$ 상기의 conditional pdf로 우리는 \\(Y_{i, mis}\\)를 impute 가능. for \\(i=1, \\cdots, n\\), \\(Y_{i, mis} \\vert Y_{i, obs}\\) 에서 \\(Y_{i, mis}\\)를 draw. 4.10.2.2 2. P-Step 베이지안 분석을 위해선 prior가 필요. 여기서 prior는 이하로 설정하자. \\(q\\)는 known integer이며, \\(q=p\\)인 상황에 이는 \\(\\Sigma\\)에 대한 Jefferey’s prior. \\[ \\pi (\\mu, \\Sigma) \\propto \\vert \\Sigma \\vert^{-\\tfrac{q+1}{2}} \\] 위와 같이 식들을 구성하였을 때, com 데이터에 대한 posterior distribution \\(\\pi(\\mu, \\Sigma \\vert Y_1 , \\cdots, Y_n)\\)는 이하와 같이 characterized 가능. 이는 inverse-Wishart 분포. \\[ \\Sigma \\vert Y_1 , \\cdots, Y_n \\sim \\dfrac{1}{\\vert \\Sigma \\vert^{-\\tfrac{q+n}{2}}} \\exp \\left\\{ -\\dfrac{1}{2} tr\\left( \\Sigma^{-1} S \\right) \\right\\} \\] 이렇게 획득해온 패러미터들을 사용해 \\(\\mu\\)의 post를 구하면 이하와 같다. \\[ \\mu \\vert \\Sigma, Y_1 , \\cdots, Y_n \\sim N_p \\left( \\bar Y \\dfrac {1}{n} \\Sigma \\right) \\] \\(\\Sigma \\vert Y_1 , \\cdots, Y_n\\)에서 \\(\\Sigma\\) 를 생산 이후 \\(\\mu \\vert \\Sigma, Y_1 , \\cdots, Y_n\\)에서 \\(\\mu\\) 를 생산 4.10.3 Bayesian adaptive clinical trial with delayed outcomes 4.10.3.1 Continual Reassessment Method Clinical Trial: Toxicity -&gt; Efficacy -&gt; Confirmation 희귀병 케이스에서는 도즈 레벨을 1~n까지 정해둔 후, 샘플을 slice 하여 1번 subsample에 도즈1 투입. 유효하면 (1차시에 3명 투입했다고 하고 그중에 1명이 독성 나왔으면 독성 확률은 1/3. 해당 여부로 도즈2로 넘어갈 것인지를 판단) 2투입. 2에서 문제 생기면 1로 복귀하고 2번 subsample에 도즈1 투입해봐서 유효한지 검증. 이렇게 모든 서브샘플에 도즈레벨 오가면서 투입해보고 최적 도즈레벨 결정. 이때 CRM을 시작하기 전 대략적으로 이정도의 도즈레벨이 최적 도즈레벨일 것이라는 예측 (Skeleton)을 정하고 CRM을 시작함. delay outcome 이전 환자들의 evaluation이 끝나기 전에 (evaluation period가 경과하기 전이나, 결과가 나오기 전에) 환자 풀이 증가하는 상황 이 상황에서는 관측이 더 된 환자보다 덜 된 환자에서 outcome이 발생할 확률이 높음. 9개월 누워있던 놈보다 2개월 누워있던 놈이 12개월 경과 전에 뭔가 변화를 보이기 쉽다는 소리. 이때 아직 결과를 관찰하지 못한 환자들을 mis로 지정. 이 상황은 누워있던 기간이 결과 발생 여부라는 variant와 직결되어 있으므로 NMAR. 그러니까, 여기서 결과값은 outcome이 발생했는지 안했는지, 그리고 variable은 환자나 누워있던 기간. 위에서 언급했듯 누워있던 기간이 짧으면 변이확률 높음. 따라서 각각에 대해 다른 survival function을 적용하여 각각의 다른 확률 뽑아낸 후 이거 기반으로 DA 진행하면 해결. 4.10.4 NMAR의 종류 \\(m_i\\)는 missing indicator. \\(Y_i\\)가 mis면 1. \\[ f(M, Y \\vert X, \\theta, \\psi) = \\prod_{i=1}^n f(m_i , y_i \\vert x_i , \\theta, psi) \\] interested in direct relationship b/w \\(X\\) and \\(Y\\), rather than in subpopulation defined by missing-data pattern. Selection Model characterize \\(y\\) missing mechanism \\(f(m_i, y_i \\vert x_i, \\theta, \\psi) =\\) \\(f_y(y_i \\vert x_i, \\theta)\\) \\(\\ast f_{m \\vert y}(m_i \\vert x_i, y_i, \\psi)\\) \\(f(m_i, y_i \\vert x_i, \\xi, \\psi) =\\) \\(f(y_i \\vert x_i, \\xi)\\) \\(\\ast f(m_i \\vert x_i, \\xi)\\) Pattern Mixture model mis 데이터의 다른 패턴들에 의해 정의되는 각각 다른 strata에서의 \\(y_i \\vert x_i\\)의 분포 probability of different patterns in missingness missing의 다른 패턴에 따라 \\(x_i\\)가 결정이 되고, 그 \\(x_i\\)를 기준으로 놓았을 때의 \\(y_i\\)의 분포가 궁금. 4.10.5 wk10) Bayesian Model Selection 해당 문제는 prior을 어떻게 고르느냐에 따라서 해결될 수 있음. 이하는 해당 문제에 대한 다양한 해결책들. 4.10.5.1 1. Spike-and-Slab prior let \\(X_{n \\times p} , Y_{n \\times 1}\\), then \\(Y = X \\beta\\), where \\(\\beta_{p \\times 1}\\). p가 많다, 즉 배리어블이 많다는 이야기는 실제 각각의 x의 정보량이 중첩될 가능성이 큼. 그러면 수학적으로는 x’x가 full rank matrix가 아닐 것이며, 이는 곧 몇몇 변수들 간에 서로간의 의존관계가 강하여 의미없는 정보를 포함하는 변수들이 많아질 것. 이러한 의미없는 변수를 삭제하고 실제로 필요한 변수들만 골라내어 y에 대한 inference를 하고 싶음. 이것이 모델 셀렉션 문제이며 이걸 베이지안적으로 풀어내는 것이 곧 Bayesian Model Selection. 무지성 prior로는 \\(\\pi(\\beta) \\sim N(0, \\sigma^2)\\)가 쓰이지만, 이로는 variable selection이 불가. \\(\\beta\\) 중 하나의 component가 0에 가깝게 나왔다고 한들 이것을 0으로 판정할 indicator가 없기 때문. (HPD interval을 구성해서 이것이 0을 포함하면 내치는 식의 방법도 있지만 일단은.) 따라서 다른 prior를 필요로 함. 바로 여기서 사용되는 것이 Spike-and-Slab prior. 이름에서 알 수 있듯이 mixture distribution을 prior로 사용함. \\(\\beta\\)의 component가 spike 부분에 포함되면 이를 0으로 판정함, 즉 not significant로 판정. 이의 역은 slab part. 이는 곧 prior로 variable selection을 한다는 이야기이다. 즉 이 상황에서는 prior가 패널티로 들어간 것이 된다. 정의적으로 엄밀하게 패널티는 아니지만 사실상의 패널티로 작동. 패널티 term (error penalty)으로 골라내는 것은 full context에서 많이 사용? 이때는 라플라스 프라이어를 쓰고, 노멀을 프라이어 주면? 패널티 텀을 베이지안 인퍼런스로 연결지어서 생각할 수 있지만, 이 배리어블 셀렉션은 디멘션 셀렉션과 연관이 있기 때문에 위와는 정확하게는 다른 개념? variable selection에는 3가지 방법: 1. 패널티 텀 2. mixture prior 3. 컴퓨테이셔널 (reversible jump, dimension selection) (gradient descent는 아님!) spike 파트 (아래에서는 \\((1-\\lambda)N(0, \\sigma^2)\\)) 에는 double exponential을 쓰거나, normal 을 변형해서 사용함. 혹은 극단적으로는 dirac 분포 (point mass) 를 쓸 수도 있음. 이하에서 예시로 제시된 수식은 SS prior이며, 이는 spike와 slab 모두 Normal을 사용하였음. \\[ \\pi(\\beta) \\sim (1-\\lambda)N(0, \\sigma^2) + \\lambda N(0, \\omega \\sigma^2), \\; \\; \\; \\; \\; w \\gg 1 \\] 위에서 \\(\\sigma^2\\)는 spike variance, \\(w \\sigma^2\\)는 slab variance. 여기서 \\(\\lambda\\)가 취할 수 있는 값은 0 아니면 1. 확인할 수 있듯이 1이면 slab part, 즉 significant하고, 0이면 역으로 not significant. 우리는 이에 MCMC 알고리즘을 적용하게 되며, 따라서 MCMC 샘플로 계산을 하면 해당 샘플에서 0인 propotion과 1인 비율이 나오게 될 것. 이때 1인 비율이 0.5 이상이면? 해당 component (변수) 는 significant 하다고 결론짓는 것이 가능하다. $$ \\[\\begin{alignat}{3} \\pi(\\beta, \\lambda, \\sigma^2, \\omega \\vert y, x) &amp;\\sim \\pi(\\beta \\vert \\lambda, \\sigma^2, \\omega ) \\pi(\\lambda, \\sigma^2, \\omega) &amp;&amp; f(y \\vert x, \\beta, \\lambda, \\sigma^2, \\omega) \\\\ &amp;\\sim \\pi(\\beta \\vert \\lambda, \\sigma^2, \\omega ) \\pi(\\lambda, \\sigma^2, \\omega) &amp;&amp;L( x, \\beta, \\lambda, \\sigma^2, \\omega \\vert y) \\\\ \\\\ \\pi(\\lambda) &amp;\\sim BETA(1,1), \\; \\; \\; \\pi(\\sigma^2) \\sim \\cdot \\tag{1}, \\; \\; \\; \\omega \\sim 1 + GAM(\\alpha, \\beta) \\end{alignat}\\] $$ 이때 \\(\\sigma^2\\)는 우리가 임의로 fixed 해서 given으로 잡거나, 위처럼 prior로 해서 시뮬레이션 중에 생산되도록 할수도 있다. 여기선 \\(\\dfrac{1}{U(4,100)}\\)을 사용. accept를 하기 위해선 위를 돌리면 됨. 이는 Stochastic Search Variable Selection (SSVS)라고 불림. 이는 GS를 통하여 패러미터를 sequentially update. 이의 결과값은 다음과 같으며, 프로세스는 그 다음과 같다. \\[ (\\beta_1 , \\cdots, \\beta_p, \\lambda_1, \\cdots, \\lambda_p, \\sigma^2, \\omega) \\] Proceeds: update model parameter \\(\\beta_i^{(t+1)} \\sim \\pi( \\beta_{i} \\vert y, x, \\beta_{-i}^{(t)}, \\lambda_{i}^{(t)}, {\\sigma^2}^{(t)}, \\omega^{(t)} )\\) where \\(\\beta_{-i}^{(t)} = \\left( \\beta_{1}^{(t+1)}, \\cdots, \\beta_{i-1}^{(t+1)}, \\beta_{i+1}^{(t)}, \\cdots, \\beta_{p}^{(t)} \\right)\\) Simple GS로도 가능하고, MH-within-Gibbs로도 가능함 update \\(\\lambda_I^{(t+1)} \\sim \\pi(\\lambda_i \\vert y, x, \\lambda_{-i}^{(t)}, \\beta_{i}^{(t+1)}, {\\sigma^2}^{(t)}, \\omega^{(t)} )\\) where $P( {i}^{(t+1)} = 1 y, x, {-i}^{(t)}, _{i}^{(t+1)}, {2}{(t)}, ^{(t)} )= {a+b} BER( {a+b} ) $. \\(a = \\pi( \\beta_{i}^{(t+1)} \\vert y, x, \\lambda_{i}^{(t+1)}=1, {\\sigma^2}^{(t)}, \\omega^{(t)} )\\). \\(b = \\pi( \\beta_{i}^{(t+1)} \\vert y, x, \\lambda_{i}^{(t+1)}=0, {\\sigma^2}^{(t)}, \\omega^{(t)} )\\), update \\(\\sigma^2\\) update \\(\\omega\\) 4.10.5.2 2. Horseshoe prior (Scale-mixture prior) distribution에서 scale이란 Variance. 위와 동일 케이스 가정. 그 경우 $$ \\[\\begin{alignat}{4} \\pi(\\beta \\vert y, x) \\propto f(y \\vert x, \\beta) \\pi(\\beta), \\; \\; \\; \\; \\; &amp;\\pi(\\beta) &amp;&amp; \\sim N(0, \\sigma^2) \\\\ \\Longrightarrow &amp;\\pi(\\beta_i \\vert \\tau, \\lambda_i) &amp;&amp; \\sim N(0, \\tau^2 \\lambda_i^2) \\end{alignat}\\] $$ where \\(pi(\\tau^2), \\pi(\\lambda_i^2) \\sim Cauchy^{+}(0,1)\\) 이때 common variance component \\(\\tau\\)는 각 component마다 공유하는 1개의 variance component, 그리고 각 component마다 indiviually 고유한 individual parameter variance component \\(\\lambda_i\\)를 설정한 것. 4.10.6 Autologistic model 4.10.7 wk10) Bayesian Model Averaging 해당 상황에서 연구자는 다양한 모델 예측 후보를 생각해볼 수 있음. 보통은 프로세스를 거쳐 이 모델들 중의 하나를 선택하게 됨. 하지만 완벽한 모델이라는 건 (보통) 존재할 수 없음, 어떤 모델 후보를 선택하든 해당 후보가 내포하고 있는 uncertainty가 존재하며 이를 수용하게 됨. 따라서 모델을 선택한다는 것은 동시에 over-confidence inference 문제를 발생시킨다. 따라서 모델 후보군을 하나만 골라야 한다는 고정관념을 벗어나 다양한 모델 후보군들 각각을 동시에 반영하자. 이 동시 반영할 때 각 모델이 내포하고 있는 확률 (uncertainty)에 의해 각 모델의 반영 정도를 가감하게 된다. BMA는 패러미터 estimate를 획득할 때, 이러한 model uncertainty를 설명하기 위한 일관된 메커니즘을 제공한다. given 데이터 \\(D\\), posterior prob of \\(\\mathcal{M}_k\\) \\(= P(\\mathcal{M}_k \\vert D) = \\dfrac{L(D \\vert \\mathcal{M}_k) P(\\mathcal{M}_k)}{\\sum_{k=1}^k L(D \\vert \\mathcal{M}_k) P(\\mathcal{M}_k)}\\). 이때 marginal likelihood under \\(\\mathcal{M}_k)\\) \\(L(D \\vert \\mathcal{M}_k) = \\int L(D \\vert \\theta \\mathcal{M}_k) \\pi(\\theta \\vert \\mathcal{M}_k) d \\theta\\)이며, integral 안의 수식은 posterior of model의 상수배 In brief, BMA는 model uncertainty를 설명할 수 있는 posterior density를 획득하기 위해 integral을 취한다 (model에 대해 마지널化). (각각의 모델에 대한 model uncertainty를 구한다) 이를 통해 최종적으로 posterior sample 같은 경우에는 각 모델 별로 model probability에 posterior sample의 probability를 다 더해준 값이 실제로 우리의 \\(\\theta\\)에 대한 post가 된다. 즉슨 BMA란 다양한 모델 후보군들이 존재할 때, 그 어떤 상황에서도 robust inference를 가능케 하는 tool이 바로 BMA. 4.10.7.1 Ex: BMA-CRM CRM 시작 전에 skeleton 정하고 시작하는 건 자명. 근데 이 skeleton이 잘못 선정되었다면 제대로된 도즈 selection이 불가능해지므로 skeleton의 선정이 잘못되어 있다면 이는 치명적임. 상식적으로, 하나의 skeleton으로만 도즈 셀렉션을 진행한다면 문제가 생길 확률은 당연히 높음. 이런 리스크를 희석하기 위해 skeleton을 다수를 정하고 CRM을 시작하면 이런 문제를 다소 회피할 수 있지 않을까? 이때 이 각각의 스켈레톤 하나하나를 모델로 인식한다. 이 각각의 모델에 따라서 CRM을, 즉 도즈레벨을 업데이트할 확률을, 즉 업데이트 할 때 패러미터 evaluation을 하는데, 그때 나오는 패러미터 값과 그 각각의 모델 probability를 비교하여 그 모델 averaging을 해주면 그 어떤 상황이 와도 굉장히 robust 한 값을 획득할 수 있을 것. Main research question Justification for your research question (why is it important to answer the question?) Data source Data analysis Summary of the data analysis results and conclusion Appendix (if needed) – R scripts (scripts or codes for any other software) 1 – Technical details regarding the statistical tools used in the analysis 비교적 오랜 기간 데이터가 잘 정립된 MLB 기록 활용을 위해 수업 시 활용하였던 Lahman package(R) 사용 야구는 공격과 수비로 이루어지며, 따라서 분석을 진행함에 있어 야구는 야구 스탯들 간에 상당한 수준의 선형성이 보장되어 이 타자의 가치 = α∗Batting+β∗Fielding, α,β는 임의의 패러미터 선수의 타격능력은 이른바 클래식 스탯으로 불리는 다양한 구형 통계량으로도 표기하는데 문제가 없지만, 수비능력은 선수별로 할당된 수비범위가 천차만별이며 선수가 수비시도를 하지 않으면 선수의 실책으로 이어지지 않는다는 점 때문에 선수 개별의 수비능력이 객관적으로 평가되기 시작한 것은 구장의 정보를 훨씬 자세하게 담을 수 있게 된 2000년대 중반부 이후부터의 이야기. 최신야구에서 선수 수비능력의 평가는 주로 Ultimate Zone Rating (UZR) 로 이루어지며, 해당 스탯은 ARM (달린거리), DPR (병살), RngR (수비커버리지), ErrR (에러빈도) 등 수비에 관련된 스탯을 총집합하여 망라하는 고밀도 스탯이다. 그러나 해당 스탯의 계산은 2002년 BIS (Baseball Info Solutions)라는 회사에서 제공하는 유료 데이터셋과 15년 도입된 스탯캐스트 데이터에 거의 전적으로 의존하고 있다. 스탯캐스트 데이터는 민간에 어느정도는 공개되어 있어 접근이 불가능하지 않지만 (https://baseballsavant.mlb.com/statcast_leaderboard), BIS 데이터는 접근이 어렵다. 이와 같은 이유로 선수별 수비 스탯을 구하기를 시도하기보단 팀별 수비력에 대한 척도인 Defensive Efficiency Ratio (DER)를 사용하고자 한다. 최대 12개의 팀인만큼 팀 간의 차이를 포착하기 쉬우며, 12개의 팀으로 표준화되니만큼 아웃라이어들이 평준화되어 전반적인 경향성으로 기능하는 것을 기대해볼 수 있다. DER의 수식은 다음과 같다. DER의 계산법은 이하와 같다. DER=1−(Hits+Reached.On.Error−HomeRunsPlate.Appearance−BB(Walks)−Strike.Out−Hit.By.Pitch−HomeRuns) Teams %&gt;% mutate(., DER = 1-((H + E - HR)/((AB + SF) - SO - HR))) %&gt;% ##select(., yearID, teamID, Rank, SO, SOA) select(.,yearID, teamID, franchID, Rank, G,DER) -&gt; Teams_DER 물론 같은 팀에 속했다는 이유만으로 모든 선수들에게 동급의 수비스탯을 배정하는 건 합리적이라고 말하기 어렵다. 팀의 수비에 기여하는 정도가 높은 선수가 있다면 낮은 선수도 있을 것이 자명하기 때문이다. 따라서 팀별로 획득한 DER을 수비에 대한 클래식 스탯인 각 선수의 Fielding Percentage(FPCT) 나 Range Factor(RF)의 비율로 스케일링해서 부여하자. 두 스탯은 각각 수비능력과 개인의 수비범위 평가를 위해 시도되었던 스탯들이지만, 전자는 개인의 수비범위가 좁으면 더 좋은 값이 나온다는 한계, 후자는 공이 본인 위치로 떨어졌을 때 스탯계산에서 이득을 본다는 한계를 넘지 못해 좋은 스탯으로는 평가받지 못했던 값들이다. 그러나 팀 단위로 한번 수비력을 표준화한 후 팀 내에서 상대적인 기여도를 보는 식으로 보정이 한 번 들어갔으므로 어느정도 기준선으로서는 기능할 것이라고 기대된다. 이를 위해 선수생활 중의 메인 수비포지션 지정하고 해당 포지션에서의 통계량만 사용. "],["mva.html", "Chapter 5 MVA ", " Chapter 5 MVA "],["overview-of-mva-not-ended.html", "5.1 Overview of mva (not ended)", " 5.1 Overview of mva (not ended) Find relationships b/w \\(\\pmb x_p, \\pmb y_q\\), e.g., * Response variables (variable directed) * PCA * Factor Analysis * mv Regression * Cannonical Correlation Analysis * Experiment units (individual directed) * Discriminant Analysis * Cluster Analysis * MANOVA Multivariate techniques tend to be exploratory. * i.e. not hypothesis testing type Experimental units must be independent. Time series data are not appropriate for this course. 5.1.1 Notation Variable \\(y_1 , \\cdots, y_p\\) One observation \\(\\pmb y &#39; = (y_1 , \\cdots, y_p )\\) Data Matrix \\(Y_{n \\times p} = \\begin{bmatrix} \\pmb y &#39; = (y_1 , \\cdots, y_p ) \\\\ \\vdots \\\\ \\pmb y &#39; = (y_1 , \\cdots, y_p ) \\end{bmatrix}\\) Random Samples: Suppose we intend to collect n sets of measurements on p variables, but not been observed yet. If $x_1 ‘, , x_n’ $ are independent observation from pdf \\(f(\\pmb x) = f(x_1, \\cdots, x_p)\\), then $x_1 ‘, , x_n’ $ are said to be rs from \\(f(\\pmb x)\\). rvec \\(\\pmb X = X_{p \\times n} = \\begin{bmatrix} \\pmb x_1 \\\\ \\vdots \\\\ \\pmb x_p \\end{bmatrix}\\) mean vector \\(\\pmb \\mu = E (\\pmb x) = \\begin{bmatrix} \\mu1 \\\\ \\vdots \\\\ \\mup \\end{bmatrix}\\) Covariance Matrix \\(\\Sigma\\) Correlation Matrix \\(\\rho\\), \\(\\rho_{ij} = \\tfrac{\\sigma_{ij}}{\\sigma_{ii}\\sigma_{jj}}\\) * Correlation measures linear association. * Correlation is 0 if symmetric non-linear association exists. 5.1.2 Summary Statistics Sample Mean Vector \\(\\bar X=\\) estimate of \\(\\pmb \\mu\\) Sample Covariance Matrix Sample Correlation Matrix 5.1.3 Statistical Inference on Correlation $ H_0: = 0 $ test stat \\(T=\\dfrac {r \\sqrt{n-2}}{\\sqrt{1-r^2}} \\sim t_{n-2}\\), where \\(r=Corr(x,y)\\) and \\(x \\sim N_2, y \\sim N_2\\) Notes: 1. Correlation measures a linear relationships 2. it is still difficult to get a CI for \\(\\rho\\). 5.1.3.0.1 CI for \\(\\rho\\) Fisher’s Method: \\(100(1-\\alpha)%\\) CI for \\(\\rho\\) $=((r) ) Ruben’s Method let \\(u=z_{\\alpha/2}, r^\\ast = \\dfrac {r}{\\sqrt{1-r^2}\\) $ \\[\\begin{align*} a &amp;= 2n-3-u^2 b &amp;= r^\\ast \\ast \\sqrt{(2n-3)(2n-5)} c &amp;= (2n-5-u^2} \\ast (r^\\ast)^2 - 2u^2 \\end{align*}\\] $ set \\(ay^2 - 2by +c =0\\), then root of this equation will be \\(y_1, y_2 = \\dfrac{b}{a} \\pm \\dfrac {\\sqrt{b^2-ac}}{a}\\). 이때 \\(100(1-\\alpha)%\\) CI for \\(\\rho\\) \\(=\\left[ \\dfrac{y_1}{\\sqrt{1+y_1^2}, \\dfrac{y_2}{\\sqrt{1+y_2^2}, \\right]\\) * 이때, input은 \\(n, \\alpha, r\\), output은 \\(\\rho\\)의 CI. 5.1.4 Standardization 5.1.5 Missing Value Treatment "],["multivariate-nomral-wk2.html", "5.2 Multivariate Nomral (wk2)", " 5.2 Multivariate Nomral (wk2) 5.2.1 Overview let rvec \\(Z=(Z_1 , \\cdots, Z_p)&#39;, Z_i \\overset {iid}{\\sim} N(0,1)\\). then \\(X=(X_1 , \\cdots, X_p)&#39; = A_{k \\times p} Z + \\pmb \\mu_{k \\times 1}\\) follows MVN. at here, if \\(rank(A)=p(\\le k), AA&#39; = \\Sigma\\), then \\(X \\sim N_p (0, \\Sigma)\\). notation: \\(\\pmb y \\sim N_p (\\pmb \\mu , \\Sigma)\\) rvec \\(\\pmb y &#39; = [y_1 , \\cdots , y_p ]\\) have Multivariate Normal Distribution, if \\(\\sum_{i=1}^p a_i y_u = \\pmb a&#39; y\\) has Univariate Normal Distribution, for every possible set of values for the elements in \\(\\pmb a\\). pdf for \\(f(\\pmb y) = \\dfrac{1}{(2\\pi)^p {\\vert \\Sigma \\vert}^{1/2}} \\exp \\left\\{ -\\dfrac{1}{2} (\\pmb y - \\pmb \\mu)&#39; \\Sigma^{-1} (\\pmb y - \\pmb \\mu) \\right\\}\\). Ellipsoid: - path of \\(\\pmb y\\) values yielding a constant height for the density, i.e., all \\(\\pmb y\\) s.t. \\(\\{ (\\pmb y - \\pmb \\mu)&#39; \\Sigma^{-1} (\\pmb y - \\pmb \\mu)=c^2 \\}\\). Standard Normal Distribtion: - \\(\\pmb z = \\left( {\\Sigma^{1/2}}\\right)^{-1} (\\pmb y -\\pmb \\mu) \\sim N_p (\\pmb 0, I_p)\\), where \\(\\left( {\\Sigma^{1/2}}\\right)^{-1}\\) satiesfy $ = ( {{1/2}}){-1} ( {{1/2}}){-1}$. Property of \\(\\Sigma\\): 1. symmetric Matrix 2. positive definite Matrix 3. $Cov(A y + b) = A A’ $. ※ if \\(A\\) is symmetric and non-singular, then \\(A=CC&#39;\\), where \\(C\\) is lower triangular Matrix. This is called Cholesky Decomposition of \\(A\\). \\(E(X)=\\mu, Cov(X)=AA&#39; = \\Sigma\\) \\(M_X (t) = \\exp (t&#39; \\mu + \\dfrac {1}{2} t&#39; \\Sigma t\\) if \\(\\Sigma=AA&#39;\\) is non-singular Matrix \\(\\iff rank(A)=p\\) \\(\\Sigma = Cov(X)\\)는 symmetric, n.n.d. 이상의 \\(X\\)에 대해 이하는 TFAE. 1. \\(X \\sim N_p (0, \\Sigma)\\). 2. 3. 4. 5. 5.2.2 Spectral Decomposition if \\(A\\) is symmetric, non-singular, then \\(A=E \\Lambda E&#39;\\), where \\(\\lambda_i\\) are ev (\\(\\lambda_1 \\ge \\cdots \\ge \\lambda_n\\)), and \\(\\pmb e_i\\) are evec (\\(E&#39;E = I_p)\\). This is called Spectral Decomposition of \\(A\\). $ = \\[\\begin{bmatrix} \\lambda_1 &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; &amp; \\lambda_n \\end{bmatrix}\\] , ; ; ; ; ; E= $ 이때 \\(\\Sigma = E \\Lambda E&#39; = E \\Lambda \\Lambda^{1/2} E&#39; = E \\Lambda E&#39; E \\Lambda^{1/2} E&#39; = \\Sigma^{1/2} \\Sigma^{1/2}\\). Center &amp; Axis of ellipsoids of \\(\\{ (\\pmb y - \\mu)&#39; \\Sigma^{-1} (\\pmb y - \\mu)=c^2 \\}\\): * center: \\(\\pmb \\mu\\) * axis : \\(\\pm c \\sqrt{\\lambda_i \\pmb e_i}\\) Square root Matrix: let symmetric non-negative Matrix \\(A_{p \\times p}\\). the square root matrix of \\(A\\) is defined as \\(A^{1/2} = E \\Lambda^{1/2} E&#39;\\), where $ ^{1/2} = \\[\\begin{bmatrix} \\sqrt{\\lambda_1} &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; &amp; \\sqrt{\\lambda_n } \\end{bmatrix}\\] $ Negative Square Root Matrix: Let \\(A\\) be of full rank and all of its \\(\\lambda_i\\) are positive, in addition to symmetry. \\(A^{-1/2} = E \\Lambda^{-1/2} E&#39;\\), where $ ^{-1/2} = \\[\\begin{bmatrix} \\dfrac{1}{\\sqrt{\\lambda_1}} &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; &amp; \\dfrac{1}{\\sqrt{\\lambda_n }} \\end{bmatrix}\\] $ Generalized Inverse: let \\(A\\) be a non-negative M. if \\(\\lambda_1&gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_r &gt; 0 = \\lambda_{r+1} = \\cdots = \\lambda_{p}\\), i.e., not full rank, then the Moore-Penrose generalized inverse of \\(A\\) is given by $ A^{-} = e_1 e_1 ’ + + e_r e_r ’ $ where $ ^{-} = \\[\\begin{bmatrix} \\dfrac{1}{\\lambda_1} &amp; &amp; &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; &amp; &amp; \\\\ &amp; &amp; \\dfrac{1}{\\lambda_n } &amp; &amp; &amp; \\\\ &amp; &amp; &amp; 0 &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix}\\] $ Marginal Distribtion: $ \\[\\begin{align*} \\pmb y \\sim N_p (\\pmb \\mu , \\Sigma) &amp;\\Longrightarrow y_i \\sim N(\\mu_i, \\sigma^{ii}), \\; \\; \\; i= 1, \\cdots, p \\\\ &amp;\\not \\Longleftarrow \\end{align*}\\] $ 5.2.3 Properties of MVN linear combination of the components of \\(\\pmb y\\) are normally distributed. any subset of \\(\\pmb y\\) have MVN. conditional distribution of the components of \\(\\pmb y\\) are MVN: $ y N_p(, ) a ’ y N( a ’ , a ’ a ) $ $ y N_p(, ) , ; ; A_{n times p} = \\[\\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{np} \\end{bmatrix}\\] A y N_n(A , A A’) $ 즉슨 dimension 변화 if $ y N_p(, )$, and cvec \\(\\pmb d\\), then $ y + d N_p(+ d , )$. If we partition y, μ, S ! ! as follows Let 1 11 2 ~ ( , ) p y y N y μ é ù ê ú = ê ú S ê ú ë û !\" ! ! ! with 5.2.4 \\(\\Chi^2\\) distribution if \\(\\pmb z \\sim N_p ( \\pmb 0 , I_p )\\), then \\(\\pmb z &#39; \\pmb z = \\sum_{i=1}^p z_i^2 \\sim \\Chi_p^2\\). if $ y N_p(, )$, then $(y - )’ ^{-1} (y - ) _p^2 $ the \\(N_p(\\pmb \\mu , \\Sigma)\\) distribution assigns probability \\(1-\\alpha\\) to the solid ellipsoid \\(\\left \\{ \\pmb y : (\\pmb y - \\pmb \\mu)&#39; \\Sigma^{-1} (\\pmb y - \\pmb \\mu) \\le \\chi_p^2 (\\alpha) \\right \\}\\), where \\(\\chi_p^2 (\\alpha)\\) denotes upper \\((100 \\ast \\alpha)\\) th percentile of the \\(\\chi_p^2\\) distribution. 5.2.5 Linear Combination of Random Vectors 5.2.6 Multivariate Normal Likelihood 5.2.7 Sampling Distribtion of \\(\\bar {\\pmb y}, S\\) let rvec $ y_1, y_n N_p(, )$. \\(\\bar {\\pmb y} \\sim N_p (\\pmb \\mu , \\dfrac{1}{n} \\Sigma)\\) (n-1) S $ Wishart distribution, with \\(df=n-1\\) * \\(S\\) is random Matrix, e.g., Wishart is distribution of rM. \\(\\bar {\\pmb y} \\perp S\\). 5.2.7.0.1 Wishart Distribtion ※ \\(\\dfrac {\\sum (x_i - \\barx )^2}{\\sigma^2} = \\dfrac {S^2} {\\dfrac{\\sigma^2}{n-1}} \\sim \\chi_{n-1}^2\\), i.e., \\(\\sum (x_i - \\barx )^2 = (n-1)S^2 \\sim \\sigma^2 \\ast \\chi_{n-1}^\\) for let rvec $ y_1, y_n N_p(, )$, $ \\[\\begin{align*} \\sum_{i=1}^n(\\pmb y - \\pmb \\mu)(\\pmb y - \\pmb \\mu)&#39; &amp;\\sim W_p (n, \\Sigma) \\\\ \\\\ (n-1)S^2 = \\sum_{i=1}^n(\\pmb y - \\bar {\\pmb y} )(\\pmb y - \\bar {\\pmb y} )&#39; &amp;\\sim W_p (n-1, \\Sigma) \\end{align*}\\] $ if \\(A \\sim W_p (n, \\Sigma), B \\sim W_p (m, \\Sigma)\\), and \\(A \\perp B:\\) \\(A+B \\sim W_p (n+m, \\Sigma)\\) if \\(A \\sim W_p (n, \\Sigma)\\), then \\(CAC&#39; \\sim W_p (n, C \\Sigma C&#39;)\\) if \\(A \\sim W_p (n-1, \\Sigma)\\), \\(f(A)\\), where gamma function. 5.2.7.0.2 MV t-Distribtion ※ univariate t-Distribtion \\(t=\\tfrac{\\tfrac{U}{\\sigma}}{\\sqrt{\\tfrac{V}{nu}}} \\sim t_{\\nu}\\), where \\(U \\sim N(0, \\sigma^2), V \\sim \\chi_{\\nu}^2\\), and \\(U \\perp V\\). let $ y = (y_1, , y_n)’ N_p(, )$, and \\(V \\sim \\chi_{\\nu}^2\\), and \\(\\pmb y \\perp V\\). assume rvec \\(\\pmb t = (t_1 , \\cdots, t_p)&#39;\\),\\(t_i = \\tfrac {\\tfrac{y_i - \\mu_I}{\\sigma_i}{\\sqrt{V/\\nu}}, i=1, \\cdots, p\\) * Note that each \\(t_i \\sim t\\). at here, joint distribution of \\(\\pmb t\\) is called MV t-distribution, with \\(df=\\nu\\) and matrix parameter \\(\\Sigma\\). denote this distribution by 5.2.7.0.3 Dirichlet Distribution ※ is MV generalization of \\(BETA\\). let $ y D_p(1 , {p+1})$ * parameters: \\(\\{\\nu_i, i=1, \\cdots, p+1\\}\\) * pdf: f(y) = _{i=1}^p y_i^{v_i - 1}$ ????????????????????????????????????????????????? 5.2.7.0.4 CLT let $ y_1 , , y_n {} , &lt; $. then $ \\[\\begin{align*} \\sqrt {n} (\\hat {\\pmb y} - \\pmb \\mu) &amp;\\overset {d} {\\rightarrow} N_p (\\pmb 0 , \\Sigma) \\\\ n (\\hat {\\pmb y} - \\pmb \\mu)&#39; S^{-1} (\\hat {\\pmb y} - \\pmb \\mu) &amp;\\overset {d} {\\rightarrow} \\chi_p^2 \\end{align*}\\] $ 5.2.8 Assessing Normality 5.2.8.0.1 1. Univariate Marginal Distribtion 5.2.8.0.1.1 a. Q-Q Plot ※ Sample quantile vs. quantile of N distribution let order statitics, or sample quantiles \\(x_{(1)} \\le \\cdots \\le x_{(n)}\\). the proportion of sample below \\(x_{(j)}\\) is approximated by \\(\\tfrac{j-\\tfrac{1}{2}}{n}\\). the quantiles \\(q_{(j)}\\) for std. N are defined as $ P(z q_{j)}) = {-}^{q{(j)}} ( - z^2 ) dz $ if the data arise from a N population, then \\((\\sigma \\ast q_{(j)} + \\mu \\congruent x_{(j)}\\). Similarly, the pairs \\((q_{(j)}, x_{(j)})\\) will be linearly related. Proceeds: 1. get \\(x_{(1)} \\le \\cdots \\le x_{(n)}\\) from original obs. 2. calculate probability values \\(\\tfrac{j-1/2}{n}, \\; \\; j= 1, \\cdots, n\\) 3. calculate standard normal quantities \\(q_{(1)}, \\cdots, q_{(n)}\\) 4. plot the pairs of observations $(q_{(1)}, x_{(1)}), , \\((q_{(n)}, x_{(n)})\\) Checking the straightness of Q-Q plot: * using corr coef * Hypothesis tesiting: \\(H_0: \\rho=0\\), $T= t_{n-2} 5.2.8.0.1.2 b. others Shapiro-Wilks Test: Test of correlation coefficient b/w \\(x_{(j)}, r_{(j)}\\). \\(r_{(j)}\\) is function of the expected value of standard normal order statistics, and their \\(Cov\\). Kolmogorov-Smirnov Test Compare cdf’s: If the data arise from a normal population, the differences are small. $ T = _x F(x) - S(x) $ where cdf \\(F(x)\\), empirical cdf \\(S(x)\\). Skewness Test skewness \\(\\sqrt{b_1} = \\tfrac{\\sqrt{n} \\sum_{i=1}^n (x_i - \\bar x)^3} {\\left[ \\sum_{i=1}^n (x_i - \\bar x)^2 \\right]^{\\tfrac{3}{2}}}\\) When the population is normal, the skewness = 0. Kurtosis Test: kurtosis \\({b_2} = \\tfrac{n \\sum_{i=1}^n (x_i - \\bar x)^4} {\\left[ \\sum_{i=1}^n (x_i - \\bar x)^2 \\right]^{3}}\\) When the population is normal, the kurtosis is 3. Lin and Mudholkar (1980): $ Z = ^{-1}(r) = ( ) $ where \\(r\\) is the sample \\(corr\\) of \\(n\\) pair \\((x_i , q_i), \\; \\; i=1, \\cdots, n\\) with \\(q_i = \\tfrac {1}{n} \\left( \\sum_{i \\not = j} x_j^2 - \\tfrac{1}{n-1} \\left( \\sum_{i \\not = j} x_j\\right)^2 \\right)^{\\tfrac{1}{3}}\\). if the data arise from a normal population, \\(Z \\sim N(0, \\tfrac 3 n)\\). 5.2.8.0.2 2. Bivariate Normality ※ If the data are generated from a multivariate normal, each bivariate distribution would be normal. Scatter Plot the contours of bivariate normal density are ellipses. The pattern of the scatter plot must be near elliptical. Squared Generalized Distances ※ \\(\\pmb y \\sim N_p (\\pmb \\mu, \\Sigma) \\; \\; \\; \\Longrightarrow \\; \\; \\; (\\pmb y - \\pmb \\mu)&#39; \\Sigma^{-1} (\\pmb y - \\pmb \\mu) \\sim \\chi_p^2\\). it means, for bivariate cases, Squared Generalized Distances \\(d_j^2 = (\\pmb x_j - \\hat {\\pmb x})&#39; S^{-1} (\\pmb x_j - \\hat {\\pmb x}) \\sim \\chi_2^2\\). Chi2 Plot (Gamma Plot) \\(d_1^2 , \\cdots, d_n^2\\) should behave like \\(\\chi_2^2\\) rv. 1. order the squared distances \\(d_{(1)}^2 \\le \\cdots \\le d_{(n)}^2\\) 2. calculate the probabilitt values \\(\\tfrac{j-1/2}{n}\\), \\(j=1,\\cdots, n\\) 3. Calculate quantiles of \\(\\chi_2^2\\) distribution \\(q_{(1)}, \\cdots, q_{(n)}\\). 4. Plot the pairs \\((q_{(j)}, d_{(j)}^2 ), \\; \\; j=1, \\cdots, n\\) where \\(q_{(j)} = \\chi_2^2 \\left( \\tfrac{j-1/2}{n} \\right)\\) The plot should resemble a straight line through the origin having slope 1. 5.2.8.0.3 2. Multivariate Normality Practically, it is usually sufficient to investigate the univariate and bivariate distributions. Chi-square plot is still useful. When the parent population is multivariate normal, and both \\(n\\) and \\(n-p\\) are greater than 25 or 30, the squared generalized distance \\(d_{1}^2 \\le \\cdots \\le d_{n}^2\\) should behave like \\(\\chi_p^2\\). 5.2.9 Power Transformation $ x^= \\[\\begin{cases} \\tfrac{1}{x}, &amp; \\lambda = -1 \\tag{\\text{Reciprocal Transformation}}\\\\ \\tfrac{1}{\\sqrt{x}}, &amp; \\lambda = -\\tfrac{1}{2} \\\\ \\ln(x), &amp; \\lambda = 0 \\\\ \\sqrt{x}, &amp; \\lambda = \\tfrac{1}{2} \\\\ x, &amp; \\lambda = 1 \\tag{\\text{No Transformation}} \\end{cases}\\] $ Examine Q-Q plot to see whether the normal assumption is satisfactory after power transformation. 5.2.9.0.1 Power Transformation $ x^() = \\[\\begin{cases} \\tfrac{x^\\lambda - 1}{\\lambda}, &amp; \\lambda \\not = 0 \\\\ \\ln(x), &amp; \\lambda = 0 \\end{cases}\\] $ at here, find \\(\\lambda\\) that maximizes $ l() = - ln+ () _{j=1}^n x_j $ where \\(\\hat{x_j}^{(\\lambda)} = \\tfrac{1}{n} \\sum_{j=1}^n x_j^{(\\lambda)}\\) x^{()} is the most feasible values for normal distribution, but not guaranteed to follow normal distribution. * Transformation (Box-Cox) usually improves the approximation to normality. * Trial-and-error calculations may be necessary to find \\(\\lambda\\) that maximizes \\(l(\\lambda)\\) * Usually, change \\(\\lambda\\) values from -1 to 1 with increment 0.1. * Examine Q-Q plot after the Box-Cox transformation. 5.2.9.0.2 nqplot, contour plot, cqplot, cqplot and box-cox plot "],["inference-about-mean-vector-wk3.html", "5.3 Inference about Mean Vector (wk3)", " 5.3 Inference about Mean Vector (wk3) 5.3.1 Overview Recall: univariate case \\(x_1 , \\cdots, x_n \\overset {iid} {\\sim} N(\\mu, \\sigma^2)\\) \\(H_0 : \\mu = \\mu_0\\) $ \\[\\begin{alignat*}{2} \\text{test stat } &amp;t &amp;&amp;=\\tfrac{\\bar X - \\mu_0}{\\tfrac{S}{\\sqrt{n}}} &amp;\\overset{H_0}{\\sim} t_{n-1} \\\\ &amp;t^2 &amp;&amp;=\\tfrac{(\\bar X - \\mu_0)^2}{\\tfrac{S^2}{n}} &amp;\\overset{H_0}{\\sim} F_{1, \\; n-1} \\end{alignat*}\\] $ reject \\(H_0\\) if as below, which means upper \\((100-\\alpha)\\)th percentile. $ \\[\\begin{alignat*}{1} \\tfrac{(\\bar X - \\mu_0)^2}{\\tfrac{S^2}{n}} = n(\\bar X - \\mu_0)\\tfrac{1}{S^2}(\\bar X - \\mu_0) &amp;&gt; F_{1,n-1}(\\alpha) \\end{alignat*}\\] $ therefore, with assumption \\(\\pmb X_1 , \\cdots, \\pmb X_n \\overset {iid} {\\sim} N_p (\\pmb \\mu , \\Sigma)\\), $ H_0 : = _0 $ $ \\[\\begin{alignat*}{3} \\text{Hotelling&#39;s }T^2 \\; \\; T^2 &amp;= n(\\bar {\\pmb X} - \\pmb \\mu_0)&#39; S^{-1} (\\bar {\\pmb X} - \\pmb \\mu_0) \\\\ &amp;\\overset{H_0}{\\sim} \\tfrac{(n-1)p}{(n-p)} F_{p,n-p} \\\\ \\iff \\; \\; \\tfrac {(n-p)} {(n-1)p} T^2 &amp;\\overset{H_0}{\\sim} F_{p,n-p} \\end{alignat*}\\] $ reject \\(H_0\\), if \\(T^2 &gt; \\tfrac{(n-1)p}{(n-p)} F_{p,n-p} (\\alpha)\\). assumption check: \\(\\pmb X_1 , \\cdots, \\pmb X_n \\overset{iid}{\\sim} N_p (\\pmb \\mu , \\Sigma)\\). 5.3.1.0.1 Remark stat \\(T^2\\)는 측정 단위에 invariant. proof) let $Y_{p } = C_{p p} X_{p } + d_{p } $. then $ \\[\\begin{align*} \\bar {\\pmb Y} &amp;= C \\bar {\\pmb X} + \\pmb d \\\\ \\\\ S_{\\pmb y} &amp;= CSC&#39;\\\\ \\\\ \\mu_y &amp;= E(\\pmb Y) \\\\ &amp;=C \\ast E(\\pmb X) + \\pmb d \\\\ &amp;= C \\pmb \\mu_0 + \\pmb d \\end{align*}\\] $ therefore, $ \\[\\begin{align*} T^2 &amp;= n(\\bar {\\pmb Y} - \\pmb \\mu_y)&#39; S_y^{-1} (\\bar {\\pmb Y} - \\pmb \\mu_y) \\\\ &amp;= n \\left[ C(\\bar {\\pmb X} - \\pmb \\mu_0) \\right]&#39; (CSC&#39;)^{-1} \\left[ C(\\bar {\\pmb X} - \\pmb \\mu_0) \\right] \\\\ &amp;= n (\\bar {\\pmb X} - \\pmb \\mu_0)&#39; C&#39; (C&#39;)^{-1} S^{-1}(C)^{-1} C(\\bar {\\pmb X} - \\pmb \\mu_0) \\\\ &amp;= n (\\bar {\\pmb X} - \\pmb \\mu_0)&#39; S^{-1}(\\bar {\\pmb X} - \\pmb \\mu_0) \\end{align*}\\] $ 여기서 \\(C^{-1}\\)이 존재한다는게 뭔수로 보장되는거지? 5.3.2 1. Confidence Region 5.3.2.0.1 Confidence Region region \\(R(\\pmb X)\\), is $100(1-) % $ CR of $ \\[\\begin{alignat*}{3} &amp;P \\left\\{ R(\\pmb X) \\text{ will cover the true } \\pmb \\theta \\right\\} &amp;&amp;= 1-\\alpha \\\\ &amp;P \\left\\{ n (\\hat {\\pmb X} - \\pmb \\mu)&#39; S^{-1}(\\hat {\\pmb X} - \\pmb \\mu) \\le \\tfrac{(n-1)p}{(n-p)} F_{p,n-p} (\\alpha) \\right\\} &amp;&amp;= \\end{alignat*}\\] $ the inequality \\(n (\\bar {\\pmb X} - \\pmb \\mu)&#39; S^{-1}(\\bar {\\pmb X} - \\pmb \\mu) \\le \\tfrac{(n-1)p}{(n-p)} F_{p,n-p} (\\alpha)\\) will define a region \\(R(\\pmb X)\\). The region is an ellipsoid centered at \\(\\bar {\\pmb X}\\). Testing \\(H_0 : \\mu = \\mu_0\\) at \\(\\alpha =.05\\) is equivalent to see whether \\(\\mu_0\\) falls within the CR. with ev \\(\\lambda_1 , \\cdots, \\lambda_p\\), evec \\(\\pmb e_1 , \\cdots, \\pmb e_p\\) of \\(S\\), CR Axis: \\(\\pm \\sqrt{\\lambda}\\sqrt{\\tfrac{(n-1)p}{(n-p)} F_{p,n-p} (\\alpha)} \\ast \\pmb e_i&#39;\\) CR half-length: $ $ 5.3.3 2. Simultaneous CI let \\(\\pmb X \\sim N_p (\\pmb \\mu, \\Sigma)\\), then linear combination \\(\\pmb a&#39; \\pmb X \\sim N_p (\\pmb a&#39; \\pmb \\mu, \\pmb a&#39; \\Sigma \\pmb a)\\) $ \\[\\begin{align*} t=\\dfrac{\\bar X - \\mu} {S / \\sqrt{n}} &amp;\\sim t_{n-1} \\tag{recall: univariate}\\\\ t= \\dfrac {\\pmb a &#39; \\bar X - \\pmb a &#39; \\pmb \\mu} {\\sqrt{\\pmb a &#39; S \\pmb a / n } } = \\dfrac {\\sqrt{n}(\\pmb a &#39; \\bar X - \\pmb a &#39; \\pmb \\mu)} {\\sqrt{\\pmb a &#39; S \\pmb a} } &amp;\\sim t_{n-1} \\tag{MV} \\end{align*}\\] $ therefore, \\(100(1-\\alpha)\\%\\) CI for \\(\\pmb a &#39; \\mu\\) (at here, \\(\\pmb a\\)is fixed) is \\(\\pmb a &#39; \\bar {\\pmb X} \\pm t_{n-1} \\dfrac {\\alpha} {2} \\dfrac{\\sqrt{\\pmb a &#39; S \\pmb a} } {\\sqrt{n}}\\). This is not a simultaneous CI. let each \\((a_1 , a_2), (b_1, b_2)\\) be CI for \\(\\mu_1 , \\mu_2\\). then simultaneous CI \\((a_1 , a_2), (b_1, b_2)\\) has confidence \\(95\\% \\ast 95\\% = 90.25\\%\\). need a wider interval. let rs \\(\\pmb X_1 , \\cdots, \\pmb X_n \\overset {iid} {\\sim} N_p (\\pmb \\mu , \\Sigma)\\). then, simultaneously for all \\(\\pmb a\\), the interval \\(\\pmb a &#39; \\bar {\\pmb X} \\pm \\sqrt{\\dfrac{n-1}{n} \\dfrac{p}{n-p} F_{p,n-p} (\\alpha) \\pmb a &#39; S \\pmb a}\\) will contain \\(\\pmb a &#39; \\pmb \\mu\\) with probability \\(1-\\alpha\\). $ \\[\\begin{alignat*}{3} \\because 1-\\alpha &amp;= P \\left[ n (\\bar {\\pmb X } - \\pmb \\mu)&#39; S^{-1} (\\bar {\\pmb X } - \\pmb \\mu) \\le (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\right] \\\\ &amp;= P \\left[ (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu)&#39; (\\pmb a&#39; S \\pmb a)^{-1} (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu) \\le \\dfrac{1}{n} (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\right] \\\\ &amp;= P \\left[ (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu)&#39; (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu) \\le \\dfrac{1}{n} (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\; \\ast \\; (\\pmb a&#39; S \\pmb a) \\right] \\tag{∵ Scalar} \\\\ &amp;= P \\left[ (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu)^2 \\le \\dfrac{1}{n} (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\; \\ast \\; (\\pmb a&#39; S \\pmb a) \\right] \\\\ &amp;= P \\left[ - \\sqrt{\\dfrac{1}{n} (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\; \\ast \\; (\\pmb a&#39; S \\pmb a)} \\le (\\pmb a&#39; \\bar {\\pmb X } - \\pmb a&#39; \\pmb \\mu) \\le \\sqrt{\\dfrac{1}{n} (n-1) \\dfrac {p}{n-p} F_{p,n-p} (\\alpha) \\; \\ast \\; (\\pmb a&#39; S \\pmb a)} \\right] \\end{alignat*}\\] $ 5.3.3.0.1 Simultaneous CI for \\(\\mu_i - \\mu_k\\) let \\(\\pmb a &#39; = [0,\\cdots, 0, a_i, 0, \\cdots, 0, a_k, 0, \\cdots, 0]\\). then as below, where \\(S =\\begin{bmatrix} S_{11} &amp; \\cdots &amp;S_{1p} \\\\ &amp; \\ddots &amp; \\\\ S_{p1} &amp; \\cdots &amp; S_{pp} \\end{bmatrix}\\). $ \\[\\begin{align*} \\pmb a &#39; \\pmb \\mu &amp;= \\mu_i - \\mu_k \\\\ \\pmb a &#39; S \\pmb a =S_{ii} -2 S_{ik} + S_kk \\end{align*}\\] $ therefore, the simultaneous CI for \\(\\mu_i - \\mu_k\\), is \\((\\bar x_i - \\bar x_k ) \\pm \\sqrt{\\dfrac{n-1}{n} \\dfrac{p}{n-p} F_{p, n-p}(\\alpha)S_{ii} -2 S_{ik} + S_kk}\\). at here, if we let \\(\\pmb a &#39; = [1, 0, \\cdots, 0]\\). then $ \\[\\begin{align*} \\pmb a &#39; \\pmb \\mu &amp;= \\mu_1\\\\ \\pmb a &#39; S \\pmb a =S_{11} \\end{align*}\\] $ therefore, the simultaneous CI for $_1 $, is \\(\\bar x_1 \\pm \\sqrt{\\dfrac{n-1}{n} \\dfrac{p}{n-p} F_{p, n-p}(\\alpha)S_{11}}\\). 5.3.4 3. Note: Bonferroni Multiple Comparison Bonferroni’s CI, \\(\\bar x_1 \\pm \\left\\{ t_{n-1} \\left( \\dfrac{\\alpha}{2p} \\right) \\right\\} \\sqrt{\\dfrac{S_11}{n}}\\), is more precise (narrower) than simultaneous CI. 5.3.5 4. Large Sample Inferences about a Mean Vector Recall mv CLT: let \\(\\pmb X_1 , \\cdots, \\pmb X_n {\\sim} ?(\\pmb \\mu, \\Sigma)\\) and for \\(n-p\\) large. then $ \\[\\begin{align*} \\sqrt{n} (\\bar {\\pmb X} - \\pmb \\mu) &amp;\\overset {d}{\\Longrightarrow} N_p (\\pmb 0, \\Sigma) \\\\ n (\\bar {\\pmb X} - \\pmb \\mu)&#39; S^{-1}(\\bar {\\pmb X} - \\pmb \\mu) &amp;\\overset {d}{\\Longrightarrow} \\chi^2_p \\end{align*}\\] $ when the sample size is large, the MVN assumption is less critical. therefore, let \\(\\pmb X_1 , \\cdots, \\pmb X_n {\\sim} ?(\\pmb \\mu, \\Sigma)\\). $ H_0: = _0 $ when \\(n-p\\) is large, the \\(H_0\\) is rejected if \\(n (\\bar {\\pmb X} - \\pmb \\mu)&#39; S^{-1}(\\bar {\\pmb X} - \\pmb \\mu) &gt; \\chi^2_p (\\alpha)\\). Note: \\((n-1) \\dfrac{p}{n-p} F_{p,n-p} )\\alpha \\simeq \\chi_p^2(\\alpha)\\), for large \\(n-p\\). CI: $ P = 1- $ the inequality $ n ({X } - )’ S^{-1} ({X } - ) _p^2 () $ will define a region, which means, \\(100(1-\\alpha) \\%\\) region. Simultaneous CI: let \\(\\pmb X_1 , \\cdots, \\pmb X_n {\\sim} ?(\\pmb \\mu, \\Sigma)\\) and for \\(n-p\\) large. then \\(\\forall \\pmb a\\), \\(100(1-\\alpha) \\%\\) simultaneous CI for \\(\\pmb a &#39; \\pmb \\mu\\) \\(= \\pmb a &#39; \\bar {\\pmb X} \\pm \\sqrt{ \\chi_p^2 (\\alpha)} \\sqrt{ \\dfrac{\\pmb a &#39; S \\pmb a} {n}}\\). Simultaneous CI for \\(\\mu_i\\) $ x_i $ Bonferroni’s CI for \\(\\mu_i\\) $ x_i z_{} $ - Bonferroni’s CI is more precise. as also. 5.3.6 1. Profile Analysis (wk4, 5) if \\(\\pmb X \\sim N_p (\\pmb \\mu, \\Sigma)\\), and the variables in \\(\\pmb X\\) are measured in the same unit, we may with to compare the means \\(\\mu_1 , \\cdots, \\mu_p\\) in \\(\\pmb \\mu\\). ex) repeated measure: a measurement is taken at the same experimental unit \\(p\\) successive times. A profile is a plot, connecting \\((i, \\mu_i), i= 1, \\cdots, p\\) Question: is the profile flat? $ \\[\\begin{align*} &amp;H_0: \\mu_1 = \\cdots = \\mu_p \\\\ \\iff &amp;H_0: C_1 \\pmb \\mu = \\pmb 0 , \\left[ C_1\\right]_{(p-1) \\times p} \\\\ \\iff &amp;H_0: C_2 \\pmb \\mu = \\pmb 0 , \\left[ C_2\\right]_{(p-1) \\times p} \\end{align*}\\] $ if \\(\\pmb X \\sim N_p (\\pmb \\mu, \\Sigma)\\), then \\(C \\pmb X \\sim N_{p-1} (C \\pmb \\mu, C \\Sigma C&#39;)\\), thus when \\(H_0 : C \\pmb \\mu = 0\\) is true, then \\(C \\bar X \\sim N_{p-1} (C \\pmb \\mu, C \\Sigma C&#39;)\\). test stat \\(T^2 = n (C \\bar {\\pmb X})&#39; (C S C&#39;)^{-1} (C \\bar {\\pmb X}) \\overset{H_0}{\\sim} (n-1) \\dfrac{p-1}{n-p+1} F_{p-1,n-p+1}\\) reject \\(H_0\\), if \\(T^2 &gt; (n-1) \\dfrac{p-1}{n-p+1} F_{p-1,n-p+1} (\\alpha)\\). **Note: \\(C_{(p-1) \\times p}\\) is not square, so there’s no inverse. thus \\(C\\) in test stat doesn’t be canceled. $ H_0 : C = 0 $ where \\(C_{q \\times p} (q \\le p)\\), and \\(rank(C)=q\\). then test stat \\(T^2 = n (C \\bar {\\pmb X})&#39; (C S C&#39;)^{-1} (C \\bar {\\pmb X}) \\overset{H_0}{\\sim} (n-1) \\dfrac{q}{n-q} F_{q,n-q}\\) which means \\(p-1\\) become \\(q\\). 5.3.7 2. Test for Linear Trend suppose \\(p\\) variables are measured across equally spaced time periods. Also suppose \\(H_0 : \\mu_1 = \\cdots = \\mu_p\\) is rejected. Question: Do the means fall onto a straight line? $ \\[\\begin{align*} &amp;H_0: \\mu_2-\\mu_1 = \\cdots = \\mu_p-\\mu_{p-1} \\\\ \\iff &amp;H_0: \\mu_3 -2 \\mu_2+\\mu_1 = 0, \\; \\; \\cdots, \\; \\; \\mu_p - 2 \\mu_{p-1} + \\mu_{p-2} = 0 \\\\ \\iff C_{(p-2) \\times p}, &amp;H_0: C \\pmb \\mu = \\pmb 0 \\end{align*}\\] $ at here, we acquire test stat \\(T^2 \\overset {H_0} {\\sim} (n-1) \\dfrac{p-2}{n-p+2} F_{p-2,n-p+2}\\). 5.3.8 3. Inferences about a Covariance Matrix let rs \\(\\pmb X_1 , \\cdots, \\pmb X_n \\overset {iid} {\\sim} N_p (\\pmb \\mu , \\Sigma)\\). $ H_0 : = _0 $ let \\(W = (n-1)S = \\sum_{i=1}^n (\\pmb X_i - \\bar {\\pmb X})(\\pmb X_i - \\bar {\\pmb X})&#39;\\). then $ ^ = ( )^{} _0^{-1} W ^{} , ; ; ; ; ; ; ; v=n-1 $ then calculate \\(L=-2 ln \\Lambda^\\ast \\; \\; \\; \\; \\; \\; \\; \\overset {H_0}{\\sim}\\) function of \\(\\chi^2\\)-distribution. Test for Sphericity (Test for no Correlation) $ H_0 : = ^2 I $ $ = $ function of \\(\\chi^2\\)-distribution. Test for Compound Symmetry if \\(\\Sigma = \\begin{bmatrix} \\sigma^2 &amp; \\rho &amp; \\cdots &amp; \\rho \\\\\\rho &amp; \\sigma^2 &amp; &amp; \\vdots \\\\ \\vdots &amp; \\rho &amp; \\ddots &amp; \\rho \\\\ \\rho &amp; \\cdots &amp; \\rho &amp; \\sigma^2 \\\\ \\end{bmatrix}\\), then \\(\\Sigma\\) has compound symmetry. $ H_0: $ Compute \\(\\Lambda = \\dfrac{\\vert S \\vert} {(S^2)^p (1-r)^{p-1} (1+ (p-1)r)}\\), where - $S^2 = {i=1}^p S{ii} $. - $r = {i&lt;j}^p S{ij} $. reject \\(H_0\\) if \\(Q&gt; \\chi_f^2 (\\alpha), \\; \\; \\; \\; \\; f= \\tfrac{p(p+1)-4}{2}\\) - \\(Q = -\\dfrac{(N-1)-p(p+1)^2(2p-3)}{6(p-1)(p^2+p-4)} \\ast \\ln\\Lambda\\). "],["comparison-of-several-mv-means-wk5.html", "5.4 Comparison of Several MV Means (wk5)", " 5.4 Comparison of Several MV Means (wk5) 5.4.1 Paired Comparison Recall: for univariate, let \\(X_i - Y_i = D_i \\sim N(\\delta, \\sigma_d^2)\\), \\(i=1, \\cdots, n\\) then for \\(H_0 : \\delta = 0\\), test stat \\(t = \\tfrac{\\bar D}{\\tfrac{S_d}{\\sqrt{n}}} \\overset {H_0}{\\sim} t_{n-1}\\). Assume independent rvec \\(\\pmb D_1 , \\cdots, \\pmb D_n \\sim N_p (\\pmb \\delta , \\Sigma_{\\pmb d})\\). then test stat \\(T^2 = n(\\bar {\\pmb D} - \\pmb \\delta)&#39; S^{-1}_{\\pmb d} (\\bar {\\pmb D} - \\pmb \\delta) \\sim (n-1)\\tfrac{p}{n-p} F_{p, n-p}\\). Hypothesis Testing: $ H_0 : = $ $ T^2 = n({D} )’ S^{-1}{d} ({D} ) F{p, n-p} $ reject \\(H_0\\) if \\(T^2 &gt; \\tfrac{(n-1)p}{n-p} F_{p, n-p} (\\alpha)\\). $100(1-) % $ CR for \\(\\pmb \\delta\\): $ ({D} - )’ S^{-1}{d} ({D} - ) F{p, n-p} () $ $100(1-) % $ simultaneous CI for individual \\(\\delta_i\\): Bonferroni’s $100(1-) % $ simultaneous CI for individual \\(\\delta_i\\): $ \\[\\begin{alignat*}{3} \\bar d_i \\pm &amp;\\sqrt{\\tfrac{(n-1)p}{n-p} F_{p, n-p} (\\alpha)} &amp;\\sqrt{\\tfrac{S^2_{d_i}}{n}} \\tag{2} \\\\ \\bar d_i \\pm &amp;t_{n-1} \\left( \\tfrac {\\alpha} {2p} \\right) &amp;\\sqrt{\\tfrac{S^2_{d_i}}{n}}\\tag{3} \\end{alignat*}\\] $ – ==== 5.4.1.0.1 Different Approach let \\(\\pmb X = \\left[ x_{11}, \\cdots, x_{1p}, x_{21}, \\cdots, x_{2p} \\right]_{1 \\times 2p}&#39; \\sim N_{2p}(\\pmb \\mu, \\Sigma)\\). then \\(\\pmb D = C \\pmb X\\), where $C = ( \\[\\begin{matrix} 1 &amp; &amp; \\pmb 0 &amp; \\vdots &amp; -1 &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; &amp; \\vdots &amp; &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; &amp; 1 &amp; \\vdots &amp; \\pmb 0 &amp; &amp; -1 \\end{matrix}\\] )_{p 2p} $. at here, $ \\[\\begin{align*} E(\\pmb D) &amp;= E(C \\pmb X) = C \\pmb \\mu \\\\ &amp;= \\pmb \\delta\\\\ \\\\ Cov(\\pmb D) &amp;= Cov(C \\pmb X) = C \\Sigma C&#39; \\\\ &amp;= \\Sigma_d\\\\ \\\\ \\pmb D &amp;= C \\pmb X \\sim N_p (C \\pmb \\mu, C \\Sigma C&#39;) \\end{align*}\\] $ therefore, given \\(H_0 : C \\pmb \\mu = \\pmb 0\\), test stat \\(T^2 = n (C \\bar {\\pmb X})&#39; (CSC&#39;)^{-1} (C \\bar {\\pmb X}) \\overset {H_0}{\\sim} \\tfrac{(n-1)p}{n-p} F_{p, n-p}\\) graph, check normality: 5.4.2 Comparing Mean Vectors from Two Populations Recall: univariate, $ t = {sqrt{S_p^2 ( + )}} t_{n_1 + n_2 - 2}$ for MV, assume below, where \\((\\pmb X_{11}, \\cdots, \\pmb X_{1n_1})\\) and \\((\\pmb X_{21}, \\cdots, \\pmb X_{2n_2})\\) are independent. $ X_{11}, , X_{1n_1} N_p (_1 , _1 ) $ $ X_{21}, , X_{2n_2} N_p (_2 , _2 ) $ at here, $ H_0 : _1 - _2 = $ 5.4.2.0.1 case 1: $ _1 = _2 = $ 이하 대부분은 벡터에 관한 이야기이다. \\(\\bar X_i\\) estimates \\(\\mu_i\\), \\(i=1,2\\). \\(S_p\\) estimates \\(\\Sigma\\), where $S_p = {(n_1-1) + (n_2-1)} $. the test stats Hotelling’s $ T^2 = ( {X_1 } - {X_2} ) ’ S_p^{-1} ( {X_1 } - {X_2} ) $ where $ {p [ (n_1 - 1) + (n_2 - 1) ]} ; T^2 = {p [ (n_1 + n_2 - 2) ]} ; T^2 F_{p, n_1 + n_2 -p - 1}$. (p.285 for pf) CR for \\(\\mu_1 - \\mu_2\\) will be $ Pr = 1- $ where $ c^2= {n_1 + n_2 - p - 1} ; T^2 F_{p, n_1 + n_2 -p - 1} ()$. * 이때 constant가 역수가 되었음을 눈치. * The equality will define the boundary of a region. * The region is an ellipsoid centered at \\((\\bar X_1 - \\bar X_2)\\). 5.4.2.0.1.1 Example) Testing \\(H_0 : \\mu_1 - \\mu_2 = 0\\) at \\(\\alpha=0.05\\) is equivalent to see whether falls within the confidence region Axes of the confidence region * let \\(\\lambda_1 , \\cdots, \\lambda_p\\) are ev of \\(S_p\\). * let \\(e_1 , \\cdots, e_p\\) are evc of \\(S_p\\). then \\(e_i\\)’s are the direction of CI $ $are the half-length of the CR Link let $ c^2= {n_1 + n_2 - p - 1} ; T^2 F_{p, n_1 + n_2 -p - 1} ()$. \\(100(1-\\alpha)%\\) simultaneous CI for \\(a&#39;(\\mu_1 - \\mu_2)\\), \\(\\forall a\\): $ a’ ( X_1 - X_2 ) c $ 5.4.2.0.1.2 Example) simultaneous CI for \\((\\mu_{1i} - \\mu_{2i}), i=1, \\cdots, p\\). let \\(a&#39; = \\left[0, \\cdots, 0, 1, 0, \\cdots, 0 \\right]\\). 이때 \\(a&#39;\\)가 하나만 1이고 나머지 0이면, 어떤 특별한 한 axis로 proj하라는 의미. link let \\(\\mu_1 - \\mu_2 = \\left[ \\mu_{1i} - \\mu_{2i} \\right]_{i=1,\\cdots,p}\\). $ a’(X_1 - X_2) = X_{1i} - X_{2i}$, \\(a&#39; \\left( \\dfrac {1}{n_1} + \\dfrac {1}{n_2} \\right) S_p a = \\left( \\dfrac {1}{n_1} + \\dfrac {1}{n_2} \\right) S_{p \\; ii}\\) * \\(S_{p \\; ii}\\) : p번째 변수의 표본 cov. 이는 단변량에서 나왔던 공통 cov, 즉 샘플 se와 표기법이 동일해지며 유사하다. (ch1) link the Bonferroni’s $100(1-)% $ simultaneous CI for \\((\\mu_{1i} - \\mu_{2i})\\) is $ (X_1 - X_2) t_{n_2 + n_2 -2, ()} $. 5.4.2.0.2 case 2: $ _1 = _2 $ assume \\(n_1 - p , \\; n_2 - p\\) are large. for \\(H_0 : \\mu_1 - \\mu_2 = 0\\), test stat becomes \\(T^2 = (\\bar X_1 - \\bar X_2 )&#39; \\left[ \\dfrac{1}{n_1} S_1 + \\dfrac {1}{n_2} S_2 \\right]^{-1} (\\bar X_1 - \\bar X_2 ) \\overset{H_0}{\\sim} \\chi_p^2\\). $ E(\\bar X_1 - \\bar X_2 ) = \\mu_1 - \\mu_2 $ $ Cov(\\bar X_1 - \\bar X_2 ) = Cov(\\bar X_1) + Cov(\\bar X_2 ) - 2 Cov(\\bar X_1, \\bar X_2 ) = \\dfrac{1}{n_1} \\Sigma_1 + \\dfrac {1}{n_2} \\Sigma_2 - 0 $ $ \\bar X_1 - \\bar X_2 \\overset{\\cdot}{\\sim} N_p \\left( \\mu_1 - \\mu_2, \\dfrac{1}{n_1} \\Sigma_1 + \\dfrac {1}{n_2} \\Sigma_2 \\right) \\tag{∵ CLT} $ $\\\\[3ex] $ $ \\text{under } H_0, $ $ S_1 \\overset{p}{\\to} \\Sigma_1, S_2 \\overset{p}{\\to} \\Sigma_2 \\tag{∵ WLLN} $ $ (\\bar X_1 - \\bar X_2 )&#39; \\left[ \\dfrac{1}{n_1} S_1 + \\dfrac {1}{n_2} S_2\\right]^-1 (\\bar X_1 - \\bar X_2 ) \\overset{app}{\\sim} \\chi_p^2 \\tag{∵ Slutsky&#39;s thm} $ why Cov become 0??? i.e. reject \\(H_0\\) if \\(T^2 &gt; \\chi_p^2 (\\alpha)\\). CI becomes $ Pr = 1- $ 차이는~~ Remark: if \\(n_1 = n_2 = 2\\), $ \\[\\begin{align*} \\dfrac{1}{n_1} S_1 + \\dfrac{1}{n_2} S_2 &amp;= \\dfrac{1}{n} (S_1 + S_2) \\\\ &amp;= \\dfrac{1}{n} \\left[ \\dfrac{1}{n-1} \\sum_{n=1}^n (\\pmb X_{1i} - \\bar {\\pmb X_1})(\\pmb X_{1i} - \\bar {\\pmb X_1})&#39; + \\dfrac{1}{n-1} \\sum_{n=1}^n (\\pmb X_{2i} - \\bar {\\pmb X_2})(\\pmb X_{2i} - \\bar {\\pmb X_2})&#39; \\right] \\\\ &amp;= \\dfrac{1}{n} \\dfrac{1}{n-1} S_p \\ast 2(n-1) = \\dfrac{2}{n} S_p \\end{align*}\\] $ i.e. case 1 and case 2 are the same procedure when the sample sizes are the same for large sample sizes. \\(100(1-\\alpha)%\\) simultaneous CI for \\(\\pmb a&#39;(\\pmb \\mu_1 - \\pmb \\mu_2)\\), \\(\\forall \\pmb a\\): $ a’ ( {X_1} - {X_2} ) $ 5.4.2.0.3 Other Statistics for Testing two Mean Vectors let \\(W=(n_1-1)S_1 + (n_2-1)S_2\\): within SS, \\(B=n_1 (\\bar {\\pmb X_1} - \\bar {\\pmb X})(\\bar {\\pmb X_1} - \\bar {\\pmb X})&#39; + n_2 (\\bar {\\pmb X_2} - \\bar {\\pmb X})(\\bar {\\pmb X_2} - \\bar {\\pmb X})&#39;\\) Wilk’s Lambda: when two-sample procedure, Hotelling’s \\(T^2\\) $ ^= $ Lawley-Hotelling’s Trace: $ tr(BW^{-1}) $ Pillai Trace: $ tr $ Roy’s Largest Root: maximum ev of \\(B(B+W)^{-1}\\). 5.4.2.0.4 Testing Equality of Covariance Matrices $ H_0 : _1 = _2 $ let \\(S_p = \\dfrac{1}{n_1 + n_2 - 2} \\left[ (n_1 - 1) S_1 + (n_2 - 1) S_2 \\right]\\). $ \\[\\begin{align*} M &amp;= (n_1 + n_2 - 2) \\ln \\vert S_p \\vert - (n_1 - 1) \\ln \\vert S_1 \\vert - (n_2 - 1) \\ln \\vert S_2 \\vert \\tag{test stat} \\\\ C^{-1} &amp;= 1 - \\dfrac{2p^2 + 3p -1}{6(p+1)} \\left( \\dfrac {n_1 + n_2 - 2}{(n_1-1)(n_2 - 1)} - \\dfrac {1}{n_1 + n_2 - 2} \\tag{Scale Factor} \\\\ MC^{-1} &amp;\\sim \\chi_v^2, \\; \\; \\; \\; \\; v=\\dfrac{p(p+1)}{2} \\end{align*}\\] $ reject \\(H_0\\) if \\(MC^{-1} &gt; \\chi_v^2(\\alpha)\\) 5.4.3 Profile Analysis (for \\(g=2\\)) Recall: \\(H_0: \\pmb \\mu_1 = \\pmb \\mu_2\\), when \\(\\Sigma_1 = \\Sigma_2 = \\Sigma\\) $ \\[\\begin{align*} T^2 &amp;= (\\bar {\\pmb X_1} - \\bar {\\pmb X_2})&#39; \\left[ \\left( \\tfrac{1}{n_1} + \\tfrac{1}{n_2} \\right) S_p \\right]^{-1} (\\bar {\\pmb X_1} - \\bar {\\pmb X_2}) \\\\ &amp;\\overset {H_0} {\\sim} \\tfrac {(n_1 + n_2 -2)p} {n_1 + n_2-p-1} F_{p, \\; \\; n_1 + n_2 -p -1} \\end{align*}\\] $ let’s \\(H_0: C \\pmb \\mu_1 = C \\pmb \\mu_2\\), when \\(\\Sigma_1 = \\Sigma_2 = \\Sigma\\), where \\(C_{q \\times p}\\), \\(q \\le p\\) and \\(rank(C)=q\\). $ \\[\\begin{align*} T^2 &amp;= (\\bar {\\pmb X_1} - \\bar {\\pmb X_2})&#39; C&#39; \\left[ \\left( \\tfrac{1}{n_1} + \\tfrac{1}{n_2} \\right) CS_p C&#39;\\right]^{-1} C(\\bar {\\pmb X_1} - \\bar {\\pmb X_2}) \\\\ &amp;\\overset {H_0} {\\sim} \\tfrac {(n_1 + n_2 -2)q} {n_1 + n_2-q-1} F_{p, \\; \\; n_1 + n_2 -p -1} \\end{align*}\\] $ Profiles are constructed for each group. Consider two groups. Questions: Are the profiles parallel? $ \\[\\begin{alignat*}{3} &amp;&amp;H_0 : \\mu_{11}-\\mu{12} = \\mu_{21}-\\mu{22}, \\mu_{12}-\\mu{13} = \\mu_{22}-\\mu{23}, \\mu_{13}-\\mu{14} = \\mu_{23}-\\mu{24}, \\cdots, \\mu_{1,p-1}-\\mu{1,p} = \\mu_{2,p-1}-\\mu{2,p} \\\\ &amp;\\iff &amp; H_0 : \\mu_{11}-\\mu{21} = \\mu_{12}-\\mu{22} = \\cdots = \\mu_{1p}-\\mu{2p}} \\\\ &amp;\\iff C_{(p-1) \\times p} &amp;H_0: C \\pmb \\mu_1 = C \\pmb \\mu_2 \\end{alignat*}\\] $ This is equivalent to test the equal mean vector of the transformed data \\(C \\pmb X_1\\) and \\(C \\pmb X_2\\). Populations 1: \\(C \\pmb X_{11}, \\cdots, C \\pmb X_{1n_1} \\sim N_{p-1} (C \\pmb \\mu_1 , C \\Sigma C&#39;)\\) Populations 2: \\(C \\pmb X_{21}, \\cdots, C \\pmb X_{2n_2} \\sim N_{p-1} (C \\pmb \\mu_2 , C \\Sigma C&#39;)\\) reject \\(H_0: C \\pmb \\mu_1 = C \\pmb \\mu_2\\) (i.e. paralle profiles), if $ T^2 = ({X_1} - {X_2})‘C’ ^{-1} C({X_1} - {X_2}) &gt; d^2 = (n_1 + n_2 - 2) F_{p-1,n_1+n_2-p} () $ 5.4.3.0.1 2. Coincident Profiles Assuming that the profiles are parallel, are the profiles coincident? $ \\[\\begin{align*} &amp;H_0 : \\mu_{1i} = \\mu_{2i}, i=1, \\cdots, p \\\\ \\iff &amp; H_0 : \\pmb 1 &#39; \\pmb \\mu_1 = \\pmb 1 &#39; \\pmb \\mu_2 \\end{align*}\\] $ is the case where \\(C\\) is replaced by \\(\\pmb 1 &#39;\\). reject \\(H_0\\) if $ \\[\\begin{alignat*}{2} T^2 &amp;= \\pmb 1 &#39; (\\bar {\\pmb X_1} - \\bar {\\pmb X_2}) \\left[ \\left(\\dfrac{1}{n_1} + \\dfrac{1}{n_2} \\right) \\pmb 1 &#39; S_p \\pmb 1 \\right]^{-1} (\\bar {\\pmb X_1} - \\bar {\\pmb X_2}) &amp;&amp; \\\\ &amp;= \\left( \\dfrac{\\pmb 1 &#39; (\\bar {\\pmb X_1} - \\bar {\\pmb X_2})}{\\sqrt{\\left(\\dfrac{1}{n_1} + \\dfrac{1}{n_2} \\right) \\pmb 1 &#39; S_p \\pmb 1}} \\right)^2 &amp;&amp;&gt; F_{1, n_1 + n_2 -2} (\\alpha) (n_1 + n_2 - 2) \\dfrac{p-1}{n_1 + n_2 - p } F_{p-1,n_1+n_2-p} (\\alpha) \\end{alignat*}\\] $ 5.4.3.0.2 3. Flat Profiles 3.Assuming that the profiles are coincident, are the profiles level? $ H_0 : {11} = {12} = } = {1p} = {21} = {22} = } = {2p} $ by 1 and 2, we can collapse two groups into one. $ X_{11}, , X_{1n_1}, X_{21}, , X_{2n_2} N_p (, ) $ this is one population problem $ C_{(p-1) p}, H_0: C = 0 $ reject \\(H_0\\), iff $ T^2 = (n_1+n_2) {X}‘C’ [CSC’]^{-1} C {X} &gt; d^2 = (n_1 + n_2 - 1) F_{p-1,n_1+n_2-p+1} () $ 이는 1번에서의 그것과는 \\(F\\)분포의 df가 변화했다는 점에 주목. - \\(\\bar {\\pmb X} = \\tfrac{1}{n_1 + n_2} \\left( \\sum_{j=1}^{n_1} \\pmb X_{1j}+ \\sum_{j=1}^{n_2} \\pmb X_{2j} right)\\). - \\(S = n_1 + n_2\\) sample covariance matrix, using data. 5.4.4 Comparing Several Multivariate Population Means Recall: In univariate, two-sample t-test is extended to Analysis of Variance(ANOVA). $ H_0:_1 = =_g $ $ F^= {SSE/df_2} F_{df_1 , df_2} $ - where - SSR: sum of squared regression, - SSE: sum of squared error, - SST: sum of squared total - \\(df_1 = g-1, df_2 = N-g, N=\\sum_{i=1}^g n_i\\). Assume \\(g\\) population or treatment groups, and each groups are independent. 각 population은 같은 Cov를 갖고 같은 숫자의 패러미터를 갖되 총 observation 숫자랑 각각의 population mean은 다름. Population 1~g: \\(\\pmb X_{i1}, \\cdots, \\pmb X_{in_i} \\sim N_p(\\pmb \\mu_i , \\Sigma)\\). Model $ X_{ij} = {i} + {ij}, ; ; ; ; ; i=1, , g, ; ; j = 1, , n_i $ $ H_0: _1 = _g $ $ X_{ij} = \\[\\begin{bmatrix} X_{ij1} \\\\ X_{ij2} \\\\ \\vdots \\\\X_{ijp} \\end{bmatrix}\\] {p } , {ij} = \\[\\begin{bmatrix} \\mu_{i1} \\\\ \\mu_{i2} \\\\ \\vdots \\\\ \\mu_{ip} \\end{bmatrix}\\] {p }, {ij} = \\[\\begin{bmatrix} \\epsilon_{ij1} \\\\ \\epsilon_{ij2} \\\\ \\vdots \\\\ \\epsilon_{ijp} \\end{bmatrix}\\] _{p } $ Assumptions The random samples from different populations are independent. All populations have a common covariance matrix \\(\\Sigma\\). Each population is Multivariate Normal. This assumption can be relaxed by C.L.T., when the sample sizes \\(n_1 , \\cdots, n_g\\) are large. 5.4.4.0.1 One-Way MANOVA The quantities SSR, SSE and SST become matrices in MANOVA. $ \\[\\begin{align*} B &amp;= \\sum_{i=1}^g n_i (\\pmb X_i - \\pmb X) (\\pmb X_i - \\pmb X)&#39; \\tag{SSR} \\\\ W &amp;= \\sum_{i=1}^g \\sum_{j=1}^{n_i} (\\pmb X_{ij} - \\pmb X_i) (\\pmb X_{ij} - \\pmb X_i)&#39; \\\\ &amp;= (n_1 -1)S_1 + \\cdots + (n_g -1)S_g \\tag{SSE} \\end{align*}\\] $ Note: $ \\[\\begin{alignat*}{3} (\\pmb X_{ij} - \\bar {\\pmb X}) &amp;= (\\bar {\\pmb X_i} - \\bar {\\pmb X}) + (\\pmb X_{ij} - \\bar {\\pmb X_i})&amp;&amp; \\\\ (\\pmb X_{ij} - \\bar {\\pmb X}) (\\pmb X_{ij} - \\bar {\\pmb X}) &#39; &amp;= (\\bar {\\pmb X_i} - \\bar {\\pmb X}) (\\bar {\\pmb X_i} - \\bar {\\pmb X}) &#39; + &amp;&amp;(\\bar {\\pmb X_i} - \\bar {\\pmb X}) (\\pmb X_{ij} - \\bar {\\pmb X_i})&#39; + (\\pmb X_{ij} - \\bar {\\pmb X_i}) (\\bar {\\pmb X_i} - \\bar {\\pmb X}) &#39; + (\\pmb X_{ij} - \\bar {\\pmb X_i})(\\pmb X_{ij} - \\bar {\\pmb X_i})&#39; \\\\ \\sum_{i=1}^g \\sum_{j=1}^{n_i} (\\pmb X_{ij} - \\bar {\\pmb X}) (\\pmb X_{ij} - \\bar {\\pmb X}) &#39; &amp;= \\sum_{i=1}^g n_i (\\bar {\\pmb X_i} - \\bar {\\pmb X}) (\\bar {\\pmb X_i} - \\bar {\\pmb X}) &#39; &amp;&amp;+ \\sum_{i=1}^g \\sum_{j=1}^{n_i} (\\pmb X_{ij} - \\bar {\\pmb X_i})(\\pmb X_{ij} - \\bar {\\pmb X_i})&#39; \\\\ T &amp;= B &amp;&amp;+ W \\end{alignat*}\\] $ B: Between Sum of Squares W: Within Sum of Squares Any test statistic will be a function of B and W. Popular test statistics use eigenvalues of \\(BW^{-1}\\). let \\(\\lambda_1, \\cdots, \\lambda_r\\) be ev of \\(BW^{-1}\\), where \\(r=\\) ## of non-zero ev’s. Wilk’s Lambda (LRT) $ = = = _{i=1}^r (1+_1)^{-1} $ Pillai’s Trace $ \\[\\begin{align*} V &amp;= tr[B(B+W)^{-1}] = tr[B(B(I+B^{-1}W))^{-1}] = tr[B(I+B^{-1}W)^{-1}B^{-1}] \\\\ &amp;=tr[B^{-1}B(I+B^{-1}W)^{-1}] = tr[(I+B^{-1}W)^{-1}] = tr[I+(B^{-1}W)^{-1}]\\\\ &amp;=\\sum_{i=1}^r \\left( \\dfrac{\\lambda_i}{1+\\lambda_i}\\right) \\end{align*}\\] $ Lawley-Hotelling’s Trace $ T = tr(BW^{-1}) = _{i=1}^r _i $ Roy’s Largest Root $ U = _{i=1,,r} { _i } $ Sampling Distribution of Wilk’s Lambda $ \\[\\begin{alignat*}{2} p=1, g \\ge 2: &amp;\\left(\\dfrac{\\sum_{i=1}^g n_i - g}{g-1}\\right) \\left(\\dfrac{1-\\Lambda^\\ast}{\\Lambda^\\ast}\\right) &amp;&amp;\\overset{H_0}{\\sim} F_{g, \\sum_{i=1}^g n_i - g} \\\\ p=2, g \\ge 2: &amp;\\left(\\dfrac{\\sum_{i=1}^g n_i - g-1}{g-1}\\right) \\left(\\dfrac{1-\\sqrt{\\Lambda^\\ast}}{\\sqrt{\\Lambda^\\ast}}\\right) &amp;&amp;\\overset{H_0}{\\sim} F_{2(g-1), 2(\\sum_{i=1}^g n_i - g-1)} \\\\ p\\ge1, g = 2: &amp;\\left(\\dfrac{n_1 + n_2 - p -1}{p}\\right) \\left(\\dfrac{1-\\Lambda^\\ast}{\\Lambda^\\ast}\\right) &amp;&amp;\\overset{H_0}{\\sim} F_{p, n_1 + n_2 - p -1} \\\\ p \\ge 1, g \\ge 3: &amp;\\left(\\dfrac{\\sum_{i=1}^3 n_i - p-2}{p}\\right) \\left(\\dfrac{1-\\sqrt{\\Lambda^\\ast}}{\\sqrt{\\Lambda^\\ast}}\\right) &amp;&amp;\\overset{H_0}{\\sim} F_{2p, 2(\\sum_{i=1}^g n_i - p-2)} \\\\ \\text{large sample sizes}: &amp;- \\left( \\sum_{i=1}^g n_i -1 -\\dfrac{p+q}{2}\\right) \\ln \\Lambda^\\ast &amp;&amp;\\overset{H_0}{\\sim} \\chi^2_{p(g-1)} \\tag{Why?} \\end{alignat*}\\] $ "],["multivariate-multiple-regression-wk6.html", "5.5 Multivariate Multiple Regression (wk6)", " 5.5 Multivariate Multiple Regression (wk6) 5.5.1 Overview Recall: univariate Linear Regression: repsponse variable \\(Y\\), \\(r\\) predictor variables \\(Z_1 , \\cdots, Z_r\\). model: $ \\[\\begin{alignat*}{3} Y_j &amp;= \\beta_0 + \\beta_1 Z_{j1} + \\cdots + \\beta_j Z_{jr} + \\epsilon_j , \\; \\; \\; \\; \\; &amp;E(\\epsilon_j) = 0, Var(\\epsilon_j) = \\sigma^2 \\pmb Y_{n \\times 1} &amp;= \\pmb Z_{n \\times (r+1)} \\pmb \\beta_{(r+1) \\times 1} + \\pmb \\epsilon_{n \\times 1}, \\; \\; \\; \\; \\; &amp;E(\\pmb \\epsilon) = 0, Var(\\pmb \\epsilon) = \\sigma^2 I \\end{alignat*}\\] $ estimation: $ \\[\\begin{alignat*}{3} \\hat {\\pmb \\beta} &amp;= (\\pmb Z &#39; \\pmb Z )^{-1} \\pmb Z &#39; \\pmb Y \\\\ \\hat {\\pmb \\epsilon} &amp;= (\\pmb Y - \\pmb Z \\hat {\\pmb \\beta}) = \\pmb Y - \\pmb Z (\\pmb Z &#39; \\pmb Z )^{-1} \\pmb Z &#39; \\pmb Y = (I - \\pmb Z (\\pmb Z &#39; \\pmb Z )^{-1} \\pmb Z &#39;) \\pmb Y \\\\ &amp;= (I-H)\\pmb Y \\end{alignat*}\\] $ inference: let \\(\\epsilon \\sim N_n (\\pmb 0, \\sigma^2 I)\\). then $ \\[\\begin{alignat*}{3} \\hat {\\pmb \\beta} &amp;\\sim N_{r+1} (\\pmb \\beta , \\sigma^2(\\pmb Z &#39;\\pmb Z )^{-1}) \\\\ \\hat {\\pmb \\epsilon} &#39; \\hat {\\pmb \\epsilon} &amp;\\sim \\sigma^2 \\chi^2_{n-r-1} \\\\ \\\\ E(\\hat {\\pmb \\epsilon} ) &amp;= \\pmb 0 \\\\ Cov(\\hat {\\pmb \\epsilon} ) &amp;= \\sigma^2 (I - \\pmb Z (\\pmb Z &#39; \\pmb Z )^{-1} \\pmb Z &#39;) \\\\ E\\left( \\dfrac{\\hat {\\pmb \\epsilon} &#39; \\hat {\\pmb \\epsilon}}{n-r-1} \\right) &amp;= \\sigma^2 \\end{alignat*}\\] $ 5.5.2 Multivariate Multiple Regression Notation Model $ Y_{n m} = Z_{n (r+1)} {(r+1) m} + {n m}, ; ; ; ; ; E({(i)} ) = , Cov({(i)}, {(j)}) = {ik} I, ; ; ; i,k = 1, , m $ Cov of \\(m\\) responses: $ \\[\\begin{alignat*}{3} &amp;\\Sigma = \\begin{bmatrix} \\sigma_{11} &amp; &amp; \\sigma_{1m} \\\\ &amp; \\ddots &amp; \\\\ \\sigma_{m1}&amp;&amp; \\sigma_{mm} \\end{bmatrix}, \\; \\; \\; \\; \\; &amp;&amp;Var(\\pmb \\epsilon_{(i)}) = \\sigma_{ii} I,\\; \\; \\; \\; \\; &amp;&amp;Cov(\\pmb \\epsilon_{(i)}, \\pmb \\epsilon_{(j)}) = \\begin{bmatrix} \\sigma_{ik} &amp; &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; \\\\ \\pmb 0 &amp;&amp; \\sigma_{ik} \\end{bmatrix} \\end{alignat*}\\] $ the meaning of \\(0\\): observations from different trials, are uncorrelated \\(\\sigma_{ik}\\): errors for different responses on the same trial are correlated \\(i\\)th response \\(\\pmb Y_{(i)}\\): $ Y_{(i)} = Z {(i)} + {(i)}, ; ; ; ; ; Corr({(i)}) = {ii}I , = (Z ’ Z )^{-1} Z ’ Y_{(i)} $ 5.5.2.0.1 Least Square Collecting Univariate Least Squares Estimates (LSE) Errors $ Y - Z = $ Error Sum of Squares (SSE) diagonal elements: Error SS for univariate least squares \\((\\pmb Y_{(i)}-\\pmb Z \\pmb \\beta_{(i)})&#39; (\\pmb Y_{(i)}-\\pmb Z \\pmb \\beta_{(i)})\\) is minimized. the generalized \\(Var\\) \\(\\lvert (\\pmb Y-\\pmb Z \\pmb \\beta)&#39; (\\pmb Y-\\pmb Z \\pmb \\beta) \\rvert\\) is also minimized. Properties $ \\[\\begin{align*} \\hat {Y} &amp;= Z \\hat \\beta = Z(Z&#39;Z)^{-1} Z&#39; Y \\\\ &amp;= HY \\tag{Predicted Values}\\\\ \\hat {\\pmb \\epsilon} &amp;= Y - \\hat Y = \\left[ I - Z(Z&#39;Z)^{-1} Z&#39; \\right] Y \\\\ &amp;= (I-H)Y \\tag{residuals} \\\\ Z&#39; \\hat {\\pmb \\epsilon} &amp;= Z&#39; \\left[ I - Z(Z&#39;Z)^{-1} Z&#39; \\right] Y \\\\ &amp;= [Z-Z&#39;] Y =\\pmb 0 \\tag{3} \\hat Y&#39; \\hat {\\pmb \\epsilon} &amp;= \\hat {\\beta} &#39; Z&#39; \\left[ I - Z(Z&#39;Z)^{-1} Z&#39; \\right] Y \\\\ &amp;= [\\hat {\\beta} &#39; Z- \\hat {\\beta} &#39; Z&#39;] Y =\\pmb 0 \\tag{4} \\end{align*}\\] $ - - by (3), residuals are orthogonal to \\(Z\\) - by (4), residuals are orthogonal to \\(\\hat Y\\) Error Sum of Squares $ \\[\\begin{align*} Y&#39;Y &amp;= (\\hat Y \\hat {\\pmb \\epsilon} ) &#39; (\\hat Y \\hat {\\pmb \\epsilon} ) \\\\ &amp;= \\hat Y &#39; \\hat Y + \\hat{\\pmb \\epsilon}&#39; \\hat{\\pmb \\epsilon} \\\\ \\\\ \\hat {\\pmb \\epsilon}&#39; \\hat {\\pmb \\epsilon}&amp;= Y&#39;Y - \\hat Y &#39; \\hat Y \\\\ &amp;= \\hat Y &#39; \\hat Y - \\hat \\beta &#39; Z&#39; Z \\hat \\beta \\end{align*}\\] $ Results 1 $ \\begin{alignat*}{2} E() &amp;= , ; ; ; ; ; Cov(, ) &amp;= _{il} (Z’Z)^{-1} \\ \\ E() = , ; ; ; ; ;E ( ’ ) = \\end{alignat*} $ - - at here, \\(\\hat {\\pmb \\epsilon}\\) and \\(\\hat {\\pmb \\beta}\\) are correlated. Results 2 If \\(\\pmb \\epsilon_j\\) has a \\(N_m (\\pmb 0 , \\Sigma)\\), then \\(\\hat {\\pmb \\beta}= (\\pmb Z &#39; \\pmb Z )^{-1}\\pmb Z &#39;Y\\) is MLE of \\(\\pmb \\beta\\) $ \\[\\begin{align*} \\hat {\\pmb \\beta_{(i)}} &amp;\\sim N_{r+1} ({\\pmb \\beta_{(i)}}, \\sigma_{ii} (\\pmb Z &#39; \\pmb Z )^{-1}) \\\\ \\hat \\Sigma &amp;= \\dfrac{1}{n} \\hat {\\pmb \\epsilon} &#39; \\hat {\\pmb \\epsilon} \\\\ &amp;= \\dfrac{1}{n} (\\pmb Y - \\pmb Z \\hat {\\pmb \\beta}) &#39; (\\pmb Y - \\pmb Z \\hat {\\pmb \\beta}) \\tag{5} \\end{align*}\\] $ is MLE of \\(\\Sigma\\) \\(n \\hat \\Sigma \\sim W_{p,n-r-1} (\\Sigma)\\). Comment Multivariate regression requires no new computational problems. Univariate least squares \\(\\hat {\\pmb \\beta_{(i)}}\\) are computed individually for each response variable. Diagnostics check must be done as in univariate regression. Residual vectors \\([ \\pmb \\epsilon_{j1}, \\cdots, \\pmb \\epsilon_{jm} ]\\) can be examined for multivariate normality. 5.5.3 Hypothesis Testing Note: $ \\[\\begin{align*} &amp;H_0: \\text{ responses do not depend on } Z_{q+1}, Z_{q+2}, \\cdots, Z_{r} \\\\ \\iff &amp;H_0: \\begin{bmatrix} \\beta_{(q+1)1} &amp; \\beta_{(q+2)1} &amp; \\cdots &amp; \\beta_{(q+1)m} \\\\ \\vdots &amp;&amp;&amp; \\vdots \\\\ \\beta_{r1} &amp; \\beta_{r1} &amp; \\cdots &amp; \\beta_{rm} \\end{bmatrix} = 0 \\\\ \\iff &amp;H_0: \\pmb{\\beta_{(2)}} = \\pmb 0, \\; \\; \\; \\; \\; \\ \\pmb \\beta = \\begin{bmatrix} \\pmb{\\beta_{(1)}}_{(q+1) \\times m} \\\\ cdots \\\\ \\pmb{\\beta_{(2)}}_{(r-q) \\times m} \\end{bmatrix} \\end{align*}\\] $ 5.5.3.0.1 Full Model vs. Reduced Model let \\(Z = \\begin{bmatrix} Z_1 &amp; \\vdots Z_2 \\end{bmatrix}\\), then \\(Z \\beta = Z_1 \\beta_{(1)} + Z_2 \\beta_{(2)}\\). under \\(H_0\\), \\(Y = Z \\beta_{(1)} + \\epsilon\\), let $ \\[\\begin{align*} E &amp;= n \\hat \\Sigma &amp;\\\\ &amp;= (\\pmb Y - \\pmb Z \\hat{\\pmb \\beta})&#39;(\\pmb Y - \\pmb Z \\hat{\\pmb \\beta})&amp; \\tag{Full Model}\\\\ \\\\ H &amp;= n(\\hat \\Sigma_1 - \\hat \\Sigma), &amp;\\text{ where } E_1 = n(\\hat \\Sigma_1) = (\\pmb Y - \\pmb Z \\hat{\\pmb \\beta_{(1)}})&#39;(\\pmb Y - \\pmb Z \\hat{\\pmb \\beta_{(1)}}) \\tag{under H0} \\end{align*}\\] $ $ E=n $. 여기서 E라는 것은 오차행렬이기 때문에, 즉 univariate 를 4번 반복해서 나온 오차를 모은 것이 바로 이 \\(E\\)라는 행렬. let \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_s\\) be non-zero ev of \\(HE^{-1}\\), \\(s=min(m, r-q)\\). Four Test Stat: Wilk’s Lambda: $ = _{i=1}^s $ Pillai Trace: $ tr = _{i=1}^s $ Lawley-Hotelling’s Trace: $ tr(HE^{-1}) = _{i=1}^s {_i} $ Roy’s Largest Root: maximum ev of \\(H(H+E)^{-1} = \\lambda_1\\). 5.5.4 Example) fit FM \\(Y = Z \\beta + \\epsilon\\). fit \\(Y_1 , Y_2 , Y_3 , Y_4 = X_1,X_2,X_3\\), then we acquire \\(E=n \\hat \\Sigma\\). 1. $~H_0: \\begin{bmatrix} \\beta_{31},\\beta_{32},\\beta_{33},\\beta_{34} \\end{bmatrix} =0~$, \\(H_0: \\begin{bmatrix} \\beta_{21},\\beta_{22},\\beta_{23},\\beta_{24}\\\\\\beta_{31},\\beta_{32},\\beta_{33},\\beta_{34} \\end{bmatrix} =0\\), under \\(H_0\\), \\(Y=Z \\beta_{(1)} + \\epsilon\\) $ Z_1 = \\[\\begin{bmatrix} 1 &amp; X_{11} \\\\ \\cdots &amp; \\cdots \\\\ 1 &amp; X_{n1} \\end{bmatrix}\\] _{n }, ; ; ; ; ; _{(1)} = \\[\\begin{bmatrix} \\beta_{01} &amp; \\cdots &amp; \\beta_{0m} \\\\ \\beta_{11} &amp; \\cdots &amp; \\beta_{1m} \\end{bmatrix}\\] _{2 m} $ now, fit \\(Y_1 , Y_2 , Y_3 , Y_4 = X_1\\) (X_2, X_3 excluded), then we acquire \\(E_1 =n \\hat \\Sigma_1, H = n \\hat \\Sigma_1 - n \\hat \\Sigma = E_1 - E\\). let’s calculate ev of \\(HE^{-1}\\), and compute Wilk’s Lambda \\(\\Lambda^\\ast = \\dfrac{\\vert E \\vert }{\\vert E+H\\vert }\\). 5.5.4.0.1 Sampling Distribution of the Wilk’s Lambda let Z be full rank of \\(r+1\\), and \\((r+1) + m \\le n\\). let \\(\\epsilon\\) be normally distributed. under \\(H_0\\), $ - (^) ^2_{m(r-q)}$. 5.5.4.0.2 Prediction $ {n m} = Z {(r+1) m} $ assume fixed values \\(\\pmb {Z_0}_{(r+1) \\times 1}\\) of the predictor variables. then \\(\\hat {\\pmb \\beta}&#39;_{m \\times (r+1)} \\pmb Z_0 \\sim N_m(\\pmb \\beta &#39; \\pmb Z_0 , \\pmb Z_0 &#39; (\\pmb Z &#39; \\pmb Z)^{-1} \\pmb Z_0 \\Sigma)\\). \\(100(1-\\alpha)\\%\\) simultaneous CI for \\(E(Y_i) = \\pmb Z_0 &#39; \\pmb \\beta_{(i)}\\): $ Z_0 ’ _{(i)} , ; ; ; ; ; i=1,, m $ where \\(\\pmb \\beta_{(i)}\\) is the \\(i\\)th column of \\(\\pmb \\beta\\). \\(\\hat \\sigma_{ii}\\) is the \\(i\\)th diagonal element of \\(\\hat \\Sigma\\). \\(100(1-\\alpha)\\%\\) simultaneous C.I. for the individual responses \\(Y_{0i} = \\pmb Z_0 &#39; \\pmb \\beta_{(i)} + \\epsilon_{0i}\\): $ Z_0 ’ _{(i)} , ; ; ; ; ; i=1,, m $ "],["pca.html", "5.6 PCA", " 5.6 PCA PCA는 상관관계 있는 반응변수 \\(y\\)의 집합을 상관관계 없는 더 작은 집합으로 바꿈. 이 더 작은 직합들의 이름은 principal components. 이는 더 작은 principal components들이 어쩌면 원본 데이터에 들어있는(available) 거의 대부분의 정보를 보유하고 있을지도 모른다는 생각에서 출발함. 1. Outlier 2. Cluster 3. Discriminant: Cov 매트릭스 invert 하려면 필요. 샘플 사이즈 작으면 \\((n&lt;p)\\) 문제터져서 변수 갯수를 줄임. 4. Regression: predictors 사이에 multicollinearity 존재하는지 체크 5. Multivariate Nomality semi-positive definite 벡터의 매트릭스 \\(\\textbf {X}_{1 \\times p}\\) 의 Cov 매트릭스 \\(\\Sigma\\), 이의 \\(ev\\) \\(\\lambda_1 \\le \\cdots \\le \\lambda_p \\le 0\\). \\(\\textbf a&#39;_i\\)는 $ p $인 열벡터. 이것이 \\(i=1~p\\)개만큼 존재. \\(Y_i = \\textbf a&#39;_i \\textbf {X}_{i}\\), 즉 \\(Y\\)는 \\(a\\)와 \\(X\\)의 선형결합. $Cov(Y_1 , Y_2) = Cov(’_1 , ’_2 ) = _1 ’ _2 ( = 0 ) $ 벡터와 스칼라 여부 주의. Transpose 여부 주의. 0이 되는 건 $_1 ’ $과 $ _2 $ 가 orthogonal. Var가 클수록 정보량 많음. 1번은 분산이 가장 큼. 2번은 분산이 2번째로 크되 1번째의 ${1} $과 orthogonal 해야함. e.g. $ Cov ( {1}’ _{2}’ )$. 이를 반복. 1st principal component: \\(= \\textbf e_1 &#39; \\textbf X\\). * \\(Var \\left( \\textbf e_1 &#39; \\textbf X \\right)= \\textbf e_1 &#39; \\Sigma \\textbf e_1 = \\lambda_1\\). * 이때, \\(e \\textbf v\\)의 정의에 의해 $_1 = _1 _1 $ . * \\(Var \\left( \\textbf e_1 &#39; \\textbf X \\right)\\) 는 $_1 ’ _1 $ 를 만족하는 값들 중 \\(Var \\left( \\textbf e_1 &#39; \\textbf X \\right)\\)를 최대화시키는 값. 2nd principal component: \\(= \\textbf e_2 &#39; \\textbf X\\). * \\(Var \\left( \\textbf e_2 &#39; \\textbf X \\right)= \\textbf e_2 &#39; \\Sigma \\textbf e_2 = \\lambda_2\\) 는 모든 \\(\\textbf a_2 &#39; \\textbf X\\) 중 $Cov ( _1 ’ _1 _2 ’ ) = 0 $ 과 $_2 ’ _2 $를 만족하는 녀석. 즉 PC 자체는 \\(\\textbf e_i &#39; \\textbf X\\) 로 정해짐. note ***이건 proj의 일종인 모양.*** 근데 이걸로 정해지는 이유가 상기의 조건을 만족해야 한다는 거고, 해당 체크 조건들을 \\(\\textbf e_i &#39; \\textbf X\\) 가 모두 통과할 수 있으므로 이걸 PC로 삼는 것에 문제가 없다는 것. $ \\[\\begin{align*} \\sum_{i=1}^p Var(\\textbf X_i) &amp;=tr(\\Sigma) \\\\ &amp;= \\sigma_{11} + \\sigma_{22} + \\cdots + \\sigma_{pp} \\\\ &amp;= \\lambda_1 + \\lambda_2 + \\cdots + \\lambda_p \\\\ &amp;= \\sum_{i=1}^p Var(\\textbf Y_i) \\end{align*}\\] $ 따라서 kth PC에 의해 유발되는 총 Var의 비율은 $ = $. 이인즉 PCA를 거쳐도 p개의 variable 갯수를 유지한다면 설명력의 총합은 동일함. 하지만 우리는 설명력을 1만큼 잃고 변수를 10만큼 줄이기를 원함. 따라서 어느정도 설명력을 잃더라도 그 이상으로 변수의 갯수를 줄이는 선이면 하꼬변수를 쳐냄. 이는 PCA 분석때 기본적으로 분산의 80% 설명을 기준으로 함. Cov 매트릭스 \\(\\Sigma\\), PC \\(Y_i = \\textbf e_i &#39; \\textbf X\\). 이때 \\(\\rho_{Y_i , X_k } = Corr (Y_i , X_k ) = \\dfrac {e_{ik} \\sqrt{\\lambda_i}} {\\sqrt{\\sigma_{kk}}}, \\; \\; \\; i,k=1,\\cdots,p\\). 다룰 때의 편의를 위해 PC 구성 단계에서 \\(Y_i =\\textbf {e}_i ( \\pmb {X} - \\pmb {\\mu} )\\) 로 구성하는 경우도 잦음. PC Score. n개의 관측 중에서 r번째 관측의 variable의 벡터를 $r $이라고 설정하자. 그렇다면 \\(Y_{ri} = \\textbf e_i &#39; (\\textbf X_r - \\pmb \\mu_r)\\). 이때 \\(r=1,\\cdots, n\\). 이때 PC Score는 $ Y{ri} = ’ (_r - { _r})$ 로 추정될 수 있다. ***elbow*** PCA prerequisite * variable들이 same unit * variable들이 have similar Var 해결책 * $ $로 표준화하고 PCA. \\(E(\\textbf Z) = 0, Cov(\\textbf Z )=\\rho\\) * PCA 자체를 corr 매트릭스에 적용 $ \\[\\begin{align*} \\sum_{i=1}^p Var(\\textbf Y_i) &amp;= \\sum_{i=1}^p Var(\\lambda_i) \\\\ &amp;= tr(\\pmb \\rho) \\\\ &amp;= \\sum_{i=1}^p Var(\\textbf Z_i) \\\\ &amp;= p \\end{align*}\\] $ 따라서 이때의 kth PC에 의해 유발되는 총 Var의 비율은 $ = $. \\(Corr\\)을 썼을 때 PC를 어디까지 쓸지를 솎아낼 때는 scree plot이나 \\(ev&gt;1\\)인지를 기준으로 한다. 모든 기존 변수들의 분산이 1이므로 최소한의 설명력이 1이라는건데, 1도 안되면 그냥 쓰레기들이므로. Checking Multivariate Normal: 기존 데이터가 mv normal이라면, 각 PC Score는 normal로 분포되어 있다. 각 PC들을 QQ plot 사용해서 체크하면 답나옴. "],["factor.html", "5.7 Factor", " 5.7 Factor PCA FA concern with explaining \\(Cov\\) and/or \\(Corr\\) structure among measured variables Variability in the variables Objectives 1. Partition the \\(p\\) response variables into \\(m\\) subsets, each consisting of a group of variables tending to be more highly related to others. 2. Create a new set of uncorrelated variables, called underlying factors or underlying characteristics. 3. Use the new variables in future analysis. Warnings 1. If the original variables are already uncorrelated, no reason to consider FA.2. Subjective decisions are necessary to determine number of factors, to determine the method to get the underlying factors 3. FA solutions are not unique. 5.7.0.0.1 Orthogonal Factor Model \\(\\pmb X \\sim \\pmb \\mu , \\Sigma\\). Common Factors $F_1 , F_m $은 $X $와 linearly dependent. errors, Specific Factors $_1 , , _p $. $ $ loading \\(l_{ij}\\)은 \\(i\\)번째 variable의 \\(j\\)번째 factor에 대한 loading. matrix of Factor Loadings \\(\\pmb L\\) 즉슨, \\(X_i - \\mu_i\\)는 \\(F_j\\)의 선형결합과 \\(\\epsilon_i\\)를 더하는 것으로 서술될 수 있다는 게 요지. 다만 관측되지 않은 quantity가 너무 많아서 Factor Model의 직접적인 검증은 사실상 불가능함. 따라서 \\(\\pmb F\\)와 \\(\\pmb \\epsilon\\)에 추가적인 조건을 덧붙인 후, \\(Cov\\) 관계성을 체크하는 것으로 대신한다. \\(E(\\pmb F) = \\pmb 0, Cov(\\pmb F) = E(\\pmb F \\pmb F&#39; ) = I_{m \\times m}\\) $E() = , Cov() = E(’) = _{p p} = \\[\\begin{bmatrix}\\Psi_1 &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; \\Psi_p \\end{bmatrix}\\] _{p p} $ $ F $, so $Cov(, F ) = E(’) = _{p m} $ 이때 $ \\[\\begin{align*} \\Sigma = Cov(\\pmb X) &amp;= E \\left[ (\\pmb X - \\pmb \\mu) (\\pmb X - \\pmb \\mu) &#39; \\right] \\\\ &amp;= E \\left[ \\pmb{LF (LF)&#39; + \\epsilon(LF)&#39; + (LF) \\epsilon&#39; + \\epsilon \\epsilon&#39;} \\right] \\\\ &amp;= \\pmb {LE(FF&#39;)L&#39; + E(\\epsilon \\epsilon&#39;)} \\\\ &amp;= \\pmb{LL&#39; + \\Psi} \\end{align*}\\] $ ㅁㄴㅇㄹ $ \\[\\begin{align*} Cov (\\pmb {X, F}) = \\pmb {E \\left[ (X-\\mu)(F-0)&#39; \\right]} &amp;= E \\left[ \\pmb {(X-\\mu)F&#39; }\\right] \\\\ &amp;= \\pmb { E \\left[ (LF + \\epsilon)F&#39; \\right]} \\\\ &amp;= \\pmb { LE(FF&#39;) + E(\\epsilon F&#39;) }\\\\ &amp;= \\pmb { L} \\end{align*}\\] $ 따라서 Total Variation은: communality \\(h_j^2 = \\sum_{j=1}^m l_{ij}^2\\). * contribution by \\(m\\) column factors * $ ’ = \\[\\begin{bmatrix}h_1^2 &amp; &amp; \\sigma_{1p} \\\\ &amp; \\ddots &amp; \\\\ \\sigma_{p1} &amp; &amp; h_p^2 \\end{bmatrix}\\] _{p p}$. specific variance \\(\\Psi_i\\) $ Var(X_i) = {j=1}^m l{ij}^2 + i \\ Cov(X_i, X_k) = {j=1}^m l_{ij}l_{kj} \\ Cov(X_i, F_j) = l_{ij} $ Notes: * When \\(m=p\\), any \\(Cov\\) matrix \\(S\\) can be reproduced exactly as \\(\\pmb{LL}&#39;\\), so \\(\\pmb \\Psi\\) is the zero matrix. * When \\(m &lt; p\\), the FA is most useful. The FA model provides a “simple” explanation of covariance in \\(\\pmb X\\). * When \\(m \\lll p\\), most \\(Cov\\) matrices cannot be factored as \\(\\pmb{LL}&#39;+\\pmb \\Psi\\)(while maintaining basic statistical properties). 5.7.0.0.2 Uniqueness Orthogonal factor model is not unique, b/c rotation. 5.7.1 Method of Estimation Choosing the appropriate Number of Factors: 1. Similar to PCA. Determine the number of factors using scree plot or eigenvalue \\(\\ge 1\\) 2. Do not include trivial factors (only one variable assigned to one factor). 3. Test the adequacy of the chosen number of factors.(Use ML method and LRT) for standardized variable. 4. Use AIC. Choose m that produces the minimum value for AIC. 5. Use SBC (Schwarz’s Bayesian Criterion). Notes: * $= ’ + * objective is estimating \\(\\pmb L\\) 5.7.1.0.1 1. Principal Component Method $ = = $. 여기서 기여도가 낮은 \\(\\lambda_i\\)에 해당하는 ev를 뒤에서부터 쳐내서 적당한 ev만으로 구성. 그 경우 \\(=\\pmb {L_{p \\times m} L_{m \\times p}&#39;}\\). 여기서 specific factors \\(\\pmb \\Psi\\)의 \\(Var\\)을 \\(\\Sigma - \\pmb {LL&#39;}\\)의 diagonal elements 를 사용해서 구할 수 있다. 근사는 \\(\\Sigma \\approx \\pmb {LL&#39; + \\Psi}\\). $ i = i^2 - {j=1}^m l{ij}^2 = i^2 - {j=1}^m j e{ij}^2 $ 이는 위에서 \\(l_{ij} = \\sqrt {\\lambda_j e_{ij}}\\)임을 보여놨기에 가능. the importance of \\(j\\)th factor $ \\[\\begin{align*} = \\dfrac {\\lambda_j}{\\sum_{i=1}^p \\lambda_i} &amp;= \\dfrac {\\sum_{i=1}^p l_{ij}^2} {\\sum_{i=1}^p \\sigma^2} \\\\ &amp;= \\dfrac {\\sum_{i=1}^p l_{ij}^2} {p} \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\text{if} \\; \\; \\Sigma=\\pmb \\rho \\end{align*}\\] $ at here, communality \\(h_i^2 = \\sum_{j=1}^m l_{ij}^2\\). 5.7.1.0.2 3. ML Method assumption is needed: \\(\\pmb X \\sim N_p (\\pmb mu , \\Sigma)\\), where $= + $. 이때 \\(L(\\pmb \\mu, \\Sigma)\\)는 $= + $ 이기에 \\(\\pmb L\\), \\(\\Psi\\)에 의존. \\(\\hat \\pmb L_{MLE}\\), \\(\\hat \\pmb \\Psi_{MLE}\\)는 수치해석으로 찾아짐. estimated communalities들은 $ h_i^2 = {j=1}^m l{ij}^2$, \\(i=1, \\cdots, p\\). The importance of \\(j\\)th factor는~. 5.7.1.0.2.1 3.5. Test for the number of factors \\(H_0: \\; \\; \\Sigma_{p \\times p} = \\pmb L_{p \\times m} \\pmb L&#39;_{m \\times} + \\pmb \\Psi_{p \\times p}\\) \\(H_1: \\Sigma_{p \\times p}\\)는 any other positive definite matrix. assume \\(\\pmb X \\sim N_p (\\pmb mu , \\Sigma)\\). under \\(H_0\\), $= + $. 이때 $_{MLE} = + $. under \\(H_1\\), \\(\\hat \\Sigma_{MLE} = S_n\\). 이떄 $ S_n$은 sample \\(Cov\\) matrix. LRT for testing \\(H_0\\): $ -2log = n log ( ) = n log ( ) $ 5.7.1.0.2.2 3.7. Bartlett’s Approx. reject \\(H_0\\) if~. 5.7.1.0.3 3. Minimum Residual Method let $Cov(X) = = + $, and mv regression \\(pmb{Y_{n \\times m}=Z_{n \\times (r+1)} \\beta_{(r+1)\\times m)} + \\epsilon_{n \\times m}\\)와 유사한 개형. estimate factor loadings so that the sum of squares of off-diagonal residuals be minimized. \\(\\hat \\pmb L_{MLE}\\), \\(\\hat \\pmb \\Psi_{MLE}\\)는 수치해석으로 찾아짐. estimated communalities들은 $ h_i^2 = {j=1}^m l{ij}^2$, \\(i=1, \\cdots, p\\). The importance of \\(j\\)th factor는~. 5.7.2 Factor Rotation All factor loadings obtained from the initial loading by an orthogonal transformation have the same ability to reproduce the covariance matrix. * $= + = + = + $. at here, must be \\(TT&#39; = I\\) by characteristics of rotation in linear algebra. From matrix algebra, we know that an orthogonal transformation corresponds to a rigid rotation of the coordinate axes. An orthogonal transformation of factor loading is called factor rotation. The communalities \\(\\hat h_i^2\\) and the specific variances \\(\\hat \\Psi_i\\) are not changed, b/c $ = $, and diagonal elements of this is communalities. Rationale: Since the original loadings \\(L\\) may not be easily interpretable, it is usual practice to rotate them until a “simpler structure” is achieved. 5.7.3 Varimax Criterion define \\(\\hat l_{ij}^\\ast = \\dfrac {l_{ij}^\\ast}{\\hat h_j}\\) to be the rotated coefficients. Then the Varimax procedure selects the orthogonal transformation T that makes \\(V=\\) as large as possible. 이는 일종의 분산으로서 관점될 수 있다. In words, $V _{j=1}^mVar( l_j^2 ) $ , which is variance of squares of loading for jth factor. Maximizing \\(V\\) corresponding to “spreading out” the squares of loadings on each factor as much as possible. 5.7.3.0.1 Oblique Rotation It is not possible to rotate the axes so that one axes goes through each cluster of variables while keeping the axes orthogonal to one another. Such rotation can be achieved by multiplying L by a matrix Q where Q is not an orthogonal matrix. Oblique rotations do not produce new factors that remain uncorrelated, which is a contradiction of the initial FA assumptions \\(\\rightarrow\\) not good! 5.7.4 Factor Scores In "],["discrimination-and-classification.html", "5.8 Discrimination and Classification", " 5.8 Discrimination and Classification 여러개의 다른 모집단으로부터 나온 데이터들을 설명. discriminants (구분자) 들의 발견. 관찰치를 분류하여 이를 클래스로 묶고 싶다. why인지는 크게 중요하지 않고, 예측을 하고 싶다. 5.8.1 Bayes Rule let r\\(v\\) for populations \\(\\pi_1 , \\pi_2\\), \\(\\pmb X = (x_1 , \\cdots, x_p)&#39;\\). 이때, \\(f_i(\\pmb X)\\)는 \\(\\pi_i\\)의 pdf. \\(R_i\\)는 우리가 해당 object를 \\(\\pi_i\\)로 분류하는 \\(\\pmb X\\) 값들의 set. $ R_1 : {f_2 ( X)} , ; ; ; ; ; R_2 : {f_2 ( X)} $ let prior of each \\(P_i\\) be \\(\\pi_i\\), and \\(\\sum_{i=1}^n P_i = 1\\). $ R_1 : {f_2 ( X)} , ; ; ; ; ; R_2 : {f_2 ( X)} $ let the costs of misclassification can be defined by a cost matrix: | classify \\(\\pi_1\\)| \\(\\pi_2\\) | True Population \\(\\pi_1\\) | 0 | \\(C(2 \\vert 1)\\) | \\(\\pi_2\\) | \\(C(1 \\vert 2)\\) | 0 | $ R_1 : {f_2 ( X)} , ; ; ; ; ; R_2 : {f_2 ( X)} $ and let both. $ R_1 : {f_2 ( X)} , ; ; ; ; ; R_2 : {f_2 ( X)} $ 5.8.2 Classification with Two mv \\(N\\) Populations assume \\(\\pmb X_1 \\sim N_p( \\pmb \\mu_1 , \\Sigma_1), \\pmb X_2 \\sim N_p( \\pmb \\mu_2 , \\Sigma_2)\\) 5.8.2.0.1 1. if \\(\\Sigma_1 = \\Sigma_2 = \\Sigma\\) (LDA) this is called Linear Discriminant Analysis, e.g., LDA. $ f_i ( X ) = {(2)^{p/2} {}^{1/2}} $ suppose the populations parameters, \\(\\pmb \\mu_1, \\pmb \\mu_2, \\Sigma\\) are known. The minimum expected cost rule is $ \\[\\begin{alignat*}{2} &amp;R_1 : \\dfrac {f_1 ( \\pmb X)} {f_2 ( \\pmb X)} &amp; &amp;\\ge \\dfrac {P_2}{P_1} \\ast \\dfrac {C(1\\vert2)}{C(2\\vert1)} \\\\ &amp;\\exp \\left[ -\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_1)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_1)&#39; +\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_2)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_2) \\right]&amp; &amp;\\ge \\\\ &amp;-\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_1)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_1)&#39; +\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_2)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_2) &amp; &amp;\\ge \\log \\left[ \\dfrac {P_2}{P_1} \\ast \\dfrac {C(1\\vert2)}{C(2\\vert1)} \\right] \\\\ \\\\ \\\\ \\Rightarrow \\; \\; \\; \\; \\; &amp;(\\pmb \\mu_1 - \\pmb \\mu_2)&#39; \\Sigma^{-1} \\pmb X - \\dfrac {1}{2} (\\pmb \\mu_1 - \\pmb \\mu_2)&#39; \\Sigma^{-1} (\\pmb \\mu_1 + \\pmb \\mu_2) &amp; &amp;\\ge \\\\ \\\\ \\Rightarrow \\; \\hat R_1 \\colon \\; \\; \\; \\; \\; &amp;(\\bar {\\pmb X}_1 - \\bar {\\pmb X}_2)&#39; {S_p}^{-1} \\pmb X - \\dfrac {1}{2} (\\bar {\\pmb X}_1 - \\bar {\\pmb X}_2)&#39; {S_p}^{-1} (\\bar {\\pmb X}_1 + \\bar {\\pmb X}_2) &amp; &amp;\\ge \\tag{1} \\\\ \\\\ \\Rightarrow \\; \\hat R_1 \\colon \\; \\; \\; \\; \\; &amp; \\hat {\\pmb a}&#39; \\pmb X - \\dfrac {1}{2} ( \\hat {\\pmb a}&#39; \\bar {\\pmb X_1} + \\hat {\\pmb a}&#39; \\bar {\\pmb X_2}) &amp; &amp;\\ge \\tag{2} \\end{alignat*}\\] $ Allocate (Classify) \\(\\pmb X_0\\) to \\(\\pi_1\\) if \\(R_1\\) holds. Note that \\(R_1\\) has changed to $ R_1$ at last expression. \\((1):\\) However, in practice, \\(\\pmb \\mu_1, \\pmb \\mu_2, \\Sigma\\) are unknown. 따라서 해당 룰은 상응하는 패러미터를 샘플 패러미터로 대체해서 이루어짐. \\(\\pmb \\mu_i\\)는 ${X}_i $ 로 대체. we assumed \\(\\Sigma_1 = \\Sigma_2 = \\Sigma\\), therefore \\(\\Sigma\\) can be replaced by \\(S_p = \\dfrac{n_1 - 1 } {n_1 + n_2 -2} S_1 + \\dfrac{n_2 - 1 } {n_1 + n_2 -2} S_2\\). \\((2):\\) Note that \\((\\bar {\\pmb X}_1 - \\bar {\\pmb X}_2)&#39; {S_p}^{-1} \\pmb X\\) is linear combination of variable \\(\\pmb X\\). let \\((\\bar {\\pmb X}_1 - \\bar {\\pmb X}_2)&#39; {S_p}^{-1} = \\hat {\\pmb a}&#39;\\). 5.8.2.0.1.1 LDA intuition 5.8.2.0.1.2 Posterior assuming equal prior, equal misclassification cost: $ \\[\\begin{alignat*}{1} P(\\pi_1 \\vert \\pmb X) &amp;= \\dfrac{f_1 ( \\pmb X) } {f_1 ( \\pmb X) + f_2 ( \\pmb X) } \\\\ &amp;= \\dfrac {\\exp \\left[ - \\dfrac {1}{2} (\\pmb X - \\pmb \\mu_1)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_1)&#39; \\right] } {\\exp \\left[ - \\dfrac {1}{2} (\\pmb X - \\pmb \\mu_1)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_1)&#39;\\right] + \\exp \\left[ - \\dfrac {1}{2} (\\pmb X - \\pmb \\mu_2)&#39; \\Sigma^{-1} (\\pmb X - \\pmb \\mu_2)&#39;\\right]} \\end{alignat*}\\] $ Allocate \\(\\pmb X_0\\) to \\(\\pi_1\\), if $P(_1 X_0) P(_2 X_0) $. This is equivalent to the Bayes Rule $R_1 f_1 (X) f_2 (X) $. 5.8.2.0.2 2. \\(\\Sigma_1 \\not = \\Sigma_2\\) (QDA) This is called Quadratic Discriminant Analysis (QDA). suppose the populations parameters, \\(\\pmb \\mu_1, \\pmb \\mu_2, \\Sigma_1, \\Sigma_2\\) are known. $ \\[\\begin{alignat*}{2} R_1 \\colon \\; &amp;-\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_1)&#39; \\Sigma_1^{-1} (\\pmb X - \\pmb \\mu_1)&#39; +\\dfrac {1} {2} (\\pmb X - \\pmb \\mu_2)&#39; \\Sigma_2^{-1} (\\pmb X - \\pmb \\mu_2)&#39;&amp; &amp;\\ge \\log \\left[ \\dfrac {P_2}{P_1} \\ast \\dfrac {C(1\\vert2)}{C(2\\vert1)} \\right] \\\\ \\\\ \\Rightarrow \\; \\; \\; \\; \\; &amp;(\\pmb \\mu_1 \\Sigma_1 ^{-1} - \\pmb \\mu_2 \\Sigma_2 ^{-1} )&#39; \\pmb X - \\dfrac {1}{2} \\pmb X &#39; (\\Sigma_1^{-1} - \\Sigma_2^{-1} ) \\pmb X \\; \\; \\; \\; \\; -k&amp; &amp;\\ge \\tag{3} \\\\ \\\\ \\Rightarrow \\; \\hat R_1 \\colon \\; \\; \\; \\; \\; &amp;(\\pmb \\mu_1 S_1 ^{-1} - \\pmb \\mu_2 S_2 ^{-1} )&#39; \\pmb X - \\dfrac {1}{2} \\pmb X &#39; (S_1^{-1} - S_2^{-1} ) \\pmb X \\; \\; \\; \\; \\; - k&amp; &amp;\\ge \\end{alignat*}\\] $ allocate \\(\\pmb X_0\\) to \\(\\pi_1\\) if \\(R_1\\) holds. \\((3)\\) where $ \\[\\begin{alignat*}{2} k &amp;= \\dfrac {1} {2} \\log \\left( \\dfrac {\\vert \\Sigma_1 \\vert}{\\vert \\Sigma_2 \\vert}\\right) + \\dfrac {1}{2} (\\pmb \\mu_1 &#39; \\Sigma_1^{-1} \\pmb \\mu_1 - \\pmb \\mu_2 &#39; \\Sigma_2^{-1} \\pmb \\mu_2) \\\\ &amp;= \\dfrac {1} {2} \\log \\left( \\dfrac {\\vert S_1 \\vert}{\\vert S_2 \\vert}\\right) + \\dfrac {1}{2} (\\hat {\\pmb X}_1 &#39; S_1^{-1} \\hat {\\pmb X}_1 - \\hat {\\pmb X}_2 &#39; S_2^{-1} \\hat {\\pmb X}_2) \\end{alignat*}\\] $ When \\(\\Sigma_1 = \\Sigma_2\\), \\(\\pmb X &#39; (\\Sigma_1^{-1} - \\Sigma_2^{-1} ) \\pmb X\\) disappears, which means LDA. This regions \\(R_1\\) is defined by quadratic functions of \\(\\pmb X\\). 5.8.3 Evaluating Classification Functions The misclassification probability is \\(P(1 \\vert 2) + P(2 \\vert 1)\\). . If \\(R_1, R_2\\) are selected by Bayes Rule, then this misclassification probability is the minimum. Sample Misclassification Probability: $P_1 {R_2} f_1 (x) dx + P_2 {R_1} f_2 (x) dx $ assume \\(f_1(\\pmb x), f_2(\\pmb x)\\) are unkown. 우리가 분포를 가정하지 않으면 이를 측정하는 것은 상당히 어렵다. 5.8.3.0.1 Apparent Error Rate (APER) 이는 training 샘플 중에서 sample classification function에 의해 분류될 때 잘못 분류된 관찰값들의 분수. * Training Sample: classification function 제작을 위해 사용되는 데이터 * Test Sample: classification function 평가를 위해 사용되는 데이터. 이는 training sample과는 독립. 5.8.3.0.1.1 Confusion Matrix | Predicted Membership \\(\\pi_1\\) | \\(\\pi_2\\) | | Actual\\(\\pi_1\\) | \\(n_{11}\\) | \\(n_{12}\\) | \\(n_1\\) | \\(\\pi_2\\) | \\(n_{21}\\) | \\(n_{22}\\) | \\(n_2\\) | the APER \\(= \\dfrac {n_{21} + n_{12}} {n_1 + n_2}\\): 오분류된 item들의 비율. APER은 true 오분류 확률을 과소평가한다. 이는 classification function 생산에 활용된 데이터들이 또한 이를 평가하기 위해서도 사용되기 때문. 생산에 쓰였던 놈들인만큼 생산된 classification function은 얘들한테 좀 더 최적화되어있을 수밖에 없고 이에 의해 에러율이 낮아진다. 5.8.3.0.2 Test Sample Error Rate training 샘플과 독립인, test 샘플이 따로 존재한다면,우리는 misclassification probability를 test 샘플에서 오분류된 비율로 misclassification probability를 계산하는 것이 가능하다. test 샘플이 없다면, 총 데이터를 training과 test 샘플로 쪼갠다. training 샘플은 classification function의 구축에 사용되고, test 샘플은 이를 평가하는데 쓰인다. 이 과정은 large sample을 필요로 한다. 5.8.3.0.3 Hold-out Error Rate \\(= \\dfrac {n_{21}^{(H)} + n_{12}^{(H)}} {n_1 + n_2}\\) Also called ‘leave-one-out’ or ‘cross-validation’ error rate. 1. 관측값 1개를 뽑아서 (omit) 제외한 후 나머지 데이터들을 사용해서 cf 생산. 2. 위에서 생산한 function을 써서 hold-out 관측값을 분류. 3. 모든 관측값들이 분류될 때까지 1, 2를 반복. 이에 의해, 역으로 1-LOO는 accuracy rate. 5.8.4 Classification with several Populations (wk13) let associated with \\(\\pi_i , i = 1, \\cdots, g\\): * density $f_i (x) $ * Prior distribution \\(P_i\\) * misclassification cost \\(C(k \\vert i)\\) * set of \\(\\pmb x\\) classified as, \\(R_k\\) \\(R_k\\) is the region that * \\(f_k(\\pmb x) \\propto P_k f_k(\\pmb x)\\) is largest * \\(\\sum_{\\not k} f_i(\\pmb x) \\propto \\sum_{\\not k} P_i f_i(\\pmb x) \\propto \\sum_{\\not k} C(k \\vert i) P_i f_i(\\pmb x)\\) is smallest under equal misclassification cost, allocate \\(\\pmb X_0\\) to \\(\\pi_k\\) if: $ \\[\\begin{alignat*}{3} \\forall i \\not = &amp;k: P_k f_k (\\pmb x) &amp;&amp;&gt;P_i f_i (\\pmb x) \\\\ &amp;\\log P_k f_k (\\pmb x) &amp;&amp;&gt;\\log P_i f_i (\\pmb x) \\\\ \\end{alignat*}\\] $ the Bayes rule is identical to the rule that maximizes Posterior Probability \\(P(\\pi \\vert \\pmb X) = \\dfrac{P_k f_k (\\pmb x)}{\\sum_{i=1}^g P_i f_i (\\pmb x)}\\) 5.8.4.0.1 Classification with several Normal Populations assume \\(f_i (\\pmb x) \\sim N_p (\\pmb \\mu_i , \\Sigma_i )\\), and equal misclassification cost. 5.8.4.0.1.1 1. \\(\\Sigma_1 = \\cdots = \\Sigma_g = \\Sigma\\) (LDA) MVN을 따르는 것에서 \\(f_k\\)의 형은 알 수 있다. according to Bayes rule, we allocate \\(\\pmb x_0\\) to \\(\\pi_k\\) if $ P_k f_k ( x_0 ) = _i P_i f_i ( x_0 ) $ 이때 constant $- (2 ) - () - x_0 ’ ^{-1} x_0 $는 모든 \\(\\log P_i f_i (\\pmb x_0)\\)에 대해 공통 (\\(k\\)에 의존하지 않으므로). 따라서 해당 constant 부위는 비교 목적으로는 무시될 수 있음. $ \\[\\begin{align*} d_i (\\pmb x) &amp;= \\pmb \\mu_i &#39; \\Sigma^{-1} \\left( \\pmb x \\right) - \\tfrac{1}{2} \\pmb \\mu_i &#39; \\Sigma^{-1} \\pmb \\mu_i + \\log P_i \\tag{1} \\\\ \\\\ S_p &amp;= \\tfrac{1}{(n_1 + \\cdots +n_g) - g} \\left[ (n_1-1)S_1 + \\cdots (n_g - 1)S_g \\right] \\tag{2} \\\\ \\\\ \\Longrightarrow \\hat d_i (\\pmb x) &amp;= \\bar { \\pmb x_i} S_p^{-1} \\ast \\pmb x - \\tfrac{1}{2} \\bar { \\pmb x_i} &#39; S_p^{-1} \\bar { \\pmb x_i} + \\log P_i \\tag{3} \\end{align*}\\] $ define linear discriminant function (LDF) \\(d_i (\\pmb x)\\), where \\(i=1, \\cdots, g\\). \\(\\Sigma\\)’s pooled estimate \\(S_p\\) \\(d_i (\\pmb x)\\)’s estimate \\(\\hat d_i (\\pmb x)\\), 실질적으로 사용할 LDF function. Estimated Bayes Rule: allocate \\(\\pmb x_0\\) to \\(\\pi_k\\), if \\(\\hat d_k(\\pmb x_0) = \\max \\{ \\hat d_1(\\pmb x_0), \\cdots, \\hat d_g(\\pmb x_0) \\}\\). 뒤의 조건을 만족하는 것이 Likelihood의 키가 가장 큰 population이므로. 5.8.4.0.1.2 2. \\(\\not = \\Sigma\\) (QDA) $ P_k f_k ( x_0 ) = _i P_i f_i ( x_0 ) $ constant \\(-\\dfrac{p}{2} \\log(2 \\pi)\\)는 모든 \\(\\log P_i f_i (\\pmb x_0)\\)에 대해 공통, 무시 가능. define quadratic discriminant function \\(d_i^Q (\\pmb x)\\), where \\(i=1, \\cdots, g\\): $ \\[\\begin{align*} d_i^Q (\\pmb x) &amp;= -\\dfrac{1}{2} \\log\\vert\\Sigma_i \\vert -\\dfrac{1}{2} (\\pmb x - \\pmb \\mu_i)&#39; \\Sigma_i^{-1} (\\pmb x - \\pmb \\mu_i) +\\log P_i \\\\ \\\\ \\hat {d}_i^Q (\\pmb x) &amp;= -\\dfrac{1}{2} \\log\\vert S_i \\vert -\\dfrac{1}{2} (\\pmb x - \\bar {\\pmb x_i})&#39; S_i^{-1} (\\pmb x - \\bar {\\pmb x_i}) +\\log P_i \\tag{Sample} \\end{align*}\\] $ Estimated Bayes Rule: allocate \\(\\pmb x_0\\) to \\(\\pi_k\\), if \\(\\hat d_k^Q(\\pmb x_0) = \\max \\{ \\hat d_1^Q(\\pmb x_0), \\cdots, \\hat d_g^Q(\\pmb x_0) \\}\\) 5.8.5 Other Discriminant Analysis Methods Nearest Neighbor Discriminant Analysis (거리 함수 사용) Nonparametric approach – no assumption on distribution Idea *For a new observation, first find the observation in the training sample that is closest to the new observation. (i.e. its Mahalanobis distance is smallest) Then assign the new observation to the group from which the observation’s nearest neighbor comes. Variations: K-nearest neighbor assign each new observation to the group to which a majority of its k nearest neighbors belongs. e.g. k=5. 5.8.5.0.1 KNN \\(K_1\\) belongs to group 1, \\(K_2\\) belongs to group 2. 우리는 \\(K_1 + K_2 =K =5\\) 로 설정함. 즉 가장 가까운 이웃 5개를 뽑되 Group 1과 Group 2에서 뽑은 애들을 합하면 총 5개여야 함. assign \\(\\pmb x_0\\) to group 1 (\\(\\pi1\\)) if \\(K_1 \\ge K_2\\). - if \\(n_1 \\not = n_2\\), then assign \\(\\pmb x_0\\) to \\(\\pi1\\) if \\(\\dfrac{K_1}{n_1} \\ge \\dfrac{K_2}{n_2}\\). - if \\(P_1 \\not = P_2\\), then assign \\(\\pmb x_0\\) to \\(\\pi1\\) if \\(P_1\\dfrac{K_1}{n_1} \\ge P_2\\dfrac{K_2}{n_2}\\). choice of hyper-parameters \\(K\\): \\(K = \\sqrt{n_1}\\). select \\(K\\) s.t. minimizes the error rate 5.8.5.0.2 Kernel (Density Estimation) Discriminant Analysis (KDA) Bayes Rule 이론에서 출발, Likelihood 함수 사용 (KNN과는 이 부분부터 다름. KNN은 Bayes Rule 안썼음): allocate \\(\\pmb x_0\\) to \\(\\pi_k\\), if \\(\\dfrac{f_1 (\\pmb x)}{f_2 (\\pmb x)} \\ge \\dfrac{P_2 C(1 \\vert 2)}{P_1 C(2 \\vert 1)}\\) 분포에 대한 가정 없이 개시하므로, 밀도함수 자체를 추정해버리자. density estimation: estimate f_1 (x) and f_2 (x) for each point \\(\\pmb x\\), where \\(N(\\pmb x_0)\\) is neighborhood around \\(\\pmb x_0\\) of width \\(\\lambda\\) 이동 히스토그램 람다는 벽돌 하나의 넓이이며, 람다값이 달라지면 추정된 pdf의 형 또한 조금씩 바뀔 수 있음 $ f(x_0) = $ this estimate is bumpy. 더 발전된 추정법을 찾아내자. 개선된 추정법: Parzen Estimate $ f(x_0) = {i=1}^N K{} (x_0 , x_i) $ 위의 초기형 추정에서 사용된 커널함수는 uniform. 가우시안 커널은 정규분포의 형을 따르므로 이는 당연히 분산을 필요로 함. 여기서 분산 부분에 들어가는건 람다이며, 따라서 람다는 벽돌의 넓이, width를 결정하게 된다. 따라서 람다는 called as smoothing parameters, or bandwidth. 추정의 성능은 거의 전적으로 람다의 selection에 달려있음. 람다 잘 고르면 추정 성능 높고, 람다 잘못 고르면 떡락함. 람다를 너무 좁게 잡으면 삐쭉삐쭉해서 과반영되고, 너무 넓게 잡으면 민둥산이 나와서 값 간의 density가 다 비슷비슷한 나쁜 pdf가 추정됨. at here, popular choice of \\(K_{\\lambda}\\) is Gaussian kernal: $ K_{} (x_0 , x) = ( ) = { ( x_i - x_0 )^2 } $ estimated Bayes Rule: allocate \\(\\pmb x_0\\) to \\(\\pi_1\\), if \\(\\dfrac{\\hat f_1 (\\pmb x_0)}{\\hat f_2 (\\pmb x_0)} \\ge \\dfrac{P_2 C(1 \\vert 2)}{P_1 C(2 \\vert 1)}\\) 이런 식의 비율 접근법은 클래스가 2개인 경우 한정. 늘어나면 달라? 5.8.5.0.3 Modern Classification Methods: Decision Trees Classification Trees Regression Trees Neutral Networks Support Vector Machines Ensemble "],["clustering-distance-methods-and-ordination.html", "5.9 Clustering, Distance Methods, and Ordination", " 5.9 Clustering, Distance Methods, and Ordination 5.9.1 Overview Example: Customer Segmentation 5.9.1.0.1 Clustering 군집화의 기준 동일한 군집에 속하는 개체 (또는 개인) 은 여러 속성이 비슷하고, 서로 다른 군집에 속한 관찰치는 그렇지 않도록 (여러 속성이 비슷하지 않도록) 군집을 구성 군집화를 위한 변수: 전체 개체 (개인) 의 속성을 판단하기 위한 기준 인구통계적 변인 (성별, 나이, 거주지, 직업, 소득, 교육 등) 구매패턴 변인 (상품, 주기, 거래액 등) 군집분석에서는 관측값들이 서로 얼마나 유사한지, 또는 유사하지 않은지를 측정할 수 있는 측도가 필요하다. - 군집분석에서는 보통 유사성(similarity)보다는 비유사성(dissimilarity)를 기준으로 하며, 거리(distance)를 사용한다. \\(x\\)가 연속형일 때 CA의 위력이 최고로 발휘됨. 유사성의 척도로 거리가 사용되는데, 카테고리컬 변수에는 거리 계산이 불가능하기 때문. 꼭꼭 카테고리컬 변수로 CA를 해야겠다면 지시변수로 대체하여 CA를 시도할 수는 있겠으나, 이는 어느정도 억지로 하는 것이고 오점없는 CA는 아님. 5.9.1.0.2 Distance Measures 거리 (Distance) 라는 함수. CA에서 사용되는 모든 거리는 pairwise 거리. Euclid 거리 (Euclidean) : 가장 메이저함 p차원 공간에서 주어진 두 점 \\(\\pmb x=(x_1 , \\cdots, x_p), \\; \\; \\pmb y=(y_1 , \\cdots, y_p)\\) 사이의 유클리드 거리는 $ d(x, y) = $ if \\(p=2\\), {:start=“2”} Minkowski 거리 $ d(x, y) = { {_{i=1}^p (x_i - y_i)^m} }^{} $ \\(m=2\\)일 때 이는 Euclidean과 같아진다. 보통은 m의 값으로 짝수를 많이 씀. 민코프는 결국 Euclidean의 일반화. {:start=“3”} Mahalanobis 거리 위에서 A와 B의 거리만을 보는 것이 아니라 위의 점들의 군집의 패턴 또한 고려함. x축과 y축에 해당하는 변수들 사이에 correlation이 있다는 것을 반영함. 중앙의 \\(S^{-1}\\)으로 corr 구조를 반영하는 것. 뭔 메커니즘으로? 위 케이스를 생각하면 A는 전체적인 패턴의 연장선 상에서 멀리 있는데, B는 패턴에서 직교해서 벗어나면서 가까이 있음. 따라서 A보다 B가 멀다고 평가 가능. $ d(x, y) = $ {:start=“4”} Manhattan 거리 $ d_{Manhattan} (x, y) = {_{i=1}^p x_i - y_i } $ Standardization CA는 자료 사이의 거리를 이용하여 수행되기 때문에, 각 자료의 단위가 결과에 큰 영향을 미친다. 이러한 문제를 해결하기 위하여, 가장 널리 쓰이는 방법이 표준화 방법이다. 표준화 방법이란 각 변수의 관찰값으로부터 그 변수의 평균을 빼고, 그 변수의 표준편차로 나누는 것이다. 표준화된 모든 변수가 평균이 0이고 표준편차가 1이 된다. 사실상 필수. Graphical Tools Scatter Plot Scatter Plot using PCA Andrews Plot Star Plot Chernoff Faces 5.9.2 Hierarchical Clustering Start with \\(N\\) clusters, each containing a single entity and an \\(N \\times N\\) symmetric matrix of distances, \\(D=\\{d_{ik}\\}\\). Search the distance Matrix \\(D\\) for the nearest pair of clusters. Let the distance b/w the most similar (가장 거리가 작은) clusters \\(U\\) and \\(V\\) be \\(d_{UV}\\). Mearge clusters \\(U\\) and \\(V\\). Label the newly formed cluster \\((UV)\\). Update the entries in the distance Matrix \\(D\\) by squences below. The distance b/w \\((UV)\\) and other cluster \\(W\\) is denoted by \\(d_{(UV)W}\\). deleting rows and columns corresponding to clusters \\(U\\) and \\(V\\), then adding a row and a column giving the distance b/w \\((UV)\\) and the remaining clusters. repeat steps 2 and 3 a total of \\(N-1\\) times. Then, all observations will be in single clusters. Record the identity of clusters that are merged and the levels at which the mergers take place. 5.9.2.0.1 계층적 군집분석 Example distance Matrix \\(D\\)는 \\(n^2\\)에 의존하여 변수 숫자가 증가하면 연산 시간도 기하급수적으로 증가. {:start=“5”} 5. 계층적 분석에서만 덴드로그램을 그릴 수 있음. a graphical tool to illustrate the merges or divisions. python 라이브러리 함수 기준 총 distance의 70%에서 짤라서 clutser를 판정. color_threshold. 5.9.2.0.2 HCA의 종류 Single Linkage, 단일 연결 (mimum distance, or nearest neighbor) $ d_{(UV)W} = ( d_{UW}, d_{VW} ) $ {:start=“2”} 2. Complete Linkage, 완전 연결 (maximum distance, or farthest neighbor) $ d_{(UV)W} = ( d_{UW}, d_{VW} ) $ {:start=“3”} 3. Average Linkage, 평균 연결 (average distance) - 위의 둘이 변동이 너무 심해서 이를 해결하기 위해 제시됨 $ d_{(UV)W} = ( {i=1}^{n{UV}} {j=1}^{n{W}} d_{ij} ) $ {:start=“4”} 4. Centriod Method, 중심점 연결 (For each cluster, compute the centroid) $ d_{(UV)W} = U V $ {:start=“5”} 5. Ward’s Method bold들이 무난함 5.9.2.0.3 HCA의 장단점 Advantage: - cluster의 수를 알 필요가 없음 - 덴드로그램 통해 군집화 프로세스와 결과물을 표현 가능 Disadvantage: - 계산속도가 느림 - 아웃라이어 (이상치) 가 존재할 경우, 초기 단계에 잘못 분류된 군집은 분석이 끝날때까지 소속 cluster가 변하지 않음 - 아웃라이어에 대한 사전검토 필요, Centroid 방법이 아웃라이어에 덜 민감함 5.9.3 K-means Clustering K-평균 군집분석법. 사전에 결정된 군집수 \\(k\\)에 기초하여 전체 데이터를 상대적으로 유사한 k개의 군집으로 구분한다. Proceeds: 1. 군집수 k를 결정한다 2. 초기 k개 군집의 중심을 선택한다 (랜덤하게) 3. 각 관찰치를 그 중심과 가장 가까운 거리에 있는 군집에 할당한다. 4. 형성된 군집의 중심 (centroid) 을 계산한다. 5. 3-4의 과정을 기존의 중심과 새로운 중심의 차이가 없을 때까지 반복한다. 5.9.3.0.1 Determination of K KCA의 결과는 초기 군집수 k의 결정에 민감하게 반응한다. 여러가지의 k값을 선택하여 CA를 수행한 후 가장 좋다고 생각되는 k값을 이용. Elbow point 계산하여 k 선택 Silhouette plot으로 k 선택 자료의 시각화를 통하여 K를 결정 (ex. star plot을 2차원 df로 바꾸어 평균 체크했었음) 자료의 시각화를 위해서는 차원축소가 필수적이고, 이를 위하여 PCA가 널리 사용된다. 대용량 데이터에서 sampling한 데이터 (이것이 스몰데이터가 됨) 로 HCA를 우선 수행하여 (여기서 덴드로그램이 얻어짐) k의 값을 선택 (즉 HCA와 KCA를 둘 다 쓰므로 hybrid) 5.9.4 군집의 평가방법 Silhouette Score (Silhouette Plot) $ s(i) = = \\[\\begin{cases} 1-\\dfrac{a(i)}{b(i)}, &amp; if \\; \\; a(i) &lt; b(i) \\\\ 0, &amp; if \\; \\; a(i) = b(i) \\\\ \\dfrac{b(i)}{a(i)} - 1, &amp; if \\; \\; a(i) &gt; b(i) \\end{cases}\\] $ \\(a(i)\\): 개체 \\(i\\)로부터 같은 군집 내에 있는 모든 다른 개체들 사이의 평균 거리. 작을수록 좋다. 작을수록 해당하는 군집 안에서 중앙 부분에 components가 모여 있다는 소리이므로. \\(b(i)\\): 개체 \\(i\\)로부터 다른 군집 내에 있는 개체들 사이의 평균 거리 중 가장 작은 값. 클수록 좋다. 클수록 다른 군집에 헷갈려서 속할 일 없이 확실하게 현재 소속되어 있는 군집에 소속되어 구분된다는 소리이므로. 1을 넘어갈 수 없으며, 1에 가까울수록 군집화가 잘 된 관찰값. 몇개의 cluster가 설정되었을 때 가장 해당 stat이 높게 나오는지를 통해 판정하는 것이 이 접근법. 평균 Silhouette Score는 모든 obs마다 \\(s(i)\\)를 구하여 이를 평균낸 값이므로, 평균 Silhouette Score가 1에 가까울수록 군집분석이 잘됐다고 판단 가능. 5.9.5 Clustering using Density Estimation (wk14) Based on nonparametric density estimation The clusters may be viewed as high-density regions in the space separated by low-density regions between them. No need to specify the number of clusters. It is determined by the method itself. 밀도기반 추정에 요구되는 (hyper) Parameter: bandwidth. 해당 값이 달라지면 결과도 달라짐. Iris 데이터 예 5.9.5.0.1 Kernel Density Estimation (KDE) $ f(x_0) = _{i=1}^N K ( ) , ; ; ; ; ; x R $ N은 샘플사이즈, 람다는 밴드위스, K는 스무딩 커널, x_i는 obs closed form처럼 보이지만 그냥 상징적인 공식일 뿐. closed form이 있는게 아니라 데이터 포인트마다 고유한 값이 추정되는 것으로 진행됨. 밀도추정에서 가장 많이 쓰는 방법. 추정하고 싶은 포인트는 \\(x_0\\). \\(x_0\\)라는 포인트에 대해 density를 추정하고 싶다. \\(x_0\\) 인근의 관찰치는 더 많은 가중치를 가짐. \\(x_0\\) 로 부터 멀어질수록 가중치는 감소함. 각 obs 별로 커널함수 부여하고 최종적으로 그 커널함수 다 더한 다음에 스케일링하면 끝. K의 가장 흔한 선택은 정규분포함수, 즉 Gaussian Kernel Bandwidth 의 효과: 커널함수의 좌우 넓이에 해당하는 것으로서, 가우시안 커널에서는 표준편차에 해당함. Bandwith가 크면 x값들 간에 차별화가 덜되어서 추정 위력이 떨어짐 봉우리의 갯수는 군집의 갯수로 생각할 수 있음. 지나치게 밴드위스가 좁으면 뾰족한 부분이 다수 튀어나와 군집의 과다추정 발생 그래프는 1차원 밀도 추정에 해당 회색: 정답. 표준정규분포 붉은색: undersmoothed,  = 0.05 (too small) 녹색: oversmoothed,  = 2 (too large) 검정색: optimally smoothed,  = 0.337 Bandwidth 추정 2D Kernel Density Estimation: 2차원에서의 KDE는 어떻게 확장될 것인가? 5.9.6 Multidimensional Scaling (MDS) Dimension Reduction Methods - PCA : x변수들끼리의 분산을 최대화시키는 방향으로 차원축소. 한 변수의 분산이 최대화되어야 함 - Factor: 변수간의 correlation을 최대한 깨트리지 않고 반영하는 방향으로 DR. Corr 구조가 최대한 유지 - MDS - Canonical Discriminant Analysis 이중 위의 둘은 original data의 Variance 설명에 집중함. (ex. 1명이 401호, 1명이 501호에 있다고 하면, 둘의 직선 거리가 그렇게 크게 떨어져있다고 하기는 어렵지만 위의 두 분석법은 멀리 떨어져 있는 것처럼 그래프에 표현될 수 있음. 거리 개념이 없기 땨문) MDS Fit (projection) the original data into a low-dimensional coordinate system such that any distortion caused by a reduction in dimensionality is minimized. Map the distances between points in a high dimensional space into a lower dimensional space. distortion이란? dissimilarity (distance) among the original data points For a given set of observed similarities (or distances) between every pair of N items, find a representative of the items in as few dimensions as possible such that the similarities (or distances) in the lower dimensions match, as close as possible with the original similarities (or distances). Nonmetric MDS Only the rank orders of the N(N-1)/2 original similarities are used to arrange N items in a lower-dimensional coordinate system. 거리 없이 rank만 주어져있음. rank만 안무너지도록 Metric MDS (자주씀. Principal Coordinate Analysis) The actual magnitudes of the original similarities are used to obtain a geometric representation. 5.9.6.0.1 Kruskal’s Stress so-called Badness of fit criterion. MDS가 잘됐다면 기존 오리지널 차원의 거리나 차원축소된 이후의 거리나 비슷해야 함. 크루스칼 스트레스가 작으면 왜곡도 작은 것. 스트레스가 최소인 DR이 최고의 DR. Let \\(D_{rs}\\) denote the actual distance (or dissimilarity) between item r and item s, then the ordered distances are $D_{r_1 s_1 } &lt;D_{r_2 s_2 } &lt; &lt; D_{r_M s_M }, ; ; ; M= \\[\\begin{pmatrix} N \\\\ 2 \\end{pmatrix}\\] $. Let \\(d_{rs}\\) denote the distance between item r and item s in the lower dimensional space. MDS seeks (iteratively) to find a set of \\(d\\)’s such that \\(d_{r_1 s_1 } &lt;d_{r_2 s_2 } &lt; \\cdots &lt; d_{r_M s_M }\\) and \\(Stress = \\left\\{ \\dfrac{\\sum_{i=1}^N \\sum_{j=1}^{i-1}(D_{ij} - d_{ij})^2} {\\sum_{i=1}^N \\sum_{j=1}^{i-1} \\left( D_{ij} \\right)^2} \\right\\}^{\\tfrac{1}{2}}\\) is minimized. Interpretation Guideline Stress Goodness of Fit 20% Poor 10% Fair 5% Good 2.5% Excellent 0% Perfect Goodness of fit = monotonic relationship between the similarities and the final distances. Takane’s Stress $ Stress = { {{i=1}^N {j=1}{i-1}(D_{ij}2)^2} }^{} $ Algorithm: 1. For N items, obtain \\(M=\\dfrac{N(N-1)}{2}\\) 개의 distances \\(D_{r_1 s_1 }, D_{r_2 s_2 } , \\cdots , D_{r_M s_M }\\). Tehn an \\(N \\times N\\) matrix \\(D = \\{D_{ij} \\}\\) is constructed. Using a trial configuration in q dimensions, determine distances \\(d_{ij}^{(q)}\\). The method to get initial \\(d_{ij}^{(q)}\\) is given later. Using the \\(d_{ij}^{(q)}\\), move the points around to obtain an improved configurations. A new configuration: new \\(d_{ij}^{(q)}\\) and smaller stress (e.g. Newton-Raphson method) The process is repeated until the best (minimum stress) representation is obtained. Plot minimum stress (q) versus q and choose the best number of dimensions, \\(q^\\ast\\) from an examination of this plot. x축은 축소된 차원, y축은 stress. 차원이 작아질수록 Stress는 높고, 차원이 p라면 (original 차원과 같다면) Stress는 0. PCA와 달리 여기서는 elbow에서 멈춤. similar to scree plot Note: 1. The larger the dimension, the better the fit. 2. Higher dimension means harder to visualize. 5.9.6.0.2 Algorithm to find 초기값 \\(d_{ij}^{(q)}\\) q값을 줄이려면 수치해석을 시작하기 전에 넣어줄 초기값에 해당하는 초기좌표들이 필요함. 그 값을 구하는 방법. Construct the \\(N \\times N\\) matrix \\(A = \\{ a_{ij} \\} = \\left\\{ -\\dfrac{1}{2} D_{ij}^2 \\right\\}\\). Construct the \\(N \\times N\\) matrix \\(B = \\left(I - \\dfrac{1}{N} J \\right) A \\left(I - \\dfrac{1}{N} J \\right) = \\{ b_{ij} \\} = \\{ \\bar a_{ij} - \\bar a_{i.} - \\bar a_{.j} + \\bar a_{..} \\}\\). where $ a_{..} = ^N ^N , ; ; ; ; ; J = \\[\\begin{bmatrix} 1 &amp; \\cdots &amp; 1 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; \\cdots &amp; 1 \\end{bmatrix}\\] $ {:start=“3”} D행렬은 distance들의 SSE 행렬 정도에 해당. Since \\(B\\) is a symmetric matrix, use the spectral decomposition to write \\(B\\) in the \\(B = V \\Lambda V&#39;\\). If B is positive semidefinite of rank q (p차원 아님!! \\(q \\le p\\). 거리행렬이 일정 ev까지는 유의할 수 있는데 그 후로는 0만 튀어나올 수 있으며 DR은 바로 이상황에서 일어남. p는 위에서 보였던 유사 scree plot에서 original data의 차원으로 지정되었던 숫자) , there are q positive eigenvalues. if $ _1 = \\[\\begin{bmatrix} \\lambda_1 &amp; \\cdots &amp; \\pmb 0 \\\\ &amp; \\ddots &amp; \\\\ \\pmb 0 &amp; \\cdots &amp; \\lambda_q \\end{bmatrix}\\] _{q q}, ; ; ; ; ; V_1 = \\[\\begin{bmatrix} \\pmb v_1 , \\pmb v_2 , \\cdots, \\pmb v_q \\end{bmatrix}\\] _{N q} $ then we can express $ B = { V_1 }{N q} { 1 }{q q} { V_1 ’ }{q N} = V_1 _1^{1/2} _1^{1/2} V_1 ’ = ZZ’ $ where $ Z = V_1 _1^{1/2} = \\[\\begin{bmatrix} \\sqrt{\\lambda_1} \\pmb v_1 , \\sqrt{\\lambda_2} \\pmb v_2 , \\cdots, \\sqrt{\\lambda_q} \\pmb v_q \\end{bmatrix}\\] = \\[\\begin{bmatrix} \\pmb z_1 &#39; \\\\ \\pmb z_2 &#39; \\\\ \\vdots \\\\ \\pmb z_q &#39; \\end{bmatrix}\\] _{N q} $ {:start=“4”} The rows $z_1 ’ , z_2 ’ , , z_q $ of \\(Z\\) are the points whose interpoint distance \\(d_{ij}^{(q)} = (\\pmb z_i - \\pmb z_j)&#39;(\\pmb z_i - \\pmb z_j)\\) match $D_{ij} $s in the original distance matrix \\(D\\). Since \\(q\\) will typically be too large to be of practical interest and we would prefer a smaller dimension \\(k\\) for plotting, we can use the first \\(k\\) eigenvalues and corresponding eigenvectors to obtain \\(N\\) points whose distances \\(d_{ij}^{(k)}\\) are approximately equal to the corresponding \\(D_{ij}\\). 오리지널 데이터의 차원 p가 15개였다면, 이 차원을 SVD했을 때 ev 중 5개가 0이어서 q는 15개로 하였다. 여기서 차원을 더 줄이고 싶다면, 가령 k=5개까지 임의로 내려버리고 싶다면, 뒤쪽의 ev 10개에 해당하는 걸 쳐내는 것 Rank is clearly rank 2. 즉 차원을 2차원까지 줄여도 손실되는 정보가 전혀 없다. 따라서 orginal data Distance Matrix에서 보였던 특성이 그대로 똑같이 드러난다. "],["linear.html", "Chapter 6 Linear ", " Chapter 6 Linear "],["svd.html", "6.1 SVD", " 6.1 SVD 6.1.1 Spectral Decomposition $ \\[\\begin{align} A = \\begin{pmatrix} a_{11} &amp; &amp; \\cdots &amp; &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; &amp; &amp; \\vdots \\\\ a_{i1} &amp; &amp; \\ddots &amp; &amp; a_{1n} \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; &amp; \\cdots &amp; &amp; a_{mn} \\end{pmatrix} = (a_{ij}) &amp;\\; \\; \\; \\; \\; \\; \\;= \\begin{pmatrix} \\pmb r_1 \\\\ \\vdots \\\\ \\pmb r_m \\end{pmatrix} \\\\\\ \\\\\\ &amp;\\; \\; \\; \\; \\; \\; \\;= \\begin{pmatrix} \\pmb c_1 &amp; \\cdots &amp; \\pmb c_m \\end{pmatrix} \\end{align}\\] $ for symmetric matrix \\(A\\): $ A_{p p} = ^{T} = _{j=1}^p _j _j _j^T $ $ \\[\\begin{alignat}{2} &amp;\\Lambda = diag \\{\\lambda_1 , \\cdots, \\lambda_p \\} &amp;&amp;\\; \\; \\; \\; \\;= \\begin{pmatrix} \\lambda_1 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; \\lambda_p \\end{pmatrix} _{p \\times p} \\\\ &amp;\\Gamma &amp;&amp;\\; \\; \\; \\; \\;= \\begin{pmatrix} \\pmb\\gamma_1 ,&amp; \\cdots, &amp;\\pmb\\gamma_p \\end{pmatrix}_{p \\times p} \\end{alignat}\\] $ where \\(\\pmb \\gamma_j\\) is evec of \\(A\\). Therefore \\(\\Gamma\\) is orthogonal Matrix. let symmetric Matrix of rank \\(r\\), \\(A_{p \\times p}\\), \\((r \\le p)\\). Then there exists orthogonal Matrix \\(\\Gamma_{p \\times p}\\), which means \\(\\Gamma^T \\Gamma = I_p\\) and $ A = ^T = \\[\\begin{pmatrix} \\Lambda_1 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}\\] ^T = _1 _1 ^T_1 $ where letting \\(\\delta_i=\\) i-th ev, \\(i=1, \\cdots, r\\), then $ \\ \\ = \\[\\begin{pmatrix} \\{\\Gamma_1\\}_{p \\times r} , \\; \\; \\; \\{\\Gamma_0\\}_{p \\times (p-r)} \\end{pmatrix}\\] \\ \\ = diag {_1 , , _r } = \\[\\begin{pmatrix} \\lambda_1 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; \\lambda_r \\end{pmatrix}\\] _{r r} $ then \\(\\Gamma_1^T \\Gamma_1 = I_r, \\; \\; \\; \\; \\Gamma_1^T \\Gamma_0 = 0\\) and $ A^2 = A’A = AA’ = (_1 _1 ^T_1)’ _1 _1 ^T_1 = _1 _1 ^T_1 _1 _1 ^T_1 = _1 _1^2 ^T_1 $ let \\(\\{\\pmb \\gamma_i\\}_{p \\times 1}\\) be i-th column vector of \\(\\Gamma\\). then $ _i’ _i = \\[\\begin{cases} 1 &amp; &amp; i=j \\\\ 0 &amp; &amp; i \\not = j \\end{cases}\\] $ thus $ \\[\\begin{alignat}{2} A &amp;= \\Gamma_1 \\Lambda_1 \\Gamma^T_1 &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ A&#39;A &amp;= \\Gamma_1 \\Lambda_1^2 \\Gamma^T_1 &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ AA&#39;&amp;= &amp;&amp; \\\\ \\gamma_k &#39; A &amp;= \\lambda_k \\gamma_k &#39; \\gamma_k \\gamma_k &#39; &amp;&amp;= \\lambda_k \\pmb\\gamma_k &#39; \\\\ A \\gamma_k &amp;= \\lambda_k \\gamma_k \\gamma_k &#39; \\gamma_k &amp;&amp;= \\lambda_k \\pmb \\gamma_k \\end{alignat}\\] $ 6.1.1.0.1 remark let orthogonal Matrix \\(\\Gamma\\), therefore \\(\\Gamma &#39; \\Gamma = I\\), and \\(\\det(\\Gamma) = \\vert \\Gamma \\vert = 1\\). let symmetric Matrix \\(A_{p \\times p}\\) with full rank. then by SVD, $ (A) = A = ^T = = \\[\\begin{vmatrix} \\lambda &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\lambda_p \\end{vmatrix}\\] = _{i=1}^p _i $ let symmetric Matrix \\(A_{p \\times p}\\) with full rank. then by SVD, $ n: ; ; ; A^n = (‘) (’)(‘) = ^n ’ $ in particular, a Cov Matrix \\(\\Sigma\\) can be written by $ = ^T = _{i=1}^r _i _i _i’ \\ ^{-1} = ^{-1} ^T = _{i=1}^r _i^{-1} _i _i’ \\ ^{-} = ^{-} ^T = _{i=1}^r _i^{-} _i _i’ $ 6.1.2 Singular value Decomposition: General-version decomposition of any aribtrary Matrix with rank \\(r\\), \\(A_{n \\times p} = \\Gamma_{n \\times r} \\Lambda \\triangle_{p \\times r} &#39; = \\sum_{j=1}^r \\lambda_j \\pmb \\gamma_j \\pmb \\delta_j &#39;\\). \\(\\Lambda = diag(\\lambda_1 , \\cdots, \\lambda_r), \\; \\; \\; \\; \\lambda_j &gt;0\\). 이때 \\(\\lambda_i\\)는 \\(AA&#39;\\)나 \\(A&#39;A\\)의 non-zero ev. \\(\\Gamma, \\triangle\\)는 \\(AA&#39;\\)와 \\(A&#39;A\\)의 corresponding \\(r\\) evec으로 구성. 따라서 Both $’ = ’ = I_r $, i.e., are column orthogonal. thus $ \\[\\begin{alignat}{2} A &amp;= \\Gamma \\Lambda \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\delta_i &#39; \\\\ A&#39;A &amp;= \\triangle \\Lambda^2 \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\delta_i \\pmb \\delta_i &#39; \\\\ AA&#39;&amp;= \\Gamma \\Lambda^2 \\Gamma^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ \\gamma_k &#39; A &amp;= \\gamma_k &#39; \\Gamma \\Lambda \\triangle^T &amp;&amp;= \\lambda_k \\pmb \\delta_k &#39; \\\\ A \\delta_k &amp;= \\Gamma \\Lambda \\triangle^T \\delta_k &amp;&amp;= \\lambda_k \\pmb \\gamma_k \\end{alignat}\\] $ a b \\(\\begin{alignat}{2} A &amp;= \\Gamma \\Lambda \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\delta_i &#39; \\\\ A&#39;A &amp;= \\triangle \\Lambda^2 \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\delta_i \\pmb \\delta_i &#39; \\\\ AA&#39;&amp;= \\Gamma \\Lambda^2 \\Gamma^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ \\gamma_k &#39; A &amp;= \\gamma_k &#39; \\Gamma \\Lambda \\triangle^T &amp;&amp;= \\lambda_k \\pmb \\delta_k &#39; \\\\ A \\delta_k &amp;= \\Gamma \\Lambda \\triangle^T \\delta_k &amp;&amp;= \\lambda_k \\pmb \\gamma_k \\end{alignat}\\) $\\[\\begin{alignat}{2} A &amp;= \\Gamma_1 \\Lambda_1 \\Gamma^T_1 &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ A&#39;A &amp;= \\Gamma_1 \\Lambda_1^2 \\Gamma^T_1 &amp;&amp;=\\sum_{i=1}^r \\lambda_i^2 \\pmb \\gamma_i \\pmb \\gamma_i &#39; \\\\ AA&#39;&amp;= &amp;&amp; \\\\ \\gamma_k &#39; A &amp;= \\lambda_k \\gamma_k &#39; \\gamma_k \\gamma_k &#39; &amp;&amp;= \\lambda_k \\pmb\\gamma_k &#39; \\\\ A \\gamma_k &amp;= \\lambda_k \\gamma_k \\gamma_k &#39; \\gamma_k &amp;&amp;= \\lambda_k \\pmb \\gamma_k \\end{alignat}\\] $ c d therefore, generalized inverse matrix, G-inverse Matrix \\(A^-\\) will be $ \\[\\begin{alignat}{2} A &amp;= \\Gamma \\Lambda \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\delta_i &#39; \\\\ A^- &amp;= \\triangle \\Lambda^{-1} \\Gamma&#39; &amp;&amp;=\\sum_{i=1}^r \\lambda_i^{-1} \\pmb \\delta_i \\pmb \\gamma_i &#39; \\\\ AA^- A &amp;= \\Gamma \\Lambda \\triangle^T \\ast \\triangle \\Lambda^{-1} \\Gamma&#39; \\ast \\Gamma \\Lambda \\triangle^T &amp;&amp;=\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\delta_i &#39; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; = A \\end{alignat}\\] $ 6.1.3 Singular value Decomposition: Another-version rank \\(r\\) arbitrary Matrix \\(A_{n \\times p} = \\Gamma_{n \\times r} \\Lambda \\triangle^T_{p \\times r} =\\sum_{i=1}^r \\lambda_i^{\\tfrac{1}{2}} \\pmb \\gamma_i \\pmb \\delta_i &#39;\\) \\(\\Lambda = diag(\\lambda_1^{\\tfrac{1}{2}} , \\cdots, \\lambda_r^{\\tfrac{1}{2}}), \\; \\; \\; \\; \\; \\lambda_j^{\\tfrac{1}{2}} &gt;0\\). 이때 \\(\\lambda_i\\)는 \\(AA&#39;\\)와 \\(A&#39;A\\)의 non-zero ev. \\(\\Gamma, \\triangle\\)는 \\(AA&#39;\\)와 \\(A&#39;A\\)의 corresponding \\(r\\) evec으로 구성. 따라서 Both $’ = ’ = I_r $, i.e., are column orthogonal. 6.1.4 Quadratic Forms for symmetric Matrix \\(A_{p \\times p}\\), vector \\(\\pmb x \\in \\mathbb{R}^p\\): $ Q(x) = x’ A x = {i=1}^p {j=1}^p a_{ij} x_i x_j $ $ \\[\\begin{align} \\forall x \\not = 0: Q(x) &amp;&gt; 0 \\tag{positive definite} \\\\ \\\\ \\forall x \\not = 0: Q(x) &amp;\\ge 0 \\tag{positive semi-definite} \\end{align}\\] $ if corresponding quadratic form \\(Q(\\cdot)\\) is positive definite(semi-definite), \\(A\\) is called positive definite(semi-definite). This is written by \\(A&gt;0 \\; \\; \\; (\\ge 0)\\). 6.1.4.0.1 propositions if \\(A=A&#39;\\), and \\(Q(x) = \\pmb x &#39; A \\pmb x\\) is corresponding quadratic form, then $y = ^T x: ; ; ; Q(x) = x’ A x = _{i=1}^p _1 y_i^2 $, \\(\\lambda_i\\) is ev of \\(A\\). $ A&gt;0 _i&gt;0, ; ; ; i=1, , p $ $ A&gt;0 ; ; ; ; ; ; A &gt;0, ; A^{-1} $ \\(A=A&#39;, \\; B=B&#39;, \\; B&gt;0\\), then maximum of \\(\\dfrac{\\pmb x &#39; A \\pmb x}{\\pmb x &#39; B \\pmb x}\\) is given by the largest ev of \\(B^{-1}A\\). the vector which maximizes(minimizes) \\(\\dfrac{\\pmb x &#39; A \\pmb x}{\\pmb x &#39; B \\pmb x}\\) is the corresponding evec of \\(B^{-1}A\\) for largest(smallest) ev of \\(B^{-1}A\\). more generally, for ev of \\(B^{-1}A\\), \\(\\lambda_1, \\cdots, \\lambda_p\\), $ ( ) = ; ; ; ; ;_1 _p ; ; ; ; ; = ( ) $ if \\({\\pmb x &#39; B \\pmb x}=1\\), then $ ( {x ’ A x} ) = ; ; ; ; ;_1 _p ; ; ; ; ; = ( {x ’ A x} ) $ 6.1.5 Partitioned Matrices \\(A_{n \\times p}, B_{p \\times n}\\) and \\(n \\ge p\\). then $ \\[\\begin{vmatrix} -\\lambda I_n &amp; -A \\\\ B &amp; I_p \\end{vmatrix}\\] = (-)^{n-p} BA - I_n = AB - I_n $ for \\(A_{n \\times p}, B_{p \\times n}\\), the non-zero ev of \\(AB\\) and \\(BA\\) are the same and have the same multiplicity. if \\(\\pmb x\\) is evec of \\(AB\\) for an ev \\(\\lambda \\not = 0\\), then \\(y=B \\pmb x\\) is an evec of \\(BA\\). for \\(A_{n \\times p}, B_{q \\times n}, \\pmb a_{p \\times 1}, \\pmb b_{q \\times 1}\\), if \\(rank \\left( A \\pmb a \\pmb b B \\right) \\le 1\\), then non-zero ev, if it exists, equals \\(\\pmb b&#39; BA \\pmb a\\) with evec \\(A \\pmb a\\). 6.1.6 Geometrical Aspects mutually orthogonal \\(\\pmb x_1 , \\cdots, \\pmb x_k \\iff \\forall {i,j}: \\; \\pmb x_i &#39; \\pmb x_j\\) In that case, $X= ( x_1 , , x_k ) $ has rank \\(k\\), and \\(X&#39;X\\) is a diagonal Matrix with \\(x_i &#39; x_i\\) in the i-th diagonal position. let’s consider bivariate data \\((x_i , y_i), \\; \\; \\; i= 1, \\cdots, n\\), and let \\(\\tilde x_i = x_i - \\bar {\\pmb x}, \\; \\tilde y_i = y_i - \\bar {\\pmb y}\\). then correlation b/w \\(x\\) and \\(y\\) is $ {_{i=1}^n (x_i - {x})(y_i - {y})} {} = {x ’ y} {x y} = () $ where \\(\\theta\\) is the angle b/w the deviation vectors \\({\\tilde x}\\) and \\({\\tilde y}\\). For two dimensions, the rotation can be expressed: $ \\[\\begin{alignat}{2} \\pmb y &amp;= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} &amp;&amp;= \\begin{pmatrix} \\cos(\\theta) &amp; \\sin(\\theta) \\\\ -\\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} &amp;&amp; = \\Gamma \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\\\ &amp;= \\Gamma \\pmb x \\tag{clockwise rotation} \\\\\\ \\\\\\ \\pmb y &amp; &amp;&amp;= \\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} &amp;&amp;= \\Gamma &#39; \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\\\ &amp; = \\Gamma &#39; \\pmb x \\tag{counter-clockwise rotation} \\end{alignat}\\] $ 6.1.7 Column, Row and Null Space Matrix \\(X_{n \\times p}\\): $ \\[\\begin{alignat}{2} \\mathcal{C}(X) &amp;= \\{ \\pmb x \\in \\mathbb{R}^n \\; \\vert \\; \\exists \\pmb a \\in \\mathbb{R}^p \\; \\; s.t. \\; \\; X \\pmb a = \\pmb x\\} &amp;&amp;\\subseteq \\mathbb{R}^n \\tag{column (range) space} \\\\ \\mathcal{N}(X) &amp;= \\{ \\pmb y \\in \\mathbb{R}^p \\; \\vert \\; X \\pmb y = 0 \\} &amp;&amp;\\subseteq \\mathbb{R}^p \\tag{null space} \\\\ \\mathcal{R}(X) &amp;= \\{ \\pmb z \\in \\mathbb{R}^p \\; \\vert \\; \\exists \\pmb b \\in \\mathbb{R}^n \\; \\; s.t. \\; \\; X&#39; \\pmb b = \\pmb z \\} &amp;&amp;\\subseteq \\mathbb{R}^p \\tag{row space} \\\\ &amp;= \\mathcal{C}(X&#39;) \\tag{column space of X`} \\end{alignat}\\] $ Spaces by Singular Value Decomposition: General-version, Matrix \\(X_{n \\times p}\\) with \\(rank(X)=r\\): $ \\ \\[\\begin{alignat}{2} X &amp;= \\Gamma \\Lambda \\triangle^T \\\\ &amp; =\\sum_{i=1}^r \\lambda_i \\pmb \\gamma_i \\pmb \\delta_i &#39; \\\\ \\mathcal{C}(X) &amp;= \\{ \\gamma_1 , \\cdots, \\gamma_r \\} \\\\ \\mathcal{N}(X) &amp;= \\{ \\delta_{r+1} , \\cdots, \\delta_{p} \\} \\\\ \\mathcal{R}(X) &amp;= \\{ \\delta_{1} , \\cdots, \\delta_{r} \\} \\end{alignat}\\] $ note: Matrix \\(X_{n \\times p}\\) with \\(rank(X)=r\\) $ \\[\\begin{alignat}{2} \\mathcal{N}(X) &amp;= \\mathcal{C}(X&#39;)^{\\perp} = \\mathcal{R}(X)^{\\perp} \\\\ \\mathcal{N}(X)^{\\perp} &amp;= \\mathcal{C}(X&#39;) = \\mathcal{R}(X) \\\\ \\\\ \\\\ \\mathcal{C}(X&#39;X) &amp;= \\mathcal{C}(X&#39;) = \\mathcal{R}(X) \\\\ \\\\ \\\\ \\dim \\left( \\mathcal{C}(X) \\right) &amp;= \\dim \\left( \\mathcal{R}(X) \\right) \\\\ = \\; \\; \\; \\; \\; rank(X) &amp;= rank(X&#39;) = rank(X&#39;X) \\\\ &amp;= r \\le \\min(n, p) \\end{alignat}\\] $ \\(X&#39;X\\) has full rank (is nonsingular) \\(\\iff\\) if \\(X\\) has full column rank (\\(X\\) has linearly independent columns). "],["introduction-1.html", "6.2 Introduction", " 6.2 Introduction 6.2.1 What for linear model \\[Y = X \\beta + \\epsilon\\] $$ Y_{n } = \\[\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\\] ; ; ; ; ; ; ; _{(p+1) } = \\[\\begin{pmatrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}\\] ; ; ; ; ; ; ; _{n } = \\[\\begin{pmatrix} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\] \\ \\ \\ \\ X_{n (p+1)} = \\[\\begin{pmatrix} 1 &amp; X_{11} &amp; \\cdots &amp; X_{1p} \\\\ 1 &amp; X_{21} &amp; \\cdots &amp; X_{2p} \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{n1} &amp; \\cdots &amp; X_{np} \\\\ \\end{pmatrix}\\] $$ linear regression $$ \\[\\begin{alignat}{2} y_i &amp;= \\beta_0 + \\beta_1 x_i &amp;&amp;+ \\epsilon_i \\tag{Simple} \\\\ y_i &amp;= \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} &amp;&amp;+ \\epsilon_i \\tag{Multiple} \\end{alignat}\\] $$ ANOVA $$ \\[\\begin{alignat}{2} y_{ij} &amp;= \\mu + \\alpha_i &amp;&amp;+ \\epsilon_{ij} \\tag{One-Way} \\\\ y_{ij} &amp;= \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} &amp;&amp;+ \\epsilon_{ij} \\tag{Two-Way with interaction} \\end{alignat}\\] $$ 6.2.2 Random Vectors and Matrices let rv \\[Y = \\begin{pmatrix} y_1, &amp; \\cdots &amp;, y_n \\end{pmatrix}&#39;\\] with \\[E(y_i) = \\mu_i , \\; \\; \\; Var(y_i)=\\sigma_{ii} \\; \\; (=\\sigma_i^2), \\; \\; \\; Cov(y_i , y_j) = \\sigma_{ij}\\]. define the statistics of \\[Y\\] $$ \\[\\begin{alignat}{2} &amp;E(Y) &amp;&amp;= \\begin{pmatrix} E(y_1), &amp; \\cdots &amp; E(y_n) \\end{pmatrix}&#39; = \\begin{pmatrix} \\mu_1, &amp; \\cdots &amp; \\mu_n \\end{pmatrix}&#39; &amp;&amp;= \\pmb \\mu \\tag{Expected Value of Y elementwise as } \\\\ &amp;Cov(Y) &amp;&amp;= E \\left[ (Y-\\pmb \\mu) (Y-\\pmb \\mu) &#39; \\right] &amp;&amp;= (\\sigma_{ij}) \\tag{Covariance Matrix} \\end{alignat}\\] $$ Note: $$ \\[\\begin{alignat}{2} E(AY+\\pmb b) &amp;= A \\pmb \\mu + \\pmb b \\\\ Cov(AY+\\pmb b) &amp;= A \\ast Cov(Y) \\ast A &#39; \\end{alignat}\\] $$ Prove or disprove that Cov(Y) is nonnegative definite. how? Covariance of \\[W_{r \\times 1}, \\; Y_{s \\times 1}\\] with \\[E(W)=\\gamma, \\; E(Y) = \\mu\\]: $$ \\[\\begin{alignat}{2} Cov(W, Y) &amp;= E \\left [(W-\\gamma)(Y-\\mu)&#39; \\right ]_{r \\times s} &amp;&amp; \\\\ Cov(AW+a, NY+b) &amp;= A \\ast Cov(W,Y) \\ast B &#39; &amp;&amp; \\\\ Cov(AW+NY) &amp;= A \\ast Cov(W) \\ast A&#39; + N \\ast Cov(Y) \\ast B&#39; \\\\ &amp;\\; \\; \\; \\; \\; \\; \\; + A \\ast Cov(W,Y) \\ast B&#39; + B \\ast Cov(W) \\ast A&#39; \\tag{why?} \\end{alignat}\\] $$ 6.2.3 Multivariate Normal Distributions $$ Z = (z_1 , , z_n) ’ N_n (0, ; I_n), ; ; ; ; ; z_1 , , z_n N(0,1) $$ which means \\[E(Z)=\\pmb 0, \\; Cov(Z)=I_n\\]. \\[ A_{r \\times n}, \\; b \\in \\mathbb{R}^r \\] Y has an r-dimensional MVN distribution Definition 1.2.1. Let A be r  n and b 2 Rr . Then Y has an r-dimensional multivariate normal distribution : Y = AZ + b  Nr (b;AAT ): Theorem 1.2.2. Let Y  N(;V) and W  N(;V). Then Y and W have the same distribution (Proof: p.5) The density of nonsingular \\[Y \\sim N(\\mu,V)\\] is given by $$ f(y) = (2)^{-} ^{-} $$ Theorem 1.2.3. Let Y  N(;V) and Y = Y1 Y2 ! . Then Cov(Y1;Y2) = 0 if and only if Y1 Y2 Corollary 1.2.4. Let Y  N(; 2I) and ABT = 0. Then AY BY Definition 1.3.1. Quadratic Form of Y: for n  n; A YTAY = X ij aijyiyj Theorem 6.2.4 Distributions of Quadratic Forms \\[E(Y) = \\mu, \\; Cov(Y) = V\\]. then \\[E(Y&#39;AY) = tr(AV) + \\mu &#39; A \\mu\\]. prf) let’s consider \\[Z \\sim N_n (\\mu, I_n)\\]. then \\[ Z&#39;Z \\sim \\chi^2 \\left(n, \\; \\dfrac{\\mu&#39; \\mu}{2} \\right) \\tag{second one is non-centrality parameter}\\] Let \\[Y \\sim N(\\mu , I)\\] and any orthogonal projection Matrix \\[M\\]. then \\[Y&#39;MY \\sim \\chi^2 \\left(r(M), \\dfrac{\\mu &#39; M \\mu}{2} \\right)\\] Let \\[Y \\sim N(\\mu , \\sigma^2 I)\\] and any orthogonal projection Matrix \\[M\\]. then \\[Y&#39;MY \\sim \\chi^2 \\left(r(M), \\dfrac{\\mu &#39; M \\mu}{2\\sigma^2} \\right)\\] Let \\[Y \\sim N(\\mu , M)\\]with \\[\\mu \\in \\mathcal{C}(M)\\] and \\[M\\] be an orthogonal projection Matrix. then \\[Y&#39;Y \\sim \\chi^2 \\left(r(M), \\dfrac{\\mu &#39; M \\mu}{2\\sigma^2} \\right)\\]. let \\[E(Y)=\\mu, \\; Cov(Y)=V\\]. then \\[Pr \\left[ (Y-\\mu) \\in \\mathcal{C}(V) \\right]=1\\]. Exercise 1.6. Let \\[Y\\] be a vector with \\[E(Y) = 0\\] and \\[Cov(Y) = 0\\]. Then \\[Pr(Y = 0) = 1\\]. let \\[Y \\sim N(\\mu, \\; V)\\]. then \\[Y&#39; A Y \\sim \\chi^2 \\left( tr(AV), \\dfrac{\\mu&#39; A \\mu}{2}\\right)\\], provided that 1. \\[VAVAV=VAV\\]. 2. \\[\\mu &#39; AVA \\mu = \\mu &#39; a \\mu\\]. 3. \\[VAVA \\mu = VA \\mu\\] prf) Exercise 1.7. Show that if \\[V\\] is nonsingular, then the three conditions in Theorem 1.3.6 reduce to \\[AVA = A\\]. Show that \\[Y&#39;V^{-} Y\\] has a chi-squared distribution with \\[r(V)\\] degrees of freedom when \\[\\mu \\in \\mathcal{C}(V)\\]. let \\[Y \\sim N(\\mu, \\; \\sigma^2 I)\\] and \\[BA=0\\]. then, for \\[A=A&#39;\\], 1. \\[Y&#39;AY \\perp BY\\]. 2. \\[Y&#39;AY \\perp Y&#39; BY\\] for \\[B=B&#39;\\]. let \\[Y \\sim N(\\mu, \\; V)\\] and \\[A \\ge 0, \\; B \\ge 0\\], and \\[VAVBV=0\\]. then \\[Y&#39;AY \\perp Y&#39;BY\\]. let \\[Y \\sim N(\\mu, \\; V)\\]. provided that 1. \\[VAVBV=0\\]. 2. \\[VAVB \\mu = 0\\]. 3. \\[VBVA \\mu = 0\\]. 4. \\[\\mu &#39; ABV \\mu = 0\\]. and also conditions of above thm, 1. \\[VAVAV=VAV\\]. 2. \\[\\mu &#39; AVA \\mu = \\mu &#39; a \\mu\\]. 3. \\[VAVA \\mu = VA \\mu\\] prf) hold for both \\[Y&#39;AY\\] and \\[Y&#39;BY\\], then \\[Y&#39;AY \\perp Y&#39;BY\\]. "],["estimation.html", "6.3 Estimation", " 6.3 Estimation 이하와 같은 linear model 고려. 이때 \\(x_i &#39;\\)는 \\(X\\)의 i번째 row vector이며, \\(E(\\epsilon)=0, \\; Cov(\\epsilon)=\\sigma^2 I = \\sigma^2 \\Sigma\\). \\[ Y_{n \\times 1} = X_{n \\times p} \\beta_{p \\times 1} + \\epsilon_{n \\times 1} = \\begin{pmatrix} x_i &#39; \\beta \\end{pmatrix} + \\epsilon \\] 6.3.1 Identifiability and Estimability 6.3.1.1 Identifiable 모델에서의 무한한 갯수의 관측치를 보유한다면, 모델의 underlying 패러미터의 참값을 획득하는 것이 가능한 성질. A general linear model is a parameterization $ \\[\\begin{align} E(Y) &amp;= f(X) \\\\ &amp;= E(X\\beta + \\epsilon)\\\\ &amp;= X\\beta + E(\\epsilon) \\\\ &amp;= X\\beta + 0 \\\\ &amp;= X\\beta \\end{align}\\] $ The parameter \\(\\beta\\) is identifiable if for any \\(\\beta_1\\) and \\(\\beta_2\\) \\(f(\\beta_1) = f(\\beta_2)\\) implies \\(\\beta_1 = \\beta_2\\). If \\(\\beta\\) is identifiable, we say that the parameterization \\(f(\\beta)\\) is identifiable. (패러미터 \\(\\beta\\)가 identifiable하다면, 우리는 해당 패러미터의 parameterization \\(f(\\beta)\\) 또한 identifiable 하다) Moreover, a vector-valued function \\(g(\\beta)\\) is identifiable if \\(f (\\beta_1) = f(\\beta_2)\\) implies \\(g (\\beta_1) = g(\\beta_2)\\). For regression models for which \\(r(X) = p\\), the parameters are identifiable: \\(X&#39;X\\) is nonsingular, so if \\(X\\beta_1 = X\\beta_2\\), then \\[ \\beta_1 = (X&#39;X)^{-1} X&#39;X \\beta_1 = (X&#39;X)^{-1} X&#39;X \\beta_2 = \\beta_2 \\] A function \\(g(\\beta)\\) is identifiable \\(\\iff\\) \\(g(\\beta)\\) is a function of \\(f(\\beta)\\). 6.3.1.2 Estimable The results in the last section suggest that some linear combinations of \\(\\beta\\) in the less than full rank case will not be estimable. The linear parametric function \\(c&#39;β\\) is an estimable function if there exists a vector \\(a \\in \\mathbb{R}^n\\) such that \\(\\forall \\beta: E(a &#39; y ) = c &#39; \\beta\\). A vector-valued linear function of \\(\\beta\\), \\(\\Lambda &#39; \\beta\\) is estimable if \\(\\Lambda &#39; \\beta = P &#39; X \\beta\\) for some matrix P; In other words, \\(\\Lambda &#39; \\beta\\) is estimable if \\(\\Lambda = X &#39; P \\in \\mathcal{C}(X&#39;)\\). Clearly, if \\(\\Lambda &#39; \\beta\\) is estimable, it is identifiable and therefore it is a reasonable thing to estimate. estimable \\(\\rightarrow\\) identifiable For estimable functions \\(\\Lambda&#39; \\beta = P &#39; X \\beta\\), although \\(P\\) need not be unique, its perpendicular projection (columnwise) onto \\(\\mathcal{C}(X)\\) is unique: let \\(P_1 , \\; P_2\\) be matrices with \\(\\Lambda &#39; = P_1 &#39; X = P_2 &#39; X\\), then \\[ MP_1 = X(X&#39;X)^{-}X&#39;P_1 = X(X&#39;X)^{-}\\Lambda = X(X&#39;X)^{-}X&#39;P_2 = MP_2 \\] Example 2.1.4 and 2.1.5 \\(g(\\beta)\\)’s estimate, \\(f(Y)\\), is unbiased if \\(\\forall \\beta: \\; E[f(Y)] = g(\\beta)\\). if \\(f (Y) = a_0 + a&#39; Y\\) for some scalar \\(a_0\\) and vector \\(a\\), \\(f(Y)\\) is a linear estimate of \\(\\Lambda &#39; \\beta\\). if \\(\\Lambda &#39; \\beta\\) \\(\\iff\\) \\(a_0 = 0\\) and \\(a &#39; X = \\Lambda&#39;\\); say, \\(\\Lambda = X &#39; a \\in \\mathcal{C}(X&#39;)\\), then a linear estimate \\(a_0 + a &#39; Y\\) is unbiased \\(\\Lambda &#39; \\beta\\) is estimable \\(\\iff\\) there exists \\(\\rho\\) such that \\(E(\\rho &#39; Y ) = \\Lambda &#39; \\beta\\) for any \\(\\beta\\). 6.3.2 Estimation: Least Squares Estimating \\(E(Y)\\) is to take a vector in \\(\\mathcal{C}(X)\\) closest to \\(Y\\); $ \\[\\begin{alignat}{2} E(Y) &amp;= X\\beta \\; &amp;&amp;\\in \\; \\mathcal{C}(X)\\\\ \\\\ \\hat \\beta &amp;= \\min_\\beta \\left\\{ (Y-X \\beta) &#39; (Y-X \\beta) \\right\\} \\\\ &amp;= \\min_\\beta \\left\\{ \\Vert Y-X \\beta \\Vert^2 \\right\\} \\tag{Least Squares Estimate of beta} \\end{alignat}\\] $ for any Least Squares Estimate \\(\\hat \\beta\\), LSE of \\(\\Lambda &#39; \\beta is \\Lambda &#39; \\hat \\beta\\), e.g., \\(\\hat {\\Lambda &#39; \\beta}_{LSE} = \\Lambda &#39; \\hat \\beta\\). Theorem 2.2.1 where \\(M\\) is the perpendicular projection operator onto \\(\\mathcal{C}(X)\\), then \\(\\hat \\beta\\) is a LSE of \\(\\beta\\) \\(\\iff\\) \\(X \\hat \\beta = M Y\\) Corollary 2.2.2 \\(\\hat \\beta_{LSE} = X(X&#39;X)^{-}X&#39; Y\\) Corollary 2.2.3 The unique LSE of \\(\\rho &#39; X \\beta = \\rho &#39; M Y\\). ※ Note: the unique LSE of \\(\\Lambda &#39; \\beta = \\Lambda &#39; \\hat \\beta = P&#39; M Y\\). Theorem 2.2.4 the LSE of \\(\\Lambda &#39; \\beta\\) is unique only if \\(\\Lambda &#39; \\beta\\) is estimable: \\(\\Lambda = X&#39;\\rho\\) if \\(\\Lambda &#39; \\hat \\beta_1 =\\Lambda &#39; \\hat \\beta_2\\), so that \\(X \\hat \\beta_1 = X \\hat \\beta_2 = MY\\). ※ Note: When \\(\\beta\\) is not identifiable, we need side conditions imposed on the parameters to estimate nonidentifiable parameters. ※ Note: With \\(r = r (X) &lt; p\\) (overparameterized model), we need \\(p - r\\) individual side conditions to identify and estimate the parameters. Proposition 2.2.5 If \\(\\Lambda = X &#39; \\rho\\), then \\(E(\\rho &#39; MY) = \\Lambda &#39; \\beta\\). let’s decompose $ \\[\\begin{alignat}{2} Y &amp;= X \\hat \\beta &amp;&amp;+ Y - X \\hat \\beta \\\\ &amp;= MY &amp;&amp;+ (I-M)Y \\\\ &amp;= \\hat Y &amp;&amp;+ e \\end{alignat}\\] $ 이때 $ \\[\\begin{align} \\hat Y &amp;\\in \\mathcal{C}(X) \\tag{fitted values of Y} \\\\ e &amp;\\in \\mathcal{C}(X)^{\\perp} \\tag{residuals} \\end{align}\\] $ Theorem 2.2.6 Let \\(r (X) = r\\) and \\(Cov(\\epsilon) = \\sigma^2 I\\). At below formula, denominator is degrees of freedom for error. Then an UE of \\(\\sigma^2\\), MSE, is as below. \\[ \\hat \\sigma^2 =\\dfrac{Y&#39;(I-M)Y}{rank(I-M)} =\\dfrac{Y&#39;(I-M)Y}{n-r} \\tag{MSE} \\] 6.3.3 Estimation: Best Linear Unbiased Definition 2.3.1 \\(a&#39;Y\\) is a Best Linear Unbiased Estimate(BLUE) of \\(\\lambda &#39; \\beta\\) if \\(a &#39; Y\\) is unbiased. e.g., \\(E(a &#39; Y) = \\lambda &#39; \\beta\\) and if for any other linear unbiased estimate \\(b &#39; Y\\), \\(Var(a &#39; Y) \\le Var(b&#39;Y)\\). Theorem 2.3.2: Gauss-Markov thm Consider \\(Y = X \\beta + \\epsilon\\) with \\(E(\\epsilon) = 0\\), \\(Cov(\\epsilon) = \\sigma^2 I\\). Let \\(\\lambda &#39; \\beta\\) be estimable. Then LSE of \\(\\lambda &#39; \\beta=\\) BLUE of \\(\\lambda &#39; \\beta\\). Corollary 2.3.3 Let \\(\\sigma^2 &gt; 0\\). Then there exists a unique BLUE for any estimable function \\(\\lambda &#39; \\beta\\). 6.3.4 Estimation: Maximum Likelihood Assume that \\(Y \\sim N_n(X\\beta , \\; \\sigma^2 I_n)\\). Then the Maximum Likelihood Estimates (MLEs) of \\(\\beta\\) and \\(\\sigma^2\\) are obtained by maximizing the log of the likelihood so that $ \\[\\begin{align} \\left( \\hat \\beta , \\; \\hat \\sigma^2 \\right) &amp;= \\text{ MLE of } \\left( \\beta , \\; \\sigma^2 \\right) \\\\ &amp;= \\max_{\\left( \\beta , \\; \\sigma^2 \\right)} \\left\\{ -\\dfrac{n}{2}log(2 \\pi) - \\dfrac{1}{2} \\log \\left[ (\\sigma^2 )^n\\right] - \\dfrac{(Y-X\\beta)&#39;(Y-X\\beta)}{2\\sigma^2} \\right\\} \\end{align}\\] $ $ \\[\\begin{align} \\hat \\beta &amp;= \\text{ LSE of } \\beta \\\\ \\\\\\ \\hat \\sigma^2 &amp;= \\dfrac{1}{n} \\left\\{Y&#39;(I-M)Y \\right\\} \\end{align}\\] $ 6.3.5 Estimation: Minimum Variance Unbiased Assume that \\(Y = X \\beta + \\epsilon\\) with \\(\\epsilon \\sim N_n(0, \\; \\sigma^2 I_n)\\). if \\(\\forall \\beta, \\sigma^2: \\; E \\left \\{h[T(Y)] \\right\\} = 0\\) implies that \\(Pr[h(T(Y)) = 0] = 1\\), A vector-valued sufficient statistic \\(T(Y)\\) is said to be complete If \\(T(Y)\\) is a complete sufficient statistic, then \\(f(T(Y))\\) is a Minimum Variance Unbiased Estimate (MVUE) of \\(E \\Big [ f (T(Y)) \\Big ]\\). Theorem 2.5.3 let \\(\\theta = (\\theta_1 , \\cdots, \\theta_s)&#39;\\) and let \\(Y\\) be a rvec with pdf as below. then \\(T(Y) = \\Big( T_1(Y), \\cdots, T_s(Y) \\Big)&#39;\\) is a complete sufficient statistics provided that neither \\(\\theta\\) nor \\(T(Y)\\) satisfies any linear constraints. \\[ f(Y) = c(\\theta) \\exp \\left[ \\sum_{i=1}^s \\theta_i T_i (Y) \\right] h(Y) \\] Theorem 2.5.4 MSE is a \\(\\hat {\\sigma^2 }_{MVUE}\\), and \\(\\hat { \\rho &#39; X \\beta }_{MVUE} = \\rho &#39; M Y\\) whenever \\(\\epsilon \\sim N(0, \\; I)\\). 6.3.6 Sampling Distributions of Estimates Assume that \\(Y = X \\beta + \\epsilon\\) with \\(\\epsilon \\sim N_n(0, \\; \\sigma^2 I_n)\\). Then \\(Y \\sim N_n(X \\beta, \\; \\sigma^2 I_n)\\). then $ \\[\\begin{alignat}{4} \\Lambda &#39; \\hat \\beta &amp;= P&#39; M Y &amp;&amp;\\sim N(\\Lambda &#39; \\beta , \\; &amp;&amp;\\sigma^2 P&#39;MP&amp;&amp;\\; \\; \\; ) &amp;&amp; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\;&amp;&amp; &amp;&amp; &amp;&amp; \\\\ &amp; &amp;&amp;\\sim N(\\Lambda &#39; \\beta , \\; &amp;&amp;\\sigma^2 \\Lambda &#39; (X&#39;X)^{-} \\Lambda&amp;&amp;\\; \\; \\; ) &amp;&amp; &amp;&amp; \\because &amp;&amp; \\;M &amp;&amp; =X(X&#39;X)^- X&#39; \\\\ &amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; \\; \\hat Y &amp;&amp; = MY &amp;&amp;\\sim N(X\\beta, \\sigma^2 M) \\\\ \\hat \\beta &amp;= (X&#39;X)^- X&#39;Y &amp;&amp;\\sim N(\\beta , \\; &amp;&amp;\\sigma^2 (X&#39;X)^{-1}) &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; (\\text{if X is of full rank}) \\end{alignat}\\] $ Do Exercise 2.1. Show that \\[ \\dfrac{Y&#39; (I-M) Y}{\\sigma^2} \\sim \\chi^2 \\Bigg( r(I-M), \\; \\dfrac{\\beta&#39;X&#39;(I-M)X\\beta}{2\\sigma^2} \\Bigg) \\] 6.3.7 Generalized Least Squares(GLS) Assume that for some known positive definite \\(\\Sigma\\), \\[ Y = X \\beta + \\epsilon, \\; \\; \\; \\; \\; \\] \\[ \\begin{alignat}{3} Y &amp;= X \\beta &amp;&amp;+ \\epsilon &amp;&amp; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp;&amp; E(\\epsilon)&amp;&amp;=0, \\; \\; &amp;&amp;\\; Cov(\\epsilon) &amp;&amp;= \\sigma^2 \\Sigma \\tag{1} \\\\ \\Sigma^{-\\tfrac{1}{2}}Y &amp;= \\Sigma^{-\\tfrac{1}{2}} X \\beta &amp;&amp;+ \\Sigma^{-\\tfrac{1}{2}} \\epsilon &amp;&amp; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp;&amp; E(\\Sigma^{-\\tfrac{1}{2}} \\epsilon)&amp;&amp;=0, &amp;&amp;\\; Cov(\\Sigma^{-\\tfrac{1}{2}} \\epsilon) &amp;&amp;= \\sigma^2 I \\tag{2, by SVD} \\\\ Y_\\ast &amp;= X_\\ast \\beta &amp;&amp;+ \\epsilon_\\ast &amp;&amp; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp;&amp; E( \\epsilon_\\ast)&amp;&amp;=0, &amp;&amp;\\; Cov( \\epsilon_\\ast) &amp;&amp;= \\sigma^2 I \\end{alignat} \\] \\[ \\begin{alignat}{2} \\hat \\beta_{GLS} &amp;= \\min_\\beta (Y_\\ast - X_\\ast \\beta)&#39;(Y_\\ast - X_\\ast \\beta) \\\\ &amp;= \\min_\\beta \\Vert Y_\\ast - X_\\ast \\beta \\Vert^2 \\\\ &amp;= \\min_\\beta (Y - X \\beta)&#39; \\Sigma^{-1} (Y - X \\beta) \\tag{Generalized LSE (GLSE) of β} \\end{alignat} \\] Theorem 2.7.1 \\(\\lambda &#39; \\beta\\) estimable in model (1) \\(\\iff\\) if \\(\\lambda &#39; \\beta\\) is estimable in model (2). \\(\\hat \\beta\\) is GLSE of \\(\\beta\\) \\(\\iff\\) \\(X(X&#39; \\Sigma^{-1} X)^{-}X&#39; \\Sigma^{-1}Y = X \\hat \\beta\\), which is Normal Equation of GLS. For any estimable function, there exists a unique GLSE. GLSE estimate of estimable \\(\\lambda&#39; \\beta\\), is BLUE of \\(\\lambda&#39; \\beta\\). let \\(\\epsilon \\sim N(0, \\; \\Sigma^2 \\Sigma)\\). then, GLSE of estimable \\(\\lambda &#39; \\beta\\), is MVUE. let \\(\\epsilon \\sim N(0, \\; \\Sigma^2 \\Sigma)\\). then, \\(\\hat \\beta_{GLS} = \\hat \\beta_{MLE}\\). Normal Equation of GLS can be rewritten as $ \\[\\begin{align} X(X&#39; \\Sigma^{-1} X)^{-}X&#39; \\Sigma^{-1}Y &amp;= X \\hat \\beta \\\\ AY &amp;= \\end{align}\\] $ \\(A\\) is a projection operator onto \\(\\mathcal{C}(X)\\). \\(Cov(X \\hat \\beta_{GLS}) = \\sigma^2 \\ast X(X&#39; \\Sigma^{-1} X)^{-}X&#39;\\) Let \\(\\lambda &#39; \\beta\\) be estimable. Then \\(Var(\\lambda &#39; \\hat \\beta_{GLS}) = \\sigma^2 \\ast \\lambda &#39; (X&#39; \\Sigma^{-1} X)^- \\lambda\\). Note: \\((I-A)Y\\) is residual vector of GLSE. $ \\[\\begin{align} SSE_{GLS} &amp;= (Y_\\ast - \\hat Y_\\ast)&#39; (Y_\\ast - \\hat Y_\\ast) \\\\ &amp;\\; \\; \\vdots \\\\ &amp;= Y&#39;(I-A)&#39; \\Sigma^{-1}(I-A)Y \\\\ \\\\\\ MSE_{GLS} &amp;= \\hat \\sigma^2 \\\\ &amp; = \\dfrac{1}{n-r(X)} \\ast SSE_{GLS}\\\\ \\\\\\ \\dfrac{1}{\\hat \\sigma^2} \\dfrac{\\lambda&#39; \\Big(\\hat \\beta_{GLS} - \\beta_{GLS} \\Big)}{ \\lambda &#39; (X&#39; \\Sigma^{-1} X)^- \\lambda} &amp;\\sim t\\Big( n-r(x) \\Big) \\end{align}\\] $ denominator는 \\(Var(\\lambda &#39; \\hat \\beta_{GLS}) = \\sigma^2 \\ast \\lambda &#39; (X&#39; \\Sigma^{-1} X)^- \\lambda\\). Let \\(\\Sigma\\) be nonsingular and \\(\\mathcal{C}(\\Sigma X) \\subset \\mathcal{C}(X)\\). Then least squares estimates are BLUEs. Note: for diagonal \\(\\Sigma\\), GLS is referred to as Weighted Least Squares (WLS). Exercise 2.5. Show that \\(A\\) is the perpendicular projection operator onto \\(\\mathcal{C}(X)\\) when the inner product between two vectors \\(\\pmb x\\) and \\(\\pmb y\\) is defined as \\((\\pmb x, \\pmb y)_\\Sigma \\equiv \\pmb x&#39; \\Sigma^{-1} \\pmb y\\). "],["one-way-anova.html", "6.4 One-Way ANOVA", " 6.4 One-Way ANOVA 6.4.1 One-Way ANOVA General form of One-Way ANOVA model is \\[ y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}, \\; \\; \\; \\; \\; i=1, \\cdots, a \\; \\; \\; \\; \\; j=1, \\cdots, N_i \\] \\[ n=\\sum_{i=1}^a N_i \\\\ E(\\epsilon_{ij})=0, \\; Var(\\epsilon_{ij})=\\sigma^2, \\; Cov(\\epsilon_{ij}, \\epsilon_{ab})=0 \\] i-th treatment (group) effect \\(a_i\\) Balanced model is \\(\\forall i: N_i = b\\) Unbalanced model is \\(\\forall i: N_i\\)’s are different 6.4.2 More About Models Example 4.1.1: \\(a = 3, \\; N_1 = 5, \\; N_2 = 3, \\; N_3 = 3\\), \\[ Y = X \\beta + \\epsilon = \\begin{pmatrix} J_5 &amp; J_5 &amp; 0 &amp; 0 \\\\ J_3 &amp; 0 &amp; J_3 &amp; 0 \\\\ J_3 &amp; 0 &amp; 0 &amp; J_3 \\end{pmatrix} \\begin{pmatrix} \\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\vdots \\\\ \\epsilon_{33} \\end{pmatrix} \\] let \\(N_1 = N_2 = N_3 = 5\\). then \\[ X = \\begin{pmatrix} J_3 \\otimes J_5 &amp; I_3 \\otimes J_5 \\end{pmatrix} \\] In general, balanced design such as \\(i = 1, \\cdots, a \\; \\; \\; \\; \\; j = 1, \\cdots, b\\): \\[ X = \\begin{pmatrix} J_a \\otimes J_b &amp; I_a \\otimes J_b \\end{pmatrix} \\] Notation: \\(J_r^c \\equiv J_r J_c&#39; = J_r \\otimes J^c\\) is a \\(r \\times c\\) matrix of \\(1\\)’s. Let \\(Z\\) be the model matrix for the alternative one-way analysis of variance model \\[ y_{ij} = \\mu_i + \\epsilon_{ij} \\; \\; \\; \\; \\; i=1, \\cdots, a \\; \\; \\; \\; \\; k= 1, \\cdots, N_i \\] then, letting \\(X_i X_j = \\delta_{ij}\\) with 1 for \\(i=j\\) and 0 for \\(i \\not = j\\), \\[ \\begin{align} X &amp;= \\begin{bmatrix}J &amp; Z\\end{bmatrix} &amp;&amp;= \\begin{bmatrix}J &amp; (X_1 , \\cdots, X_a)\\end{bmatrix} \\\\ \\Longrightarrow \\; \\; \\; \\; \\; \\mathcal{C}(X) &amp;=\\mathcal{C}(Z) \\\\ Z&#39;Z &amp;= diag(N_1 , N_2 , \\cdots, N_a) \\\\ Z(Z&#39;Z)^{-1}Z&#39; &amp;=Blk \\; \\; diag \\Big[ N_i^{-1} J_{N_i}^{N_i} \\Big] \\\\ M &amp;=X (X&#39;X)^{-1}X&#39; \\\\ M_\\alpha &amp;= Z_\\ast(Z_\\ast &#39; Z_\\ast)^{-1} Z_\\ast &#39; &amp;&amp;=M- M_J = M-\\dfrac{1}{n}J_n^n \\\\ Z_\\ast &amp;=(I-M_j)Z \\\\ M &amp;= M_j + M_\\alpha \\end{align} \\] 6.4.3 Estimating and Testing Contrasts A contrast in the one-way ANOVA \\[ \\lambda &#39; \\beta = \\sum_{i=1}^a \\lambda_i \\alpha_i \\; \\; \\; \\; \\; with \\; \\; \\; \\lambda &#39; J_{a+1} = \\sum_{i=1}^a \\lambda_i = 0 \\] For estimable \\(\\lambda &#39; \\beta\\), find \\(\\rho\\) so that $‘X = ’ $, \\(\\rho &#39; = \\begin{pmatrix} \\dfrac{J_{N_i} &#39; \\lambda_i}{N_i} \\end{pmatrix}\\). Proposition 4.2.1. \\(\\lambda &#39; \\alpha = \\rho &#39; X \\beta\\) is a contrast \\(\\iff\\) \\(\\rho &#39; J = 0\\). Proposition 4.2.2. \\(\\lambda &#39; \\alpha = \\rho &#39; X \\beta\\) is a contrast \\(\\iff\\) \\(M_\\rho \\in \\mathcal{C}(M_\\alpha)\\). since \\(\\sum_{i=1}^a \\lambda_i =0\\), $ _{i=1}^a _i i ={i=1}^a _i {+ i} = {i=1}^a i y{i+} $ because \\(\\mu + \\alpha_i\\) is estimable, and its unique LSE is \\(\\bar y_{i+}\\). At significance level \\(\\alpha\\), \\(H_0: \\lambda &#39; \\alpha=0\\) is rejected if $ \\[\\begin{alignat}{2} &amp;F &amp;&amp;= \\dfrac { \\dfrac{ \\Big( \\sum_{i=1}^a \\lambda_i \\bar y_{i+} \\Big) ^2} {\\dfrac{\\sum_{i=1}^a \\lambda_i^2}{N_i}} } {MSE} &amp;&amp;&gt; F \\Big(1-\\alpha, \\; \\; 1, \\; \\; dfE \\Big) \\\\ \\\\ \\\\ \\iff \\; \\; \\; \\; \\; &amp; t \\ &amp;&amp;= \\dfrac {\\Bigg \\vert \\sum_{i=1}^a \\lambda_i \\bar y_{i+} \\Bigg \\vert} {\\sqrt{MSE \\left( \\sum_{i=1}^a\\dfrac{\\lambda_i^2}{N_i}\\right) }} &amp;&amp;&gt; t \\left( 1-\\dfrac{\\alpha}{2}, \\; \\; dfE \\right) \\end{alignat}\\] $ 6.4.4 Cochran’s Theorem let \\(A_1 , \\cdots, A_m\\) be \\(n \\times n\\) symmetric Matrices, and \\(A = \\sum_{j=1}^m A_j\\) with \\(rank(A_j) = n_j\\). consider the following four statements: \\(A_j\\) is an orthogonal projection for all \\(j\\). \\(A\\) is an orthogonal projection (possibly \\(A=I\\)). \\(A_j A_k = 0\\) for all \\(j \\not = k\\). \\(\\sum_{j=1}^m n_j = n\\). If any two of these conditions hold, then all four hold. Note: Cochran’s theorem is a standard result that is the basis of the ANalysis Of VAriance. If we can write the total sum of squares as a sum of sum of squares components, and if the degree of freedom add up, then the \\(A_j\\) must be projections, they are orthogonal to each other, and they jointly span \\(\\mathbb{R}^n\\). "],["testing.html", "6.5 Testing", " 6.5 Testing 6.5.1 More About Models: Two approaches for linear model $ \\[\\begin{alignat}{2} Y &amp;= E(Y) &amp;&amp;+ Y - E(Y) \\\\ &amp;= \\mu &amp;&amp;+ \\epsilon \\tag{Parameter-free approach } \\\\ \\\\ Y &amp;= E(Y) &amp;&amp;+ Y - E(Y) \\\\ &amp;= X \\beta &amp;&amp;+ \\epsilon \\tag{Parameter approach} \\end{alignat}\\] $ $ \\[\\begin{alignat}{2} E(\\epsilon) &amp;= 0, \\; \\; \\; &amp;&amp; Cov(\\epsilon) &amp;&amp;= \\sigma^2 I \\tag{Ordinary Least Square, OLS} \\\\ E(\\epsilon) &amp;= 0, &amp;&amp; Cov(\\epsilon) &amp;&amp;= \\sigma^2 \\Sigma \\tag{Generalized Least Square, GLS} \\end{alignat}\\] $ Consider $ Y=X + , ; ; ; ; ; E()=0, ; Cov() = ^2 I $ \\(\\mathcal{C}(X)\\) \\(\\mathcal{C}(X)^\\perp\\) itslef Estimation Space Error Space orthogonal projection onto \\(M \\\\ = X(X&#39;X)^-X&#39;\\) \\(I - M \\\\= I - X(X&#39;X)^-X&#39;\\) \\(E(Y) = X \\beta \\in \\mathcal{C}(X)\\) \\(E(\\epsilon) \\in \\mathcal{C}(X)^\\perp\\) \\(Cov(Y) = \\sigma^2 I\\) \\(Cov(\\epsilon) = \\sigma^2 I\\) One-Way ANOVA $ \\[\\begin{alignat}{4} y_{ij} &amp;= \\mu_i &amp;&amp;+ \\epsilon_{ij} \\\\ &amp;= E(y_{ij}) &amp;&amp;+ \\epsilon_{ij} \\\\ &amp;= \\mu + \\alpha_i &amp;&amp;+ \\epsilon_{ij} \\\\ \\\\\\ \\bar \\mu &amp;= \\mu + \\bar \\alpha_+ \\\\ \\mu_1 - \\mu_2 &amp;= \\alpha_1 - \\alpha_2 \\end{alignat}\\] $ the parameters in the two models are different, but they are related. Simple Linear Regression $ \\[\\begin{alignat}{4} y_i &amp; = \\beta_0 + \\beta_1 x_i &amp;&amp;+\\epsilon_i \\\\ &amp; = E(y_i) &amp;&amp;+\\epsilon_i \\\\ &amp; = \\gamma_0 + \\gamma_1(x_i - \\bar x) &amp;&amp;+\\epsilon_i \\end{alignat}\\] $ $ \\[\\begin{alignat}{2} \\mathcal{C}(X_1) = \\mathcal{C}(X_2) \\; \\; \\Longrightarrow \\; \\;\\; \\; \\; X_1 &amp;= X_2 T \\\\ X_1 \\beta_1 &amp;= X_2 T \\beta_1 &amp;&amp; = X_2 \\beta_2 \\\\ &amp; &amp;&amp;= X_2 (T \\beta_1 + \\nu), \\; \\; \\; \\forall\\nu \\in \\mathcal{C}(X_2&#39;)^\\perp \\end{alignat}\\] $ ※ Note: A unique parameterization for \\(X_j, \\; j=1,2\\) occurs \\(\\iff\\) \\(X_j &#39; X_j\\) is nonsingular. Exercise: Show that a unique parameterization for \\(X_j, \\; j=1,2\\) means \\(\\mathcal{C}(X_2 &#39; )^\\perp = \\{0\\}\\). 6.5.2 Testing Models Consider $ Y=X + , ; ; ; ; ; N(0, ; I_n) $ let’s partition \\(X\\) into $X = \\[\\begin{pmatrix} X_0, &amp; X_1 \\end{pmatrix}\\] : ; (X_0) (X) $ $ \\[\\begin{alignat}{2} Y &amp;= X_0 \\beta_0 + X_1 \\beta_1 &amp;&amp;+ \\epsilon \\tag{Full Model, FM} \\\\ Y &amp;= X_0 \\gamma &amp;&amp;+ \\epsilon \\tag{Reduced Model, RM} \\end{alignat}\\] $ 이때 Hypothesis testing procedure can be described as \\(H_0:\\) Reduced Model, \\(H_1:\\) Full Model. (Example 3.2.0: pp. 52–54). Let \\(M\\) and \\(M_0\\) be the orthogonal projection onto \\(\\mathcal{C}(X)\\) and \\(\\mathcal{C}(X_0)\\) respectively. Note that with \\(\\mathcal{C}(X_0) \\subset \\mathcal{C}(X)\\), \\(M - M_0\\) is the orthogonal projection onto the orthogonal complement of \\(\\mathcal{C}(X_0)\\) with respect to \\(\\mathcal{C}(X)\\), that is, $ \\[\\begin{align} \\mathcal{C}(X_0)_{\\mathcal{C}(X)}^\\perp &amp;= \\mathcal{C}(M - M_0) \\\\ &amp;= \\mathcal{C}(M \\cap M_0^\\perp ) \\\\ \\\\\\ \\hat\\mu &amp;= \\hat E(Y) = MY \\tag{under FM} \\\\ \\hat\\mu_0 &amp;= \\hat E(Y) = M_0 Y \\tag{under RM} \\end{align}\\] $ If RM is true, then \\(MY-M_0 Y = (M - M_0)Y\\) should be reasonably small. Note that \\(E(M-M_0)Y = 0\\). The decision about whether RM is appropriate hinges on deciding whether the vector \\((M - M_0)Y\\) is large. The size of \\((M - M_0)Y\\)’s obvious measure is \\([(M - M_0)Y]&#39;[(M - M_0)Y] = Y&#39;(M-M_0)Y\\). The size of \\((M - M_0)Y\\)’s reasonable measure is given by \\(\\dfrac{Y&#39;(M-M_0)Y}{r(M-M_0)}\\). ※ Note that $E ( ) = ^2 + $. Theorem 3.2.1. Consider $ Y=X + , ; ; ; ; ; N(0, ; I_n) , ; ; ; ; ; (X_0) (X) \\ \\ \\ \\[\\begin{alignat}{2} Y &amp;= X_0 \\beta_0 + X_1 \\beta_1 &amp;&amp;+ \\epsilon \\tag{Full Model, FM} \\\\ Y &amp;= X_0 \\gamma &amp;&amp;+ \\epsilon \\tag{Reduced Model, RM} \\end{alignat}\\] $ $ \\[\\begin{alignat}{2} \\dfrac {\\dfrac{Y&#39;(M-M_0)Y}{r(M-M_0)}} {\\dfrac{Y&#39;(I-M)Y}{r(I-M)}} &amp;= \\dfrac {\\dfrac{Y&#39;(M-M_0)Y}{df_1}} {\\dfrac{Y&#39;(I-M)Y}{df_2}} &amp;&amp;\\sim F \\Bigg( df_1 , df_2, \\dfrac{\\beta&#39; X&#39; (M-M_0)X \\beta }{2 \\sigma^2} &amp;&amp; \\Bigg) \\tag{Under the FM} \\\\ \\\\\\ \\\\\\ \\dfrac {\\dfrac{Y&#39;(M-M_0)Y}{r(M-M_0)}} {\\dfrac{Y&#39;(I-M)Y}{r(I-M)}} &amp;= \\dfrac {\\dfrac{Y&#39;(M-M_0)Y}{df_1}} {\\dfrac{Y&#39;(I-M)Y}{df_2}} &amp;&amp;\\sim F \\big( df_1 , df_2, 0 &amp;&amp; \\big) \\tag{Under the RM} \\end{alignat}\\] $ Note: Example 3.2.2.; pp. 58–59 $ \\[\\begin{alignat}{2} M-M_0 &amp;= (I-M_0) &amp;&amp;-(I-M) \\\\ Y&#39;(M-M_0)Y &amp;= Y&#39;(I-M_0)Y &amp;&amp;-Y&#39;(I-M)Y \\\\ &amp;= SSE_{RM} &amp;&amp;-SSE_{FM} \\end{alignat}\\] $ 6.5.3 A Generalized Test Procedure Assume that \\(Y = X \\beta + \\epsilon\\) is correct. Want to test the adequacy of a model \\(Y = X_0 \\gamma + Xb + \\epsilon\\), where \\(\\mathcal{C}(X_0) \\subset \\mathcal{C}(X)\\) and some known vector \\(Xb=\\) offset. Example 3.2.3.; Multiple Regression $ Y = _0 J + _1 X_1 + _2 X_2 + _3 X_3 + $ want to test \\(H_0: \\beta_2 = \\beta_3+5, \\; beta_1 = 0, \\cdots\\). $ \\[\\begin{alignat}{2} Y &amp;= X \\beta &amp;&amp; &amp;&amp;+ \\epsilon \\tag{FM} \\\\ Y^\\ast &amp;\\equiv Y &amp;&amp; - X b &amp;&amp; \\\\ &amp;=X \\beta &amp;&amp; - Xb &amp;&amp;+ \\epsilon \\\\ &amp;=X (\\beta &amp;&amp; - b) &amp;&amp;+ \\epsilon \\\\ &amp;=X \\beta^\\ast &amp;&amp; &amp;&amp;+ \\epsilon \\tag{FM} \\\\ \\\\\\ \\\\\\ Y &amp;= X_0 \\gamma &amp;&amp; + Xb &amp;&amp;+ \\epsilon \\tag{RM} \\\\ Y^\\ast &amp;\\equiv Y &amp;&amp; &amp;&amp; &amp;&amp; - X b \\\\ &amp;=X \\gamma &amp;&amp; &amp;&amp;+ \\epsilon \\tag{RM} \\end{alignat}\\] $ In addition, when \\(Y^\\ast = Y_\\ast\\), $ \\[\\begin{alignat}{2} \\dfrac {\\dfrac{ Y_\\ast &#39; (M-M_0) Y_\\ast }{ r(M-M_0)}} {\\dfrac{ Y_\\ast &#39; (I-M) Y_\\ast }{r(I-M)}} &amp;\\sim F \\Big( r(M-M_0), r(I-M), \\delta^2 \\Big) \\\\ \\\\ \\delta^2 &amp;=\\dfrac{1}{2 \\sigma^2} \\Big( {\\beta^\\ast} &#39; X &#39; (M-M_0) X \\beta^\\ast \\Big) \\tag{non-centrality parameter} \\end{alignat}\\] $ $ \\[\\begin{alignat}{2} 0 &amp;= \\beta_\\ast &#39; X&#39; &amp;&amp;(M-M_0) X \\beta_\\ast \\\\ &amp;\\Updownarrow \\\\ 0 &amp;= &amp;&amp;(M-M_0)X \\beta_\\ast \\\\ &amp;\\Updownarrow \\\\ X\\beta &amp; = M_0 (X &amp;&amp;\\beta - X b) + Xb \\tag{3} \\end{alignat}\\] $ will hold if $ \\[\\begin{align} \\gamma &amp;= (X_0 &#39; X_0)^- X_0(X \\beta - Xb) \\\\ &amp;= (X_0 &#39; X_0)^- X_0 X \\beta_\\ast \\end{align}\\] $ Furthermore, $ \\[\\begin{alignat}{2} Y_\\ast &#39; (M-M_0)Y_\\ast &amp;= Y_\\ast &#39; (I-M_0)Y_\\ast &amp;&amp;- &amp;&amp;Y_\\ast &#39; (I-M)Y_\\ast \\; \\; \\; \\; \\;\\text{ , and } \\\\ Y &#39; (I-M)Y &amp;= &amp;&amp; &amp;&amp; Y_\\ast &#39; (I-M)Y_\\ast \\end{alignat}\\] $ 6.5.4 Testing Linear Parametric Functions \\(H_0: Y= X \\beta + \\epsilon, \\; \\; \\; \\; \\; \\Lambda&#39; \\beta=0 \\tag{1}\\) $ \\[\\begin{alignat}{2} \\Lambda &#39; \\beta = 0 \\; \\; \\; &amp;\\iff \\beta &amp;&amp;\\in \\mathcal{N}(\\Lambda &#39;) = \\mathcal{C}(X)^\\perp \\\\ &amp;\\iff \\beta \\perp \\mathcal{C}(\\Lambda) \\\\ &amp;\\iff \\beta \\perp \\mathcal{C}(\\Gamma) \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp;&amp;\\text{ if } \\exists\\Gamma \\; \\; s.t. \\; \\mathcal{C}(\\Gamma) = \\mathcal{C}(\\Lambda) \\\\ &amp;\\iff \\beta \\perp \\mathcal{C}(U) &amp;&amp;\\text{ if } \\exists U \\; \\; s.t. \\; \\mathcal{C}(U) = \\mathcal{C}(\\Lambda)^\\perp \\\\ &amp;\\iff \\beta = U_\\gamma &amp;&amp; \\exists \\gamma \\tag{2} \\end{alignat}\\] $ Thus, letting \\(X_0 = XU\\), (in general, \\(\\mathcal{C}(X_0) \\subset \\mathcal{C}(X)\\)), then $ \\[\\begin{alignat}{2} Y &amp;= X \\beta &amp;&amp;+ \\epsilon \\\\ &amp;= X U \\gamma &amp;&amp;+ \\epsilon \\\\ &amp;= X_0 \\gamma &amp;&amp;+ \\epsilon \\tag{3} \\end{alignat}\\] $ Suppose \\(\\mathcal{C}(X_0) = \\mathcal{C}(X)\\). Then there is nothing to test and \\(\\Lambda&#39; \\beta = 0\\) involves only arbitrary side conditions that do not affect the model. (EXAMPLE 3.3.1. pp. 62–64) $ ’ ; ; ; ; ; ; P:= X’ P $ $ \\[\\begin{align} \\mathcal{C}(MP) &amp;\\equiv \\mathcal{C}(M-M_0) \\\\ &amp;= \\mathcal{C}(X-X_0) \\\\ &amp;= \\mathcal{C}(X) \\; \\cap \\; \\mathcal{C}(X_0)^\\perp\\\\ &amp;= \\mathcal{C}(X_0)_{\\mathcal{C}(X)}^\\perp \\end{align}\\] $ thus, its distribution for testing \\(H_0: \\Lambda &#39; \\beta = 0\\) is given by $ \\[\\begin{alignat}{2} \\dfrac {\\dfrac{Y&#39;(M_{MP})Y}{r(M_{MP})}} {\\dfrac{Y&#39;(I-M)Y}{r(I-M)}} &amp;\\sim F \\Big( r(M_{MP}), r(I-M), \\delta^2 \\Big) \\tag{5} \\\\ \\\\\\ \\delta^2 &amp;= \\beta &#39; X&#39; M_{MP}X \\beta \\tag{non-centrality parameter} \\end{alignat}\\] $ Proposition 3.3.2 $ \\[\\begin{alignat}{2} \\mathcal{C}(M-M_0) &amp;= \\mathcal{C}(X_0)_{\\mathcal{C}(X)}^\\perp \\\\ &amp;= \\mathcal{C}(XU)_{\\mathcal{C}(X)}^\\perp = \\mathcal{C}(MP) \\end{alignat}\\] $ $ \\[\\begin{alignat}{2} H_0: Y=X\\beta + \\epsilon, \\; \\; \\; \\; \\; \\Lambda &#39; \\beta = 0 \\\\ \\Updownarrow \\\\ H_0: Y=X\\beta + \\epsilon, \\; \\; \\; \\; \\; P&#39;X \\beta = 0 \\\\ \\Updownarrow \\\\ H_0: Y=X\\beta + \\epsilon, \\; \\; \\; \\; \\; P&#39;MX \\beta = 0 (\\because MX = X) \\\\ \\Updownarrow \\\\ E(Y) \\in \\mathcal{C}(X), \\; \\; \\; \\; E(Y) \\perp \\mathcal{C}(MP) \\\\ \\Updownarrow \\\\ E(Y) \\in \\mathcal{C}(X) \\; \\cap \\; \\mathcal{C}(MP)^\\perp, \\; \\; \\; \\; \\mathcal{C}(X_0)=\\mathcal{C}(X) \\; \\cap \\; \\mathcal{C}(MP)^\\perp = \\mathcal{C}(MP)^\\perp_{\\mathcal{C}(X)} \\Longrightarrow \\mathcal{C}(X_0)^\\perp_{\\mathcal{C}(X)} = \\mathcal{C}(MP) \\\\ \\Updownarrow \\\\ X_0 = (I-M_{MP})X \\end{alignat}\\] $ $ \\[\\begin{align} \\mathcal{C} \\Big[ (I-M_{MP})X \\ \\Big] &amp;= \\mathcal{C} (X) \\; \\cap \\; \\mathcal{C} (MP)^\\perp \\\\ &amp;= \\mathcal{C} (X) \\; \\cap \\; \\mathcal{C} (P)^\\perp \\tag{EXAMPLE 3.3.4.: pp.66–67} \\end{align}\\] $ let \\(\\Lambda &#39; \\beta\\) is estimable, i.e., \\(\\Lambda = X&#39;P\\). then \\(\\mathcal{C}(\\Lambda) = \\mathcal{C}(X&#39;P) =\\mathcal{C}(MP)\\), and \\(X \\hat \\beta = MY\\), and \\(\\Lambda &#39; \\hat \\beta = P&#39; X \\hat \\beta = P&#39; M Y\\). then $ \\[\\begin{align} Y&#39; M_{MP}Y &amp;= Y&#39; M &amp;&amp; (P&#39; M P)^- &amp;&amp; MPY \\\\ &amp;= \\hat \\beta &#39; \\Lambda &amp;&amp; [P&#39; X(X&#39;X)^-X&#39; P]^- &amp;&amp; \\Lambda &#39; \\hat \\beta \\\\ &amp;= \\hat \\beta &#39; \\Lambda &amp;&amp; [\\Lambda&#39; (X&#39;X)^- \\Lambda]^- &amp;&amp; \\Lambda &#39; \\hat \\beta \\end{align}\\] $ 이윗부분 전혀모르겠음 thus, $ \\[\\begin{align} (5) = \\dfrac{\\dfrac{\\hat \\beta &#39; \\Lambda [\\Lambda &#39; (X&#39;X)^- \\Lambda]^- \\Lambda&#39; \\hat \\beta}{r(\\Lambda)}}{MSE} &amp;\\sim F \\Big( r(MP), r(I-M), \\delta^2 \\Big)\\\\ \\\\\\ \\\\\\ \\delta^2 &amp;= \\dfrac{\\hat \\beta &#39; \\Lambda [\\Lambda &#39; (X&#39;X)^- \\Lambda]^- \\Lambda&#39; \\hat \\beta}{2 \\sigma^2} \\\\ Cov\\Big(\\Lambda &#39; \\hat \\beta \\Big) &amp;= \\sigma^2 \\Lambda &#39; (X&#39; X)^{-} \\Lambda \\end{align}\\] $ For \\(H_0: \\lambda &#39; \\beta =0, \\; \\; \\; \\lambda \\in \\mathbb{R}^p\\), $ \\[\\begin{align} Y&#39;M_{MP}Y &amp;= \\hat \\beta &#39; \\lambda \\big [\\lambda &#39; (X&#39;X)^- \\lambda \\big]^- \\lambda&#39; \\hat \\beta \\\\ &amp;=\\dfrac{\\big( \\lambda&#39; \\hat \\beta \\big)^2}{\\lambda&#39;(X&#39;X)^-\\lambda} \\end{align}\\] $ and, under \\(H_0: \\lambda &#39; \\beta =0\\), $ F = (5) = F ( 1, ; r(I-M) ) $ Definition 3.3.5. The condition \\(E(Y) \\perp \\mathcal{C}(MP)\\) is called the constraint by \\(\\Lambda &#39; \\beta = 0\\) where \\(\\Lambda = X&#39; P\\). in other words, \\(\\mathcal{C}(MP)\\) is the constraint by \\(\\Lambda &#39; \\beta = 0\\). Do Exercise 3.5: Show that a necessary and sufficient condition for \\(\\rho_1 &#39; X \\beta = 0\\) and \\(\\rho_2 &#39; X \\beta = 0\\) to determine the orthogonal constraints on the model is that \\(\\rho_1 &#39; X \\rho_2 = 0\\) 6.5.5 Theoretical Complements Consider testing \\(\\Lambda &#39; \\beta = 0\\) when \\(\\Lambda &#39; \\beta\\) is NOT estimable. let \\(\\Lambda_0 &#39; \\beta\\) be estimable part of \\(\\Lambda &#39; \\beta\\). \\(\\Lambda_0\\) is chosen, so that \\(\\mathcal{C}(\\Lambda_0) = \\mathcal{C}(\\Lambda) \\; \\cap \\; \\mathcal{C}(X&#39;)\\), which means that \\(\\Lambda &#39; \\beta = 0\\) implies that \\(\\Lambda_0 &#39; \\beta = 0\\) but \\(\\Lambda_0 &#39; \\beta\\) is estimable, because \\(\\mathcal{C}(\\Lambda_0) \\subset \\mathcal{C}(X&#39;)\\). Theorem 3.3.6. let \\(\\mathcal{C}(\\Lambda_0) = \\mathcal{C}(\\Lambda) \\; \\cap \\; \\mathcal{C}(X&#39;)\\) and \\(\\mathcal{C}(U_0) = \\mathcal{C}(\\Lambda_0)^\\perp\\). Then \\(\\mathcal{C}(XU) = \\mathcal{C}(XU_0)\\). Thus \\(\\Lambda &#39; \\beta = 0\\) and \\(\\Lambda_0 &#39; \\beta = 0\\) induce the same RM. Proposition 3.3.7. let \\(\\Lambda_0 &#39; \\beta\\) be estimable and \\(\\Lambda \\not = 0\\). then \\(\\Lambda &#39; \\beta = 0 \\; \\; \\Longrightarrow \\; \\; \\mathcal{C}(XU) \\not = \\mathcal{C}(X)\\). Corollary 3.3.8. $ (_0) = () ; ; (X’) = {0 } \\ \\ (XU) ; ; (X) $ 6.5.6 A Generalized Test Procedure Consider as below, whose column space is solvable. \\(H_0: \\Lambda&#39; \\beta = d, \\; \\; \\; \\; \\; d \\in \\mathcal{C}(X&#39;), \\; \\; \\; \\; \\Lambda&#39; b =d\\) $ \\[\\begin{alignat}{2} \\Lambda &#39; \\beta = \\Lambda &#39; b = d \\; \\; \\; &amp;\\iff \\Lambda &#39; (\\beta - b) &amp;&amp;= 0 \\\\ &amp;\\iff (\\beta - b) &amp;&amp;\\perp \\mathcal{C}(\\Lambda) \\\\ &amp;\\iff (\\beta - b) &amp;&amp;\\in \\mathcal{C}(U) \\; \\; \\; \\; \\; \\; &amp;&amp;\\text{where } \\; \\mathcal{C}(U) = \\mathcal{C}(\\Lambda)^\\perp \\\\ &amp;\\iff (\\beta - b) &amp;&amp;= U_\\gamma &amp;&amp;\\exists \\gamma \\\\ &amp;\\iff X\\beta - Xb &amp;&amp;= XU_\\gamma \\\\ &amp; \\; \\; \\; \\Updownarrow \\\\ X\\beta &amp;= XU_\\gamma + Xb, \\\\ Y &amp;= X \\beta + \\epsilon \\\\ &amp;= X U_\\gamma + Xb + \\epsilon \\\\ &amp;= X_0 \\gamma + Xb + \\epsilon, &amp;&amp; &amp;&amp; \\text{where } \\; X_0 = XU \\end{alignat}\\] $ if \\(\\Lambda = X&#39;P\\), then \\(\\mathcal{C}(X_0)_{\\mathcal{C}(X)}^\\perp = \\mathcal{C}(MP)\\) and its test statistics is $ \\[\\begin{align} F = \\dfrac {\\dfrac{(Y-Xb)&#39;M_{MP}(Y-Xb)}{r \\Big(M_{MP} \\Big)}} {\\dfrac{(Y-Xb)&#39;(I-M)(Y-Xb)}{r \\Big(I-M \\Big)}} = \\dfrac {\\dfrac{(\\Lambda &#39; \\hat \\beta - d)&#39; \\Big[ \\Lambda&#39;(X&#39;X)^{-}\\Lambda \\Big]^- (\\Lambda &#39; \\hat \\beta - d)}{r(\\Lambda)}} {MSE} \\sim F(?, ?, ?) \\end{align}\\] $ Remark: (EXAMPLE 3.3.9.: pp.71–72, EXAMPLE 3.4.1.: pp.75) If \\(\\Lambda &#39; \\beta = d\\), the same reduced model results if we take \\(\\Lambda &#39; \\beta = d_0\\), where \\(d_0 = d + \\Lambda &#39; \\nu\\) and \\(\\nu \\perp \\mathcal{C}(X&#39;)\\). Note that, in this construction, if \\(\\Lambda &#39; \\beta = d\\) is estimable, \\(d_0 = d\\) for any \\(\\nu\\). 6.5.7 Testing Single Degrees of Freedom in a Given Subspace $ RM: Y=X_ 0 + ; ; ; ; ; vs. ; ; ; ; ; FM: Y=X + , ; ; ; ; ; with; ; (X_0) (X) $ let \\(M_\\ast = M - M_0\\), consider \\(H_0 : \\Lambda &#39; \\beta = 0\\). if \\(\\Lambda = X&#39;P\\), i.e. \\(\\Lambda \\in \\mathcal{C}(X&#39;)\\), then \\(M_\\ast = M_{MP}\\). Proposition 3.3.2 Since \\(M M_\\ast = M_\\ast\\), $ \\[\\begin{align} &amp;\\mathcal{C}(M - M_0) = \\mathcal{C}(X_0)_{\\mathcal{C}(X)}^\\perp \\equiv \\mathcal{C}(XU)_{\\mathcal{C}(X)}^\\perp = \\mathcal{C}(MP) \\\\ \\Longrightarrow \\; \\; \\; &amp;M \\rho \\in \\mathcal{C}(M_\\ast) \\\\ \\Longrightarrow \\; \\; \\; &amp;M \\rho = M_\\ast M \\rho = M_\\ast \\rho \\\\ \\Longrightarrow \\; \\; \\; &amp;\\rho &#39; \\hat \\beta = \\rho &#39; M_\\ast Y = \\rho &#39; M Y \\end{align}\\] $ thus the test statistic for \\(H_0 : \\Lambda &#39; \\beta = 0\\) is $ = $ 6.5.8 Breaking SS into Independent Components Consider \\(X = \\begin{pmatrix} X_0, &amp; X_1 \\end{pmatrix}\\). set $ \\[\\begin{alignat}{2} &amp;SSR(X_1 \\vert X_0) &amp;&amp;\\equiv Y &#39; (M-M_0)Y &amp;&amp; \\tag{Sum of Squares for regression X1 after X0}\\\\ &amp;SSR(X) &amp;&amp;\\equiv Y &#39; MY \\\\ &amp;SSR(X_0) &amp;&amp;\\equiv Y &#39; M_0 Y \\\\ &amp;SSR(X) &amp;&amp;= SSR(X_0) &amp;&amp;+ SSR (X_1 \\vert X_0) \\end{alignat}\\] $ Note: if \\(\\epsilon \\sim N(0, \\; \\sigma I)\\), then \\(SSR(X_0) \\perp SsR(X_1 \\vert X_0)\\). 6.5.9 General Theory Let \\(M\\) and \\(M_\\ast\\) be the orthogonal projection operator into \\(\\mathcal{C}(X)\\) and \\(\\mathcal{C}(X_\\ast)\\) respectively. Then, with \\(\\mathcal{C}(X_\\ast) \\subset \\mathcal{C}(X)\\), \\(M_\\ast\\) defines a test statistic as below. $ {} {} ; ; ; :Y = X_+ $ $ \\[\\begin{align} &amp;I-(M-M_\\ast ) &amp;&amp;= (I-M) + M_\\ast \\\\ &amp;\\mathcal{C}(M-M_\\ast) &amp;&amp;:\\tag{Estimation Space, under H0} \\\\ &amp;\\mathcal{C}(M_\\ast) &amp;&amp;:\\tag{Test Space, under H0} \\\\ &amp;\\mathcal{C} \\Big(I - (M-M_\\ast)\\Big) &amp;&amp;:\\tag{Error Space, under H0} \\end{align}\\] $ Using Gram-Schmidt procedure, let’s construct \\(M_\\ast\\) so that $ M_= RR’ = {i=1}^r R_iR_i ’ = {i=1}^r M_i, ; ; ; ; ; R=(R_1 , , R_r) $ and \\(M_i M_j=0\\) for \\(i \\not = j\\). By Theorem 1.3.7, $ Y’M_i Y Y’M_j Y ; ; ; ; ; ; M_i M_j =0 $ Next, $ Y’M Y = _{i=1}^r Y’M_i Y $, therefore when \\(r(M_i)=1\\), $ {} {} F ( 1, r(I-M), ’ X’ M_i X ) $ $ \\[\\begin{alignat}{2} &amp; &amp;&amp; &amp;&amp; &amp;&amp;\\beta &#39; X&#39; M_\\ast X \\beta \\; \\; &amp;&amp;= \\; \\; \\sum_{i=1}^r \\beta &#39; X&#39; M_i X \\beta &amp;&amp; =0 \\; \\; \\; \\\\ &amp;\\iff &amp;&amp; &amp;&amp; \\forall i \\; \\; : \\; \\; &amp;&amp; \\beta &#39; X&#39; M_i X \\beta &amp;&amp; &amp;&amp;=0 \\\\ &amp;\\iff &amp;&amp; &amp;&amp;\\forall i \\; \\; : \\; \\; &amp;&amp;R_i &#39; X \\beta &amp;&amp; &amp;&amp;= 0 \\\\ &amp;\\iff &amp;&amp; &amp;&amp; &amp;&amp;H_0 \\text{ is true.} \\end{alignat}\\] $ EXAMPLE 3.6.1.: Balanced design; pp.79–80 EXAMPLE 3.6.2.: Unbalanced design;pp.80–81 6.5.10 Two-Way ANOVA $ \\[\\begin{alignat}{2} y_{ijk} &amp;= \\mu + \\alpha_i + \\eta_j &amp;&amp;+ \\epsilon_{ijk} \\tag{FM} \\\\ y_{ijk} &amp;= \\mu + \\alpha_i &amp;&amp;+ \\epsilon_{ijk} \\tag{RM} \\end{alignat}\\] $ $ \\[\\begin{align} M &amp;= M_\\mu + M_\\alpha + M_\\eta \\\\ Y&#39;(M-M_0)Y &amp;= R(\\eta \\; \\Big \\vert \\; \\alpha, \\; \\mu) \\tag{1} \\end{align}\\] $ Reduction in SSE, due to fitting \\(\\eta_j\\)’s after \\(\\mu\\) and \\(\\alpha_i\\)’s. Next, $ \\[\\begin{alignat}{2} y_{ijk} &amp;= \\mu + \\alpha_i &amp;&amp;+ \\epsilon_{ijk} \\tag{FM} \\\\ y_{ijk} &amp;= \\mu &amp;&amp;+ \\epsilon_{ijk} \\tag{RM} \\\\ \\\\\\ \\\\\\ Y&#39;(M_0-M_J)Y &amp;= R(\\alpha \\; \\Big \\vert \\; \\mu) \\\\ Y&#39;(M-M_J)Y &amp;= R(\\alpha, \\; \\eta \\; \\Big \\vert \\; \\mu) \\\\ &amp;= R(\\eta \\; \\Big \\vert \\; \\mu, \\; \\alpha) &amp;&amp;+ R(\\alpha \\; \\Big \\vert \\; \\mu) \\end{alignat}\\] $ In general, $ \\[\\begin{alignat}{2} R(\\eta \\; \\Big \\vert \\; \\alpha, \\; \\mu) &amp;\\not = R(\\eta \\; \\Big \\vert \\; \\mu) \\\\ R(\\alpha \\; \\Big \\vert \\; \\eta, \\; \\mu) &amp; \\not = R(\\alpha \\; \\Big \\vert \\; \\mu) \\end{alignat}\\] $ In paricular, for balanced design, if \\(\\mathcal{C}(X_\\alpha) \\perp \\mathcal{C}(X_\\eta)\\), $ \\[\\begin{alignat}{2} R(\\eta \\; \\Big \\vert \\; \\alpha, \\; \\mu) &amp; = R(\\eta \\; \\Big \\vert \\; \\mu) \\\\ R(\\alpha \\; \\Big \\vert \\; \\eta, \\; \\mu) &amp; = R(\\alpha \\; \\Big \\vert \\; \\mu) \\end{alignat}\\] $ Proposition 3.6.3. $ \\[\\begin{alignat}{2} R(\\eta \\; \\Big \\vert \\; \\alpha, \\; \\mu) &amp; = R(\\eta \\; \\Big \\vert \\; \\mu) \\; \\; \\; \\; \\; &amp;&amp;\\iff \\; \\; \\; \\; \\; \\mathcal{C}(X_1 - M_j) \\perp \\mathcal{C}(X_0 - M_j) \\\\ \\text{that is}\\; \\; \\; \\; \\; \\; \\; M_1 - M_J&amp; = M-M_0 \\; \\; \\; \\; \\; &amp;&amp;\\iff \\; \\; \\; \\; \\; (M_1 - M_J)(M_0 - M_J) = 0, \\; \\; \\; \\; \\; \\text{where} \\; &amp;&amp;R(\\eta \\; \\Big \\vert \\; \\alpha, \\; \\mu) &amp;&amp;= Y&#39;(M-M_0)Y \\\\ &amp; &amp;&amp; &amp;&amp; R(\\eta \\; \\Big \\vert \\; \\mu) &amp;&amp;= Y&#39;(M_1 -M_0)Y \\end{alignat}\\] $ 6.5.11 Confidence Regions \\(100(1-\\alpha)\\%\\) Confidence Region(CR) for \\(\\Lambda &#39; \\beta\\) consists of all the vectors \\(d\\) satisfying the inequality $ {} {MSE} ( 1- , ; r(), ; r(I-M) ) $ These vectors form an ellipsoid in \\(r(\\Lambda)\\)-dimensional space. For regression problems, if we take \\(P&#39; = (X&#39;X)^{-1}X&#39;\\), then \\(\\Lambda&#39;\\beta = P&#39; X \\beta = \\beta = d\\). The \\(100(1-\\alpha)\\%\\) CR is $ \\[\\begin{alignat}{2} &amp; \\dfrac {\\dfrac{\\Big[\\Lambda &#39; \\hat \\beta - d\\Big]&#39; \\Big[\\Lambda &#39; (X&#39;X)^- \\Lambda\\Big]^- \\Big[\\Lambda &#39; \\hat \\beta - d\\Big]}{r(\\Lambda)}} {MSE} \\; \\; \\; &amp;&amp; = \\; \\; \\; &amp; \\dfrac {\\dfrac{\\Big(\\hat \\beta - \\beta \\Big)&#39; \\Big( X&#39;X \\Big)\\Big(\\hat \\beta - \\beta \\Big)} {p}} {MSE} \\; \\; \\; &amp;&amp;\\le \\; \\; \\; \\Big( 1- \\alpha, \\; p, \\; n-p \\Big) \\end{alignat}\\] $ 6.5.12 Tests for Generalized Least Squares Models $ \\[\\begin{alignat}{4} &amp;Y &amp;&amp;= &amp;&amp;X \\beta &amp;&amp;+ &amp;&amp;\\epsilon \\; \\; \\; \\; \\; &amp;&amp;vs. \\; \\; \\; \\; \\; &amp;&amp;Y = &amp;&amp;X_0 \\beta_0 &amp;&amp;+ &amp;&amp;\\epsilon , \\; \\; \\; \\; \\; &amp;&amp; \\epsilon \\sim N(0, \\; \\sigma^2 V) \\tag{1} \\\\ &amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; \\Updownarrow \\\\ Q^{-1}&amp;Y &amp;&amp;= Q^{-1} &amp;&amp;X \\beta &amp;&amp;+ Q^{-1} &amp;&amp;\\epsilon \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp;&amp;vs. \\; \\; \\; \\; \\; Q^{-1} &amp;&amp;Y = Q^{-1} &amp;&amp;X_0 \\beta_0 &amp;&amp;+ Q^{-1} &amp;&amp;\\epsilon , \\; \\; \\; \\; \\; Q^{-1} &amp;&amp; \\epsilon \\sim N(0, \\; \\sigma^2 I) \\tag{2} \\end{alignat}\\] $ test (1) and (2) is equal. Note: \\(\\mathcal{C}(Q^{-1}X_0) \\subset \\mathcal{C}(Q^{-1}X)\\). From Section 2.7, $ \\[\\begin{align} A &amp;= X(X&#39;V^{-1}X)^- X&#39; \\ast V^{-1} \\\\ \\\\ MSE &amp;= \\dfrac{Y&#39; (I-A)&#39; V^{-1} (I-A)Y}{n-r(X)} \\\\ \\\\ A_0 &amp;= X_0(X_0&#39;V^{-1}X_0)^- X_0&#39; \\ast V^{-1} \\end{align}\\] $ Theorem 3.8.1 $ \\[\\begin{align} \\dfrac{\\dfrac{Y&#39; (A-A_0) V^{-1} (A-A_0)Y}{r(X) - r(X_0 )}}{MSE} &amp;\\sim F \\Big( r(X)-r(X_0), \\; n-r(X) , \\; \\delta^2 \\Big) \\\\ \\\\ \\delta^2 &amp;= \\dfrac{\\beta &#39; X&#39; (A-A_0) V^{-1} (A-A_0)X \\beta}{2\\sigma^2} \\tag{1} \\\\ \\\\ \\\\\\ {\\beta &#39; X&#39; (A-A0) V^{-1} (A-A_0)X \\beta} \\; \\; \\; \\; \\; &amp;\\iff \\; \\; \\; \\; \\; E(Y) \\in \\mathcal{C}(X_0) \\tag{2} \\end{align}\\] $ Theorem 3.8.2 let \\(\\Lambda &#39; \\beta\\) be estimable. then the test statistic for \\(H_0 : \\Lambda &#39; \\beta = 0\\) is $ \\[\\begin{align} \\dfrac{\\dfrac{\\hat \\beta &#39; \\Lambda \\Big[ \\Lambda &#39; (X&#39;V^{-1}X)^- \\Lambda \\Big]^- \\Lambda &#39; \\hat \\beta}{r(\\Lambda)}}{MSE} &amp;\\sim F \\Big( r(\\lambda), \\; n-r(X) , \\; \\delta^2 \\Big) \\\\ \\\\ \\delta^2 &amp;= \\dfrac{\\beta &#39; \\Lambda \\Big[ \\Lambda &#39; (X&#39;V^{-1}X)^- \\Lambda \\Big]^- \\Lambda &#39; \\beta}{2\\sigma^2} \\tag{1} \\\\ \\\\ \\\\\\ {\\beta &#39; \\Lambda \\Big[ \\Lambda &#39; (X&#39;V^{-1}X)^- \\Lambda \\Big]^- \\Lambda &#39; \\beta} \\; \\; \\; \\; \\; &amp;\\iff \\; \\; \\; \\; \\; \\Lambda &#39; \\beta = 0\\tag{2} \\end{align}\\] $ Theorem 3.8.3 $ \\[\\begin{align} \\dfrac{Y&#39; (A-A_0) V^{-1} (A-A_0)Y}{\\sigma^2} &amp;\\sim \\chi^2\\Big(r(x) - r(X_0), \\; \\delta^2 \\Big) \\\\ \\\\ \\delta^2 &amp;= \\dfrac{\\beta &#39; X&#39; (A-A_0) V^{-1} (A-A_0)X \\beta}{2\\sigma^2}, \\\\ \\\\ \\sigma^2 = 0 \\; \\; \\; \\; \\; &amp;\\iff E(Y) \\in \\mathcal{C}(X_0) \\tag{1} \\\\ \\\\ \\\\\\ \\dfrac{\\hat \\beta &#39; \\Lambda \\Big[ \\Lambda &#39; (X&#39;V^{-1}X)^- \\Lambda \\Big]^- \\Lambda &#39; \\hat \\beta}{2\\sigma^2} &amp;\\sim \\chi^2 \\Big( r(\\Lambda) , \\; \\delta^2 \\Big) \\\\ \\\\ \\delta^2 &amp;= {\\hat \\beta &#39; \\Lambda \\Big[ \\Lambda &#39; (X&#39;V^{-1}X)^- \\Lambda \\Big]^- \\Lambda &#39; \\hat \\beta}, \\\\ \\\\ \\sigma^2 = 0 \\; \\; \\; \\; \\; &amp;\\iff \\Lambda &#39; \\beta = 0 \\tag{2} \\end{align}\\] $ "],["generalized-least-squares.html", "6.6 Generalized Least Squares", " 6.6 Generalized Least Squares Consider a full rank parameterization $ Y = X + ; ; ; ; ; ; ; ; ; ; E()=0, ; ; ; Cov() = ^2 &gt;0 $ by SVD of \\(\\Sigma\\), $ \\[\\begin{alignat}{2} \\Sigma &amp;= \\Gamma &#39; \\Lambda \\Gamma = \\Gamma &#39; \\Lambda^{\\tfrac{1}{2}} \\Lambda^{\\tfrac{1}{2}}\\Gamma = \\Gamma &#39; \\Lambda^{\\tfrac{1}{2}} \\Gamma&#39; \\Gamma \\Lambda^{\\tfrac{1}{2}}\\Gamma = \\Lambda^{\\tfrac{1}{2}} \\\\ \\\\ Z &amp;\\equiv \\Lambda^{-\\tfrac{1}{2}} Y = \\Lambda^{-\\tfrac{1}{2}}(X \\beta + \\epsilon) = \\Lambda^{-\\tfrac{1}{2}}X \\beta + \\Lambda^{-\\tfrac{1}{2}} \\epsilon = W \\beta + \\epsilon^\\ast \\end{alignat}\\] $ $ \\[\\begin{align} \\hat \\beta &amp;= (W&#39;W)^{-1} W&#39; Z = (X&#39; \\Sigma^{-1}X)^{-1}X&#39;\\Sigma^{-1}Y \\\\ E(\\hat \\beta) &amp;= (X&#39; \\Sigma^{-1}X)^{-1} X&#39;\\Sigma^{-1} X \\beta = \\beta \\\\ Cov(\\hat \\beta) &amp;= \\sigma^2 (X&#39; \\Sigma^{-1}X)^{-1} \\\\ \\hat \\sigma^2 &amp;= \\dfrac{\\Vert Z - \\mu_Z \\Vert^2}{n-p} = \\dfrac{(Y-\\hat \\mu)&#39; \\Sigma^{-1} (Y-\\hat \\mu)}{n-p} \\end{align}\\] $ the projection Matrix is $ ^{-} X (X’ {-1}X){-1}X’ ^{-}$, which is symmetric, and hence is an orthogonal projection. Now all computations have been done in the \\(z\\) coordinates, so in particular \\(x&#39; \\beta\\) estimates \\(\\mu_Z = \\Sigma^{-\\tfrac{1}{2}} \\mu\\). Since linear combinations of Gauss-Markov estimates are Gauss-Markov, it follows immediately that \\(\\hat \\mu_Z = \\Sigma^{-\\tfrac{1}{2}} \\hat \\mu\\). 6.6.1 A direct solution via inner products We can approach the problem of determining the Generalized Least Squares estimators in a different way by viewing \\(\\Sigma\\) as determining an intter product. We do this by returning to first principles, carefully defining means and covariances in a general inner product space. let \\(x, \\; y \\in \\mathbb{R}^n\\) and \\((x,y) = x&#39;y\\) be the usual innter product. choose a basis \\(\\{e_1 , \\cdots, e_n \\}\\), the usual coordinate vectors. then a rvec \\(x\\) has coordinates \\((e_i, x) = x_i\\). Definition 1. \\(E(x)=\\mu= \\begin{pmatrix} \\mu_i \\end{pmatrix}\\) where \\(\\mu_i = E(e_i , \\; x)\\). For any \\(a \\in \\mathbb{R}^n\\), $ E( (a, x) ) = E( (_{i=1}^n a_i e_i, ; x ) ) = E( _{i=1}^n a_i (e_i, ; x) ) = _{i=1}^n a_i _i = (a, ; ) $ thus, another characterization of \\(\\mu\\) is: \\(\\mu\\) is the unique vector that satisfies \\(E\\Big( (a, x) \\Big) = (a, \\; \\mu)\\) for all \\(a \\in \\mathbb{R}^n\\). Now, turn to Cov. use the same set-up as above. if \\(E(x_i^2)&lt;\\infty\\), then \\(Cov(x_i , x_j) = (x_i = \\mu_i) (x_j - \\mu_j) = \\sigma_{ij} = \\sigma_{ji}\\) exists for all \\(i,j\\), and defines \\(\\Sigma = (\\sigma_{ij})\\). For any \\(a, b \\in \\mathbb{R}^n\\), $ Cov( (a, x), (b, x) ) = E( ({i=1}^n a_i x_i, ; {j=1}^n b_j x_j ) ) = {i=1}^n {j=1}^n a_i b_j Cov(x_i, ; x_j) = {i=1}^n {j=1}^n a_i b_j _{ij} =(a, b) $ Definition 2 Assume \\(E\\Bigg( (a,x)^2 \\Bigg) &lt; \\infty\\). The unique non-negative definite linear transformation \\(\\Sigma: V \\rightarrow V\\) that satisfies \\(Cov\\Bigg( (a,x), (b,x) \\Bigg) = (a, \\Sigma b)\\) for all \\(a, b \\in V\\) is called the covariance of \\(X\\) and is denoted \\(Cov(x)\\). Theorem 1 let \\(Y \\in V\\) with innerproduct \\((\\cdot, \\; \\cdot)\\), \\(Cov(Y)=\\Sigma\\). Define another inner product \\((\\cdot, \\; \\cdot )\\) on \\(V\\) by \\([x,y] - (x, \\; Ay)\\) for some positive definite \\(A\\). Then the covariance of \\(X\\) in the inner product sapce \\(V, \\; [\\cdot, \\; \\cdot])\\) is \\(\\Sigma A\\). Note 1: This shows that if \\(Cov(X)\\) exists in one inner product, it exists in all inner products. If \\(Cov(X)=\\Sigma\\) in \\(\\begin{pmatrix} V &amp; (\\cdot, \\; \\cdot) \\end{pmatrix}\\), then if \\(\\Sigma &gt; 0\\) in the inner product \\([x,y] = (x, \\; \\Sigma^{-1}y)\\), the covariance is \\(\\Sigma^{-1} \\Sigma = I\\). Theorem 2 Suppose \\(Cov(X) = \\Sigma\\) in \\(\\begin{pmatrix} V &amp; (\\cdot, \\; \\cdot) \\end{pmatrix}\\). If \\(\\Sigma_1\\) is symmetric on \\(\\begin{pmatrix} V &amp; (\\cdot, \\; \\cdot) \\end{pmatrix}\\), and \\(Cov \\Big( (a,x) \\Big) = (a, \\; \\Sigma_1 a)\\) for all \\(a \\in V\\), then \\(\\Sigma_1 = \\Sigma\\). This implies that the covariance is unique. Consider the inner product sapce given by \\(\\begin{pmatrix} \\mathbb{R}^n &amp; (\\cdot, \\; \\cdot) \\end{pmatrix}\\), where \\([x,y] = (x, \\; \\Sigma^{-1}y)\\), \\(E(Y)=\\mu \\in \\mathcal{E}\\) and \\(Cov(Y) = \\sigma^2 \\Sigma\\). Let \\(P_\\Sigma\\) be the projection on \\(\\mathcal{E}\\) in this inner product space, and let \\(Q_\\Sigma = I - P_\\Sigma\\), so \\(y = P_{\\Sigma} y + Q_{\\Sigma} y\\). Theorem 3 with \\([x,y] = (x, \\; \\Sigma^{-1}y)\\), \\(P_\\Sigma = X(X&#39;\\Sigma^{-1} X )^{-1} X&#39; \\Sigma^{-1}\\) is an orthogonal projection. Theorem 4 let the OLS estimate \\(\\hat \\beta = (X&#39;X)^{-1}X&#39;Y\\) and the GLS estimate \\(\\tilde \\beta = (X&#39;\\Sigma^{-1}X)^{-1} X&#39; \\Sigma^{-1}Y\\). then $ = ; ; ; ; ; ; ; ; ; ; (^{-1}X) = (X) $ Corollary 1 \\(\\mathcal{C}(\\Sigma^{-1}X) = \\mathcal{C}(X)= \\mathcal{C}(\\Sigma X)\\) So \\(\\Sigma\\) need not be inverted to apply the theory. To use this equivalence theorem (due to W. Kruskal), we usually characterize the \\(\\Sigma\\)’s for a given \\(X\\) for which \\(\\hat \\beta = \\tilde \\beta\\). if \\(X\\) is completely arbitrary, then only \\(\\Sigma = \\sigma^2 I\\) works. Intra-class correlation model: let \\(J_n \\in \\mathcal{C}(X)\\). then any \\(\\Sigma\\) of the form $ = ^2 (1-)I + ^2 J_n J_n ’ $ with \\(-\\dfrac{1}{n-1} &lt; \\rho &lt; 1\\) will work. to apply the theorem, we write, $ X = ^2 (1-)X + ^2 J_n J_n ’ X $ so for \\(i&gt;1\\), the i-th coluimn of \\(\\Sigma X\\) is $ ( X )_i = ^2 (1-)X_i + ^2 J_n a_i $ with \\(a_i = J_n &#39; X\\). Thus, the i-th column of \\(\\Sigma X\\) is a linear combination of the i-th column of \\(X\\) and the column of \\(1\\)’s. For the first column of \\(\\Sigma X\\), we compute \\(a_1 = J_n\\) and \\(\\Big ( \\Sigma X \\Big)_1 = \\sigma^2 (1- \\rho) J_n + n \\sigma^2 \\rho J_n = \\sigma^2 \\Big ( 1 + \\rho(n-1) \\Big )J_n\\), So \\(\\mathcal{C}(\\Sigma X) = \\mathcal{C}(X)\\) as required, provided that \\(1+\\rho(n-1) \\not = 0\\) or \\(\\rho &gt; -\\dfrac{1}{n-1}\\). "],["flat.html", "6.7 Flat", " 6.7 Flat 6.7.1 1.Flat Sometimes in statistical applications it is useful to consider a linear subspace that is shifted or translated from the origin. This will happen, for example, in models that include an intercept. It is therefore helpful to have the following definition of a space that is displaced from the origin. Definition 1 (Flat) suppose \\(M \\subset V\\) is a linear subspace, and \\(y_0 \\in V\\). Then a flat consists of \\(\\{x + y_0 \\; \\Big \\vert \\; x \\in M\\}\\). We will write \\(y_0 +M\\) where \\(M\\) is a subspace to indicate a flat. By considering translations, flats are equivalent to vector spaces. If \\(Y\\) is a rv whose domain is the flat \\(y_0 +M\\), then, if \\(y_0\\) is fixed, \\(Y-y_0\\) has domain \\(M\\). example set \\(S_4 = \\{(1,1,1)&#39; + z, \\; z \\in S_2\\}\\) is a flat, because \\(0 \\not \\in S_4\\). example In \\(C e^2\\), consider \\(M= \\left \\{ \\alpha \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\Bigg \\vert \\; \\alpha \\in C e \\right\\}\\), and \\(y_0 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\). Then the flat \\(y_0 + M\\) is given by the set \\(y_0 + M= \\left \\{ \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\Bigg \\vert \\; \\alpha \\in C e \\right\\}\\). which is just a straight line that does not pass through the origin, but rather through the point \\((2,2)\\). The choice of \\(y_0\\) is not unique and it can be any point \\(y=y_0 + y_\\alpha\\), where \\(y_\\alpha = \\alpha(1,2)&#39;\\). For example, if \\(\\alpha = -2\\), then \\(y=(0,-2)&#39;\\) and if \\(\\alpha=+1\\), then \\(y=(3,4)&#39;\\), and so on. For any \\(y_0\\) not of this form, we simply get a different flat. This is summarized in the next remark. Theorem 1 The two spans $ \\[\\begin{align} F_1 &amp;= \\left\\{ z \\; \\Big \\vert \\; z=y_0 + x, \\; \\; \\; y_0 \\in V, \\; \\; \\; x \\in M \\subset V \\right\\} \\\\ F_2 &amp;= \\left\\{ z \\; \\Big \\vert \\; z=y_1 + x, \\; \\; \\; y_1 \\in F_1, \\; \\; \\; x \\in M \\subset V \\right\\} \\end{align}\\] $ are the same subspace, so the representation of the flat is not unique. Definition 2 (Sum and intersection of subspaces) let \\(H,K\\) be two linear subspaces. Then $ \\[\\begin{alignat}{2} H + K &amp;= \\Big\\{ x+y \\; &amp;&amp;\\Big \\vert \\; x \\in H, \\; \\; \\; y \\in K \\Big\\} \\tag{sum of H and K} \\\\ H \\cap K &amp;= \\Big\\{ x \\; &amp;&amp;\\Big \\vert \\; x \\in H, \\; \\; \\; x \\in K \\Big\\} \\tag{intersection of H and K} \\end{alignat}\\] $ Theorem 2 Both \\(H + K\\) and \\(H \\cap K\\) are linear subspaces. Definition 3 (Disjoint subspaces) Two subspaces are disjoint if \\(H \\cap K = \\big \\{ 0 \\big \\}\\), the null vector. Theorem 3 If \\(H \\cap K = \\big \\{ 0 \\big \\}\\), and \\(z \\in H +K\\), then the decomposition \\(z = x+y\\) with \\(x \\in H\\) and \\(y \\in K\\) is unique. prf) suppose \\(z=x+y\\) and \\(z=x&#39; + y&#39;\\). Then, \\(x-x&#39; \\in H\\) and \\(y-y&#39; \\in K\\). We must have \\(x+y = x&#39; + y&#39;\\) or \\(x-x&#39;=y-y&#39;\\), which in turn requires that \\(x-x&#39; = y-y&#39; = 0\\), since \\(0\\) is the only vector common to \\(H\\) and \\(K\\). Thus, \\(x=x&#39;\\) and \\(y=y&#39;\\). Theorem 4 if \\(H \\cap K = \\big \\{ 0 \\big \\}\\), then \\(\\dim(H+K) = \\dim(H) + \\dim(K)\\). In general, $(H+K) = (H) + (K) -(H K) $ Proof: Exercise. Definition 4 (Complement of a space) If \\(M\\) and \\(M^c\\) are disjoint subspaces of \\(V\\) and \\(V = M +M^c\\), then \\(M^c\\) is called a complement of \\(M\\). Remark 1: The complement is not unique. In \\(\\mathbb{R}^2\\), a subspace \\(M\\) of dimension 1 consists of a line through the origin. A complement of \\(M\\) is given by any other line \\(M^c \\not = \\alpha M\\) through the origin, because linear combinations of any two such lines span \\(Ce^2\\). In the linear model \\(Y = X \\beta + \\epsilon\\), we have that $= E(Y ) = X $, so that \\(\\mu \\in \\mathcal{C}(X)\\). To estimate \\(\\mu\\) with \\(\\hat \\mu\\), we might want to require that \\(\\hat \\mu \\in \\mathcal{C}(X)\\) (note: if \\(X\\) includes a constant, then \\(\\mathcal{C}(X)\\) is a flat; otherwise, it is a subspace). The estimate would then depend upon \\(Y\\) in a sensible way by moving \\(Y\\) to the subspace. The method of moving is via projections. The optimality of moves depends on the way we measure distance - on an inner product defined on the vector space. 6.7.2 2. Solutions to systems of linear equations Consider the Matrix equation \\(X_{n \\times p} \\beta_{p \\times 1} = y_{n \\times 1}\\). For a given \\(X\\) and \\(Y\\) does there exist \\(\\beta\\) to these equations? Is it unique? If not unique, can we characterize all possible solutions? If \\(n=p\\) and \\(X\\) is nonsingular, the unique solution is \\(\\beta = X^{-1} y\\). If \\(y \\in \\mathcal{C}(X)\\), \\(y\\) can be expressed as a linear combination of the columns of \\(X\\). If \\(X\\) is of full column rank, then the columns of \\(X\\) form a basis for \\(\\mathcal{C}(X)\\), and the solution \\(\\beta\\) is just the coordinates of \\(y\\) relative to this basis. For any g-inverse \\(X^-\\), we have \\(XX^- y = y\\) for all \\(y \\in \\mathcal{C}(X)\\), and so a solution is given by \\(\\beta=X^- y\\). If \\(\\rho (X) = rank \\Big( \\mathcal{C}(X) \\Big) &lt; p\\), then the solution is not unique. If \\(\\beta_0\\) as any solution, for example the solution is given by \\(\\beta=X^- y\\), then so is \\(\\beta_0 + z, \\;\\;\\;\\;\\; z\\in N(X)\\), which is null-space of \\(X\\). The set of solutions is given by \\(\\beta_0 + N(X)\\), which is a flat. If \\(y \\not \\in \\mathcal{C}(X)\\), then there is no exact solution. This is the usual situation in linear models, and leads to the estimation problem discussed in the next chapter. What we might do is get the closest solution by replacing \\(Y\\) by another vector \\(\\hat Y\\) that is as close to \\(Y\\) as possible; if we define close as \\(\\Vert Y - \\hat Y \\Vert^2\\) making small, we need to solve \\(X \\beta = P_{\\mathcal{C}(X)}Y\\) insetead of the original equation. If \\(X\\) has full column rank, this leads to the familiar solution: $ \\[\\begin{align} \\beta_0 &amp;= X^+ P y \\\\ &amp;= (X&#39;X)^{-1} X&#39; X (X&#39;X)^{-1}X&#39; Y \\\\ &amp; = (X&#39;X)^{-1}X&#39;Y \\tag{2} \\end{align}\\] $ which is unique. If \\(X\\) does not have not full column rank, then the set of solutions again forms a flat of the form \\(\\beta_0 + N(X)\\) with \\(\\beta_0\\) given by (2). "],["unified-approach-to-balanced-anova-models.html", "6.8 Unified Approach to Balanced ANOVA Models", " 6.8 Unified Approach to Balanced ANOVA Models We can develop a unified approach to obtaining orthogonal projection operatores in arbitrary balanced \\(k\\)-way ANOVA models by exploting the structure of design matrix. The structure of the design matrix can be easily examined using Kronecker products. Therefore, before we proceed further, we need to establish some more properties of Kronecker products. Kronecker Product \\(:= A \\otimes B = (a_{ij}B)\\). Consider the balanced two-way ANOVA model with interaction. This model is given by $ Y_{ijk} = + _ i + j + {ij} + _{ijk} $ where \\(i=1, \\cdots, a\\), \\(j=1, \\cdots, b\\), \\(k=1, \\cdots, N\\), and \\(n=abN\\). We want to write $ (M) = (M_) + (M_) + (M_) + (M_) $ and be able to compute the orthogonal projection operators in an easy and unified way. We can represent each subspace making up \\(\\mathcal{C}(M)\\) in terms of Kronecker produdcts. Once we do this, we can easily obtain the orthogonal projection operator for that space. ※ Notation: let \\(s\\) be an arbitrary index. Define \\(J_s\\) as the \\(s \\times 1\\) vector of ones, $P_s = J_s J_s ’ $ and \\(Q_s = I_s - P_s\\), where \\(I_s\\) is the \\(s \\times s\\) identity matrix. Thus, \\(P_s\\) is the orthogonal projection operator onto \\(\\mathcal{C}(J_s)\\) and \\(Q_s\\) is the orthogonal projection operator onto \\(\\mathcal{C}(J_s)^\\perp\\) ※ Facts: recall that the OPO onto \\(\\mathcal{C}(A)\\) is always given by by \\(A(A&#39;A)^{-}A&#39;\\). if \\(M\\) is an OPO, then \\(M^{-} = M\\). $ Kronecker Product forms for the OPO Computing \\(M_\\mu\\). We can write \\(J_n = J_\\a \\otimes J_b \\otimes J_N\\), so that \\(M_\\mu\\) is the OPO onto \\(\\mathcal{C} \\Big( J_\\a \\otimes J_b \\otimes J_N \\Big)\\). Thus by Fact 1 above, we have $ &amp; &amp;&amp;M_&amp;&amp; &amp;&amp; \\ &amp;= &amp;&amp;( J_J_b J_N ) &amp;&amp;( ( J_a ’ J_b ’ J_N ’ ) ( J_a J_b J_N ) )^{-} &amp;&amp;( J_a ’ J_b ’ J_N ’ ) \\ &amp;= &amp;&amp;( J_a J_b J_N ) &amp;&amp;( J_a ’ J_a J_b ’ J_b J_N ’ J_N)^{-} &amp;&amp;( J_a ’ J_b ’ J_N ’ ) \\ &amp;= &amp;&amp;( J_a J_b J_N ) &amp;&amp;( ab N)^{-} &amp;&amp;( J_a ’ J_b ’ J_N ’ ) &amp;= &amp;&amp; J_a J_a ’ + J_b J_b ’ + J_N J_N’ \\ &amp;= &amp;&amp;P_a P_b P_N $ Using the properties of Kronecker products, it can be easily shown that \\(M = I_a \\otimes I_b \\otimes P_N\\). the error space is \\(\\mathcal{C}(I-M)\\) and $ I-M &amp;= I_{abN} - M \\ &amp;= ( I_a I_b I_N ) - ( I_a I_b P_N ) \\ &amp;= ( I_a ) ( I_N - P_N ) \\ &amp;= I_a I_b Q_N $ observe that $ \\[\\begin{align} M + I - M &amp;= ( I_a \\otimes I_b \\otimes P_N ) + (I_a \\otimes I_b \\otimes Q_N) \\\\ &amp;= ( I_a \\otimes I_b) \\otimes(P_N + Q_N) \\\\ &amp;= ( I_a \\otimes I_b) \\otimes (I_N) \\\\ &amp;= I_a \\otimes I_b \\otimes I_N \\\\ &amp;= I_n \\end{align}\\] $ We can summarize the subspace and the OPO for the two-way ANOVA model as follows. Excercise Consider the three-way ANOVA model write out the subspaces and all OPO corresponding to each term in the ANOVA model completlely in terms of Kronecker. Find the simplest expression for \\(M_\\mu + M_\\alpha + M_\\eta\\). https://smartstore.naver.com/hidamari/products/5283571274 https://smartstore.naver.com/hidamari/products/3029413531 "],["network-stats.html", "Chapter 7 Network Stats ", " Chapter 7 Network Stats "],["introduction-2.html", "7.1 Introduction", " 7.1 Introduction Network = Graph: for mathematical purposes, networks are most commonly represented in a formal manner using graphs of various kinds Vertices (Vertex), Edges, directed, undirected sender -&gt; receiver. total possible dyads: \\(n(n-1)\\) 기존 상황에서는 iid 가 가정되었었음. 이는 Likelihood 를 단순히 각 pdf 의 \\(\\prod\\) 로 나타내는 것을 가능하게 했었다. 하지만 네트워크 분석 상황에서는 iid 가 보장되지 않으며 따라서 equality 붕괴. 이제 네트워크는 Adjacency Matrix 를 써서 표현된다. 7.1.1 Types of Network Analysis Visualization Numerical Summaries Transitivity (Clustering Coefficient): A-B, A-C 조합의 변호사가 동업할 때, B-C끼리도 동업할 확률은 얼마일까? 이는 social network에서의 transitivity 개념과 대응함. 소위 clustering coefficient로 요약되는, 삼각형을 이루는 (즉, 모든 세개의 vertex pair가 edge로 연결) vertex 3개 묶음들의 비율을 나열하는 것으로 수치적으로 획득 가능. networks tends to form a triangle relationship. When a edge is formed, how likely this edge forms a triangle. Assortativity Coefficient: 2가지 종류의 변호사(corporate와 litigation)이 존재할 때, 동업과 더 일을 자주하는지 다른 분야와 더 일을 자주하는지, 그 비율은 어떻게 되는지 궁금할 수 있음. 이는 social network의 assortativity 개념과 대응하며, in which labels of connected pairs of vertices들이 compared되는, 소위 assortativity coefficient라고 불리는 correlation statistic으로 quantified될 수 있다. Each node has covariate information when a pair of node has same (similar) covariate, how likely they are connected. 주된 관심은 네트워크의 vertex (변호사 케이스라면 변호사 실무) 에 있으며 네트워크 구조 레벨의 속성은 좀 더 역할이 흐릿한 편 7.1.2 Network Modeling and Inference 관찰 대상 네트워크가 어떻게 생겼는지 묻고 구조를 특성화하는 것을 넘어, 보다 근본적인 수준에서 우리는 네트워크가 어떻게 발생했는지 이해하는 데 관심이 있을 수 있다. 즉, 우리는 네트워크가 복잡한 관심 시스템과 관련된 몇 가지 기본적인 프로세스에서 비롯되었다고 생각하고 이러한 프로세스의 본질적인 측면이 무엇인지 물어볼 수 있다. 네트워크가 어떤 과정을 거쳐 획득되었는지 - 사용된 measurement와 construction process - 또한 숙고될만한 부분이다. Network Modeling: Mathematical Models: 간단한 확률 규칙에 의거하여 네트워크를 생산. 규칙은 특성한 메커니즘 혹은 원칙을 파악하기 위한 시도의 일환으로 정의됨 (ex: ‘the rich get richer’) Statistical Models: 대부분, 아니면 일부분이나마, 관측된 데이터와 맥락을 같이 하기 위해 정의되는 모델 (자주 probabilistic하기도 함, 1번의 성질도 같이 갖는다는 소리) 이며 이의 fit함은 통계적 추론의 일반적인 원칙들을 사용하여 영향을 받고, 또 평가도 받음 이러한 2개의 모델의 종류 사이에는 교집합이 존재하지만, 이 둘을 다루는 paper들 사이에는 그럼에도 불구하고 큰 차이들이 존재함 Erdos-Renyi Model: assumes each connection in a network is iid. 각 vertex 쌍마다 iid 동전던지기를 통해 해당 쌍 사이에 edge를 둘지 안둘지를 랜덤하게 결정. 랜덤 그래프의 유명한 Erdos-Renyi 공식의 변형에 해당. 이는 성질이 정말 좋음. cohesive structure가 edge 1개에서의 확률의 함수로서 나타남. 또한 다른 더 복잡한 모델들과 비교되어 이해를 돕기 위한 교과서로서도. Measure the propoertion of connection among possible dyads. Mathematical Network Model: 수학모델은 현실 네트워크 데이터에 비하면 보통 너무 간단하지만, edge 구성의 특정 메커니즘이 어떻게 네트워크의 구조에 영향을 미칠 수 있는가 하는 것과, 관측된 네트워크에서 획득할 수 있는 구조적 성질이 얼마나 “significance” 한지를 판정하기 위한 네트워크의 null classes로 작동할 수 있다는 것에서 여전히 공부할 가치가 있음. Statistical Network Models: Exponential Random Graph Models 는 Generalized Linear Models (GLM)과 유사하며, 이는 둘다 지수족 형태(exponential family form)에 기반을 두고 있다. edge들이 unmeasured, 혹은 알려지지 않은 변수에 뿌리를 두고 있다는 것이 핵심인 Latent network models은 hierarchical modeling에서의 latent 변수 사용법과 정확하게 평행하다 즉, 대비된다???. Stochastic block models는 mixture 모델의 형태로 볼 수도 있다. 여기서 중요한건 이렇게 나열해놨지만서도 고차원 데이터가 의존성 높은 데이터를 쓰면 이런 애들은 이렇게 표준화된 모델과 맞아떨어지는 정도가 낮아진다는 것이다. 7.1.3 Network Processes 복잡계의 요소들간의 상호작용을 모사하기 위해, 네트워크 그래프 자체는 보통 네트워크 분석의 주된 목표가 됨. 물론 네트워크 구성 요소 중 시스템 내의 다른 모든 요소들과 상호작용하는 변량 혹은 속성이 있다면 이녀석이 최고관심의 대상이 될 것. 그러나 그럼에도 불구하고 요소들간의 상호작용이 앞에서 언급한 최고관심 대상에게 영향을 줄 것이라고 생각하는 것이 비합리적이지 않으므로 네트워크 그래프 자체는 여전히 모델링과 분석의 대상이기에 합당함. 우리는 확률과정을 네트워크에서의 “삶”이라고 해석해볼 수 있으며 네트워크 안의 vertices에 의해 첨수(indexed)됨. 이러한 과정에 관한 다양한 질문들은 정적 network process에 관한 것이든 동적 network process에 관한 것이든 이들을 예측하고자 하는 문제로 해석될 수 있음. 7.1.3.1 Dynamic Processes network-based 관점에서 연구되는 많은 system들은 본질적으로 동적임. 동적이 얘들 특성과 더 잘 부합함. 수학적 모델링이 여전히 이러한 과정을 모델링하는데 있어 1번째로 사용되는 툴이지만, network-based 통계적 모델들이 점차적으로 그 사용이 늘어나고 있음. 왜냐고? contact network에 대한 더욱 대량의 데이터가 사용 가능해지고 있으니까. 네트워크 flow 를 분석하기 위한 통계적 방법론들. 시작점으로부터 도착점까지의 material, 사람, 상품 등의 움직임 등을 생각해보면, flow들은 커뮤니케이션 네트워크 (인터넷 패킷 등), transportation 네트워크에 필수불가결한 동적 프로세스 이며 이외에도 그러함. 이러한 동적 프로세스들은 기본적으로 특이점이 없다면 시간의 흐름에 따라 evolve 될 것이 기대되고 있음. geodesic distance b/w vertices: the length of the shortest path(s) b/w vertices. The value of longest distance: diameter. complete graph: every node is connected to the other nodes. regular graph : a graph in which every node has the same degree. tree graph: a connected graph with no cycles \\(k\\)-star graph: a special case of tree graphs, there is one root and \\(k\\) leaves directed graph out-degree in-degree "],["descriptive-statistics-of-networks.html", "7.2 Descriptive Statistics of Networks", " 7.2 Descriptive Statistics of Networks complex system 에 대한 연구에서 연구하는 문제는 대응하는 네트워크 그래프의 구조, 혹은 특성을 분석하는 문제로 동치될 수 있음. 3개의 vertex 들을 묶어 특정 형태의 triplet 을 만들어 triplet 의 특성을 분석. 상품이나 정보의 흐름 분석은 네트워크 분석에서의 path 발생 혹은 비발생 확인 문제와 동치. 각각의 시스템에서 해당 element 의 중요도를 체크하는건 vertex 의 centrality 확인과 동치. 동계통의 community 혹은 그룹을 찾는 문제는 그래프 partitioning 문제와 동치. 이런 네트워크 분석은 순혈 통계와는 살짝 차이가 있음. 보통 수학과 컴퓨터과학, 사회구조 분석사회학, 물리학 등에 의존함. 7.2.1 Vertex and Edge Characteristics 네트워크의 기본 요소는 edge 와 vertex. 이들을 characterization 하고자 하는 작업은 vertex degree 에 기반하며, 이는 각 vertex 가 얼마나 중요한지를 판단하기 위한 측도를 획득하기 위함. 7.2.1.1 Vertex Degree 네트워크 그래프 \\(G=(V, E)\\) 에서 vertex \\(v\\) 의 degree \\(d_v\\) 는 \\(v\\) 에 엮인 edge 의 갯수. \\(\\forall v \\in V, d_v = d: f_d \\coloneqq \\frac{v}{d = d_v}\\). collection \\(\\{f_d\\}_{d \\ge 0}\\) 는 \\(G\\) 의 degree distribution 이라고 부른다. 이는 결국 원본 degree sequence 를 rescaling 한 것. library(sand) data(karate) par(mfrow = c(1, 2)) hist(degree(karate), col = &quot;lightblue&quot;, xlim = c(0, 50), xlab = &quot;Vertex Degree&quot;, ylab = &quot;Frequency&quot;, main = &quot;&quot;) hist(graph.strength(karate), col = &quot;pink&quot;, xlab = &quot;Vertex Strength&quot;, ylab = &quot;Frequency&quot;, main = &quot;&quot;) FIGURE 7.1: Karate Network weighted network 케이스에서 유용하게 자주 쓰이는 degree 의 일반화는 vertex strength. 이는 해당 vertex 에 연결된 edge 의 weight 를 전부 합한 것. 이러한 strength 의 distribution 은 weighted degree distribution 이라고 불리며, 이는 일반적인 degree distribution 과 입지가 같음. Figure 2: The vertex strength distribution for the Karate club network - A Network of Interactions among Protein Pairs in Yeast library(igraphdata) data(yeast) ecount(yeast) ## [1] 11855 vcount(yeast) ## [1] 2617 히스토그램을 확인해보자. degree 가 낮은 substantial fraction of vertex 들이 존재한다. 이들의 magnitude 는 karate network 의 그것이랑 유사하지만, 이와 동시에 연속적으로 higher order of magnitude 를 가지는 vertex 들의 숫자가 non-trivial 하게 관측된다. 로그화 시킨 degree 의 경우, log frequency 에서 상당한 linear decay 관측 가능. par(mfrow = c(1, 2)) d.yeast = degree(yeast) hist(d.yeast, col = &quot;blue&quot;, xlab = &quot;Degree&quot;, ylab = &quot;Frequency&quot;, main = &quot;Degree Distribution&quot;) dd.yeast = degree.distribution(yeast) d = 1:max(d.yeast) - 1 ind = (dd.yeast != 0) plot(d[ind], dd.yeast[ind], log = &quot;xy&quot;, col = &quot;blue&quot;, xlab = c(&quot;Log-Degree&quot;), ylab = c(&quot;Log-Intensity&quot;), main = &quot;Log-Log Degree Distribution&quot;) FIGURE 7.2: The degree distribution for protein interactions in Yeast Figure 3: The degree distribution for protein interactions in Yeast 서로 다른 degree 의 vertex 들이 서로 연결되어 있다면 그 기저에 깔린 메커니즘은 무엇인가? 이 또한 흥미로운 부분. 이 문제의 해결에 주효하게 작용하는 개념은 주어진 vertex 의 average degree of the neighbors. 높은 degree 의 vertex 는 높은 degree 랑만 붙는 경향이 있는 반면 lower degree 들은 높든 낮든 들러붙는 경향. # par(mfrow=c(1,1)) a.nn.deg.yeast = graph.knn(yeast, V(yeast))$knn plot(d.yeast, a.nn.deg.yeast, log = &quot;xy&quot;, col = &quot;goldenrod&quot;, xlab = c(&quot;Log Vertex Degree&quot;), ylab = c(&quot;Log Average Neighbor Degree&quot;)) FIGURE 7.3: The degree distribution for protein interactions in Yeast 7.2.1.2 Vertex Centrality vertex 에 대해 갖는 많은 의문은 결국 해당 vertex 가 주어진 네트워크에서 얼마나 중요한가 를 알기 위한 것. centrality 에 대한 많은 측도 (measure) 들은 이러한 중요성을 측정하기 위해 개발되었음. vertex centrality 를 확인하기 위해 가장 자주 쓰이는 measure 는 vertex degree. 이외에 vertex centrality measure 로서 사용되는 건 Closeness, Betweenness, and Eigenvector 등이 존재. 이 셋이 좀 메이저, 마이저. vertex centrality 를 나타내는 가장 직관적인 방법은 radial layout 을 쓰는 것. 이는 곧 central vertex 를 중앙에 가깝게 배치하는 것. 물론 네트워크가 너무너무 커버리면 표시불가. 작거나 중간크기 네트워크에만 사용가능. Closeness centrality vertex 가 다른 다수의 vertex 와 가깝다면 이를 central 이라고 판정. 일반적으로 사용되는 기준값은: $$ \\[\\begin{alignat}{2} &amp;C_{CL} &amp;&amp;=\\frac{1}{\\sum\\limits_{u \\in V} dist(u,v)} \\tag{closeness} \\\\ 0 \\le &amp; &amp;&amp;=\\frac{1}{\\sum\\limits_{u \\in V} dist(u,v)} \\cdot (N_v-1) \\le 1 \\tag{normalized} \\end{alignat}\\] $$ denominator \\(dist(v, u)\\) 는 \\(u, v \\in V\\) 인 vertex \\(u, v\\) 사이의 geodesic distance 이의 sum 은 결국 total distance of a vertex from all others. 각각 다른 그래프에서 산출된 centrality measure 를 비교하기 위해 normalize 하는 상황 있으며 위의 factor 곱하면 \\([0,1]\\) 로 normalize. large centrality value 는 곧 small total distance 로 이어짐. Betweenness centrality 어떤 vertex 가 다른 vertex 쌍 사이에 위치하고 있는지를 확인. 이건 vertex 가 네트워크 그래프의 path 에 비추어서 어디에 위치하고 있는지가 중요하다는 관점에 기반. 현실세계에 비추어도 이러한 path 가 인간관계라고 생각한다면 path 가 다수 지나가는 vertex, 즉 인싸는 중요한 사람일 것. 일반적으로 사용되는 값은: \\[ C_B (v) = \\sum\\limits_{s \\not = t \\not = v \\in V} \\frac{\\sigma(s,t | v)}{\\sigma(s, t)} \\] numerator 는 \\(v\\) 를 통과하면서 \\(s,t\\) 사이가 최단거리인 path 의 총 숫자, denominator 는 이런 조건 없이 \\(s,t\\) 사이가 최단거리인 path 의 총 숫자. 이를 unit interval 로 scale 할 때는 \\(\\frac{(N_v - 1) (N_v - 2)}{2} = \\choose {N_v - 1}2\\) 로 나누면 됨. eigenvector centrality ‘status,’ ‘prestige,’ ‘rank’ 등에 기반한 사고방식. vertex 의 이웃이 central 하다면, 본인 vertex 자체도 central 하리라는 관점. 이는 central 의 정의만 생각해보더라도 꽤 합리적인 추론임. 이는 적절하게 정의된 방정식의 linear system 의 eigenvalue solution 으로 표현가능함. 이러한 eigenvalue centrality 관점에 쓰이는 값은 꽤 많은데 가장 대표적인 건: \\[ C_{E_i} (v) = \\alpha \\sum\\limits_{\\{u,v\\}\\in E} C_{E_i}(u) \\] What is this? we implement eigen-analysis for an adjacency matrix - \\(\\alpha\\): precision find the largest eigenvalue and its corresponding eigenvector - eigenvalue: the importance capture largest variance - eigenvector: dependency information vector \\(C_{E_i} = \\left ( C_{E_i}(1), \\cdots, C_{E_i}(N_v) \\right)&#39;\\) 는 eigenvalue problem \\(A_{C_{E_i}}\\) 의 solution. \\(A\\) 는 네트워크 그래프 \\(G\\) 의 adjacency 매트릭스. \\(\\alpha^{-1}\\) 의 optimal choice 는 \\(A\\) 의 가장 큰 eigenvalue. 따라서 \\(h_{E_i}\\) 는 상응하는 eigenvector. \\(G\\) 가 undirected 이며 connected 라면, \\(A\\) 의 largest eigenvalue 는 간단하며 이의 eigenvector는 모두 nonzero entry 이며 부호 같음. 관례 (convention) 적으로는 이 entry 들의 abs 를 보고함. 이러면 eigenvector 의 orthonormality 에 의해 얘들은 자동적으로 \\([0,1]\\) 사이로 scaled. library(network) library(sna) A = get.adjacency(karate, sparse = FALSE) g = as.network.matrix(A) par(mfrow = c(2, 2)) gplot.target(g, degree(g), main = &quot;Degree&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, vertex.col = c(&quot;blue&quot;, rep(&quot;red&quot;, 32), &quot;yellow&quot;), edge.col = &quot;darkgray&quot;) gplot.target(g, closeness(g), main = &quot;Closeness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, vertex.col = c(&quot;blue&quot;, rep(&quot;red&quot;, 32), &quot;yellow&quot;), edge.col = &quot;darkgray&quot;) gplot.target(g, betweenness(g), main = &quot;Betweenness&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, vertex.col = c(&quot;blue&quot;, rep(&quot;red&quot;, 32), &quot;yellow&quot;), edge.col = &quot;darkgray&quot;) gplot.target(g, eigenvaluecent(g), main = &quot;Eigenvalue&quot;, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, vertex.col = c(&quot;blue&quot;, rep(&quot;red&quot;, 32), &quot;yellow&quot;), edge.col = &quot;darkgray&quot;) 이러한 centrality measure 를 undirected 에만 적용해왔음. directed 에도 적용못할 이유는 없지. 소위 hub vertex 의 중요성을 정의해보자. 얼마나 많은 authority vertex 를 그들이 향하는지, 그리고 얼마나 많은 authority vertex 들이 해당 vertex 를 향하는지를 통해 판단. directed graph \\(A\\) 가 주어졌을 때 hub 는 \\(M_{hub} = AA&#39;\\) 의 eigenvector centrality 에 의해 결정됨. authority 는 \\(M_{auth} = A&#39;A\\) 에 의해 결정. l = layout.kamada.kawai(aidsblog) par(mfrow = c(1, 2)) plot(aidsblog, layout = l, main = &quot;Hubs&quot;, vertex.label = &quot;&quot;, vertex.size = 10 * sqrt(hub.score(aidsblog)$vector)) plot(aidsblog, layout = l, main = &quot;Authorities&quot;, vertex.label = &quot;&quot;, vertex.size = 10 * sqrt(authority.score(aidsblog)$vector)) FIGURE 7.4: AIDS blog network with vertex area proportional to hubs and authority centrality measures 7.2.1.3 Characterizing Edges vertex betweenness centrality 에서 edge betweenness centrality 는 직관적. 각 edge 에 해당 edge 가 최단거리 path 로 삼아지는 횟수를 고르면 됨. eb = edge.betweenness(karate) E(karate)[order(eb, decreasing = T)[1:3]] ## + 3/78 edges from 4b458a1 (vertex names): ## [1] Actor 20--John A Mr Hi --Actor 20 Mr Hi --Actor 32 하지만 이외의 vertex centrality measures 들은 edge 에 적용하려면 그렇게 쉽진 않음. 해결책 중 하나는 네트워크 그래프 \\(G\\) 의 line graph 의 vertex 에 vertex centrality measures 를 적용하는 것. \\(G\\) 의 라인그래프 \\(G&#39;=(V&#39;, E&#39;)\\) 는 \\(G\\) 의 vertex 를 edge 로, edge 를 vertex 로 바꾸는 것으로 획득됨. vertex \\(v&#39; \\in V&#39;\\) 는 원본 그래프의 edge \\(e \\in E&#39;\\) 를 의미하며, edge \\(e&#39; \\in E&#39;\\) 는 \\(G\\) 에서의 대응하는 원본 vertex 1개의 세트를 의미함. 7.2.2 Characterizing Network Cohesion • 네트워크 그래프 상에서 edge 로서 정의된 관계에 비추었을 때, vertex 들의 어느 subset 이 어느 정도로 응집되는가, 혹은 같이 들러붙는가 하는 문제에 대해 생각해보는 것이 network cohesion. SNS 에서 친구의 친구끼리는 친구가 되기 쉬운가? 세포에서 어느 protein 들끼리 협업할 가능성이 높은가? 인터넷 토폴로지에서 어느 부분이 “backbone” 을 구성하는가? 다루는 문제가 무엇이냐에 따라 network cohesion 그 자체가 무엇인지 또한 달라짐. local 에서 global (ex. giant component), 어느 정도로 확실하게 정의되는가 (ex. clique) 혹은 두루뭉술한가 (ex. cluster, community) 등 스탯이 다양함. 7.2.2.1 Subgraphs and Censuses Cliques complete 서브그래프, 즉 fully cohesive 한 vertex 들의 subset 을 Cliques 라고 통칭. 이인즉 소속된 모든 vertex 들이 edge 로 연결되어 있다는 것. 모든 사이즈의 clique 들에 대한 census 는 그래프가 어떻게 structure 되어 있는지에 대한 ‘snapshot’ 을 제공함. 더 큰 사이즈의 clique 는 필연적으로 작은 사이즈의 cluque 들을 포함함. maximal clique 는 더 큰 clique 의 subset 이 아닌 clique 를 일컫음. 큰 clique 는 본질적으로 드뭄. 큰 clique 의 존재는 결국 원본 그래프 \\(G\\) 가 일정 이상으로 dense 할 것을 요구하니까. 하지만 현실세계 네트워크는 보통 sparse 하거든. clique size: 1 (node), 2 (edge), 3 (triangle), 4 (안이 크로스된 사각형) table(sapply(cliques(karate), length)) ## ## 1 2 3 4 5 ## 34 78 45 11 2 cliques(karate)[sapply(cliques(karate), length) == 5] ## [[1]] ## + 5/34 vertices, named, from 4b458a1: ## [1] Mr Hi Actor 2 Actor 3 Actor 4 Actor 14 ## ## [[2]] ## + 5/34 vertices, named, from 4b458a1: ## [1] Mr Hi Actor 2 Actor 3 Actor 4 Actor 8 table(sapply(maximal.cliques(karate), length)) ## ## 2 3 4 5 ## 11 21 2 2 clique.number(yeast) ## [1] 23 \\(k\\)-core clique 를 약화시킨 개념. 그래프 \\(G\\) 의 \\(k\\)-core 는 \\(G\\) 의 서브그래프 중 모든 vertex degree 가 최소한 \\(k\\) 는 되는 것 서브그래프들 중에서도, 다른 서브그래프들이 \\(k\\)-core 와 동일한 condition 을 따르지 않는 것을 \\(k\\)-core 라고 말함. 즉, 해당 성질을 보유한 서브그래프들 중 maximal 한 놈. core 라는 개념은 특히 visualization 쪽에서 핫함. 네트워크를 ‘layer’ 로 decomposition 할 방법론을 제공하기 때문. cores = graph.coreness(karate) gplot.target(g, cores, circ.lab = FALSE, circ.col = &quot;skyblue&quot;, usearrows = FALSE, vertex.col = cores, edge.col = &quot;darkgray&quot;) FIGURE 7.5: Visual representation of the k-core decomposition of the karate network detach(&quot;package:sna&quot;) detach(&quot;package:network&quot;) core 에 포함된 vertex 들은 center 에서 크게 떨어지지 않았으며, 각 core 에서 일정한 거리를 유지하고 있음이 시각적으로 확인 가능. Network Cohesion 을 정의함에 있어서 쓰이는 다른 서브그래프들의 class vertex 2개를 뽑아 만든 쌍 (pair) 를 dyad 라고 부름. directed 그래프에서 dyad 는 3개의 상태를 가질 수 있다. null (no directed edges) asymmetric (one directed edge) mutual (two directed edges) 대부분의 dyad 는 보통 null 이며, non-null 중에서도 대부분은 aymmetric 이다. 후자의 경우는 블로그에서 서로이웃이 아니라 한쪽만 팔로잉하는 경우겠지. vertex 3개를 뽑아 만든 모음은 Triad. 이는 16개의 상태를 가질 수 있음. null 서브그래프부터, triad 에 속한 모든 vertex 들이 mutual directed edge 를 보유하는 서브그래프 까지. aidsblog = simplify(aidsblog) dyad.census(aidsblog) ## $mut ## [1] 3 ## ## $asym ## [1] 177 ## ## $null ## [1] 10405 해당 데이터를 살펴보면 hub 와 authority 에 대한 기존 지식과 궤를 같이함을 확인 가능. Small connected subgraphs of interest are commonly termed motifs. motif 라는 개념은 생물 네트워크에서 두드러지게 유명한 개념, 생태계 substructure 를 biological function 과 연결지을때 자주 쓰임. 7.2.2.2 Density and Related Notions of Relative Frequency Density 그래프의 Density 는 potential, 즉 잠재적으로 발생할 edge 대비 실제로 발생한 edge 들의 빈도. 예를 들어 (undirected) 그래프 \\(G\\) 가 self-loop 가 없고 multiple edge 도 없다고 할 때, 서브그래프 \\(H=(V_H, E_H)\\) 의 density 는 \\[ \\begin{align} 0 \\le den(H) &amp;= \\frac{|E_H|}{\\frac{|V_H|(|V_H|-1)}{2}}= \\frac{\\text{# of edges}}{\\text{# of dyads}} \\le 1 \\tag{undirected} \\\\ &amp;= \\frac{\\phantom{|E_H|}}{|V_H|(|V_H|-1)} \\tag{directed} \\begin{alignat} \\] 해당 값은 \\([0,1]\\) 에 존재하며 \\(H\\) 가 clique 가 되기까지의 역치에 얼마나 가까운지에 대한 측도 (measure) 를 제공함. \\(H=G\\) 인 상황이라면 전체 그래프 \\(G\\) 에 대한 density 를 생산. 반대로 vertex \\(v \\in V\\) 의 neighbor 의 set \\(H_v=H\\) 가 되게 한다면, 이들 사이의 edge 는 \\(v\\) 의 immediate 이웃의 density 의 측도 (measure) 을 생산함. immediate 이웃의 합집합으로만 생산한 ego-centric 네트워크 는 원본의 overall 네트워크보다 명백히 dense 함. ego.instr = induced.subgraph(karate, neighborhood(karate, 1, 1)[[1]]) ego.admin = induced.subgraph(karate, neighborhood(karate, 1, 34)[[1]]) graph.density(karate) graph.density(ego.instr) graph.density(ego.admin) Clustering Coefficients (transitivity) 일반적으로 이하를 일컫는다. \\[ cl_T (G) = \\frac{3\\tau_\\Delta (G)}{\\tau_3 (G)} \\] \\(\\tau_\\Delta (G)\\): 그래프 \\(G\\) 안에 있는 모든 triangle 의 숫자 \\(\\tau_3 (G)\\): connected triple, 즉 3개의 vertex 에 2개의 edge 가 놓여있는 (i.e., 2-star) vertex 들로 만든 서브그래프의 숫자. 이 Clustering Coefficients 인 \\(cl_T(g)\\) 는 그래프의 transitivity 라고 불리기도 함. 이는 소셜 네트워크 문헌에서 일반적으로 관심을 갖는 변량 중 하나임. 다른 말로 fraction of transitive triples 라고도 불림. \\(cl_T(g)\\) 는 global clustering 의 measure 이며, connected triple 이 triangle 을 형성하기까지에 얼마나 가까운지에 대한 상대적 빈도를 서술함. transitivity(karate) ## [1] 0.2556818 transitivity(karate, &quot;local&quot;, vids = c(1, 34)) ## [1] 0.1500000 0.1102941 Reciprocity directed graph 에 한정된 개념. reciprocated (mutual) 한 edge 의 숫자를 총 edge 의 숫자로 나눈 것. 이는 single, unreciprocated 한 dyad 대비 reciprocated dyad 의 비중을 나타냄. reciprocity(aidsblog, mode = &quot;default&quot;) ## [1] 0.03278689 reciprocity(aidsblog, mode = &quot;ratio&quot;) ## [1] 0.01666667 7.2.2.3 Connectivity, Cuts, and Flows 기본적으로 궁금한 건 주어진 그래프가 서로 다른 서브그래프로 쪼개질 수 있나 하는 것. 불가능하다면 해당 그래프가 이 쪼개질 수 있는 성질의 역치에 얼마나 가까운지를 체크하는 것이 목적이 된다. 만약 모든 vertex가 다른 모든 vertex에서 접근 가능하다면, 즉 adjacency Matrix가 diag 제외하고 모두 1이면, 그래프 \\(G\\)는 connected라고 칭해진다. 그리고 그래프의 connected component는 maximally connected 서브그래프이다. 그래프 \\(G\\)의 connected component 중 하나가 다른 모두를 위력에서 압도한다면, 이는 곧 해당 connected component가 \\(G\\)의 대부분의 vertex를 포함하고 있다는 이야기. 이러한 component는 giant component라고 불리며 이는 random graph theory 출신 용어. is.connected(yeast) ## [1] FALSE comps = decompose.graph(yeast) table(sapply(comps, vcount)) ## ## 2 3 4 5 6 7 2375 ## 63 13 5 6 1 3 1 결과는 false로 나오지만 이에 대해 census 돌리면 giant component의 존재 확인 가능. 아래 예시의 경우 component 1개가 2375/2617로 90퍼 vertex랑 연결중임. 이는 현실 네트워크에서의 small world property와 연결. vertex 쌍들 collection에서의 minimum path는 보통 되게 작음. 대비되게 clustring은 상대적으로 높음. (ex) protein? small world property: high connectivity b/w pairs of nodes small shortest-path distance high clustering coefficient yeast.gc = decompose.graph(yeast)[[1]] average.path.length(yeast.gc) ## [1] 5.09597 diameter(yeast.gc) ## [1] 15 transitivity(yeast.gc) ## [1] 0.4686663 해당 네트워크에서의 shortest path는 \\(N_v\\)보다 \\(\\log N_v\\)로 표현되는게 정확할 정도로 짧음. scales more like, thus considered small. 동시에 해당 네트워크에서의 clustering은 상대적으로 large, 이는 transitivity로 확인 가능. Connectivity \\(k\\)-vertex-connected the number of vertices \\(N_v &gt; k\\) cardinality \\(|X|&lt;k\\) 이며 \\(X \\subseteq V\\) 인 vertex의 subset \\(X\\) 을 지우면 connected subgraph가 아니게 됨. \\(k\\)-edge-connected \\(N_v ≥ 2\\) cardinality \\(|Y|&lt;k\\)이며 \\(Y \\subseteq E\\)인 edge의 subset \\(Y\\)을 지우면 connected subgraph가 아니게 됨. 위의 조건에 따라 그래프 \\(G\\) 는 \\(k\\)-vertex-connected 혹은 \\(k\\)-edge-connected. 즉 \\(G\\)의 vertex (edge) connectivity는 \\(G\\)의 k-vertex(k-edge-) connected가 유지되는 가장 큰 integer. 이때 vertex connectivity \\(\\le\\) edge connectivity \\(\\le\\) minimum degree among vertex in \\(G\\) (dmin). 따라서 이 서브그래프를 추가적인 component로 분해하기 위해서는 단 1개의 엄선된 vertex나 edge를 제거하는 것으로 충분하다. vertex.connectivity(yeast.gc) ## [1] 1 edge.connectivity(yeast.gc) ## [1] 1 Cut vertex (edge)의 subset \\(S\\)를 제거하는 것으로 해당 그래프가 서브그래프로 조각난다면, \\(S\\)는 vertex-cut (edge-cut). 여기서 vertex \\(S\\)의 원소가 1개라면, 즉 vertex 1개만을 제거한 것으로 그래프가 조각났다면, 이는 cut vertex, 혹은 articulation point. 이러한 vertex의 여부를 식별하는 건 해당 네트워크가 외부 공격에 취약하는지를 파악하는데 도움이 됨. 해당 포인트 끊기면 네트워크 정상작동이 안되니까. Identification of such vertices can provide a sense of where a network is vulnerable (e.g., in the sense of an attack, where disconnecting produces undesired consequences, such as a power outage in an energy network). In the giant component of the yeast network, almost 15% of the vertices are cut vertices. yeast.cut.vertices = articulation.points(yeast.gc) length(yeast.cut.vertices) ## [1] 350 nontrivial 그래프 \\(G\\)는 \\(k\\)-vertex (k-edge) connected \\(\\iff\\) 서로다른 vertex의 쌍 \\(u, v \\in V\\)가 \\(k\\) vertex-disjoint (edge-disjoint) paths에 의해 connected 가능. 이 결과는 그래프에서 특정 vertex (edge)가 제거된 상황에서도 그래프 내부에서 만들어지는 서로 다른 path 들이 얼마나 많은지를 통해 평가되는 그래프의 robust함과 연결되어 있다. 낮은 vertex (edge) connectivity 를 가지는 그래프는 따라서 path들을 가질 수 있으며, 이에 의해 그 path들을 통과했던 “information”들은 작은 숫자의 vertex (edge)를 없애는 것만으로 쉽게 방해되고 만다. shortest.paths() graph.maxfow() graph.mincu() 7.2.3 Graph Partitioning Partitioning은 elements의 집합을 “발생이 자연스러운” 부분집합으로 분할하는 과정. 더 이론적으로 말하자면, finite set \\(S\\)의 partition \\(C = \\{ C_1, \\cdots, C_K \\}\\)는 \\(S\\)를 \\(K\\) 개의 disjoint로 decomposition 한 물건으로, 이인즉 \\(\\forall C_k \\not = \\emptyset: \\cup_{k=1}^K C_k = S\\). 네트워크 그래프 분석에서, partitioning은 겉으로 드러나지 않는 관계성 측면에서 vertex의 묶음이 cohesiveness를 가지고 있는지를 확인하기에 유용한 방법이다. vertex의 cohesive한 subset은 일반적으로 이하와 같은 걸 일컬음: subset 내부에서, 동시에, 잘 connected 되어 있어야 한다 subset 외부, 즉 남아있는 vertex들과 잘 seperated - 연결성이 없음 Graph partitioning algorithms 은 보통 그래프 \\(G(V, E)\\)의 vertex set \\(V\\) 의 partition \\(C = \\{ C_1, \\cdots, C_K \\}\\)를 찾는 것을 그 목표로 함. 이를 위한 방법으로 \\(C_k\\) 안의 vertex에서 \\(C_k&#39;\\)로의 vertex로 잇는 edge의 sets \\(E(C_k, C_k &#39;)\\)는 \\(C_k\\) 내에서 vertex 를 잇는 edge들의 set \\(E(C_k) = E(C_k , C_k)\\)보다 작다는 점을 활용함. 그래프 partitioning의 이 문제는 complex networks 문헌에서의 community detection에서도 동일하게 발생함. 이에 대한 해결책으로 큰 틀에서 2가지 접근법 이 존재. 7.2.3.1 Hierarchical Clustering 그래프 파티셔닝에 사용되는 대부분의 방법은 본질적으로 Hierarchical Clustering의 변용에 불과함. 여러가지 방법론이 제시되었지만, 그 차이는 결국 이하가 다를 뿐임. proposed clusterings의 quality를 어떻게 측정하는가 연구자가 찾고 있는 해당 quality를 어떻게 최적화하는가. 보통 greedy algorithm 으로 모든 가능한 partition \\(C\\)의 space를 탐색하는 식으로 한다. 이 과정에서 계속해서 후보 partition을 갱신하고. Hierarchical methods 는 다음 둘로 분류됨. agglomerative, 파티션을 합쳐나가는 것을 계속해나가는 것으로 크기를 키워가는 것에 기반 (coarsen) divisive, 파티션을 쪼개나가는 것을 계속해나가는 것으로 연속으로 다듬어나가는 것 각 단계에서 현재의 후보 partition은 지정된 비용 측정값을 최소화한다는 목적으로 계속해서 정제되어 갑니다. agglomerative 방법에서는, 2개의 이전의 partition elements 중 가장 저렴한 merge 방법이 실행된다 divisive 방법에서는, 1개의 이전의 partition 중 가장 저렴하게 2개로 split 할 수 있는 방법이 실행된다 비용측정의 기준은 vertex의 cohesive subset을 뭘 기준으로 판정할지 하는 연구자의 주관이 개입됨. 메이저한 기준은 modularity. 계산은 이하와 같다: \\(C = \\{ C_1, \\cdots, C_K \\}\\)를 주어진 (given) 후보 (candidate) partition 으로 하자 \\(f_{ij} = f_{ij}(\\mathcal C)\\)는 \\(C_i\\) 에 있던 vertex 들을 \\(C_j\\) 에 있는 vertex 들과 연결시키는 (오리지널 네트워크의) edge 들의 fraction 이때 \\(\\mathcal C\\)의 modularity는 \\[ \\mod(\\mathcal C) = \\sum_{k=1}^K \\left[ f_{kk}(\\mathcal C) - f_{kk}^\\ast \\right] \\tag{modularity} \\] \\(f_{kk}\\) 는 within \\(C_k\\) 에서의 observed connections. \\(f_{kk}^\\ast\\)는 random edge assignment의 몇몇 모델 이하에서의 \\(f_{kk}\\)의 기댓값. \\(f_{kk}^\\ast\\)는 \\(f_{k+} \\cdot f_{+k}\\)이며 각각 \\(f\\)의 k번째 rowsum과 colsum. 즉 \\(f_{ij}\\)를 entry로 하는 \\(K \\times K\\) 매트릭스가 만들어짐. This choice corresponds to a model in which a graph is constructed to have the same degree distribution as \\(G\\), but with edges otherwise placed at random, without respect to the underlying partition elements dictated by \\(C\\). In principle the optimization of the modularity requires a search over all possible partitions C, which is prohibitively expensive in networks of moderate size and larger. • A fast, greedy approach to optimization has been proposed, in the form of an agglomerative hierarchical clustering algorithm, and implemented in igraph as fastgreedy.community. • The result of this and related community detection methods in igraph is to produce an object of the class communities, which can then serve as input to various other functions. Applying this method to the karate network, kc = fastgreedy.community(karate) length(kc) ## [1] 3 sizes(kc) ## Community sizes ## 1 2 3 ## 18 11 5 head(membership(kc)) ## Mr Hi Actor 2 Actor 3 Actor 4 Actor 5 Actor 6 ## 2 2 2 2 3 3 high modularity value nontrivial group ??? The largest community of 18 members is centered around the administrator (i.e., John A, vertex ID 34). • The second largest community of 11 members is centered around the head instructor (i.e., Mr Hi, vertex ID 1). plot(kc, karate) FIGURE 7.6: Partitioning of the Karate network obtained from hierarchical clustering Figure 9: Partitioning of the Karate network obtained from hierarchical clustering • Whether agglomerative or divisive, when used for network graph partitioning, hierarchical clustering methods actually produce, as the name indicates, an entire hierarchy of nested partitions of the graph, not just a single partition. • The resulting hierarchy typically is represented in the form of a tree, called a dendrogram. library(ape) dendPlot(kc, mode = &quot;phylo&quot;) FIGURE 7.7: The corresponding dendrogram for this partitioning 7.2.3.2 Spectral Partitioning Calculate Laplacian Grpah \\[ L = \\underbrace{D}_{\\text{diagonal matrix with degree}}-\\underbrace{A}_{\\text{Adjacency Matrix}} \\\\ = \\begin{bmatrix} degree(n_1) &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; degree(n_r) \\end{bmatrix} -A \\] Eigenvalue decomposition of \\(L\\) \\[ L = \\underbrace{\\begin{bmatrix} v_1 &amp; \\cdots &amp; v_n\\end{bmatrix}}_{eigenvector} \\underbrace{\\begin{bmatrix} \\lambda_1 &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0&amp; &amp; \\lambda_n \\end{bmatrix}}_{eigenvalue} \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n\\end{bmatrix} \\] sort eigenvalue \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_n\\). Is \\(L\\) full rank? No. Why? cause \\(L\\) is a representation of a network. Smallest eigenvalue can be shown to be identically zero and its corresponding eigenvecotr 1. Then? # of component in a grpah is directly related to # of non-zero eigenvalue. Which means \\(\\lambda_{N-1} \\approx 0 \\Rightarrow K=2\\), \\(\\lambda_{N-1} \\approx \\lambda_{N-2} \\approx 0 \\Rightarrow K=3\\)3 select \\(K\\) eigenvector. \\(v_{n-1}, \\cdots, v_{n-k}\\). apply \\(k\\)-means clustering to \\(k\\) selected eigenvector. spectral graph theory의 연구결과를 응용하여 그래프 \\(G\\)의 connectivity를 특정 매트릭스의 eigen-analysis와 연관짓는 것. adjacency matrix \\(A\\)에 대한 그래프 \\(G\\)의 그래프 Laplacian 은 \\(L = D − A\\)이며, 이때 \\(D = diag[(D_{vv} = d_v)]\\), \\(d_v\\)는 \\(G\\)의 entries of the degree sequences. spectral graph theory의 결과를 통해 우리는 다음을 파악 가능. 그래프 \\(G\\)는 \\(K\\) 개의 connected components로 구성 \\(\\iff\\) \\(\\lambda_1 (L) = \\cdots = \\lambda_K(L) = 0\\) 이며 \\(\\lambda_{K+1}(L)&gt;0\\), where \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{N_v}\\)들은 L의 (not necessarily distinct) eigenvalue이며, ordered from small to large. 그래프 안의 component의 숫자는 그래프 Laplacian의 non-zero eigenvalue의 숫자과 직접적으로 연관되어 있음. \\(L\\)의 최소 eigenvalue는 0임을 바로 보일 수 있다. eigenvector \\(x_1 = (1,\\cdots,1)&#39;\\)에 대응하므로. 따라서 우리가 그래프 \\(G\\)가 “거의” \\(K=2\\) 개의 component들로 구성되어 있다고 추론한다면, 즉슨 2개로 쪼개기에 적합하다고 생각한다면, 이는 곧 우리는 해당 대상에 대해 \\(\\lambda_2(L)\\)가 0에 가까울 것이라고 추론할 것이라는 것과 동치이다. 이러한 추론은 \\(\\lambda_2\\)가 그래프 connectivity와 structure의 측정치의 값과 깊은 연관이 있기에 합리적이다. 특히 이러한 관계성은 \\(\\lambda_2\\)가 0에 가까울 수록 서브그래프 A과 서브그래프 B 사이를 통과하는 edge가 적을 것이기에 이렇게 둘로 쪼개는 것이 합리적일 것임을 보여준다. \\(\\lambda_2\\)를 그래프의 connectivity와 연관지은 제언자는 대응하는 eigenvector \\(x_2\\) 안의 entries들의 부호에 따라 vertex들을 쪼개는 것을 주장했다. 결과는 다음과 같다: \\[ S = \\{v \\in V: x_2 (v) \\ge 0 \\} \\\\ \\bar S = \\{v \\in V: x_2 (v) &lt; 0 \\} \\] 즉, 2개의 vertex의 subset이 생산되며 (이를 보통 cut이라고 부름), 이 벡터 \\(x_2\\)는 보통 Fiedler Vector라고 불리며 이에 대응하는 eigenvalue \\(\\lambda_2\\)는 Fiedler Value라고 부른다. k.lap = graph.laplacian(karate) eig.anal = eigen(k.lap) plot(eig.anal$values, col = &quot;blue&quot;, ylab = &quot;Eigenvalues of Graph Laplacian&quot;) FIGURE 7.8: Eigenvalues of Graph Laplacian We plot the eigenvalues of the graph Laplacian. 0인 eigenvalue는 딱 하나. (해당 네트워크는 connected이므로 예상한 결과) 2번째로 작은 eigenvalue인 \\(\\lambda_2\\)는 0에 매우 가까움. f.vec = eig.anal$vectors[, 33] #Extracting the Fiedler vector faction = get.vertex.attribute(karate, &quot;Faction&quot;) f.colors = as.character(length(faction)) f.colors[faction == 1] = &quot;red&quot; f.colors[faction == 2] = &quot;cyan&quot; plot(f.vec, pch = 16, xlab = &quot;Actor Number&quot;, ylab = &quot;Fiedler Vector Entry&quot;, col = f.colors) abline(0, 0, lwd = 2, col = &quot;lightgray&quot;) FIGURE 7.9: Fiedler vector and its corresponding partition Fiedler vector를 생산하고 해당 vector의 요소들을 실제 actor number에 따라 배정한 그래프를 보면 이 spectral 방법이 faction label에 의해 네트워크 partitioning 을 획득할 수 있다는 것을 확인된다. 보통 우리는 네트워크가 서브그래프 2개보다는 더 잘게 쪼개질 수 있으리라고 예상 가능. spectral 방법을 iterative하게 적용하는 것으로 2개 이상으로 쪼갤 수 있음. 하지만 이러한 반복이 특정 목적 함수를 최적화할 수 있도록 목표하는 것이 바람직함. Newman은 spectral bisection method와 논리적 흐름이 유사하나 Laplacian \\(L\\)이 아니라 이를 대체해서 modularity와 연관된 매트릭스를 사용하는 방법을 제안했다.(leading.eigenvector.community) 7.2.3.3 Validation of Graph Partitioning validation 문제는 그래프 partitioning에 항상 중요하지만, 대부분의 경우 nontrivial 문제이다. 네트워크 그래프에 vertex의 cohesive subset 이 존재한다면, 이러한 subset의 기저에는 vertex에게 있어 vertex 간에 특정한 연관적인 특성 (또는 속성)에 일부 공통성이 있을 것으로 일반적으로 예상한다. 그래프 partitioning은 이러한 성질에 대한 지식이 없을때 그러한 subset을 발견하기 위한 도구로 인식될 수도 있다. 우리가 그래프 외부에서 정의된 클래스 멤버쉽에 대한 subset 정의를 알고 있다면, 그래프 내부에서의 partitioning으로 얻은 분절들과 비교하는 것도 흥미로움. func.class = get.vertex.attribute(yeast.gc, &quot;Class&quot;) table(func.class) ## func.class ## A B C D E F G M O P R T U ## 51 98 122 238 95 171 96 278 171 248 45 240 483 해당 예시는 cell 구축에 있어 protein이 역할하는 바로 분절했음. 단백질들이 서로 다른 단백질들과 얼마나 유사한지는 특정 세포 역할에 해당 단백질이 무슨 일을 하는지와 연관되어 있다고 알려져 있음. 그래프 외부에서 이러한 단백질들을 분류하려는 시도는 분류된 결과가 그래프 내부에서 합리적은 partitioning 과정을 걸쳐 나온 결과물과 어느정도는 연관이 있는게 맞다. 아니면 partitioning이 잘못됐던가 그래프 외부 분절이 잘못됐던가. yc = fastgreedy.community(yeast.gc) c.m = membership(yc) head(table(c.m, func.class, useNA = c(&quot;no&quot;))) ## func.class ## c.m A B C D E F G M O P R T U ## 1 0 0 0 1 3 7 0 6 3 110 2 35 14 ## 2 0 2 2 7 1 1 1 4 39 5 0 4 27 ## 3 1 9 7 18 4 8 4 20 10 23 8 74 64 ## 4 25 11 10 22 72 84 81 168 14 75 16 27 121 ## 5 1 7 5 14 0 4 0 2 3 6 1 34 68 ## 6 1 24 1 4 1 4 0 7 0 1 0 19 16 7.2.4 Assortativity and Mixing Assortative mixing 특정 성질에 따라서 vertex 중에 선별적으로 연결. Assortativity coefficients assortative mixing의 정도를 량화하는 측도. 이는 correlation coefficients의 변용. vertex 특성은 categorical, ordinal, or continuous 다 가능. categorical 케이스를 가정하고, 그래프 \\(G\\)의 각 vertex가 \\(M\\)개의 카테고리 중에 label 될 수 있다고 생각하자. 이 세팅에서의 Assortativity coefficients \\(r_a\\)는 아래와 같다. \\[ r_a = \\frac{\\sum_{i}f_{ii} - \\sum_i f_{x+}f_{+y}}{1 - \\sum_if_{x+}f_{+y}} \\] where \\(f_{ij}\\) is the fraction of edges in \\(G\\) that join a vertex in the \\(i\\)-th category with a vertex in the jth category, and \\(f_{i+}\\) and \\(f_{+i}\\) denote the ith marginal row and column sums, respectively, of the resulting matrix \\(f\\). 이때 \\(-1 \\le r_a \\le 1\\) – It is equal to zero when the mixing in the graph is no different from that obtained through a random assignment of edges that preserves the marginal degree distribution. – It is equal to one when there is perfect assortative mixing (i.e., when edges only connect vertices of the same category). – Howeigenvalueer, in the eigenvalueent that the mixing is perfectly disassortative, in the sense that eigenvalueery edge in the graph connects vertices of two different categories, the coefficient need not take the value −1. • The fact that physical binding of proteins is known to be directly releigenvalueant to functional classes suggests that there will frequently be strong assortative mixing in protein-protein interaction networks with respective to these classes as attributes. assortativity.nominal(yeast, (replace(V(yeast)$Class, is.na(V(yeast)$Class), 0) == &quot;P&quot;) + 1, directed = FALSE) ## [1] 0.5232879 assortativity.degree(yeast) ## [1] 0.4610798 • When the vertex characteristic of interest is continuous, rather than discrete, denote by (xe, ye) the values of that characteristic for the vertices joined by an edge e ∈ E. • A natural candidate for quantifying the assortativity in this characteristic is just th e Pearson correlation coefficient of the pairs (xe, ye), \\[ r = \\frac{\\sum_{x,y}xy(f_{xy} - f_{x+}f_{+y})}{\\sigma_x \\sigma_y} \\] 1st class↩︎ "],["data-collection-and-sampling.html", "7.3 Data Collection and Sampling", " 7.3 Data Collection and Sampling Difficulties in Network Data Collection. 뭔 분야든 통계의 근간은 데이터 수집. 데이터가 IID라면 이 데이터는 sample이나 실험에서 확보한 데이터. 하지만 이는 네트워크 실험에서는 사실상 불가능. 따라서 우리는 샘플을 deal with 하기가 어려우며, 이전에 해왔던 것 대비 일이 무척 어려워짐. 이러한 복잡성은 empirical networks를 다룰 때는 너무나도 자주 무시되고 있어서 안타까운 실정임. Sampling Procedures 이상적인 데이터에 해당하는 네트워크 census 를 생각해보자. 이는 모든 node 와 edge 를 기록하고 거기에 오류가 없음. 만약 완벽한 네트워크 census 데이터를 가지고 있는 케이스라면 샘플링 과정 스킵하고 바로 네트워크 formation 모델하는 단계로 넘어갈 수 있음. 하지만 그렇게 운좋을리가. 대다수의 경우에 보유한 네트워크 census 데이터는 불완전함. 보통 이런 실패는 네트워크의 성질과 mesurement process의 디테일 부족에서 옴. Survey 케이스를 생각해보자. survey 질문자, survey 답변자의 성격, survey 질문 구성 등으로 이런건 널뛰기함. 아니면 일부 질문 같은 경우에는 “가장 좋아하는 연예인 3명” 이런 식이라고 치자고. 이러면 4명 이하부터는 censoring 발생해서 이것도 완벽 데이터에서 왜곡됨. 7.3.1 Sampling Designs 우리가 true(참정보, 참값)를 확보하는 것이 불가능하다면, 우리는 IID 통계량에 의해 예시되었던 “population” graph \\(G = (V, E)\\) 확보를 포기하고 “sample” graph \\(G^\\ast = (V^\\ast, E^\\ast)\\)를 얻는 쪽으로 선회한다. 이때 \\(V^\\ast \\subset V\\), \\(E^\\ast \\subset E\\). 이러한 sampled subgraphs를 얻기 위한 다양한 방법들에 대응되는 서로 다른 sampling designs들이 존재한다. 우선 population으로부터의 units들에 대한 simple random sample (SRS)를 이해하는 것이 샘플링을 이해하기 위한 1단계가 된다. 네트워크에서는 단순 랜덤 샘플마저도 복잡한 이해를 거쳐야 한다. 7.3.1.1 Induced and Incident Subgraph node \\(V\\)의 Simpl Random Sample (SRS)인 \\(V^\\ast\\)로부터 시작하자. 이로부터 발생시킨 (induced) subgraph \\((i, j) \\in E^\\ast\\). 이때 \\((i, j) \\in E^\\ast \\Leftrightarrow (i,j) \\in E\\), \\(i \\in V^\\ast\\) and \\(j \\in V^\\ast\\) 여야만 함. 이 정제되지 않은 natural 한 과정인 induced subgraph sampling 은 정말 간단한 네트워크 stats 에 대해서도 엄청 biased. bias를 계산해낸 후에 bias 를 보정할 수 있는 경우도 있지만 여하튼 bias 가 크다는게 장점은 아니지. 반면에 우리는 edge의 SRS에서 시작해볼 수도 있다. 이 경우 \\(E^\\ast\\)는 \\(E\\)의 SRS. 이후 이 edge 양끝에 해당하는 발생을 node로서 잡는다. 이인즉슨 \\(\\exists j \\in V:(i,j) \\in E^\\ast \\Rightarrow i \\in V^\\ast\\). 고전적 survey 에 대해 쌓인 경험에 비추어볼 때 incident-subgraph sampling 는 꽤 괴상해보이지만, 그럼에도 이쪽이 natural 한 경우가 꽤 있기는 함. 7.3.1.1.1 Example of a Bias 우리가 정말정말 간단하기 그지없는 작업인 node 의 랜덤 샘플링을 진행할 때조차도 샘플링이 왜 bias 를 유발해버리는 걸까? 이는 mean degree 를 생각해보면 쉽게 알 수 있다. 직관적으로 생각해보자. induced subgraph 를 하나 가지고 있다. 이때 우리는 induced subgraph 바깥인데 전체 그래프 안에 있는, 즉 induced subgraph 에 포함되지 못한 edge 는 관측할 수가 없다. 따라서 우리가 각 node 에 대해 기록할 수 있는 degree 는 많아봤자 그것들의 degree 참값에 불과할 것이다. 따라서 샘플된 그래프들의 mean degree 는 mean degree 의 참값보다 작아져버리겠지. bias 발생. let \\(k_i = \\sum_{j=1}^n A_{ij}\\), 즉 \\(k_i\\)는 node \\(i\\)의 degree. 이 경우 모든 네트워크에 걸친 mean degree는 $k = {i=1}^n k{i} $. 여기서 \\(m\\)개의 노드를 SRS 한다면, node \\(i\\)에게 부여된 확률은 모든 각각의 node에게 부여된 확률과 같으므로, 따라서 \\(\\pi = \\frac{m}{n}\\). 여기서 \\(Z_i\\)를 \\(i \\in V^\\ast\\) 여부에 대한 indicator로 정의하자. 그렇다면 node \\(i\\)가 샘플 안에 있을 경우 \\(Z_i = 1\\). 또한 관측된 graph \\(G^\\ast\\)는 관측된 adjacency matrix \\(A^\\ast\\)를 보유하며, \\(A_{ij}^\\ast =1\\) iff \\(A_{ij}=1\\)이며 \\(i,j\\) 양쪽 모두가 샘플에 있을 경우에만. 그렇다면 plug-in estimate \\(\\bar k\\) from \\(G^\\ast\\)의 기댓값 \\(\\bar k^\\ast\\)는 어떻게 되는가? $$ \\[\\begin{alignat}{2} E \\left( \\bar k^\\ast \\right) &amp;= E \\left( \\frac{1}{m} \\sum_{i \\in V^\\ast} k_i^\\ast \\right) &amp;&amp;= E \\left( \\frac{1}{m} \\sum_{i \\in V^\\ast} \\sum_{j \\in V^\\ast} A_{ij}^\\ast \\right) \\\\ &amp;= E \\left( \\frac{1}{m} \\sum_{i=1}^n \\sum_{j =1}^n A_{ij}Z_i Z_j \\right) &amp;&amp;= \\frac{1}{m} \\sum_{i=1}^n \\sum_{j =1}^n A_{ij} E \\left(Z_i Z_j \\right) \\\\ &amp;= \\frac{1}{m} \\sum_{i=1}^n \\sum_{j =1}^n A_{ij} \\pi^2 &amp;&amp;=\\frac{1}{n \\pi} \\pi^2 \\sum_{i=1}^n \\sum_{j =1}^n A_{ij} \\\\ &amp;= \\frac{\\pi}{n } \\sum_{i=1}^n \\sum_{j =1}^n A_{ij} &amp;&amp;= \\pi \\bar k \\end{alignat}\\] $$ 7.3.1.2 Exploratory Sampling Design induced 와 incident 이외의 방법론을 쓰고 싶은 경우도 있지 않을까? induced 와 incident 서브그래프 샘플링 양쪽 모두에서 sampling frame 은 실제로 발생하는 그래프에 비하면 약간 좀 거리가 있고 이질적이다. 우리가 SRS 를 사용하는 대상인 population 은 모든 node를 포함하거나, 모든 edge를 포함해야 하지만, but doesn’t use the graph beyond that. egocentric 디자인에서 우리는 nodes 들을 샘플링한 후에 이렇게 샘플링된 nodes 들의 local 이웃에 대해서만, 혹은 ego network 에 대해서만, 정보를 수집하고 기록한다. 혹은 “ego” 케이스에서 우리는 edge 들이나 initial node 의 이웃들의 edges 들이나 non-edges 들만 기록함; 이는 때때로 star design 이라고 불림. star design 케이스에서 우리는 local 그래프 이웃에 대한 정보를 수집한 후 이들이 중복되는 지점이 있는지를 확인함. 기록 과정을 뭘 쓰느냐에 달려있긴 한데 이 정보는 보통 수집 가능함. 7.3.1.2.1 Snowball Sampling seed node 로 부터 시작. 이의 직접적인 이웃을 바로 기록. 이후 그 이웃들로 이동한 후 또 직접적인 이웃을 기록. 이 작업을 새로운 node 가 더이상 발견되지 않거나, 정해진 size 에 도달할 때까지 함. 이때 seeds 는 여럿이 있을 수 있음. 이 여럿인 경우에, 가령 seeds 가 2개라면, 진행하다가 서로 다른 seed 로부터 촉발된 2개의 snowball 이 overlap 되는 상황에 마주쳤을 때 어느 snowball을 고를지 결정하는 문제가 생김. snowball 샘플링은 그래프에 대해, incuded 나 incident 에 의해 얻어지는 것 그 어느것과도 다른 분포를 얻게 되는 결과를 초래함. seed 가 SRS에 의해 정해졌더라도 snowball 에 의해 골라지는 다른 node 들은 랜덤샘플이 아님. initial node 이외의 node 들은 seed 로부터 길을 따라서 도착할 수 있는 node 다 보니, 그들은 적어도 degree 가 1은 보장되어야 하고, 약하게나마 seed 에 연결은 되어 있어야 하마, 일반적으로 평균보다는 높은 degree 를 갖는 경향성을 보임. 7.3.1.2.2 Respondent-driven Sampling Respondent-driven 샘플링은 소셜네트워크 상황에서 snowball 샘플링의 유의한 변주. 이는 낙인되었거나 혹은 불법적이라 그들의 존재를 관계적으로 잘 발견해내기 어려운 sub-populations 을 찾아내기 위한 방법으로서 태초의 목적은 이것이었음. 이건 연구중인 문제에 해당하는 그룹 안의 멤버를 한둘 골라내서 이들을 이니셜 멤버로 한 뒤 걔들한테 주위 사람들 좀 여기 참가시켜보라고 설득하는 거. 때때로는 이 이니셜 멤버들한테 물리적 토큰(표식)을 준 뒤 이 물리적 표식을 여기 참가하라고 꼬실 대상들한테 뿌리라고 하는 식으로 link 를 트랙하기도 함. 이 물리적 토큰 자체가 인센티브일 수도 있고. 이때 응답자 별로 줄 수 있는 (허락되는) 토큰의 총량이 정해져 있다면 이건 곧 degree 의 censoring 으로 기능함. 7.3.1.2.3 Trace-route Sampling Trace-route 샘플링은 네트워크를 통과하는 각 route 들을 추적하여 네트워크를 검색함. 절차는 아래와 같다: source node의 set 을 지정 target node의 set 을 지정 각각의 source-target의 조합에 대해서, source 에서 target으로 도착하는 path 하나를 찾고, 그후 이 path에서 거친 모든 edge와 node를 기록함. 물론 이 프로세스는 어떤 path 가 탐색되었는가에 크게 의존하긴 하는데 이건 적용 층위의 문제지 메커니즘 자체가 문제가 있다고 할 건 아님. route 추적이 어떻게 이루어졌는지에 따라 연구자는 “실패” (source 에서 target 으로 도착 못했음) 한 route 에 대한 정보를 얻을수도 있고 못얻을수도 있음. Trace-route 샘플링은 체계적으로 degree 분포를 왜곡하며 모든 종류의 그래프들로 하여금 그들이 heavy-tail 인 것처럼 보이게 할 수 있다. 그들이 실제로 heavy-tail 이었든 아니든. 7.3.2 Coping Strategies 7.3.2.1 Head in Sand 이인즉 샘플링으로 인한 왜곡이나 bias 를 싹 무시하고 우리가 현재 보고 있는 그래프가 그래프의 참값이라고 가정하는 것. 당연히 좋은 생각은 아님. incuded 서브그래프 샘플링의 경우에 mean degree는 real degree 에서 bias 되어 있는데, 이 bias 는 계산 가능함. 실제로 모든 motif에 대해 motif count 의 샘플값도 또한 (얘도 계산 가능한 방법으로) 편향되어 있다. 얘들을 사후적으로 보정하는 건 꽤 쉬운 편. 하지만 다른 놈들은 복잡하게 꼬여있는데, 꼬여있는 놈들 중 일례로 degree 분포의 경우에는 매우 복잡하게 왜곡되어 있어서 사후적으로 보정하기가 드럽게 어렵다. 이건 induced 상황에서도 마찬가지로 복잡해서 사후적 보정이 난해함. 7.3.2.2 Learn Sampling Theory Classical sampling theory은 통계적 추론에 대한 이론으로, probability assumption은 오직 샘플링 프로세스에 대해서만 (성립)만들어진다는 것을 그 골자로 한다. population의 참값은 unknown 하나 fixed 되어 이 참값이 어떻게 생산되었는지에 대해서는 어떤 stochastic 가정도 만들어지지 않는다. (이를 unknown population 에 대해 조건부를 건다고 생각해도 틀리지 않다.) 모든 probability assumption 들이 샘플링 디자인에 대해서만 논하며, 추론의 타당성은 오직 디자인이 정확히 모델링되었는지 여부에만 의존하므로, 이는 때때로 design-based 추론이라고도 불린다. 크기가 n 인 어떤 finite population 에 대한 어떤 quantity \\(X_i\\) 의 평균을 a sample of units \\(S\\) 를 사용해구하고자 하는 상황이라고 해보자. 간단하고 고전적인 해는 Horvitz-Thompson estimator: \\[ \\hat \\mu_{HT} \\equiv \\frac{1}{n} \\sum_{i \\in S}\\frac{X_i}{\\pi_i} \\] \\(\\pi_i\\)는 unit \\(i\\)의 (assumed-known) 포함확률, 즉 unit \\(i\\)가 샘플에 포함될 확률. 포함 확률은 \\(\\pi = \\frac{|S|}{n}\\)로 모두 동일하다는 것을 notice. 즉 우리는 다시 sample mean \\(X\\)로 되돌아감. 이에 대한 직관은 곧 우리가 1개의 unit을 보았고 그 unit의 포함확률이 \\(\\pi_i\\)라면, 우리가 보지 못한 \\(\\frac{1}{\\pi_i}\\)개의 다른 것들이 있다는 것이 골자이다. 더 이론적으로 들어가자면 우리는 이것이 UE임을 보일 수 있다. indicator 변수 \\(Z_i = I(i \\in S), i \\in 1:n\\)을 도입하자. 이를 사용하여 \\(\\hat \\mu_{HT}\\)의 기댓값을 구하면 $$ \\[\\begin{alignat}{2} E \\left( \\hat \\mu_{HT} \\right) &amp;= E \\left(\\frac{1}{n} \\sum_{i \\in S} \\frac{X_i}{\\pi_i} \\right) &amp;&amp;= E \\left(\\frac{1}{n} \\sum_{i \\in 1:n} \\frac{X_i}{\\pi_i} Z_i \\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i \\in 1:n} \\frac{X_i}{\\pi_i} E \\left( Z_i \\right) &amp;&amp;= \\frac{1}{n} \\sum_{i \\in 1:n} \\frac{X_i}{\\pi_i} P \\left( Z_i =1 \\right) \\\\ &amp;= \\frac{1}{n} \\sum_{i \\in 1:n} \\frac{X_i}{\\pi_i} \\pi_i &amp;&amp;= \\frac{1}{n} \\sum_{i \\in 1:n} {X_i} \\\\ &amp;= \\mu \\end{alignat}\\] $$ 또한 \\[ Var \\left ( \\hat \\mu_{HT} \\right )= \\frac{1}{n^2} \\sum_{i \\in 1:n} \\sum_{j \\in 1:n} X_i X_j \\left( \\frac{\\pi_{ij}}{\\pi_i \\pi_j} -1 \\right) \\] 이때 \\(\\pi_{ij}\\)는 joint 포함확률. 즉슨 \\(i,j\\)가 한번에 샘플에 들어있을 확률. (\\(\\pi_{ii} = \\pi_i\\)로 취급) 모든 \\(\\pi_i \\rightarrow 1\\)로 가게 된다면, \\(Var \\rightarrow 0\\). 이 Var 참값을 정확히 계산하는 건 불가능. 우리는 population 안의 모든 unknown units의 합을 구하는건 불가능하기 때문. 그러가 empirical 대체값은 주어져 있다. 이는 \\[ \\hat {Var} \\left ( \\hat \\mu_{HT} \\right) = \\frac{1}{n^2} \\sum_{i \\in 1:n} \\sum_{j \\in 1:n} X_i X_j \\left( \\frac{\\pi_{ij}}{\\pi_i \\pi_j} -1 \\right) \\] sampling-theory approach는 population quantity의 평균 (혹은 총량) 으로 나타낼 수 있는 대상에게 적합. 혹은 샘플링 디자인에 대한 지식으로부터 포함 (inclusion) 확률을 파악할 수 있는 상황에 대해서도 쓸만하다. 많은 네트워크 stats는 평균으로 표현될 수 있지만 (때때로 “unit”을 정의하여 해결하기도 함, node 의 dyad 같은 거), inclusion 확률을 정확히 계산해내는 건 평균 구하는 것보다는 더 빡셈. 7.3.2.3 Missing Data Tools 다른 방법은 네트워크에서 unobserved 된 부분을 missing data로 처리하고 이를 추론해버리는 것. 이건 simple imputation 전략부터 시작해서, EM 알고리즘과 같이 추론에 대한 복잡한 모델-based 전략에 이르기까지 다양한 것들이 속한다. EM 혹은 성공적인 imputation은 design-based 가 아니라 model-based 이며, 네트워크와 샘플링 프로세스 양쪽 모두에 대한 모델을 필요로 한다. 실전에서 “missing at random” 상황은 진짜 엄청나게 드물며, “missing completely at random” 상황조차도 흔하지 않다. let alone 7.3.2.4 Model the Effective Network 마지막 전략은 observed 네트워크를 모델링하는 것. 즉 observation / 샘플링 프로세스와 실제 네트워크 양쪽 모두를 모델링하지만, 그 후 이 둘을 합치는 것으로 observed 그래프에 대한 확률 분포의 family 를 얻는 것이 가능함. 그 observed 네트워크는 underlying generative 모델의 패러미터에 대해 여전히 informative. 이게 알고 싶은 전부라면, 여기까지만 진행한 후에 종료해버릴 수 있음. 전부가 아니라면 이것 이후에 EM이나 imputation 써서 full 그래프를 복원하고자 시도하게 될 것이고. 7.3.3 Big Data Solves Nothing “\\(n =\\) all” 로 설정되고 모든 데이터가 자동적으로 기록된 경우에도 우리가 겪어온 모든 네트워크 샘플링 문제는 여전히 남아있음. 이런 상황에서도 우리가 얻은 데이터는 결국 biased convenience 샘플을 갖고 있는 것이지 가지고 있는 모든 자료가 참값이라고 말할 수가 없기 때문임. 네트워크에서는 특히 아래와 같은 3가지 문제가 두드러짐. Even when, as the promoters say, “n = all,” and the data are automatically recorded (voluntarily or involuntarily), almost all the network sampling issues we’ve gone over remain. After all, as the promoters do not say, you’re getting all of a biased convenience sample, not all of the truth. Three issues are particularly prominent for network. Entity Resolution Diffusion Performativity 7.3.3.1 Entity resolution Entity resolution, 혹은 record linkage 라 불리는 것은 데이터 분석에서 메이저한 문제 중 하나. 이는 간단하게 말하면 동일 대상에 대해 서로 다른 시간대에 기록된 자료가 있다면 이 중 무엇을 쓸 것인가 하는 문제. 혹은 겉보기에 같은 대상에 대해 서술하는 것 같은 (co-referent) 기록들이 실제로는 다른 것들에 대해 이야기하고 있는 상황 . 네트워크에서 이는 보통 같은 underlying entity 로 직결되는 2개의 다른 node 들 중에 뭘 고를까 하는 문제가 됨. 7.3.3.2 Diffusion diffusion은 우리에게 빅데이터를 제공하는 많은 자동적으로 기록된 네트워크들이 다른 오래된 소셜 네트워크로 퍼져나가는 것. provide A with B 예를 들어 페북의 tie는 pre-페북의 소셜 네트워크과 diffusion 프로세스의 결과물이다. 이 결과물을 이해하는데에는 비교적 약간의 노력만이 이루어졌다. diffusion 프로세스가 모든 node 를 균질하게 취겁하더라도, diffusion을 당한 네트워크는 기반 네트워크와 그 특성이 근본적으로 다를 수 있다. 7.3.3.3 Performativity 이론이 자기실현적 예전이 되어버리는 상황을 Performativity라고 함. 온라인 소셜 네트워크를 운영하는 회사들은 사용자들의 크고 조밀한 네트워크를 만들어낼 수 있도록 엄청나게 투자중. 이것이 그들이 link 제안 혹은 link 추천 서비스를 제공하는 이유임. 왜 당신이 아실지도 모르는 친구 이런거. 이러한 추천의 이면에 있는 알고리즘들은 소셜 네트워크가 어떻게 만들어지는지에 대한 이론과 그들이 어떤 link 패턴을 가져야하는지에 대한 이론 등이 반영되어 있다. 유저들이 이러한 추천 친구와 link 를 수락하는 순간 이 알고리즘 이면에 반영된 이론의 입맛에 맞는 케이스가 강화되는 거. "],["mathematical-models-for-network-graphs.html", "7.4 Mathematical Models for Network Graphs", " 7.4 Mathematical Models for Network Graphs By a model for a network graph we mean effectively a collection \\(G\\) 는 가능한 그래프들의 collection (혹은 ‘ensemble’) \\(P_\\theta\\)는 \\(G\\)의 확률분포 (간단하게 쓰면 \\(\\cdot_\\theta\\) 생략하고 \\(P\\)만 씀) \\(\\theta\\)는 \\(\\Theta\\) 내부에서 가능한 값들 안에서 펼쳐져있는 (ranging over) 패러미터들(패러미터값들)의 벡터 \\[ \\Big \\{ P_\\theta (G), \\; G \\in \\mathcal G \\; \\; : \\; \\; \\theta \\in \\Theta \\Big \\} \\] Variety of Purposes The testing for ‘significance’ of a pre-defined characteristic(s) in a given network graph The study of proposed mechanisms for generating certain commonly observed properties in real-world networks (such as broad degree distributions or small-world effects), The assessment of potential predictive factors of relational ties. The richness of network graph modeling derives largely from how we choose to specify P(·), with methods in the literature ranging from the simple to the complex. It is useful for our purposes to distinguish, broadly speaking, between models defined more from (i) a mathematical perspective, versus (ii) a statistical perspective. • Those of the former class tend to be simpler in nature and more amendable to mathematical analysis yet, at the same time, do not always necessarily lend themselves well to formal statistical techniques of model fitting and assessment. • On the other hand, those of the latter class typically are designed to be fit to data, but their mathematical analysis can be challenging in some cases. • Nonetheless, both classes of network graph models have their uses for analyzing network graph data. 7.4.1 Classical Random Graph Models random graph model이라는 용어는 collection \\(\\mathcal G\\)과 \\(\\mathcal G\\)에 대한 uniform probability \\(P(\\cdot)\\)을 묶어 일컬음. 수학적 관점에서 가장 잘 정의된 네트워크 그래프 모델. 주어진 order와 size를 따르는 그래프에 대한 모든 후보군에 동일 확률 부여. \\(|V|=N_v\\), \\(|E| = N_e\\)를 만족하는 모든 그래프 \\(G=(V,E)\\)의 collection \\(\\mathcal G_{N_v, N_e}\\)을 정의하고, 각각의 \\(G \\in \\mathcal G_{N_v, N_e}\\)에 확률 \\(P(G) = {N \\choose N_e}^{-1}\\)을 부여함. 이때 \\(N= {N_v \\choose 2}\\)는 서로 다른 vertex 2개를 묶은 쌍의 총 숫자. \\(\\mathcal G_{N_v, N_e}\\)의 변용이 실전에서는 더 자주 보임. 이 공식에서, \\(\\mathcal G_{N_v, p}\\)는 order \\(N_v\\)의 모든 그래프 \\(G\\)로 구성되어 있다. 이는 서로 다른 vertex의 쌍에 \\(p \\in (0,1)\\)의 확률로 edge 1개를 독립적으로 부여하는 것으로 얻어질 수 있다. 이러한 종류의 모델은 Bernoulli random graph model라고 불림. \\(p\\)가 \\(N_v\\)의 적절하게 정의된 함수이며, \\(N_e \\sim p N_v^2\\)하면, 이 모델들의 두 클래스는 large \\(N_v\\)와 거의 동치된다. The function erdos.renyi.game in igraph can be used to simulate classical random graphs of either type. The choice of Nv = 100 vertices and a probability of p = 0.02 of an edge between any pair of vertices. library(sand) g.er = erdos.renyi.game(100, 0.02) par(mfrow = c(1, 2)) plot(g.er, layout = layout.circle, vertex.label = NA) hist(igraph::degree(g.er), col = &quot;lightblue&quot;, xlab = &quot;Degree&quot;, ylab = &quot;Frequency&quot;, main = &quot;&quot;) is.connected(g.er) ## [1] FALSE table(sapply(decompose.graph(g.er), vcount)) ## ## 1 82 ## 18 1 \\(\\exists c&gt;1:p = \\frac{c}{N_v}\\)가 성립한다면, classical random graph \\(G\\)는 giant component를 보유할 확률이 높다. 위와 \\(p\\)를 동일하게 정의한다면, \\(c&gt;0\\)에 대해, degree distribution은 large \\(N_v\\)에 대해 \\(POI(c)\\)로 잘 모사된다. 이게 사실이라는 건 직관적으로 보이기도 쉽다. 아무 vertex나 하나 뽑았을 때 이의 degree가 \\(B(N_v-1, p)\\)를 따르기 때문이다. 이는 곧 mean degree는 \\(p(N_v-1)\\)에 근접한다는 소리. classical random graph의 다른 성질은 vertex 쌍 사이에서 shortest path 위에는 상대적으로 적은 숫자의 vertex가 존재한다는 것이며 이로 인해 clustering도 low하다는 것이다. path가 길어야 상대적으로 공간이 넉넉해서 여러개의 vertex가 그 위에 안착할 가능성이 높은데 짧으면 그만큼 공간 좁아서 없는게 정상일테니까. average.path.length(g.er) ## [1] 5.777477 diameter(g.er) ## [1] 13 transitivity(g.er) ## [1] 0 7.4.2 Generalized Random Graph Models 이하의 성질을 갖고, fixed order \\(N_v\\)를 따르는 모든 그래프의 collection \\(\\mathcal G\\)를 정의한다. 각 그래프 \\(G \\in \\mathcal G\\)에 동일 확률 부여 가장 자주 부여되는 성질은 fixed degree sequence의 그것. \\(\\mathcal G\\)를 모든 그래프 \\(G\\)의 collection으로 정의, 이때 이는 미리 정해진 degree sequence를 따름. 이를 ordered form으로 적으면 \\(\\{d_{(1)}, \\cdots, d_{(N_v)} \\}\\). 이 조건을 따르면서도 다른 모양의 그래프는 얼마든지 그려지며 따라서 isomorphic이 아님. degs = c(2, 2, 2, 2, 3, 3, 3, 3) g1 = degree.sequence.game(degs, method = &quot;vl&quot;) g2 = degree.sequence.game(degs, method = &quot;vl&quot;) par(mfrow = c(1, 2)) plot(g1, vertex.label = NA) plot(g2, vertex.label = NA) # 해당 케이스에선 N_v=8 vertex, 이중 절반은 degree=2, # 나머지 절반은 3. graph.isomorphic(g1, g2) ## [1] FALSE c(ecount(g1), ecount(g2)) ## [1] 10 10 고정된 숫자의 vertex \\(N_v\\)에서 fixed degree sequence를 따르는 랜덤 그래프들의 collection들은 모두 egde 숫자 \\(N_e\\)로 동일하다. 이는 mean degree of sqeuence \\(\\{d_{(1)}, \\cdots, d_{(N_v)}\\}\\)는 \\(\\tilde d = \\frac{2N_e}{N_v}\\)이니까. 따라서 이 collection은 랜덤 그래프의 collection \\(\\mathcal G_{N_v , N_e}\\) 안에 strictly 들어있음. 따라서 degree sequence의 가정된 형태의 추가는 원본 collection \\(\\mathcal G_{N_v , N_e}\\)에 조건부 분포를 걸어 우리의 모델을 특정짓은 것과 동일함. 다른말로 이는 degree sequence에 의해 제약되지 않은 부분은 얼마든지 vary 가능하다는 것을 의미. data(yeast) degs = igraph::degree(yeast) fake.yeast = degree.sequence.game(degs, method = c(&quot;vl&quot;)) all(igraph::degree(yeast) == igraph::degree(fake.yeast)) ## [1] TRUE diameter(yeast) ## [1] 15 diameter(fake.yeast) ## [1] 8 transitivity(yeast) ## [1] 0.4686178 transitivity(fake.yeast) ## [1] 0.04015224 하지만 이렇게 고삐를 풀어버리면 원본 네트워크의 직경은 시뮬레이션된 물건의 2배에 달하며 사실상 만들어두었던 clustering들도 다 날아가버렸음. 원칙적으로 class \\(\\mathcal G\\)의 정의를 제한하는 편이 훨씬 쉬우며 그렇기에 degree sequence 이외의 다른 추가적인 특성들은 그냥 고정해버림. 이러한 collection으로부터 랜덤 그래프들 \\(\\mathcal G\\)을 생산해내는데에는 MCMC 방법론이 유명하다. 이때 MC에 의해 액세스되는 상태들 그 자체 각각들이 graph \\(\\mathcal G\\)에 해당함. 7.4.3 Network Graph Models Based on Mechanisms 모던 네트워크 그래프 모델링에서 가장 중요한 혁명 중 하나가 전통적인 랜덤 그래프 모델에서 실제 세계의 성질을 모사하는 쪽으로 옮겨갔다는 거임. 이건 그냥 간단한 몇몇 메커니즘 도입하는 것으로 성공되었음. 7.4.3.1 Small-World Models small-world network 대부분의 node가 다른 node들과 이웃이 아닌 케이스. 그러나 다른 node를 작은 횟수 거치면 모든 노드에 액세스 가능한 케이스. 해당하는 케이스 생산하는 방법으로 lattice 구조로 우선 잔 후에 적은 확률 부여해서 무작위로 각 node로 rewiring. We begin with a set of Nv vertices, arranged in a periodic faction, and join each vertex to r of its neighbors to each side. For each edge, independently and with probability p, one end of that edge will be moved to be incident to another vertex, where that new vertex is chosen uniformly, but with attention to avoid the construction of loops and multi-edges. 추가적인 조치 없이 lattice만 단독으로 있으면 (\\(p=0\\) 상황에서 생산됨) 이 경우에는 clustering의 양이 상당하게 나오지만, vertex간의 거리는 non-trivial해짐. 이런 lattice에 상대적으로 적은 숫장 edge 대상으로 rewiring을 거치는 것만으로 vertex 사이의 거리를 극적으로 줄일 수 있다. 이때 clustering level은 높게 유지된다는 것이 포인트. 이 효과는 \\(p\\)가 극적으로 작더라도 여전히 얻어질 수 있다. g.ws = watts.strogatz.game(1, 25, 5, 0.05) plot(g.ws, layout = layout.circle, vertex.label = NA) g.lat100 = watts.strogatz.game(1, 100, 5, 0) transitivity(g.lat100) ## [1] 0.6666667 plot(g.lat100, layout = layout.reingold.tilford, vertex.label = NA) diameter(g.lat100) ## [1] 10 average.path.length(g.lat100) ## [1] 5.454545 g.ws100 = watts.strogatz.game(1, 100, 5, 0.05) diameter(g.ws100) ## [1] 5 average.path.length(g.ws100) ## [1] 2.678182 transitivity(g.ws100) ## [1] 0.4914586 steps = seq(-4, -0.5, 0.1) len = length(steps) cl = numeric(len) apl = numeric(len) ntrials = 100 for (i in 1:len) { cltemp = numeric(ntrials) apltemp = numeric(ntrials) for (j in 1:ntrials) { g = watts.strogatz.game(1, 1000, 10, 10^steps[i]) cltemp[j] = transitivity(g) apltemp[j] = average.path.length(g) } } cl[i] = mean(cltemp) apl[i] = mean(apltemp) plot(steps, cl/max(cl), ylim = c(0, 1), lwd = 3, type = &quot;l&quot;, col = &quot;blue&quot;, xlab = expression(log[10](p)), ylab = &quot;Clustering and Average Path Length&quot;) lines(steps, apl/max(apl), lwd = 3, col = &quot;red&quot;) 이 결과는 (normalized 평균적인 path의 길이들과 clustering coefficient들의) 개략적인 기댓값을 p 대비로 plot한 것으로, p가 엄청나게 극단적인 값으로 가지 않은 이상 네트워크가 계속 높은 수준의 clustering을 유지하면서 작은 평균 거리를 보이는 것이 확인된다. 7.4.3.2 Preferential Attachment Models 대부분의 네트워크는 시간에 따라 변화함. 주어진 시간 \\(t\\)에 네트워크가 어떻게 변화하는지는 vertex preferences, fitness, age 등에 따라서든 다양. Preferential attachment는 많은 노드와 연결된 대형 노드가 추가적인 link를 확보할 가능성이 높다는 것. SNS에서의 셀럽 생각하면 됨. undirected 네트워크의 Barabasi-Albert (BA) model 은 다음과 같음. 시작 vertex \\(N_v^{(0)}\\), 시작 edge \\(N_e^{(0)}\\) 가지는 시작 그래프 \\(G^{(0)}\\)로 시작. 각 시간 \\(t = 1, 2, \\cdots\\)에서 \\(G^{(t-1)}\\) 기반으로 \\(G^{(t)}\\) 생산. 추가적인 vertex 를 넣되 이 추가되는 vertex들의 degree는 \\(m \\ge 1\\)이고, \\(m\\)개의 새로운 edge들이 \\(m\\)개의 서로 다른 vertex에 들러붙음. 이때 새로운 vertex가 기존의 vertex에 들러붙을 확률은 \\(\\frac{d_v}{\\sum_{v&#39; \\in V}d_{v&#39;}}\\) 을 따름. 즉 기존 그래프에서 높은 degree를 가지고 있던 vertex가 새 vertex와도 들러붙을 확률 높음. t번의 이터레이션 후에 결과값 그래프 \\(G(t)\\)는 \\(N_v^{(t)} = N_v^{(0)} + t\\) 개의 vertex와 \\(N_e^{(t)} = N_e^{(0)} + tm\\) 개의 edge 를 가진다. 이런 preferential attachment 경향성 때문에 이터레이션이 쌓일수록 높은 degree를 보유하는 vertex가 많아질 것을 기대할 수 있다. g.ba = barabasi.game(100, directed = FALSE) par(mfrow = c(1, 2)) plot(g.ba, layout = layout.circle, vertex.label = NA) hist(igraph::degree(g.ba), col = &quot;lightblue&quot;, xlab = &quot;Degree&quot;, ylab = &quot;Frequency&quot;, main = &quot;&quot;) 고전적인 랜덤 그래프 (SRS) 대비 vertex 쌍 사이의 edge가 uniform 분포를 따르지 않음을 유의. 이런 경향에 의해 edge를 다수 끌어모으고 있는 “hub”라 불릴만한 vertex가 있는 것으로 사료됨. 여기서 전체 분포는 꽤 heterogeneous한 감이 있는데, 대부분의 vertex는 degree가 2를 넘지도 못함. summary(igraph::degree(g.ba)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.00 1.00 1.98 2.00 20.00 average.path.length(g.ba) ## [1] 5.268889 diameter(g.ba) ## [1] 13 transitivity(g.ba) ## [1] 0 이러한 preferential attachment 모델에서 특필될만한 특징은 \\(t \\rightarrow \\infty\\) 따라서 그래프 \\(G^{(t)}\\)의 degree distribution이 \\(d^{-\\alpha}\\), \\(\\alpha = 3\\)의 형을 갖는 경향을 보인다는 것이다. 이러한 경향은 고전적인 랜덤 그래프와 큰 차이점임. 반면에 BA 모델에 의해 생성된 네트워크 그래프는 vertex 쌍 사이의 shotest path 위에 위치하는 vertex 숫자가 점점 적어지며 또한 low clustering이라는 고전적인 랜덤 그래프의 특징을 어느정도 공유한다. 뭔소리야 BA가 preferential attachment 모델을 생산하려고 고안된게 아냐? BA가 preferential attachment 모사하려고 고안되긴 했는데 preferential attachment의 특성을 모두 복사하진 못했고 BA의 한계상 asymptotic 상황에서 고전적 랜덤 그래프 성질을 가져버렸다는건가? 7.4.4 Assessing Significance of Network Graph Characteristics 위에서 설명한 네트워크 모델들은 관측된 네트워크들로 통계적 모델링하긴 너무너무 간단함. 간단해서 현실모사를 제대로 못해서 쓸모가 없음. 그래도 중요하긴 함. 통계적 가설 검정 측면에서는 말이지. 특히 네트워크 그래프 성질의 significance를 측정(test)하는데 있어 자주 쓰임. 우리가 observations으로부터 얻은 그래프 \\(G^{obs}\\)를 가지고 있다고 가정하자. 우리는 이때 임의의 structural 특성인 \\(\\eta(\\cdot)\\)에 관심있음. 이 경우 \\(\\eta(G^{obs})\\)가 unusual 이든 unexpected 이든 significant 한지를 체크하는 것은 중요함. 지금까지 위에서 언급해온 네트워크 모델들에 대한 이론은 바로 이 경우에 기준으로서 동작한다는 점에서 중요. 이인즉 우리가 가지고 있는 그래프들의 collection \\(\\mathcal G\\) 에 대해 \\(\\eta(G^{obs})\\)를 각 그래프들을 넣어본 값들의 collection \\(\\{ \\eta(G): G \\in \\mathcal G \\}\\) 과 비교해본다는 것이다. 이때 해당 collection에 들어있는 값들 대비 \\(\\eta(G^{obs})\\)가 극단적이라고 판정되면 \\(\\eta(G^{obs})\\)가 보유한 값이 unusual하다는 것을 판정내릴 수 있는 재료가 됨. 랜덤 그래프 모델을 사용할 경우 척도가 될 수 있는 distribution을 개발하고자 하는 움직임은 지극히 정상적임. 이는 \\(\\mathcal G\\) 안에 있는 각 그래프들 \\(G\\) 각각에 uniform 확률을 부여해서 만들어보는 게 합리적. 이는 곧 \\[ P_{\\eta, \\mathcal G} (t) = \\frac{\\text{#}\\{ g \\in \\mathcal G \\; : \\; \\eta(G) \\le t \\}}{| \\mathcal G |} \\] 만약 \\(\\eta(G^{obs})\\)가 이 분포를 따르지 않을 것 같은 확률이 높다면 \\(G^{obs}\\)는 \\(\\mathcal G\\)로부터의 uniform draw가 아니라는 쪽에 힘이 실림. 이는 랜덤 그래프를 예쁘게 따르는 자주 발생하지 않는 상황 대비 실제 상황으로부터 서브그래프를 뽑아내게 되는 자주 마주치는 상황에 활용가능하다는 점에서 실용적. 7.4.4.1 Assessing the Number of Communities in a Network 위에서 karate 데이터에 hierarchical clustering 사용했더니 clustering 3개 발견했음. 이 karate 데이터에 비추어볼 그래프로 2개의 \\(G\\)를 생각하자. karate 네트워크와 동일하게 same order \\(N_v = 34\\), size \\(N_e = 78\\) 1번에 더해서 원본과 동일한 degree distribution 따름 MCMC 써서 생산. 뭐 deterministic 관점이었던 물건에 시뮬레이션을 통한 랜덤 관점으로 접근하는 거라는 이야기? 단점이란 소리인가? data(karate) nv = vcount(karate) ne = ecount(karate) degs = igraph::degree(karate) ntrials = 1000 num.comm.rg = numeric(ntrials) for (i in 1:ntrials) { g.rg = erdos.renyi.game(nv, ne, type = &quot;gnm&quot;) c.rg = fastgreedy.community(g.rg) num.comm.rg[i] = length(c.rg) } # We then generate classical random graphs of this same # order and size and, for each one, we use the same # community detection algorithm to determine the number of # communities. num.comm.grg = numeric(ntrials) for (i in 1:ntrials) { g.grg = degree.sequence.game(degs, method = &quot;vl&quot;) c.grg = fastgreedy.community(g.grg) num.comm.grg[i] = length(c.grg) } # Similarly, we do the same using generalized random graphs # constrained to have the required degree sequence. rslts = c(num.comm.rg, num.comm.grg) indx = c(rep(0, ntrials), rep(1, ntrials)) counts = table(indx, rslts)/ntrials barplot(counts, beside = TRUE, col = c(&quot;blue&quot;, &quot;red&quot;), xlab = &quot;Number of Communities&quot;, ylab = &quot;Relative Frequency&quot;, legend = c(&quot;Fixed Size&quot;, &quot;Fixed Degree Sequence&quot;)) # The results may be summarized and compared using side by # side bar plots. • Clearly the actual number of communities detected in the original karate network (i.e., three) would be considered unusual from the perspective of random graphs of both fixed size and fixed degree sequence. • Accordingly, we may conclude that there is likely an additional mechanism(s) at work in the actual karate club, one that goes beyond simply the density and the distribution of social interactions in this network. 7.4.4.2 Assessing Small World Properties small-world 여부를 확인하는 일반적인 방법론은 대상 네트워크의 clustering coefficient와 average (shortest) path 길이를 보정된 고전적 랜덤 그래프에서 확인할 수 있는 그것들과 비교하는 것이다. 고전적 랜덤 그래프의 그것과 비교했을 때, 만약 대상 네트워크가 small-world라면, clsutering coefficient는 고전적 랜덤 그래프보다 크되, average path 길이는 대충 비슷할 것으로 예상함. library(igraphdata) data(macaque) summary(macaque) ## IGRAPH f7130f3 DN-- 45 463 -- ## + attr: Citation (g/c), Author (g/c), shape (v/c), name (v/c) 해당 네트워크에서 clustering을 평가하기 위해 directed 네트워크에 해당하는 clustering coefficient 의 변형 사용. 이 변량은 모든 vertex \\(v\\)에 대해 vertex 각각의 clsutering coefficient를 평균낸 것이 된다. A는 adjacency Matrix \\(d_v^{tot}\\)는 vertex \\(v\\)의 총 degree (i.e., in-degree plus out-degree) \\[ cl(v) = \\frac{\\left( A+ A&#39; \\right)^3_{vv}}{ 2 \\left [ d_v^{tot}(d_v^{tot} - 1) - 2 (A^2)_{vv} \\right]} \\] clust.coef.dir &lt;- function(graph) { A = as.matrix(get.adjacency(graph)) S = A + t(A) deg = igraph::degree(graph, mode = c(&quot;total&quot;)) num = diag(S %*% S %*% S) denom = diag(A %*% A) denom = 2 * (deg * (deg - 1) - 2 * denom) cl = mean(num/denom) return(cl) } # 비교하려면 고전적 랜덤 그래프를 생산하고 이의 각각의 # clustering과 평균 path length를 계산하는 과정 필요. ntrials = 1000 nv = vcount(macaque) ne = ecount(macaque) cl.rg = numeric(ntrials) apl.rg = numeric(ntrials) for (i in 1:ntrials) { g.rg = erdos.renyi.game(nv, ne, type = &quot;gnm&quot;, directed = TRUE) cl.rg[i] = clust.coef.dir(g.rg) apl.rg[i] = average.path.length(g.rg) } summary(cl.rg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2150 0.2301 0.2340 0.2341 0.2377 0.2545 summary(apl.rg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.812 1.828 1.833 1.833 1.839 1.860 clust.coef.dir(macaque) ## [1] 0.5501073 average.path.length(macaque) ## [1] 2.148485 위 예시에서 랜덤 네트워크에서 계산된 것 대비 clustering 숫자가 엄청 크지만, 동시에 vertex 쌍 사이의 shortest path들의 평균적인 길이 또한 눈에 띄게 길다. 따라서 small-world 여부는 명확하지 않으며, 이 결과는 분석 대상 네트워크가 고전적 랜덤 그래프보다도 훨씬 lattice에 가깝다는 것을 보여준다. "],["introduction-to-ergm.html", "7.5 Introduction to ERGM", " 7.5 Introduction to ERGM 7.5.1 Exponential Random Graph Models 7.5.1.1 What Is a Network? := “relational data” 를 수학적 그래프로 나타낸 것. node의 set과 edge set의 복합이며, edge는 일부 node를 이음. 7.5.1.2 Exponential Random Graph Model (ERGMs) \\[ P_\\theta (X=x) = \\frac{1}{\\kappa(\\theta)} \\exp \\Big( \\theta&#39; g(x) \\Big) \\] \\(X\\): adjacency Matrix 로 서술된 랜덤 네트워크 \\(X_{ij}\\): node \\(i\\) 로부터 node \\(j\\) 로의 edge 여부 indicator \\(g(x)\\): 연구자의 관심대상인 네트워크 statistics 의 vector \\(\\theta\\): vector \\(g(x)\\) 에서의 상응하는 entry 들의 strengths of the effects 를 측정 (measure) 하는 패러미터 들의 vector. ERGM 을 해석하는 방법은 이하와 같다. \\(\\theta &gt;0\\): \\(X_{ij}\\) 값이 0에서 1로 변할 때 \\(g(x)\\) 를 형성하는 경향이 있음을 의미 \\(\\theta &gt;0\\): \\(X_{ij}\\) 값이 0에서 1로 변할 때 \\(g(x)\\) 를 형성하지 않는 경향이 있음을 의미. \\(g(x)\\) 형성의 역방향. \\(\\kappa (θ)\\): A normalizing constant ERGM 은 네트워크의 전체 구조를 형성하는 로컬적인 selection force를 간략하게 설명함. 네트워크 데이터셋은 리그레션에서의 response 같은 것으로 간주될 수 있으며, 이때 predictor들은 “파트너십에서 개인들이 삼각형을 형성하는 성향” 과 같은 것임. 즉, ERGM은 local transtivity의 정도, 위력을 량화하는데 도움을 줌. EGRM을 사용해 획득하는 정보는 특정 현상을 이해하거나 특정 네트워크로부터의 랜덤한 실현값을 시뮬레이션하는데에 쓰일 수 있음. 이때 랜덤한 실현값은 당연히 원본의 성질을 유지해야 하고. 7.5.1.3 Network Statistics knitr::include_graphics(&quot;images/knit-logo.png&quot;) FIGURE 7.10: Basic Markov Network Statistics 7.5.1.3.1 Degree and Shared Partnership Distribution Degree: 특정 node 가 다른 node 와 연결되어 있는, 즉 보유하고 있는 edge 의 수 \\(D_k (x)\\): degree 가 \\(k\\) 인 node 들의 갯수. 이때 \\(\\sum D_k(x) = n\\). unordered pair Shared Partnership Distribution: \\(i\\)와 \\(j\\) 가 정확히 \\(k\\) 개의 공통된 neighbor 를 가지고, 이와 동시에 이하의 각각의 조건을 만족한다면, unordered pair \\((i,j)\\) 의 갯수는 이하로 notation. \\(EP_k (x)\\): \\(X_{ij} = 1\\). 즉 픽된 서로가 connected. \\(NP_k (x)\\): \\(X_{ij} = 0\\). 즉 픽된 서로가 unconnected. \\(DP_k (x)\\): regardless of value \\(X_{ij}\\). 픽된 서로의 connect 여부 무관. \\(\\sum EP_k(x) = S_1(x)\\) (edge counts) 이며, \\(\\sum DP_k (x) = {n \\choose x}\\) (dyad counts). knitr::include_graphics(&quot;images/knit-logo.png&quot;) FIGURE 7.11: Degree and Shared Partnership Distribution Geometrically Weighted Statistics (GW statistics) for degree and shared partnership distribution 는 이하와 같이 정의된다. 여기에 추가된 패러미터 \\(\\tau\\)는 higher order terms 때 부과되는 weight의 decreasing rate를 나타냄. 위에서 언급한 statistics 들 중 \\(NP\\) 만 안쓰였음. $$ \\[\\begin{alignat}{2} u(x | \\tau) &amp;= e^\\tau \\sum_{i=1}^{n-2} \\left \\{ 1- \\left ( 1-\\frac{1}{e^\\tau} \\right)^i \\right \\} &amp;&amp;\\cdot D_i(x) \\tag{GWD} \\\\ v(x | \\tau) &amp;= \\ditto &amp;&amp;\\cdot EP_i(x) \\tag{GWESP} \\\\ w(x | \\tau) &amp;= \\ditto &amp;&amp;\\cdot DP_i(x) \\tag{GWDSP} \\end{alignat}\\] $$ 이들을 통해 우리는 can capture high-order interaction. \\(\\tau\\) can be either pre-specified (general exponential families) estimated (curved exponential families) 7.5.2 Difficulty in Parameter Estimation 7.5.2.1 Intractable Normalizing Constants ERGMs의 normalizing constant 는 \\(\\kappa (\\theta) =\\sum_{\\text{all possible }x} \\exp \\Big \\{ \\theta&#39; g(\\mathbf x) \\Big \\}\\). undirected 인 경우에조차도 \\(2^{n \\choose x}\\) 개의 네트워크가 존재하므로, \\(\\kappa(\\theta)\\) 를 직접 계산하는건 불가능함. 이렇게 직접 계산하는게 불가능하기 때문에 MCMC 가 시뮬레이션과 통계적 추론 양쪽에 있어서 핵심이 된다. 하지만 일반적은 MH 알고리즘에 있어서는 acceptance probability에 알려지지 않은 constant ratio 인 \\(\\frac{\\kappa(\\theta)}{\\kappa(\\theta&#39;)}\\) 가 끼어있으므로 이를 직접적으로 계산하는 것 또한 실패하게 됨. 이때 \\(\\theta &#39;\\) denotes the proposed value. 7.5.2.2 Model Degeneracy \\(\\theta\\)를 어떻게 설정하느냐에 따라서 ERGM은 full (모든 연결이 존재하는, \\(J\\)) 혹은 empty (연결이 없는, \\(\\mathbf 0\\)) 네트워크를 거의 1에 가까운 확률로 생산하기도 한다. Example: Basic Markovian Statistics. 네트워크에서 하나의 edge가 추가되거나 제거될때, 다른 통계량들이 비교적 크게 변하지 않을 때 basic Markovian 통계량만 엄청나게 요동치는 상황 발생할 수 있음. 따라서 dyadic dependence effects만 빠르게 뻥튀기되어서 모델이 degenerate 될 수 있음. 현재 사용되는 방법인 MCMLE and stochastic approximation 는 시작값이 degeneracy 영역에 있었다면 \\(\\theta\\)의 degenerate 추정값을 생산하기도 한다. 이러한 문제점을 일컫는 용어가 Local convergence property. knitr::include_graphics(rep(&quot;images/knit-logo.png&quot;, 3)) FIGURE 7.12: Model Degeneracy for Basic Markov Statistics, Figure: Visualization of the degeneracy (black) and non-degeneracy (white) region of an ERGM with edge counts and K2-star. "],["parameter-estimation-of-ergm.html", "7.6 Parameter Estimation of ERGM", " 7.6 Parameter Estimation of ERGM 7.6.1 Current Methods for ERGM Approximation-based Algorithm: MCMC 샘플들로 likelihood function 을 최대화. typically used in finding MLE. Maximum Pseudo-likelihood Estimation (MPLE, 1974). Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMCMLE, 1994). Markov Chain Monte Carlo Stochastic Approximation (MCMCSA, 2002). 이 위까지로는 model degeneracy issue 를 해결하지 못했음. 아래의 VTSAMCMC 가서야 해결함. Varying Truncation Stochastic Approximation MCMC Method with Trajectory Averaging (VTSAMCMC, 2013). Auxiliary Variable Markov Chain Monte Carlo (MCMC) Algorithm: normalizing constant 를 근사하거나, normalizing constant ratio \\(\\frac{\\kappa(\\theta)} {\\kappa(\\theta&#39;)}\\) 를 cancel 하기 위해 auxiliary variable 들을 도입 (introduce). 베이지안 추론을 위해 사용. Bayesian inference. The Exchange Algorithm. Auxiliary Variable Metropolis-Hasting Algorithm (AVMH). Adaptive Exchange Monte Carlo Algorithm (AEXMC). 7.6.2 Approximation-based Algorithm 7.6.2.1 Maximum Pseudo Likelihood Estimation (MPLE) \\(A\\) 내부의 성분 간의 의존성 (high-order dependence wihin network) 을 무시한 채로, conditional independence 를 가정하여 조건부 likelihood 함수들의 series를 곱하는 것으로 pseudo-likelihood 함수를 근사함. ERGMS 의 조건부이며 pseudo-likelihood 는 아래와 같음. 이렇게 근사한 pseudo-likelihood 로 maximum likelihood estimator 를 approximate. 물론 이건 어디까지나 가장 간단한 방법으로서 제시되었을 뿐이고, 이 방법론은 의존성 구조를 완전히 무시했기 때문에 퍼포먼스가 구림. \\[ \\begin{align} &amp;logit \\Bigg \\{ P_\\theta \\Big (X_{ij} = 1 \\Big | X_{ij}^c = x_{ij}^c \\Big ) \\Bigg \\} &amp;&amp;= \\theta &#39; g(x_{ij}^c) \\\\ \\iff &amp;\\log PL(\\theta, x) &amp;&amp;= \\sum_{ij} \\theta &#39; g(x_{ij}^c) \\cdot x_{ij} - \\sum_{ij}\\log \\left \\{ 1+ \\theta &#39; g(x_{ij}^c) \\right \\} \\end{align} \\] \\[ \\begin{align} &amp;logit \\Bigg \\{ P_\\theta \\Big (X_{ij} = 1 \\Big | y_{ij}^c \\Big ) \\Bigg \\} &amp;&amp;= \\underbrace{\\sum_{i=1}^P \\theta_i &#39; S_i(y_{ij}^c)}_{\\text{log of unnormalized density with \\(y_{ij}^c\\)}} \\\\ \\iff &amp;\\log PL(\\theta, y) &amp;&amp;= \\underbrace{\\sum_{ij} \\theta &#39; S(y_{ij}^c) \\cdot y_{ij} - \\sum_{ij}\\log \\left \\{ 1+ \\theta &#39; S(y_{ij}^c) \\right \\}}_{\\text{not a proper likelihood}} \\end{align} \\] advantage: simple one / don’t need to generate auxiliary networks disadvantage: parameter estimates is not good because MPLE ignores high-order depedence 7.6.2.2 MCMC MLE with initial guess of parameter -&gt; generate auxiliary networks with auxiliary networks -&gt; update parameter estimates 패러미터 \\(\\theta^{(0)}\\) 에 대해 이하와 같은 initial guess 가진다고 하자. \\[ l(\\theta) = \\theta &#39; S(y) - \\log k(\\theta)\\\\ l(\\theta^{(0)}) = \\theta^{(0)} &#39; S(y) - \\log k(\\theta^{(0)}) \\] \\[ l(\\theta)-l(\\theta^{(0)})=(\\theta-\\theta^{(0)})^{t}S(y)-\\log \\frac{\\kappa(\\theta)}{\\kappa(\\theta^{(0)}} \\] distribution \\(f(x|\\theta^{(0)})\\) 에서 생산된 MC 샘플들로부터 \\(\\kappa(θ)\\) 근사. 이때 \\(\\theta^{(0)}\\) 는 \\(\\theta\\) 의 initial 측정값. MCMC 시뮬레이션을 통해 \\(f(x_{obs}|\\theta^{(0)}\\) 으로부터 랜덤샘플 \\(x_1 , \\cdots, x_m\\) 을 draw. 이 경우 log-ratio likelihood 는 \\(m \\rightarrow \\infty\\) 에서 이하와 같이 근사 가능. \\[ I(\\theta)-I(\\theta^{(0)})=(\\theta-\\theta^{(0)})^{t}g(x_{\\mathrm{obs}})-\\log\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\exp\\left\\{(\\theta-\\theta^{(0)})^{t}g(x_{i})\\right\\}\\right] \\] 따라서 \\(\\theta\\) 를 경유해서 \\(I(\\theta)-I(\\theta^{(0)})\\) 를 극대화하는 작업을 통해 결국 \\(\\hat \\theta_{MLE}\\) 를 근사할 수 있게 된다. MCMLE 의 퍼포먼스는 \\(\\theta^{(0)}\\) 의 선택에 좌우된다. \\(\\theta^{(0)}\\) 이 MLE 의 참값의 attraction region 에 존재하지 않는다면, 해당 방법론은 suboptimal 해로 수렴해버리거나, 최악으로는 converge 실패 가능성도 있음. 7.6.2.3 MCMC Stochastic Approximation exponential family 에 대한 이론을 통해, 우리는 ERGMs을 maximize 하는 것은 \\(E_{\\hat \\theta} \\Big \\{ g(X) \\Big \\} = g(x_{obs})\\) 라는 system 을 푸는 것과 동일하다는 것을 알 수 있다. 이 system 의 성립 여부는 이하와 iff (\\(\\iff\\)). \\(\\hat \\theta = \\hat \\theta_{MLE}\\) Independent network generation Parameter estimation update with stochastic approximation 이 방법론은 independent 네트워크 샘플을 생산하는데에는 비효율적이다. 각 샘플 \\(x_{k+1}\\)을 생산하기 위해 요구되는 이터레이션 스텝의 갯수는 order of \\(100 n^2\\). 이때 \\(n\\)은 node 의 갯수. 7.6.3 Auxiliary Variable MCMC-based Approaches 7.6.3.1 Exchange Algorithm normalizing constant ratio \\(\\frac{\\kappa(\\theta)}{\\kappa(\\theta&#39;)}\\) 따위의 auxiliary 변수로 분포 \\(f(x | \\theta)\\) 를 augment. 이는 시뮬레이션 중에 canceled. 후보 point \\(\\theta &#39; \\sim q(\\theta &#39; | \\theta, x)\\) 를 생산 perfect sampler 사용해서 auxiliary 변수 \\(y \\sim f(y | \\theta &#39;)\\) 를 생산 \\(\\theta&#39;\\)를 with probability \\(1 \\wedge r(\\theta, \\theta &#39; \\Big | x)\\)로 채택. 이때 \\[ r(\\theta, \\theta &#39; \\Big | x) = \\frac{\\pi(\\theta&#39;)}{\\pi(\\theta)} \\cdot \\frac{f(x \\Big | \\theta &#39; )}{f(x \\Big | \\theta )} \\cdot \\frac{f(y \\Big | \\theta)}{f(y \\Big | \\theta&#39;)} \\cdot \\frac{q(\\theta \\Big | \\theta &#39; , x)}{q(\\theta &#39; \\Big | \\theta , x)} \\] exchange 알고리즘은 Ising 이나 autologistic 과 같은 일부 discrete 모델에는 잘 작동. 하지만 perfect sampling 이 적용되지 않는 많은 다른 모델에는 적용할 수 없다. 우리는 MCMC 샘플을 통해 auxiliary 변수 \\(y\\)를 생산할 수 있지만, 수렴 문제 부분에서 이론적인 흠결이 있음. 만약 MCMC 샘플의 mixing이 매우 느리다면 딴놈이 아니라 \\(\\theta_0\\) 가 매우 높은 확률로 채택되어버림. 그리고 이 MCMC 샘플의 mixing 이 느린 것 자체가 ERGMs 의 일반적인 특징임. 이래서 문제. 7.6.3.2 Monte Carlo MH Algorithm MH 알고리즘의 MC 버전. 각 이터레이션에서 MCMH 알고리즘은 unknown normalizing constant ratio \\(\\frac{\\kappa(\\theta)}{\\kappa(\\theta&#39;)}\\) 를 MC estimate와 importance 샘플링 접근법으로 대체한다. exchange 알고리즘과 다르게, MCMH 알고리즘은 perfect sampler 요구조건을 빗겨간다. 따라서 perfect sampler 를 못 쓰는 다수의 통계문제에 대해서도 얘를 쓸 수 있음. 하지만 얘도 여전히 수렴 문제가 존재함. 얘의 경우에는 importance sampling estimator 가 유한한 숫자의 샘플로는 ratio의 참값 \\(\\frac{\\kappa(\\theta)}{\\kappa(\\theta&#39;)}\\) 에 수렴하지 못할 수도 있음. FIGURE 7.13: Goodness-of-fit Plots, for ERGMs - for the high school student friendship network 7.6.4 Varying Trunction Stochastic Approximation MCMC ERGM 의 likelihood 함수는 \\[ \\begin{align} &amp;f(y | \\theta ) = \\frac{1}{\\kappa(\\theta )} \\exp \\Big \\{ \\theta&#39; S(y)\\Big \\}, &amp;&amp;\\theta = (\\theta_1 , \\cdots, \\theta_d)&#39;, &amp;&amp;S(y) = \\Big( S_1 (y), \\cdots, S_d(y) \\Big)&#39; \\end{align} \\] 이때 \\(\\kappa(θ)\\)를 특정할 수 없다 (intractability)는 점과 모델 degeneracy 때문에, \\(\\theta\\)를 정확하게 추정하는 것은 어렵다. 이 문제는 varying truncation stochastic approximation MCMC 를 사용하는 것으로 해결 가능. Normalizing constant 는 \\(\\kappa(\\theta) = \\sum_{\\text{all possible }y} \\exp \\Big \\{ \\theta &#39; S(y)\\Big\\}\\). 이로 인해 촉발되는 문제는? MPLE: dyadic 독립이 가정되어 있다. 이건 observed 네트워크가 무엇이냐에 대해 과하게 의존. MCMLE: \\(\\theta^{(0)}\\) 을 어떻게 고르느냐에 대해 과하게 의존. local 최적해로 수렴해버리거나 모델 degeneracy 로 수렴 자체가 실패하기도. 7.6.4.1 MCMC Stochastic Approximation \\(h(\\theta) = 0\\) 형의 system 을 풀자. 고전적인 SA 알고리즘은 이하와 같은 형태를 띈다. (\\(a_k\\) , \\(h\\), \\(\\omega_k\\) 에 대한) 적절한 조건 하에서, 이 알고리즘은 해로 수렴한다는 것을 실제로 보이는 것이 가능하다. $$ \\[\\begin{align} \\theta_{k+1} &amp;= \\theta_k + a_k Y_k \\\\ &amp;=\\theta_k + a_k \\Big \\{ h(\\theta_k) + \\omega_k \\Big \\}, &amp;&amp; k \\ge 0 \\end{align}\\] $$ \\(Y = h(\\theta) + \\omega\\) 는 noisy estimate \\(\\omega\\) 는 mean-zero noise. \\(\\theta_{MLE}\\) 를 찾는 것은 exponential family 안에서 \\(E_\\theta \\{ S(Y) \\} = S(\\mathbf y_{obs})\\) 의 해를 찾는 것과 equivalent. 이는 이하의 단계를 거친다. 다만 이는 independent 네트워크 샘플을 생산하는데 있어 비효율적일 수밖에 없음. order of \\(100n^2\\) 이라 연산을 엄청 먹으니까. \\(\\mathbf y^{k+1} \\sim f(\\mathbf y | \\theta^{(k)}\\)를 샘플링 (independence 네트워크 생산) 각 arc 변수 \\(Y_{ij}\\)가 독립적으로 정해지는 랜덤 그래프에서 시작. 이 변수에는 0 혹은 1이 0.5 확률로 할당. 이 랜덤 그래프를 Gibbs 샘플러든 MH 알고리즘이든 써서 업데이트. SA 를 통해 패러미터 estimate \\(\\theta^{(k+1)} = \\theta^{(k)} - a_k D^{-1} \\Big \\{ U( \\mathbf y_{k+1}, \\bar {\\mathbf y}_{k+1} ) - \\mathbf S (\\mathbf y_{obs} ) \\Big \\}\\) 절차 따라서 업데이트. where $$ \\[\\begin{align} U( \\mathbf y_{k+1}, \\bar {\\mathbf y}_{k+1} ) - \\mathbf S (\\mathbf y_{obs} ) \\Big \\} &amp;= P(\\bar {\\mathbf y}_{k+1} \\Big | \\mathbf y_{k+1} ) \\cdot \\mathbf S (\\mathbf {\\bar y}_{k+1} ) \\Big \\{ 1-P(\\mathbf {\\bar y}_{k+1} \\Big | \\mathbf y_{k+1} ) \\Big \\} \\cdot \\mathbf S (\\mathbf {\\bar y}_{k+1} ) \\\\ \\mathbf {\\bar y}_{k+1} &amp;= 1- \\mathbf y_{k+1} \\end{align}\\] $$ 7.6.4.1.1 Model Degeneracy \\(\\theta\\)을 어떻게 정하느냐에 따라 모델은 그것의 확률을 (Complete (fully connected) 거나 empty (entirely unconnected) 네트워크와 같은) 1개 혹은 소수의 그래프에만 한정시켜서 부어버릴 수도 있다. (탐색 효율 쓰레기됨) 이 문제를 해결하기 위해선 탐색 전에 degeneracy 리전을 거의 포함하지 않는 패러미터 스페이스를 가지는 모델로 특정할 필요가 있다. 근데 이게 진짜 드럽게 어려움. 7.6.4.2 Varying Truncation SAMCMC for ERGMs 알고리즘 proceeds 전 Setup: for some constants \\(k_0&gt;1\\), \\(\\eta \\in (\\frac{1}{2},1)\\), \\(\\xi \\in (\\frac{1}{2},\\eta)\\), \\(C_a &gt; 0\\), \\(C_b &gt;0\\), set as below. $$ \\[\\begin{align} &amp;a_k = C_a \\left( \\frac{k_0}{(k_0 \\vee k)}\\right)^\\eta, &amp;&amp;b_k = C_b \\left( \\frac{k_0}{(k_0 \\vee k)}\\right)^\\xi \\tag{C_1} \\end{align}\\] $$ Next, \\[ \\bigcup_{s \\ge 0} \\mathcal K_s = \\Theta, \\; \\; \\; \\text{where } \\mathcal K_s \\subset \\text{int}(\\mathcal K_{s+1}) \\tag{C_2} \\] And also, \\(\\mathcal X\\): a space of social network \\(\\mathcal T\\): \\(\\mathcal X \\times \\Theta \\rightarrow \\mathcal X_0 \\times \\mathcal K_0\\) (reinitialization mechanism) \\(\\sigma_k\\): 이터레이션 \\(k\\) 까지 수행되는 reinitialization 의 숫자. (\\(\\sigma_0 = 0\\)) Proceeds: Gibbs 샘플러 사용해 \\(m\\) sweeps 번 interate 해서 auxiliary network \\(y_{k+1} \\sim f(y | \\theta^{(k)})\\) 생산 Set \\(\\theta^{(k + \\frac{1}{2})} = \\theta^{(k)} + a_k \\Big \\{ S(y_{k+1}) - S(y_{obs}) \\Big \\}\\) Set $$ \\[\\begin{cases} \\sigma_{k+1} = \\sigma_k \\; \\text{ and } \\left( y_{k+1}, \\theta^{(k + {1})} \\right) = \\left( y_{k+1}, \\theta^{(k + \\frac{1}{2})} \\right) \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; &amp; \\| \\theta^{(k + \\frac{1}{2})} - \\theta^{(k)} \\| \\le b_k, \\; \\; \\theta^{(k + \\frac{1}{2})} \\in \\mathcal K_{\\sigma_k} \\\\ \\sigma_{k+1} = \\sigma_k+1 \\; \\text{ and } \\left( y_{k+1}, \\theta^{(k + {1})} \\right) = \\mathcal T \\left( y_{k}, \\theta^{(k)} \\right) &amp; o.w. \\end{cases}\\] $$ 7.6.4.2.1 Trajectory averaging estimator trajectory averaging estimator \\(\\bar \\theta_n = \\frac{1}{n}\\sum_{k=1}^n \\theta^{(k)}\\) 에 의해 \\(\\theta\\) 를 estimate 가능. 실전에서는 estimate 의 variation 을 줄이기 위해 대신 \\(\\theta\\) estimate 에 \\(\\bar \\theta (n_0 , n) = \\frac{1}{n-n_0}\\sum\\limits_{k=n_0+1}^n \\theta^{(k)}\\) 을 자주 사용함. 이 때 \\(n_0\\) 는 burn-in 이터레이션의 숫자. Free parameters Free parameters: \\(\\{a_k\\}\\), \\(\\{b_k\\}\\), \\(\\{\\mathcal K_s, \\; s \\le 0\\}\\), \\(m\\) \\(k_0 = 100\\), \\(\\eta = 0.65\\), \\(\\xi = \\frac{0.5 + \\eta}{2}\\). \\(C_a\\), \\(C_b\\): adjusted for different examples choose \\(\\mathcal K_0\\) to be around MPLE. 이 article 에선 \\(\\mathcal K_{s, 1} = \\Big [ -4(s+1), 4(s+1) \\Big]\\), \\(\\mathcal K_{s, 2} = \\cdots = \\mathcal K_{s, d} = \\Big [ -2(s+1), 2(s+1) \\Big]\\) 로 설정. \\(m=1\\) 7.6.4.2.2 Numerical Examples Methods: MCMLE, SAA (Stochastic Approximation Algorithm), Varying truncation SAMCMC SAMCMC: independent 5 runs(each of 200,000 iterations) (m=1, Ca = 0.01, Cb = 1000, η, ξ, κs are defalut) ?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\?????????????????\\ 7.6.5 Conclusion SAMCMC 알고리즘의 varying truncation 은 ERGM 에 적용 가능. 여기서는 estimator 가 degeneracy region 으로 향하는 걸 reinitialization 에 의해 방지. estimator 는 ERGM 의 MLE로 converge 하며, 이는 asymptotic 하게 \\(N\\) 을 따르며 asymptotic 하게 efficient. 이는 normalizing constant 를 감당하고서라도 model degeneracy problem 를 해결할 수 있다는 점에서 유용. "],["ergm-for-dynamic-networks.html", "7.7 ERGM for Dynamic Networks", " 7.7 ERGM for Dynamic Networks However, a need for statistical models representing the evolving phenomena ⇒ ”Dynamic Models” with a temporal structure ERGM → TERGM → STERGM One-step transition probability \\((t-1) → (t)\\) (Markov Assumption) \\[ P_{\\eta, g} \\Big ( Y^t = y^t \\Big | Y^{t-1} \\; \\; ; \\; \\; \\theta \\Big ) = \\frac{\\exp \\Big \\{ \\eta(\\theta) \\cdot g(y^t, y^{t-1})\\Big \\}}{c_{\\eta, h}(\\theta, y^{t-1})} \\] 7.7.1 Temporal ERGM (TERGM, T ERGM) 시간 t에서의 네트워크는, t-1 시점의 (어쩌면 t-2도 가능하고) 네트워크로 조건을 건 ERGM 으로부터의 단일 생산으로 생각될 수 있음. 소셜 네트워크의 변화를 간단히 하기 위해 Markov assumption 적용. 이는 곧, \\(A^t\\)가 t 시점에서의 단일-관계 소셜 네트워크의 weight 매트릭스를 표현하고, 우리에게 \\(A^{t-1}\\) 의 값이 주어져 있다면, \\(A^t\\) 는 \\(A^1, \\cdots, A^{t-2}\\) 으로부터는 독립임을 의미한다는 뜻. 수식으로 표현하면 아래와 같다. \\[ P\\Big(A^2, A^3, \\cdots, A^t \\Big | A^1 \\Big ) = P\\Big(A^t \\Big | A^{t-1} \\Big ) P\\Big(A^{t-1} \\Big | A^{t-2} \\Big ) \\cdots P\\Big(A^2 \\Big | A^1 \\Big ) \\tag{Temporal ERGM} \\] 마르코프 가정이 주어져 있음을 고려하면, evolving 네트워크 전반에 대해 ERGM 을 일반화하는 방법이란 \\(A^t \\vert A^{t-1}\\) 가 ERGM 표현법을 채택했음을 가정하는 것. to assume \\(A^t \\vert A^{t-1}\\) admits an ERGM representation. 함수 \\(\\Psi : \\mathbb R_{n \\times n} \\times \\mathbb R_{n \\times n} \\rightarrow \\mathbb R^k\\) 를 생각해보자. 이는 시간적으로 인접한 2개의 네트워크 (\\(t\\), \\(t-1\\) 등) 에 걸친 cliques 들의 잠재적은 potential로 인지될 수 있다. 이때 패러미터 벡터 \\(\\theta \\in \\mathbb R^k\\) 는 이하와 같은 conditional pdf를 가지며, \\(\\theta_i\\) 는 곧 tendency to have more \\(S_i\\) network statistics as time evolves. \\[ P \\bigg( A^t \\Big | A^{t-1}, \\theta \\bigg) = \\frac{1}{\\kappa(\\theta, A^{t-1})} \\exp \\left\\{ \\theta&#39; \\Psi \\left ( A^t, A^{t-1} \\right ) \\right\\} \\] joint likelihood 에서 \\(\\theta &#39;\\) 는, as time evolves, how likely to have this network statistics. 특히 우리는 해당 모델에서 이하와 같은 특수한 경우에 관심이 있다. \\[ \\Psi \\left ( A^t, A^{t-1} \\right ) = \\sum_{ij}\\Psi_{ij} \\left ( A^t_{ij}, A^{t-1} \\right ) \\] 이 형의 temporal potential 함수는 \\(A^t | A^{t-1}\\)의 조건부 분포의 This form of the temporal potential function represents situations where the conditional distribution of \\(A^t | A^{t-1}\\) factors over the entries \\(A^t_{ij}\\) of \\(A^t\\). 7.7.1.1 Network Statistics for Temporal ERGM 여기서 4개의 network Statistics 를 제안하자. 여기서 \\(n\\) 은 # of dyad. $$ \\[\\begin{align} S_D = \\Psi_D \\left ( A^t , A^{t-1}\\right) &amp;= \\frac{1}{n-1} \\sum_{ij = (i&lt;j)} A^t_{ij} \\tag{Density} \\\\ \\Psi_S \\left ( A^t , A^{t-1}\\right) &amp;= \\frac{1}{n-1} \\sum_{ij} \\left \\{ A^t_{ij} A^{t-1}_{ij} + \\left (1-A^t_{ij} \\right) \\left (1-A^{t-1}_{ij} \\right) \\right \\} \\tag{Stability} \\\\ \\Psi_R \\left ( A^t , A^{t-1}\\right) &amp;= n \\left ( \\frac{\\sum_\\limits{ij} A_{ij}^t A_{ij}^{t-1}}{\\sum_\\limits{ij}A_{ij}^{t-1}}\\right ) \\tag{Reciprocity} \\\\ \\Psi_T \\left ( A^t , A^{t-1}\\right) &amp;= n \\left ( \\frac{\\sum_\\limits{ijk} A_{ik}^t A_{ij}^{t-1}A_{jk}^{t-1}} {\\sum_\\limits{ijk}A_{ij}^{t-1}A_{jk}^{t-1}}\\right ) \\tag{Transitivity} \\end{align}\\] $$ Density : 전체 네트워크에 들어있는 총 tie의 숫자. 단순 density. Stability : \\(t-1\\) 시점에 존재했던 link가 \\(t\\) 시점에도 여전히 존재하는 경향성 Reciprocity : \\(t-1\\) 시점에 \\(i\\) 에서 \\(j\\) 로 향하는 link가 있었다면 \\(t\\) 시점에 \\(j→i\\) 링크가 생겨날 경향 Transitivity : \\(t-1\\) 시점에 \\(i→j\\) 와 \\(j→k\\) 인 tie가 존재한다면, \\(t\\) 에 \\(i→k\\) tie의 발생으로 이어지는 경향 Claim TERGM can avoid the model degeneracy problem: NOT CORRECT!! 7.7.1.2 Estimation Notation &amp; Algorithm &amp; Convergence observed 네트워크의 sequence \\(N^1 , N^2 , \\cdots, N^T\\) 를 사용하자. 무엇을 위해? 실제 패러미터값 \\(\\theta\\) 에 가까운 estimator \\(\\hat \\theta\\) 을 찾기 위해. normalizing constant 는 보통 계산해내는 것이 불가능하여 MLE 방법론의 도입은 불가. 따라서 MCMC stochastic approximation 를 사용해 패러미터 estimate. 이하와 같이 notation 한다. 이때 t 시점의 네트워크인 랜덤변수 \\(\\underline N^t\\) 에 대해 기댓값들이 계산되었음을 notice. \\[ \\begin{align} L\\Bigl(\\theta:N^{1},{ N}^{2},\\cdots ,{ N}^{T}\\Bigr) =\\log P\\Bigl({ N}^{2},{ N}^{3},\\cdots,{ N}^{t}\\,\\Bigg \\vert\\,{ N}^{1},\\theta\\Bigr) \\\\ M\\Bigl(t,\\theta\\Bigr) = E_{\\theta}\\Bigl(\\downarrow\\phi^{t},\\Lambda\\not v^{t-1}\\Bigr)\\mid{{N}}^{t-1}\\Bigr) \\\\ G\\Bigl(t,\\theta\\Bigr) &amp;=E_{\\theta}\\Bigl(\\Psi\\Bigl(\\underline{{{\\Lambda}}}^{t},\\Lambda\\not{N}^{t-1}\\Bigr)\\Psi\\Bigl(\\underline{{{\\Lambda}}}^{t},\\Lambda\\not{N}^{t-1}\\Bigr)^{T}\\mid\\Lambda\\not=\\Lambda\\prime\\prime \\end{align} \\] 이때 이하의 기댓값들은 조건부 분포로부터 Gibbs 샘플링을 돌리는 것으로 근사 가능. Newton 방법론과 유사한 과정을 통해 unconstrained optimization 을 하자. 기댓값을 근사하고, Likelihood를 증가시키는 방향으로 패러미터값을 업데이트. 이 과정을 수렴하기까지 반복. \\[ \\Delta{ L}\\Bigl(\\theta:N^1 , N^2 , \\cdots, N^T \\Bigr) = \\sum_{t=2}^{T}\\Bigl(\\Psi\\big( N^t, N^{t-1} \\Bigr)-M\\big(t,\\theta\\big)\\Bigr) \\\\ \\triangle^{2} L\\Big(\\theta:N^1 , N^2 , \\cdots, N^T\\Big)= \\sum_{t=2}^{T}\\Big(\\mathrm{M}\\big(t,\\theta\\big)\\mathrm{M}\\big(t,\\theta\\big)^{\\prime}-{{C}}\\big(t,\\theta\\big)\\Big) \\] algorithm randomly initialize \\(\\pmb \\theta^{(1)}\\). $$ \\[\\begin{alignat}{2} \\hat{N}_{(i)}^{t,1},\\cdots, \\hat{N}_{(i)}^{t,B} &amp; \\sim\\,{P}\\left(\\underline N^t\\,\\mid\\,{N^{t-1}}_{\\cdot}\\,\\pmb{\\theta}^{(i)}\\right) \\\\ \\hat{\\mu}_{(i)}^{t} &amp; = {\\frac{1}{B}}\\sum_{b=1}^{B}\\Psi\\Big(\\hat{ N}_{(i)}^{t,b},N^{t-1}\\Big) \\\\ \\hat{C}_{(i)}^{t} &amp; = \\frac{1}{B}\\sum_{b=1}^{B}\\Psi\\left(\\hat{N}_{(i)}^{t,b},N^{t-1}\\right)\\Psi\\left(\\hat{N}_{(i)}^{t,b},N^{t-1}\\right)&#39; \\\\ \\hat{H}_{(i)} &amp; {=}\\sum_{i=2}^{T}\\left(\\mu_{(i)}^{t}\\hat{\\mu}_{(i)}^{t}-\\hat{C}_{(i)}^{t}\\right) \\tag{after iteration} \\\\ \\end{alignat}\\] $$ \\[ \\pmb \\theta^{(i+1)} = \\pmb \\theta^{(i)}-\\hat{{H}}_{(i)}^{-1}\\sum_{t=2}^T\\Bigg\\{\\Psi\\Big(\\hat{N}_{(i)}^{t,b},N^{t-1}\\Big)-\\hat{\\mu}_{(i)}^{t}\\Bigg\\} \\] 7.7.1.3 Degeneracy of Temporal ERGMs 간단한 케이스에는, where the transition distribution factors over the edges, 이 모델들은 그러한 문제들에서 완전히 자유롭다는 것이 알려져 있음. 이는 직관적으로도 와닿음. \\(A^t_{ij} | A^{t−1}\\) 의 개개의 조건부 분포가 과하게 극단적이지 않은 한, \\(A^{t-1}\\)이 주어졌을 때 \\(A^t\\)의 edge는 조건부 독립이기 때문이지. 따라서 \\(A^t | A^{t−1}\\) 의 조건부 엔트로피는 커야 하며, 이에 의해 \\(A^t\\)의 조건부 엔트로피도 커야 할테니까. 물론 이 명제는 \\(A^{t−1}\\) 에 대한 \\(A^t\\) 의 의존이 그렇게 강하지 않을 때만 성립하는 것임. 이때 이 의존의 위력은 패러미터들의 위력에 의해 결정되지. 그러니까 패러미터가 이상하게 잡히면 해당 명제의 전제가 깨진다는 거. 동일한 확률값을 가진다는 것을 analytic 하게 보일 수 있는 그래프들의 class들, 즉 equivalence 클래스들에 대해서 이를 계산해보고, 엔트로피 계산에서 각각의 클래스의 크기에 따라서 weight를 부여하자. 첫 플랏의 경우를 생각해보자. \\(A^2 | A^1\\) 의 조건부 분포는 결국 \\(A^2\\) 에 존재하는 edge의 갯수와, 얼마나 많은 \\(ij\\) 값들에게서 \\(A^2_{ij} = A^1_{ij}\\)가 성립하고 있느냐, 의 2개의 값에 대한 함수일 뿐이다. 이에 더해 \\(A^1\\)의 edge들은 exchangeable 하다는 점도 있다. 이들을 모두 생각해보면 결국 우리는 \\(A^2\\)의 marginal 분포를 순수하게 edge의 숫자를 통해서만 서술하는 것이 가능하다. 따라서 우리는 \\(n(n − 1)\\)의 확률값만 계산해내면 되며, 따라서 엔트로피는 weighted sum이다. 이때 weight는 각각의 edge 숫자에 대해, that many edges 를 가지고 있는 그래프의 숫자가 반영된 combinatorial quantities 가 된다. 7.7.1.4 Assessing Statistic Importance and Quality of Fit Description of Network Statistics - 108th U.S. Senate Network Example Three Parameter Model, Seven, Nine \\(\\Psi_{R T}\\left(\\mathcal{A}_{\\ \\cdot}\\mathcal{A}_{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu}^{t-1}\\right)\\equiv\\mathcal{H}\\!\\left(\\sum_{i j k}\\mathcal{A}_{i j}^{t}\\mathcal{A}_{j k}^{t-1}\\mathcal{A}_{k i}^{t-1}\\right)\\big/\\left(\\sum_{i j k}\\mathcal{A}_{j k}^{t-1}\\mathcal{A}_{k i}^{t-1}\\right).\\) \\(\\Psi_{C S o}\\Bigl(\\lambda_{~,}^{t},\\lambda_{~}^{t-1}\\Bigr)\\mathop{\\displaystyle=\\,1\\atop i j k}\\mathcal{A}_{i j}^{t}\\bar{A}_{k j}^{t-1}\\mathcal{A}_{k j}^{t-1}\\Bigr)\\Bigl(\\sum_{i j k}\\mathcal{A}_{k i}^{t-1}\\mathcal{A}_{k j}^{t-1}\\Bigr)\\dots\\) \\(\\Psi_{C S d}\\Bigl(\\lambda_{\\phantom{0},K}^{t},\\lambda_{\\phantom{-1}}^{t-1}\\Bigr)\\not=\\imath\\Bigl(\\sum_{i j k}\\lambda_{i j}^{t}\\lambda_{i k}^{t-1}\\lambda_{j k}^{t-1}\\Bigr)\\Bigl/\\Bigl(\\sum_{\\textstyle{\\frac{i j k}{j k}}}\\lambda_{i k}^{t-1}\\lambda_{j k}^{t-1}\\Bigr).\\) \\(\\Psi_{P}\\Bigl(\\mathcal{A}_{\\phantom{0},}^{t},\\mathcal{A}^{t-1}\\Bigr)\\equiv\\mathcal{H}\\Bigl(\\sum_{j k}\\mathcal{A}_{k j}^{t}\\mathcal{A}_{j j}^{t-1}\\Bigr)\\big/\\left(\\bigotimes_{i j}\\mathcal{A}_{k j}^{t-1}\\right).\\) \\(\\Psi_{G}\\Bigl(\\partial_{~,}^{t},\\partial_{}^{t-1}\\Bigr)\\Longrightarrow\\left(\\sum_{i j k}\\mathcal{A}_{i k}^{t}\\mathcal{A}_{i j}^{t-1}\\right)\\Bigl/\\left(\\sum_{i j}\\mathcal{A}_{i k}^{t-1}\\right).\\) Reverse-Transitivity: Co-Supported: Co-Supporting: Popularity Generosity 7.7.2 Separable Temporal ERGM (STERGM, ST ERGM) STERGM = A Separable Model for Dynamic Network Dynamic: social networks that evolve over time Time(discrete): \\(\\cdots (t-2) → (t-1) → (t) → \\cdots\\) Shows longitudinal properties based on the ERGM Separable Temporal ERGM formation of an edge: new ties duration of an edge: lasting ties 즉, model formation / duration seperatively. 7.7.2.1 Temporal ERGM Interpretation 하지만 이때 패러미터 해석할 때 주의해야할 부분이 있음. Property1: incidence of ties / tie formation (the rate at which new ties are formed) \\[ Y^+ = Y^{t-1} \\cup Y^{t} \\] Property2: duration of ties / tie dissolution (how long they tend to last once they do) \\[ Y^- = Y^{t-1} \\cap Y^{t} \\Rightarrow Y^t = Y^- \\cup \\Big( Y^+ \\setminust Y^{t-1}\\Big) \\] given \\(Y^{t-1}\\), we assume \\(Y^+\\) and \\(Y^-\\) are conditionally independent. \\[ \\begin{align} P(Y^+ = y^+ | Y^{t-1} = y^{t-1} ; \\theta^+) = \\frac{1}{\\kappa(\\theta^+} \\exp \\Big( \\sum_{i=1}^P \\theta_i^+ S_i^+(Y^+ , y^{t-1})\\Big) \\\\ P(Y^- = y^- | Y^{t-1} = y^{t-1} ; \\theta^-) = \\frac{1}{\\kappa(\\theta^-} \\exp \\Big( \\sum_{i=1}^P \\theta_i^- S_i^-(Y^- , y^{t-1})\\Big) \\end{align} \\] \\(S_i^\\pm (Y^\\pm , y^{t-1})\\) 는 \\(S_i(y^\\pm)\\), \\(S_i(y^{t-1})\\) 사이의 difference. \\(S_i(\\cdot)\\) 는 ERGM 상에서의 아무 network statistics \\[ P(Y^t = y^t | Y^{t-1} = y^{t-1}) = P(Y^+ = y^+ | Y^{t-1} = y^{t-1} ; \\theta^+) \\cdot P(Y^- = y^- | Y^{t-1} = y^{t-1} ; \\theta^-) \\] Network statstic (ex) edge count \\(g(y^t , y^{t-1}) = | y^t |\\) coefficient on \\(g\\) \\(\\propto\\) possibility of a network with many ties. 따라서 \\(g\\)의 계수가 올라가면 tie가 많은 네트워크의 발생 확률 올라감. But, this term simultaneously increases the weight of preservation of extant ties (fewer dissolved) ⇒ Both incidence and duration ↑ The two-sided nature of these effects tends to muddle parameter interpretation. ⇒ STERGM which separates the incidence and duration of ties and allows for the separate interpretation. : incidence/tie formation \\(y^+ = y^{t-1} \\cup y^t\\) – : duration/tie dissolution \\(y^- = y^{t-1} \\cap y^t \\Rightarrow y^t = y^- \\cup (y^+ \\setminus y^{t-1})\\) \\(P r(y^{+}=y^{+}\\vert Y^{t-1}=y^{\\iota-1};\\theta^{+})=\\frac{\\theta x p(\\eta^{+}(\\theta^{+})*\\mathcal{O}^{+}(y^{+},y^{I-1}))}{G_{\\eta^{+},g^{+}}(\\theta^{+},y^{\\iota-1})}\\) \\(P r(Y^{-}=y^{-}|Y^{t-1}=y^{t-1};\\theta^{-})=\\frac{\\theta x p(\\eta^{-}(\\theta^{-})*g^{-}(y^{-},y^{t-1}))}{c_{\\eta^{-},g^{-}}(\\theta^{-},y^{t-1})}\\) \\({\\cal P}_{I}(\\mathcal{V}^{t}\\underline{{{-}}}\\mathcal{V}^{t-1}\\underline{{{-}}}\\mathcal{D}^{t-1}\\underline{{{-}}}\\mathcal{J}) \\times \\text{incidence} \\times \\text{duration}\\) \\(P r(y^{+}=y^{+}\\vert Y^{t-1}=y^{\\iota-1};\\theta^{+})\\) \\(P r(y^{-}=y^{-}\\vert Y^{t-1}=y^{\\iota-1};\\theta^{-})\\) :::{.definition name = “1”} Definition 1 We say that a dynamic model is separable if Y+ is conditionally independent of Y− given Y t−1 and the parameter space of θ is the product of the individual parameter spaces of θ+ and θ−. ::: Assumption: During a given discrete time step, the process by which the ties form does not interact with the process by which they dissolve. Lost: In the parameterization in terms of formation and dissolution, some flexibility(formation and dissolution processes interact within a given time step) is lost Gain: Ease of specification, tractability of the model and substantial improvement in interpretability of TERGM Now the parameter and its interpretation have an implicit direction. (formation or duration) - Formation network (+) is related to formation network only \\(\\Pr(\\mathbf{V}^{-}={\\boldsymbol{y}}^{-}|\\mathbf{V}^{t-1}={\\boldsymbol{y}}^{t-1};{\\boldsymbol{\\theta}}^{-})={\\frac{\\exp\\{(\\theta^{+})^{\\cdot7}g^{+}(y^{+},y^{\\dot{t}-1})}{c_{g^{+}}(\\theta^{+},y^{t-1})}}\\) Duration network (or Dissolution network) \\(\\Pr(\\mathbf{V}^{-}={\\boldsymbol{y}}^{-}|\\mathbf{V}^{t-1}={\\boldsymbol{y}}^{t-1};{\\boldsymbol{\\theta}}^{-})={\\frac{\\exp\\{({\\boldsymbol{\\theta}}^{-}),{\\boldsymbol{\\gamma}}^{t-1}\\}}{c_{o}({\\boldsymbol{\\theta}}^{-},{\\boldsymbol{\\gamma}}^{t-1})}}\\) (-) is related to duration network only Example of Parameter Interpretation (Edge Count) Now the parameter and its interpretation have an implicit direction. (formation or duration) Formation network Edge count g + (y + , y t−1 ) = |y + |, y + = y t−1 ∪ y t Recall, y + is network about formation θ + means log-odds of gaining new tie from y t−1 =⇒ y t Dissolution network Edge count g −(y −, y t−1 ) = |y −| , y − = y t−1 ∩ y t Recall, y − is network about duration θ − means log-odds of existing tie to survive at y t−1 =⇒ y Likelihood-based Inference for STERGM Fit STERGM by finding conditional MLE under an order 1 Markov assumption: $$ \\[\\begin{align} \\hat{\\theta} &amp;= \\arg\\mathrm{max}_{\\theta}&amp;&amp;\\prod_{t=1}^{T}\\mathrm{Pr} \\Big ({Y}^{t}=y^{t} \\Big | {Y}^{t-1}=y^{t-1} \\Big) \\\\ &amp;= &amp;&amp;\\prod_{t=1}^{T}\\frac{\\exp \\Big \\{ (\\theta^{+})^{\\cdot T}g^{+}(y^{+},y^{t-1}) \\Big\\} } {c_{g^{+}}(\\theta^{+},y^{t-1})} \\cdot \\frac{\\exp\\Big\\{(\\theta^{-})\\cdot g^{-}(y^{-},y^{t-1})\\Big\\}}{c_{g^{-}}(\\theta^{-},y^{t-1})}. \\end{align}\\] $$ where \\(c_{g}(\\theta,y^{t-1})=\\sum_{y^{\\prime}\\in\\psi}\\exp \\Big \\{(\\theta)^{\\cdot T}g(y,y^{t-1}) \\Big\\}\\) In practical, MLE can be obtained by maximizing the log-likelihood using numerical optimization. The normalizing constant \\(c_{g^+}(\\theta^+,y^{t-1})\\) 와 \\(c_{g^-}(\\theta^-,y^{t-1})\\) 는 계산 불가. 각각은 시뮬레이션을 통해 (e.g. MCMCMLE) 를 통해 획득됨 i.e. maximizing \\(I(\\theta)-I(\\theta^{0})=\\{\\theta-\\theta^{0}\\}\\sum_{t=1}^{T}g(y^{t},y^{t-1})-\\log\\Big\\{\\prod_{t=1}^{T}\\frac{c_{g}(\\theta,y^{t-1})}{c_{g}(\\theta^{0},y^{t-1})}\\Big\\}\\) 7.7.2.2 Application Study 7.7.2.3 Conclusion Introduce statistical model for networks that evolve over time. Separable parameterization of incidence and duration. Greatly improve interpretability of model parameters, with sacrificing a little. Identify the structure of incident and durational structure "],["latent-network-models.html", "7.8 Latent Network Models", " 7.8 Latent Network Models 7.8.1 Latent Position Model ERGMs 와 다르게, 이 모델은 소셜 스페이스의 개념을 도입함. 이 소셜 스페이스에서는, 네트워크 관계에 있어 unobserved latent 특성이 potential transitive 경향을 represent, 즉 위력? 을 나타낼 수 있음. 이때 이 소셜 스페이스에서 각 actor (혹은 node) \\(i\\)는 알려지지 않은 포지션 \\(z_i\\)를 각각 차지하게 됨. 우리는 이를 latent position 이라고 부름. 여기서 우리는 주된 가정으로 ties 간의 조건부 독립을 가정한다. latent position이 주어진다면, 네트워크 안의 ties들은 조건부 독립임이 가정된다. 두 개인들 간의 특정한 tie의 확률은 그들의 positions 들의 함수로 모델링된다. 가령 소셜 스페이스 안에서의 두 actor 사이의 거리라던가. main assumption given the latent position, each connection is conditionally independent each others latent positions can capture all dependence structures the prob of having an edge is a function of latent positions 이때 주로 사용되는 function 은 distance. 이때 이를 노테이션으로 표기하자면 다음과 같다. 아래는 joint likelihood function. \\[ P(Y | Z, X, \\theta) = \\prod_{i \\not = j, i&lt;j} P(y_{i,j} | z_i , z_j , x_{i,j}, \\theta) \\] \\(i&lt;j\\) 는 undirected network 상황에서 sociomatrix (binary network) \\(Y_{n \\times n}\\), 이때 요소 \\(y_{i,j}\\)는 actor \\(i\\)로부터 \\(j\\)로의 관계를 의미하는 값. additional covariate information \\(X\\) (nodal covaraites) 이때 \\(X\\)와 \\(x_{i,j}\\)는 unobserved 성질이며, \\(\\theta\\)는 estimate되어야 하는 패러미터, \\(Z\\)는 estimate 되어야 하는 포지션. \\(z_i\\): node \\(i\\) 의 latent position. 7.8.1.1 Methods 7.8.1.1.1 Distance Models \\(P(y_{i,j} | z_i , z_j , x_{i,j}, \\theta)\\)는 logistic regression framework 를 활용하는 것으로 편하게 패러미터化 (설명) 할 수 있다. 이렇게 패러미터化 할 때 tie의 확률은 \\(z_i , z_j \\in \\mathbb R\\) 인 \\(z_i , z_j\\) 사이의 euclidean distance 에 의존한다. 수식은 아래와 같다. \\[ \\eta_{i,j} = logodds (y_{i,j} = 1 | z_i , z_j , x_{i,j}, \\alpha, \\beta) = \\underbrace{\\alpha}_{\\text{intercept}} + \\underbrace{\\beta &#39; }_{\\substack{\\text{effect of}\\\\\\text{nodal covariates}}} x_{i,j}- \\underbrace{|z_i - z_j |}_{\\text{euclidean}} \\] \\[ \\begin{alignat}{2} P(Y|Z,X,\\theta) &amp;= \\prod_{i&lt;j} &amp;&amp;\\pi_{ij}^{y_{ij}}&amp;&amp;(1-\\pi_{ij})^{1-y_{ij}} &amp;= \\prod_{i&lt;j} &amp;&amp;\\left(\\frac{\\exp(\\alpha + \\beta x_{ij} - \\|z_i - z_j \\|)} {1+\\exp(\\alpha + \\beta x_{ij} - \\|z_i - z_j \\|)} \\right)^{y_{ij}} &amp;&amp;\\left(\\frac{1}{1+\\exp(\\alpha + \\beta x_{ij} - \\|z_i - z_j \\|)} \\right)^{1-y_{ij}} \\end{alignat} \\] 이때 distance \\(|z_i - z_j |=d_{ij}\\) 는 그 어떤 metric으로도 대체될 수 있다는 것을 notice. 삼각부등식 \\(d_{i,j} \\le d_{i,k} + d_{k,j}\\)만 만족하면 됨. \\(d_{ij}\\) 이 증가하면 edge 를 가질 prob은 감소. 역도 성립. out of sight, out of mind. latent 포지션 모델은 본질적으로 reciprocal 하고 transitive 함. 왜? 만약 \\(i \\rightarrow j\\) 이고 \\(j \\rightarrow k\\) 이라면, \\(d_{i,j}\\) 와 \\(d_{j,k}\\) 는 어쩌면 지나치게 크지는 않을 수도 있는 것이고, 이 경우에는 이하로 이어짐: events \\(j \\rightarrow i\\) (reciprocity) 그리고 \\(i \\rightarrow k\\) (transitivity) when a node \\(i,j,k\\) are connected, the latent position of \\(z_i\\) and \\(z_j\\) are close. benefits: visualize the relationship / very easy to capture the cluster - structures disadvantages: even though a latent space model can capture the transitivities with triangle inequalities, it still has a limitatino in capturing a high-order dependence 7.8.1.1.2 Projection Models Invariance of distance. distance 모델은 본질적으로 symmetric 임. 즉 \\(p(i → j) = p(j → i)\\). 하지만 많은 (directed) 모델에서 이런 symmetry 는 성립을 안함. 예를 들어 actor \\(i\\) 가 대량의 ties 들을 보내는 반면 \\(j\\) 가 \\(i\\) 에게서 ties 들을 받은 actors 전체 중 작은 subset 에게만 보낸다면? 따라서 행위의 변수 레벨은 관계에서 확률의 transitivity 를 allow하는 latent 포지션 모델의 맥락 속에서 모델링될 필요가 있다. 개개인의 소셜 활동의 특정한 수준도 고려되어야 함은 물론이다. 그러나 likelihood 는 only include a distance function -&gt; distance 는 2개의 latent position 사이의 infinite realization 을 보유. latent position 들 자체도 무한한 realization 이 존재. -&gt; Bayesian Inference -&gt; post-process for posterior sample (Procrustes Matching: based on a reference point, all posterior samples are moved / rotated tward a reference point. commonly used in statistical shape analysis) set up a reference point generally a posterior sample that yiedls MAP (Maximum A Posterior) all other posterior samples are transformed by procrustes matching -&gt; \\(procrustes(X_{ref}, X_{ref})\\) in MCMCpack pacakage of R. distance -&gt; undirected network projection method -&gt; directed network actor \\(i\\)의 특성의 벡터 \\(v_i\\)를 unit \\(k\\)-dim 이라고 가정. \\(i,j\\) 사이의 angle 에 따라 둘 사이에 tie 가 존재할 가능성이 이하와 같이 영향받는다. 예각: high 직각: neutral 둔각: low actor \\(i\\) 의 활동 레벨 \\(a_i &gt;0\\) 를 설정한 후, \\(i→j\\)의 tie의 존재 확률을 \\(a_i v_i &#39; v_i = \\frac{z_i&#39; z_j}{| z_j |}\\) (이때 \\(z_i = \\underbrace{a_i}_{\\text{strength (length)}} \\underbrace{v_i}_{\\text{unit vector}}\\)) 라고 설정한다면 이하의 등식이 성립. \\[ \\eta_{i,j} = logodds (y_{i,j} = 1 | z_i , z_j , x_{i,j}, \\alpha, \\beta) = \\alpha + \\beta &#39; x_{i,j} + \\frac{z_i&#39; z_j}{| z_j |} \\] from note: \\[ \\pi_{ij} = logodds(y_{ij} | z_i , z_j , \\alpha) = \\alpha + \\frac{z_i &#39; z_j}{\\|z_i \\|} \\\\ \\pi_{ji} = \\phantom{logodds}(y_{ji} | \\phantom{z_i , z_j , \\alpha) = \\alpha} + \\frac{z_j &#39; z_i}{\\|z_j \\|} \\] Latent Space Model (LSM) -&gt; map each node into unobserved latent space -&gt; easy to visualized -&gt; easy to observe cluster structures LSM \\(z_i \\sim (0, \\sigma^2)\\), \\(\\pi(z_i) \\sim N(0, \\sigma^2)\\) -&gt; need a model that capture clusture structures using a model 여기서 \\(z_i\\) 는 mixture normal prior (one of the typical way to implement clustering) 7.8.1.2 Estimation Distance 모델 상황을 생각해보자. 유클리드 공간에서 set points 간의 거리는 회전, 반사, 이동에 불변 (invariant). 따라서 모든 각각의 latent postion의 행렬 \\(Z_{k \\times n}\\) 에 대해 같은 log-likelihood 를 갖는 다른 positions 을 표상하는 행렬이 존재한다. \\(\\mathcal Z\\) 를 회전, 반사, 이동에 불변한 \\(Z\\)와 equivalent 한 postions들의 class 라고 하자. 각각의 \\(\\mathcal Z\\)에 대해 node 들 간의 거리를 모아 set 1개가 나옴. 이러한 positions 들의 class를 configuration 이라고 부름. 마찬가지로 Projection 모델에서의 \\(Z\\) 에 대해서도 Projection 모델들은 positions 들의 회전과 반사에는 불변하지만, 이동에 대해서는 불변이 아님. 조건부 독립 모델의 log-likelihood 모델은 다음과 같다: \\[ \\log P(Y | \\eta ) = \\sum_{i \\not = j} \\Big \\{ \\eta_{i,j}y_{i,j} - \\log (1+\\exp(\\eta_{i,j}) \\Big \\} \\] ※ Steps: 각 j 에서 \\(z_j &#39;\\) 샘플링하기 위해 MH 스텝 거침. proposal 분포 \\(\\varphi(\\cdot)\\) 으로부터 \\(z_j&#39;\\) 생산하고 이를 이하의 확률 \\(r_z \\left ( z_j &#39; , z_j^{(t)} \\right)\\) 로 채택. 비슷환 과정을 따라서 \\(\\alpha, \\beta\\) 도 MH 이용해서 생산. \\[ r_z \\left ( z_j &#39; , .z_j^{(t)} \\right) = \\frac{\\pi \\Big (z_j &#39; \\Big \\vert Y, \\alpha , \\beta \\Big )}{\\pi \\Big (z_j^{(t)} \\Big \\vert Y, \\alpha , \\beta \\Big )} \\cdot \\frac{\\varphi \\Big (z_j &#39; \\rightarrow z_j^{(t)} \\Big )}{\\varphi \\Big ( z_j^{(t)}\\rightarrow z_j &#39; \\Big )} \\] Procrustes 매칭 사용해서 MCMC 샘플 후처리. latent positions 들의 reference set 을 찾기 위해, MCMC 샘플로부터 latent positions들 중 full log posterior density가 가장 높은 latent positions들 \\(Z_0\\)를 하나 뽑아서 쟁여둠 \\(Z_0\\)를 사용해서 각각의 MCMC 샘플에 Procrustes 매칭 적용 \\[ Z^\\ast = \\arg \\min_{TZ} tr \\Big \\{ (Z_0 - TZ) &#39; (Z_0 - TZ) \\Big \\} \\] 7.8.1.3 Advantages 네트워크 관계에 대한 시각적이고 모델에 기반한 공간적인 표현을 제공. 해석 용이함. It is flexible and can be easily generalized to allow for multiple relationships, ties with varying strengths, and time-varying relations deal easily with missing data, at least if information on ties is missing at random the model is inherently transitive, and so we can expect an improved fit over models lacking such structure when the relations are transitive in nature 7.8.2 Latent Position Cluster Model $$ \\[\\begin{align} \\log \\frac{P(y_{i, j} = 1\\Big | z_i , z_j , x_{i,j}, \\beta)}{1 - P(y_{i, j} = 1 \\Big | z_i , z_j , x_{i,j}, \\beta)} &amp;= \\beta_0 &#39; x_{i,j} - \\beta_1 |z_i - z_j | \\\\ P(Y | Z, X, \\beta) &amp;= \\prod_{i \\not = j} P(y_{i, j} \\Big | z_i , z_j , x_{i,j}, \\beta) \\tag{Likelihood} \\end{align}\\] $$ \\(z_i \\sim \\sum\\limits_{g=1}^G \\lambda_g \\cdot MVN_d )\\mu_g , \\sigma^2_g I_d )\\), where \\(\\sqrt{\\frac{1}{n} \\sum_i |z_i|^2} = 1\\). \\(\\lambda_g\\)는 individual distribution의 비율 Latent position cluster model \\[ z_i \\sim \\sum^G_{g=1} \\lambda_g \\cdot MVN(\\mu_g , \\sigma^2_g I) \\] \\(\\lambda_g\\): proportion of an individual distribution (probability of belonging to cluster \\(g\\) from node \\(i\\)) \\(\\mu_g\\): center of cluster position \\(\\sigma_g^2\\): size of cluster 이때 \\(\\sqrt{\\frac{1}{n} \\sum\\limits^n_{i=1}\\|z_i\\|^2} = 1\\) \\[ logodd \\pi_{ij} = \\beta_0 &#39; x_{ij} - \\beta_1 |z_i - z_j| = \\alpha - \\beta \\|z_i - z_j\\| \\] 7.8.2.1 Bayesian Estimation ※ Fully Bayesian Estimation Procedure 모델 패러미터 \\(\\beta, \\lambda_g, \\mu_g, \\sigma_g^2\\) 들의 prior 분포 특정 $$ \\[\\begin{align} \\beta &amp;\\sim MVN_p \\left( \\xi , \\Psi \\right) \\\\ \\lambda &amp;\\sim Dirichlet(\\nu) \\\\ \\sigma^2_g &amp;\\sim \\sigma_0^2 Inv- \\chi_\\alpha^2 &amp;&amp; g = 1, \\cdots, G \\\\ \\mu_g &amp;\\sim MVN_d \\left( 0, \\omega^2 \\cdot I_d \\right) &amp;&amp; g = 1, \\cdots, G \\end{align}\\] $$ \\(\\mathbf z_i , \\beta, \\lambda, \\mu_g ,\\sigma^2_g, K_i\\)의 full 조건부 posterior 분포 특정 $$ \\[\\begin{align} {\\bf z}_{i}\\mid K_{i}=g,\\mathrm{others} &amp;\\sim \\phi_{d}\\bigl({\\bf z}_{i};\\mu_{g},\\sigma_{g}^{2}I_{d}\\bigr) \\cdot P\\bigl(Y\\mid Z,X,\\beta\\bigr),\\quad &amp;&amp;i=1:n, \\\\ &amp;= MVN(\\bf z_i ; \\mu_g , \\sigma_g^2 I_d) \\cdot P\\bigl(Y\\mid Z,X,\\beta\\bigr),\\quad \\tag{1} \\\\ \\beta\\mid \\bf{Z},\\mathrm{others} &amp;\\sim \\phi_{p}\\Bigl(\\beta;\\xi,\\Psi\\Bigr) \\cdot P\\Bigl({Y}\\mid\\bf {Z},\\bf {X},\\beta\\Bigr), \\\\ &amp;= MVN(\\beta ; \\xi, \\Psi) \\cdot P\\Bigl({Y}\\mid\\bf {Z},\\bf {X},\\beta\\Bigr), \\tag{2} \\\\ \\lambda\\mid{\\mathrm{others}} &amp;\\sim {Dirichlet\\Big (m + \\nu \\Big)} \\\\ \\mu_{g}\\mid\\mathrm{others} &amp;\\sim MVN_{d}\\left(\\frac{m_{g}\\bar z_{g}}{m_{g}+\\sigma_{g}^{2}/\\omega^{2}},\\,\\frac{\\sigma_{g}^{2}}{m_{g}+\\sigma_{g}^{2}/\\omega^{2}} \\cdot I\\right),\\quad &amp;&amp;g=1:G, \\\\ \\sigma_{g}^{2}\\mid\\mathrm{others} &amp;\\sim \\left(\\sigma_{0}^{2}+d s_{g}^{2}\\right) \\cdot \\mathrm{Inv-}\\chi^2_{\\alpha + m_s d}, &amp;&amp;g=1:G, \\\\ P\\Bigl(K_{i}=g \\bigg | \\mathrm{others}\\Bigr) &amp;= \\frac{\\lambda_{g}\\phi_{d}\\Bigl(z_{i};\\mu_{g},\\;\\sigma_{g}^{2} \\cdot I_{d}\\Bigr)} {\\sum_{r=1}^{G}\\lambda_{r}\\phi_{d}\\Bigl(z_{i};\\mu_{r}, \\; \\sigma_{r}^{2} \\cdot I_{d}\\Bigr)},\\quad &amp;&amp;g=1:G. \\end{align}\\] $$ \\(K_i = g\\): the membership of node \\(i\\) \\(MVN(\\bf z_i ; \\mu_g , \\sigma_g^2 I_d)\\): normal for membership \\(g\\) \\(P\\bigl(Y\\mid Z,X,\\beta\\bigr)\\): we cannot calculate the conditional posterior distribution analytically. for same reason, for (1) and (2), use MH algorithm \\(m_g = \\sum\\limits^n_{i=1}I(K_i = g)\\) \\(\\bar z_g\\): mean of latent positions that belongs to cluster \\(g\\) \\(d\\): dimension \\(s_g^2 = \\frac{1}{d}\\sum\\limits_{i=1}^n (z_i - \\mu_g) &#39; (z_i - \\mu_g) I(K_i = g)\\) ※ Steps. MH 스텝 이용해서 \\(Z_{t+1}\\) 업데이트 1. proposal \\(2_{i}^{\\ast}\\sim\\left|l\\right|/|\\bigvee_{i}(\\Sigma_{i},(\\hat{L}_{i}^{\\ast}|_{i})\\), \\(g=1, \\cdots, G\\) 2. accept \\(Z_i^\\ast\\) as the i-th element of \\(z_{t+1}\\) with probability \\(\\frac{P\\big(\\mathbf{V}|\\mathbf{Z}^{*},\\mathbf{X},\\beta_{t}\\big)\\phi_{d}\\big(\\mathbf{Z}_{i}^{*},\\mu K_{i},\\sigma_{K}^{2}|_{d}\\big)}{P\\big(\\mathbf{V}|\\mathbf{Z}_{t},\\mathbf{X},\\beta_{t}\\big)\\phi_{d}\\big(\\mathbf{Z}_{i i};\\mu K_{i},\\sigma_{K}^{2}|_{d}\\big)}\\) MH 스텝 이용해서 \\(\\beta_{t+1}\\) 업데이트 1. \\(2_{i}^{\\ast}\\sim\\left|l\\right|/|\\bigvee_{i}(\\Sigma_{i},(\\hat{L}_{i}^{\\ast}|_{i})\\), \\(g=1, \\cdots, G\\) 2. accept \\(Z_i^\\ast\\) as the i-th element of \\(z_{t+1}\\) with probability \\(\\frac{{\\cal P}\\big(\\mit{W}\\lbrack\\mathbf{Z}_{t+1},\\mit{X},\\beta^{*}\\big)\\phi_{\\rho}\\big(\\beta^{*};\\xi,\\mit{\\mit\\Psi}\\big)}{{\\cal P}\\Big(\\mit{\\bf V}\\lbrack\\Z_{t+1},\\mit{\\bf X},\\beta_{t}\\big)\\phi_{\\rho}\\big(\\beta_{t};\\xi,\\mit{\\mit\\Psi}\\Big)}.\\) Update λ, µg, σ, g, Ki using full conditional posterior distributions Pros and Cons 장점: 성능이 더 나음 단점: 더 복잡함 7.8.2.2 Identifiability of Positions and Cluster Labels 2개의 identifiability issues invariant property of distance label-switching problem common problem in a Bayesian Mixture Model Cluster label does not change the likelihood -&gt; changing the cluster label will make problems for making inferences of individuals in a cluster. Solution: Minimize, expectation of loss function, which is Bayes risk Likelihood 에만 의존해서 cluster化 성능 평가하면 문제생김. positions 과 cluster labels 들의 Non-identifiabilities 문제. Likelihood 는 이하에 불변. latent positions 들의 반사, 회전, translation cluster 들의 relabeling. 이는 Label switching problem[^cluster 의 label 을 permute 하는 것은 Likelihood 에 변화를 가져오지 않지만, obs 들을 그룹에 넣는 과정에서 우리가 문제를 겪게 됨. Likelihood 는 같지만 label 이 다른 순간 이건 cluster 의 구성이 다른 것과 동일하니까.] 으로 이어짐. 이를 해결하기 위한 방법으로 Minimizing Bayes risk 가 제시. estimate 되는 Bayes risk4 를 최소화하는 actor 들의 position (i.e., reference point) 탐색 latent position 의 posterior 추출값 (draw), (i.e. 모든 다른 MCMC 샘플들) 를 Procrustes Transform 하고 동일한 Transformation Matrix 를 사용하여, cluster mean 와 Cov 를 transform Estimate 된 Bayes risk 를 최소화하는 cluster membership probability 을 탐색 에 대한 계산은 이하와 같다. 이를 통하여 cluster 의 갯수를 정한다. Bayesian estimation - 이하의 equation, 즉 higehest posterior probability 를 가지는 model 을 선택. $$ \\[\\begin{array} &amp;P(Y,{\\hat{Z}}|G) &amp;= &amp;&amp;\\underbrace {\\int P(Y|{\\hat{Z}},X,\\beta)p(\\beta)d\\beta} _{\\substack{\\text{integrated likelihood for the logistic regression} \\\\ \\approx BIC_{lr}(\\text{logistic regression})}} &amp;&amp;\\cdot &amp;&amp;\\underbrace {\\int P({\\hat{Z}}|\\theta)p(\\theta)d\\theta} _{\\substack{\\text{integrated likelihood for the mixture model} \\\\ \\approx BIC_{mbc}(\\text{mixture model})}} \\\\ BIC &amp;= &amp;&amp;BIC_{lr} &amp;&amp;+ &amp;&amp;BIC_{mbc} \\\\ &amp;= &amp;&amp; \\left \\{ 2 \\log \\Big [ P \\Big \\{ Y \\Big | \\hat Z , X, \\hat \\beta ( \\hat Z ) \\Big \\} \\Big ] - d_{logit} \\log (n_{logit}) \\right \\} &amp;&amp; + &amp;&amp; \\left\\{ 2 \\log \\Big [ P \\Big \\{ \\hat Z \\Big | \\hat \\theta ( \\hat Z ) \\Big \\} \\Big ] - d_{mbc} \\log (n) \\right \\} \\end{array}\\] $$ \\(d_{logit} =\\) # of parameters in the logistic regression \\(n_{logit} =\\) # of ties in data (# of edges) \\(d_{mbc} =\\) # of parameters in the clustering model Bayes risk = Expecation of loss function. 이때 loss function 으로는 Kullback-Leibler loss 를 사용한다.↩︎ "],["additive-and-multiplicative-effects-network-models.html", "7.9 Additive and Multiplicative Effects Network Models", " 7.9 Additive and Multiplicative Effects Network Models 7.9.1 Introduction 7.9.2 Social Relations Regression Social Relations Regression additive Effect Model (ANOVA Model): iid model Social Relations Model Social Relations Covariance Model Social Relations Regression Model Multiplicative Effect Models additive &amp; Multiplicative Effect Models $$ \\[\\begin{align} &amp;y_{i,j} &amp;&amp;= &amp;&amp;\\; \\; \\; \\; \\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{AEM, ANOVA model} \\\\ &amp;y_{i,j} &amp;&amp;= &amp;&amp;\\; \\; \\; \\; \\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{SRM} \\\\ &amp;y_{i,j} &amp;&amp;= \\beta&#39; \\mathbf x_{i,j} &amp;&amp; +\\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{SR Regression M} \\\\ &amp;y_{i,j} &amp;&amp;=\\beta^{T} \\mathbf x_{i,j} &amp;&amp; +\\mathbf u_{i}^{T} \\mathbf v_{j} &amp;&amp; +{a}_{i}+b_{j}+\\epsilon_{i,j} \\tag{AMEM} \\\\ &amp; \\tag{Random effects AMEM} \\\\ &amp;V &amp;&amp;=M(X,\\beta)+U V^{T} &amp;&amp; &amp;&amp; +a{1}^{T}+1b^{T}+E \\tag{Gibbs Sampling for the AME} \\end{align}\\] $$ 7.9.2.1 additive Effect Model (iid model) $$ \\[\\begin{alignat}{2} y_{i,j} &amp;= &amp;&amp;\\; \\; \\; \\; \\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{AEM, ANOVA model} \\\\ y_{i,j} &amp;= &amp;&amp;\\; \\; \\; \\; \\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{SRM} \\\\ y_{i,j} &amp;= \\beta&#39; \\mathbf x_{i,j} &amp;&amp; +\\mu &amp;&amp; +a_{i}+b_{j}+\\epsilon_{i,j} \\tag{SR Regression M} \\end{alignat}\\] $$ \\(a_i\\): sender effect (sociomatrix 의 rowmean) \\(b_j\\): receiver effect (sociomatrix 의 colmean) additive Effect Model (iid model) Social Relations Model (SRM) Social Relations Regression Model Goal Consider Dependency sender-receiver correlations (dyadic correlations) Quantify the association between a particular dyadic variable and some other dyadic or nodal variables5 (2) Limitation: Unable to represent higher-order network patterns (lack of fit) SRM 에서 \\(a_i &#39; s , b_j s , \\epsilon_{ij} &#39;\\) 들은 mean-zero random variable 들. with effects otherwise being independent 이며 이하를 따름. \\[ V a r\\left[\\begin{pmatrix}a_i \\\\ b_j\\end{pmatrix}\\right]=\\Sigma={\\left(\\begin{array}{l l}{\\sigma_{a}^{2}}&amp;{\\sigma_{a b}}\\\\ {\\sigma_{a b}}&amp;{\\sigma_{b}^{2}}\\end{array}\\right)} \\\\ V a r\\left[\\begin{pmatrix}\\epsilon_{i,j} \\\\ \\epsilon_{j,i}\\end{pmatrix}\\right]=\\sigma^{2} \\begin{pmatrix}1 &amp; \\rho \\\\ \\rho &amp; 1\\end{pmatrix} \\] \\(a_i\\): sender effect (row means of the sociomatrix), \\(b_j\\): receiver effect (column means of the sociomatrix) 목표: dependency 고려 Social Relations Covariance Model: You decompese the variance of \\(y_{ij}\\) into three parts: \\[ Var(y_{ij}) = \\underbrace{\\sigma^2_a }_{\\text{variance of sender}} + \\underbrace{\\sigma^2_b }_{\\text{variance of receiver}} + \\underbrace{\\sigma^2 }_{\\text{common variance}} \\] \\(Cov(y_{ij}, y_{ik}) = \\sigma_a^2\\): within-row covariance \\(Cov(y_{ij}, y_{kj}) = \\sigma_b^2\\) : within-col covariance \\(Cov(\\underbrace{y_{ij}, y_{jk}}_{i\\rightarrow j \\rightarrow k}) = \\sigma_{ab}\\): row-col covariance \\(Cov(\\underbrace{y_{ij}, y_{ji}}_{ i\\rightarrow j \\\\j \\rightarrow i}) = 2 \\sigma_{ab} + \\rho \\sigma^2\\): row-col covariance + reciprocity with all other Cov b/w elements of \\(\\bf Y\\) being 0. Social Relations Regression Model: linear regression model을 SRM 의 covariance structure 를 사용하여 combine. 그 결과값이 가장 위의 수식. 여기서 \\(x_{ij}\\) 는 regressor 들의 \\(p\\)-dimensional vector 이며, \\(\\beta\\) 는 estimate 된 regression coefficient 들의 vector. 한계: high-order network pattern 을 드러내는 것은 불가능. (lack of fit) 7.9.3 Multiplicative Effects Models \\[ \\begin{alignat}{2} y_{i,j} &amp;=\\beta^{T} \\mathbf x_{i,j} &amp;&amp; +\\mathbf u_{i}^{T} \\mathbf v_{j} &amp;&amp; +{a}_{i}+b_{j}+\\epsilon_{i,j} \\tag{AMEM} \\\\ \\tag{Random effects AMEM} \\\\ V &amp;=M(X,\\beta)+U V^{T} &amp;&amp; &amp;&amp; +a{1}^{T}+1b^{T}+E \\tag{GS for AME} \\end{alignat} \\] Goal Multiplicative Effect Models Random effect AME model Capture higher-order network patterns prevent overfitting &amp; provide summaries of certain network dependencies \\[ \\begin{align} y_{i,j} &amp;=\\beta^{T} \\mathbf x_{i,j} +\\mathbf u_{i}^{T} \\mathbf v_{j} +{a}_{i}+b_{j}+\\epsilon_{i,j}, &amp;&amp; \\forall i&lt;j: \\left(\\epsilon_{i,j},\\epsilon_{j,i}\\right) \\overset{iid}{\\sim} N_{2} \\Bigg( \\mathbf 0 , \\; \\sigma^2 \\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix} \\Bigg) \\tag{AMEM} \\end{align} \\] Random effects AME model \\[ \\begin{alignat}{2} &amp;(\\mathbf u_1, \\mathbf v_1), \\cdots, (\\mathbf u_n, \\mathbf v_n) &amp;&amp;\\overset{iid}{\\sim} N_{2r}(\\mathbf 0 , \\Phi) \\\\ &amp; (a_1 , b_1), \\cdots, (a_n , b_n) &amp;&amp;\\overset{iid}{\\sim} N_2(\\mathbf 0 , \\Sigma) \\tag{Random effects AME} \\end{alignat} \\] Transformation Models for Non-Gaussian: Continuous dyadic variable Discrete dyadic variable Gaussian AME model Binary (not friend / friend) Probit AME model Ordinary (dislike / neutral / like) Ordinal Probit AME model continuous 에는 binary, ordinary 구분 없음. ERGM Latent Variable Models(latent network)(Stochastic Block Model) AME global properties evaluating specific global network patterns of interest simply by including an appropriate sufficient statistic in the model Description of local, micro-level patterns of relationships among specific nodes global patterns via estimating the parameters \\((\\beta,\\underbrace{\\Sigma}_{\\substack{\\text{Cov of}\\\\\\text{mean effect}}}, \\underbrace{\\Psi}_{\\substack{\\text{Cov of}\\\\\\text{high-order}}}, \\underbrace{\\sigma^2 , \\rho}_{\\text{error}})\\) local properties local patterns via estimating the node-specific effects \\((\\underbrace{a_i , b_i}_{\\text{mean effects}} , \\underbrace{\\bf u_i , \\bf v_i}_{\\substack{\\text{high-order}\\\\ \\text{dependency}}})\\) Comparisons b/w ERGMs vs. SRM \\[ P(Y)\\sim\\exp\\left(\\mu\\sum_{i,j}y_{i,j}+\\sum_{i} \\bigg \\{a_{i}\\sum_{j}y_{i,j}+b_{i}\\sum_{j}y_{j,i} \\bigg \\}+\\rho\\sum_{i,j}y_{i,j}y_{j,i}\\right) \\tag{p2 model} \\] p1 model: \\(\\sum_{i,j}y_{i,j}\\): sufficient statics, the total # of ties \\(\\sum_{i,j}y_{i,j}y_{j,i}\\): the # of reciprocated ties \\(\\sum_{j}y_{i,j}\\), \\(\\sum_{j}y_{j,i}\\): in- &amp; out-degrees SRM \\(\\mu\\): overall mean of the relations \\(\\rho\\): dyadic correlations \\(a_i , b_i\\): Heterogeneity in row &amp; col means p2 model extends the p1 model by including regressors (as does the SRRM). Treats the node-level parameters \\(a_i\\) and \\(b_i\\) as potentially correlated random effects (as do SRM and SRRM). P1 model is unable to describe more complex forms of dependency such as transitivity or clustering. P2 model or SRRM can represent some degree of higher-order dependency, still exhibit lack-of-fit and so more complex models are desired. ERGMs approach to describe higher-order dependencies is to include additional sufficient statistics. However, it can lead to model degeneracy - How to solve? Constraining the parameter space Finding alternative summary statistics AME Approach to Represent Complex Patterns AME 는 low-rank Matrix \\(UV&#39;\\) 를 사용하여 complex pattern 표시. \\(Y_{n \\times n}\\) 는 \\(U_{n \\times r}\\) 과 \\(V_{n \\times r}\\) 를 사용하여 \\(UV&#39;\\) 의 형으로 arbitrary degree of precision 으로 approximate 가능. AME model 은 observed network 의 model-based low-dimensional 표현 (representation) 을 제공함. Limitation of Multiplicative Effects Approach Not all higher-order moments can be represented by the random effects model for the multiplicative effects (Gaussian random effect model) 예를 들어 dimenison \\(r=1\\) 일 때, \\[ \\begin{alignat}{2} &amp;E[\\gamma_{i,j}\\gamma_{j,k}\\gamma_{k,l}\\gamma_{l,i}&amp;&amp;]&amp;&amp;=tr(\\Psi^4_{uv}) = \\sigma^4_{uv} \\\\ &amp;E[\\gamma_{i,j}\\gamma_{j,k}\\gamma_{k,l}&amp;&amp;] &amp;&amp;=tr(\\Psi^3_{uv}) = \\sigma^3_{uv} \\end{alignat} \\] These moments are not separately estimable, because both completely determined by the single parameter \\(\\sigma_{uv}\\). To separately estimate such moments requires the higher dimension, which is very tricky. Pros of AME Models Multiplicative effects matrix \\(UV&#39;\\) 는 sociomatrix \\(Y\\) 의 reduced-rank representation 을 생산. \\(\\Psi\\) 의 estimate 는 \\(u_i\\) 와 \\(v_i\\) 간의, network dependency 를 유도하는 것인, node heterogeneity 에 걸친 summary 를 제공한다. multiplicative effect 는 simple random effect model 보다 훨씬 더 넓은 range 의 pattern 을 설명해내는 것이 가능. Cons of AME models \\(\\Psi\\) 의 estimate 는 imcomplete summary. 이는 오직 node heterogeneity effect 에 대한 (across) Covariance 를 설명해낼 뿐이다. potential network dependency 에 대한 제한된 summary 만을 제공. higher-dependency 는 다소 모호 (opaque) 한 채로 남음. 목적이 특정한 종류의 higher-order network dependencies 를 측정하고자 하는 것이라면, ERMGs 쪽이 더욱 straightforward. Block Model Latent Distance Model Assumption Each node belongs to an unobserved latent class or “block” “stochastic equivalence” Each node has some unobserved location in a latent Euclidean “social space” Relations b/w nodes two nodes are determined (statistically) by their block membership within-group density of ties is lower then b/w-group density the strength of a relation (prob of tie) b/w two nodes is decreasing in the distance b/w them in latent space how to define Membership members of the same group with the same distribution of relationships to other nodes closeness b/w two nodes useful when there exists subgroups of nodes with strong withi-group relationships latent 와 비교시, SBM 은 large number of block 요구 SBM 과 비교시, latent에서 social space 에서 같은 위치에 존재할 경우, 2개의 node 는 stochastically equivalent Limitation of Latent Variable Models Real networks exhibit combinations of stochastic equivalence and transitivity in varying amounts Providing incomplete description of the heterogeneity across nodes: How to solve? Latent variable models based on multiplicative effects Represent both types of network patterns Generalization of both models 7.9.4 Inference via Posterior Approximation Gibbs Sampling for the AME \\[ V =M(X,\\beta)+U V^{T} +a{1}^{T}+1b^{T}+E \\tag{GS for AME} \\] 7.9.5 Discussion and Example with R Combines a linear regression model with the covariance structure of the SRM as follows, where \\(\\mathbf x_{i,j}\\) is \\(p\\)−dimensional vector of regressors and \\(\\beta\\) is a vector of regression coefficient to be estimated.↩︎ "],["stochastic-block-models.html", "7.10 Stochastic Block Models", " 7.10 Stochastic Block Models 7.10.1 Stochastic Block Model Latent Class Model for each actor (node), we assign group memberships two components in the block model, the vector of grop membership : \\(z\\) conditional on the group membership, the block matrix represents the edge probability of two nodes $$ \\[\\begin{align} &amp; \\begin{bmatrix} 0.8 &amp; 0.05 &amp; 0.02 \\\\ 0.05 &amp; 0.9 &amp; 0.03 \\\\ 0.02 &amp; 0.03 &amp; 0.7 \\end{bmatrix} &amp;&amp; \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} \\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{32} \\end{bmatrix} \\end{align}\\] $$ \\(b_{11}\\) 에 부여된 확률 0.8: the connection prob within group 1 \\(b_{22}\\) 에 부여된 확률 0.9: the connection prob within group 1 \\(b_{11}\\) 에 부여된 확률 0.05: the connection prob b/w group 1 and group 2 ※ notations: \\(Y\\): adjacency Matrix \\(K\\): the total membership groups (\\(&lt;n\\)) \\(Z_i\\): \\(k\\)-vector. node \\(i\\) 가 속하게 될 group 에만 \\(1\\) 부여하고 이외에는 모두 \\(0\\). \\(Z := (z_1 , \\cdots, z_n)&#39;\\) \\(C\\): \\(k \\times k\\) block matrix entry \\(C_{ij} \\in [0,1]\\): prob of occurence of a edge from a node in \\(g\\) block Matrix \\(C\\) 라는 발상은, block membership 이 given 일 때, edge 들이 conditionally independent 라는 것. \\(\\Rightarrow\\) 1. \\(Y_{ij} \\sim\\) Bernoulli distribution with success probability \\(z_i &#39; C z_j\\) 2. independent of \\(Y_{kl}\\) for \\((i,j) \\not = (k,l)\\), given \\(z_i\\) and \\(z_j\\). 7.10.1.1 Likelihood function \\[ P (Y \\Big | Z, C) = \\prod_{i&lt;j} P (y_{ij} \\Big | Z, C) = \\prod_{i&lt;j} \\Bigg [ \\Big (z_i &#39; Cz_j \\Big )^{y_{ij}} \\Big (1-z_i &#39; Cz_j \\Big )^{1-y_{ij}} \\Bigg ] \\tag{1} \\] In real data, \\(z\\) and \\(C\\) are unknown. \\(\\Rightarrow\\) we need additional assumptions. \\(\\underbrace{1}_{2}\\) 1. \\(z_i\\) is independent of \\(z_j\\). 2. \\(P(z_{i \\underbrace{k}_{\\text group}} =1) = \\theta_k\\), \\(\\sum_{j=1}^k \\theta_k = 1\\) \\(\\Rightarrow\\) the latent group \\(z_p\\) follows multinomial distribution with \\(\\theta\\), i.e., \\[ \\pi(z \\Big | \\theta) = \\prod_{i=1}^n z_i &#39; \\theta = \\prod_{k=1}^K \\theta_k^{n_k} \\tag{2} \\] , where \\(n_k\\) is the total number of nodes that belongs to group \\(K\\). In a Bayesian inference, we can assign \\(Dirichlet\\) prior \\(\\alpha, \\cdots, \\alpha\\) \\(\\pi(\\alpha) \\sim Gamma (a,b)\\). 7.10.1.2 Inference By combining equation (1) and (2), we can make an inference by the frequentist way using EM algorithm. 7.10.1.3 Bayesian we need to assign prior for \\(C \\sim BETA\\), \\(C_{ij} \\sim BETA(A_{ij}, B_{ij})\\), where \\(A, B\\) are \\(k \\times k\\) Matrix. 7.10.1.4 Posterior Distribution $$ \\[\\begin{alignat}{2} \\pi(Z , \\theta, C , \\alpha \\Big | Y) &amp; \\propto f(Y \\Big | Z , \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(Z , \\theta, C , \\alpha) \\\\ &amp; = f(Y \\Big | Z , \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(Z \\Big | \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(\\theta \\Big | C , \\alpha) &amp;&amp; \\cdot \\pi(C, \\alpha) \\\\ &amp; = f(Y \\Big | Z , C) &amp;&amp; \\cdot \\pi(Z \\Big | \\theta) &amp;&amp; \\cdot \\pi(\\theta \\Big | \\alpha) &amp;&amp; \\cdot \\pi(C) \\cdot \\pi(\\alpha) \\\\ &amp; \\propto \\prod_{i&lt;j} \\Big[ (z_i&#39; C z_j)^{y_{ij}}(1- z_i&#39; C z_j)^{1-y_{ij}} \\Big] &amp;&amp; \\cdot \\prod_{k=1}^K \\theta_k^{n_k} \\Bigg[\\Gamma(k \\alpha) \\Big \\{ \\sum^k_{i=} \\theta_k = 1 \\prod_{i=1}^k \\frac{\\theta_z^{\\alpha-1}}{\\Gamma(\\alpha)}\\Big \\} \\Bigg] &amp;&amp; \\cdot\\prod^k_{i&lt;j} \\Big[ C_{ij}^{A_{ij}-1} (1-C_{ij})^{B_{ij}-1} \\Big] &amp;&amp; \\cdot \\alpha^{a-1} e^{-b \\alpha} \\end{alignat}\\] $$ \\(\\Rightarrow\\) Computation is veery straightforward via MCMC. 7.10.2 Mixed Membership Block Model (MMBM) 상기의 SBM 에서, 각 actor 는 오직 1개의 membership 을 가짐. 이러한 SBM 의 제약을 완화 (alleviate) 하기 위해 MMBM 이 제언되었다. 해당 모델에서는 특정 membership 에 1을 부여하는 것이 아니라, 각 membership 발생 여부에 prob 을 할당하여 다중 membership 을 각 ndoe 에 할당한다. \\[ \\begin{align} z_i &amp;= (1, 0, 0) \\tag{SBM} \\\\ \\theta_i &amp;= (0.7, 0.2, 0.1) \\tag{MMSB} \\end{align} \\] 이때 MMSB 의 vector 의 각 항은 각각 group 1, group 2, group 3 일 prob. \\(\\theta_i\\): weights or probabilities in the groups that have to be non-negative and sum to 1. For each dyad \\(y_{ij}\\), a latent membership \\(z_{ij}\\) is drawn from the multinomial distribution with \\(\\theta_p\\). \\(\\pi(z \\Big | \\Theta) = \\prod\\limits_{i \\not = j} (z_{ij}&#39;\\theta_i x z_{ji}&#39;\\theta_j)\\), where \\(\\Theta := (\\theta_1 , \\cdots, \\theta_k)\\) is \\(n \\times k\\) Matrix of membership probability. The likeliood is \\(f(Y \\Big | Z, C) = \\prod\\limits_{i&lt;j} \\Big[ (z_{ij}&#39; C z_{ji})^{y_{ij}}(1- z_{ij}&#39; C z_{ji})^{1-y_{ij}} \\Big]\\). The goal of inference estimating not \\(z\\) but the mixed memberships \\(\\Theta\\). 7.10.2.1 Degree-corrected SBM use Poisson Model. \\(y_{ij}\\): the number of edges from dyad (i,j) following a Poisson dist \\(C_{ij}\\): the expected number of edges from a node in group \\(i\\) to a node in group \\(j\\) The likelihood is \\[ \\pi(Y_{ij} \\Big | Z, C) = (Y_{ij}!)^{-1} \\exp \\Big \\{ (-z_i &#39; C z_j ) (z_i &#39; C z_j )^{y_{ij}} \\Big \\} \\tag{3} \\] In a large sparse graph, where the edge probability equals the expected number of edges, DC-SBM is asymptotically equivalient to the Bernoulli counterpart in equation (3). \\(\\phi_p\\): the ratio of node \\(i\\)’s degree to the sum of degrees in node \\(i\\)’s group. Under constraint \\(\\sum_{i=1}^n \\phi_i \\cdot I \\Big ( z_{ik} = 1 \\Big) =1\\), then equation (3) becomes \\[ f(y_{ij} \\Big | Z, C, \\phi) = (y_{ij}!)^{-1} \\exp \\Big \\{ (- \\phi_i \\phi_j z_i &#39; C z_j ) (\\phi_i \\phi_j z_i &#39; C z_j )^{y_{ij}} \\Big \\} \\] which is DC-SBM. \\(\\Rightarrow\\) SBM ignores the variation of node degrees in a real network. With DC-SBM, we can make correction better. "],["high-dimension.html", "Chapter 8 High Dimension ", " Chapter 8 High Dimension "],["introduction-4.html", "8.1 Introduction", " 8.1 Introduction "],["concentration-inequalities.html", "8.2 Concentration inequalities", " 8.2 Concentration inequalities 다양한 경우에 랜덤변수의 tails의 bound, 혹응 랜덤변수가 mean이나 median에 close 임을 보이기 위한 쌍방향 부등식의 획득은 괘 매력적임. 8.2.1 Motivation \\(X_1 , \\cdots, X_n \\overset{iid}{\\sim}N(\\mu, \\sigma^2)\\) 일 경우에 \\(\\bar X \\sim N(\\mu, \\frac{\\sigma^2}{n})\\) 이며, 노멀분포의 tail bound (혹은 Chernoff bound 에 의해) 는 이하를 생산함: \\[ \\forall t \\ge 0 : P( | \\bar X - \\mu | \\ge t) \\le 2 \\exp \\left( - \\frac{nt^2}{2\\sigma^2}\\right) \\] 따라서 샘플평균 \\(\\bar X\\)가 population 평균 \\(\\mu\\)와 크게 떨어져있을 확률은 빠르게 decay. 이를 응용해 finite 숫자의 샘플에 대해 과하게 많은 수의 가정 없이도 랜덤 샘플에 대한 bound 를 획득해보자. 8.2.2 From Markov to Chernoff Markov’s Inequality given \\(X \\ge 0\\) (nonnegative) 이며 \\(|E(X)| &lt; \\infty\\) (finite mean), 이하가 성립한다. \\[ \\forall t \\ge 0: P(X\\ge t) \\le\\frac{E(X)}{t} \\] Chebyshev’s inequality \\(Var(X) &lt; \\infty)\\) (finite variance) 일 때 이하가 성립. 이는 Markov 부등식을 nonnegative 랜덤변수 \\((X-\\mu)^2\\) 에 적용한것. \\[ \\forall t \\ge 0: P(|X - \\mu| \\ge t) \\le\\frac{Var(X)}{t^2} \\] 이는 즉 \\(Var\\) 가 작을 때 \\(X\\) 가 \\(\\mu\\) 와 가깝다는 것을 보장한다는 점에서 가장 기초적인 contentration ineqaulity. Markov 와 Chebyshev 는 sharp 이며, 이는 곧 이 둘이 일반적으로는 imporve 될 수 없다는 것을 뜻함. Polynomial Markov \\(X\\)가 order \\(k\\)의 central moment 를 가진다면, 랜덤변수 \\(|X-\\mu|^k\\) 에 Markov 부등식 적용하면 이하를 생산한다. 모든 integer \\(k=1,2,\\cdots\\)에 대해 order \\(k\\) 의 central moment 가 존재한다면 2번째 ineq 도 성립. $$ \\[\\begin{alignat}{2} \\forall t \\ge 0: P(|X-\\mu| \\ge t) &amp;\\le &amp;&amp;\\frac{E \\Big [ |X-\\mu|^k \\Big]}{t^k} \\\\ &amp;\\le \\lim_{k=0,1,2…} &amp;&amp;\\frac{E \\Big [ |X-\\mu|^k \\Big]}{t^k} \\tag{2.1} \\end{alignat}\\] $$ Chernoff bound 랜덤변수 \\(X\\) 가 0의 neighborhood 에서 mgf 를 가진다면, 즉 \\(\\exists b&gt;0, \\forall \\lambda\\le|b|: \\varphi(\\lambda) = E \\Big\\{ e^{\\lambda(X-\\mu)}\\Big \\}\\) 라고 가정하자. 이때 \\(\\forall \\lambda \\in [0, b]\\) 에서 랜덤변수 \\(Y = e^{\\lambda(X-\\mu)}\\) 에 대해 Markov 부등식을 적용할 수 있으며, 이에 의해 upper bound 를 획득할 수 있다. 우리가 선택하는 \\(\\lambda\\)를 Chernoff bound 에 최적화 시키면 (2.2)와 같이 나온다. $$ \\[\\begin{alignat}{2} &amp;P(X-\\mu &gt; t) = P(e^{\\lambda(X-\\mu)} \\ge e^{\\lambda t}) &amp;&amp;\\le \\frac{E(e^{\\lambda(X-\\mu)})}{e^{\\lambda t}} \\\\ \\log &amp;P(X-\\mu &gt; t) &amp;&amp;\\le \\inf_{\\lambda \\in [0, b]} \\left \\{ {\\log E(e^{\\lambda(X-\\mu)})} - {{\\lambda t}} \\right\\} \\end{alignat}\\] $$ 8.2.3 sub-Gaussian random variables 모든 랜덤변수 \\(X\\)에 대해 그것의 mgf 가 \\(\\forall \\lambda \\in \\mathbb R : E \\left \\{ e^{\\lambda (X-\\mu)} \\right\\} \\le e^{\\frac{\\sigma^2 \\lambda^2}{2} }\\) 를 만족하면 특정 tail bound 가 성립된다. 이를 응용하면 아래의 개념을 얻을 수 있다. Definition 8.1 (sub-Gaussian) 평균이 \\(\\mu = E(X)\\) 인 랜덤변수 X에 대해 \\(\\exists \\sigma &gt;0\\) 에 대해 이하가 성립하면 이는 sub-Gaussian 이며 \\(X \\in SG(\\sigma^2)\\). \\[ \\forall \\lambda \\in \\mathbb R : E \\left \\{ e^{\\lambda (X-\\mu)} \\right\\} \\le e^{\\frac{\\sigma^2 \\lambda^2}{2} } \\] \\(sigma\\) 는 sub-Gaussian 패러미터 \\(sigma^2\\) 는 variance proxy symmetry 해보면 \\(X \\in SG(\\cdot)\\) 일 경우에만 \\(-X \\in SG(\\cdot)\\). 이하의 값은 Example 2.1 과 동일하며, 따라서 모든 SG 랜덤변수는 이하의 ineq 를 만족한다. \\[ \\forall t \\ge 0: P(|X-\\mu| \\ge t) \\le 2 e^{-\\frac{t^2}{2 \\sigma^2}} \\tag{2.4} \\] Jensen’s Ineq convex 함수 \\(g: \\mathbb R \\mapsto \\mathbb R\\) 에 대해 \\(E \\Big \\{ g(X) \\Big \\} \\ge g \\Big \\{ E(X) \\Big \\}\\). g가 concave 면 逆. Proof: \\(\\mu = E(X)\\) 로 하고, \\(L_\\mu (x) = a + bx\\) 가 \\(\\mu\\) 에서의 \\(g\\) 에 대한 tangent line, i.e. \\(L_\\mu (\\mu) = g(\\mu)\\). convexity 에 의해 \\(\\forall x:g(x) \\ge L_\\mu (x)\\). 따라서 \\[ E[g(X)] \\ge E[L_\\mu (X)] = E(a+bX) =a+b\\mu= L_\\mu (\\mu) = g(\\mu) \\] 8.2.4 Properties of sub-Gaussian random variables \\(X \\in SG(\\cdot)\\) 일 경우 \\(Var(X) \\le \\sigma^2\\) Hoeffding’s lemma: almost surely 하게 \\(a \\le X-\\mu \\le b\\) 한 실수 \\(a, b\\)가 있다면, \\(X \\in SG \\Big ( (\\frac{b-a}{2})^2\\Big )\\). \\(X \\in SG(\\sigma^2)\\) 이며 \\(Y \\in SG(\\tau^2)\\) 일 경우, $a R: aX SG ( a^2 ^2 ) \\(X+Y \\in SG \\Big ( (\\sigma + \\gamma)^2\\Big )\\) if \\(X \\perp Y\\), then $X + Y SG ( ^2 + ^2 ) Proof for each. \\(\\forall \\lambda \\in \\mathbb R: E \\left \\{ e^{\\lambda (X-\\mu)} \\right\\} \\le e^{\\frac{\\sigma^2 \\lambda^2}{2} }\\), Taylor’s thm 에 의해 이하가 성립. 쌍방을 \\(\\labmda^2 &gt;0\\) 으로 나누고 \\(\\labmda \\rightarrow 0\\) 를 취하는 것으로 (1) 성립. \\[ 1 + \\lambda \\underbrace{E[(X - \\mu)]}_{=0} + \\frac{\\lambda^2}{2} \\underbrace{E[(X - \\mu)^2]}_{=Var(X)} + o(\\lambda^2) \\le 1 + \\frac{\\lambda^2 \\sigma^2}{2} + o(\\lambda^2 ) \\] \\(\\forall \\lambda \\in \\mathbb{R}:E [e^{\\lambda\\left(x-\\mu\\right)}]\\ \\le {\\exp\\left \\{ \\frac{\\lambda^{2}\\left(b-a\\right)^{2}}{8} \\right \\}}\\) 인 것만 보이면 됨. WLOG, \\(\\mu=0\\) 임을 가정. \\(\\forall \\lambda \\in \\mathbb R :e^{\\lambda x}\\) 는 x의 convex 이므로, $ a x b :e{x}e{a}+e^{b}$. 따라서 \\(\\mu=0\\) 를 가정하면, \\(\\mathbb{R}[e^{\\lambda X}]\\leq\\frac{b}{b-a}e^{\\lambda a}-\\frac{a}{b-a}e^{\\lambda b}\\). 이때 \\(h = \\lambda(b-a)\\) 와 \\(p = -\\frac{a}{b-a}\\) 라고 하고, \\(L(h) = -hp \\log(1-p + pe^h)\\) 라고 하자. 이때 우리는 이하가 증명된다. \\({\\frac{b}{b-a}}e^{\\lambda a}-{\\frac{a}{b-a}}e^{\\lambda b}\\equiv e^{L(h)}\\). 이에 더해 \\(L(0) = L&#39;(0) = 0\\) 이며 \\(\\forall h: L&#39;&#39;(h) \\le \\frac{1}{4}\\). 따라서 Taylor expansion 에 의해 \\(L(h)\\le\\frac{1}{8}h^{2}=\\frac{1}{8}\\lambda^{2}(b-a)^{2}\\) 이며 따라서 \\(E [e^{\\lambda\\left(x-\\mu\\right)}]\\ \\le {\\exp\\left \\{ \\frac{\\lambda^{2}\\left(b-a\\right)^{2}}{8} \\right \\}}\\). 는 SG 랜덤변수의 정의에 의해 trivial. (b) 를 증명하자. \\(E(X) = E(Y)=0\\) 임을 가정. 이때 이하가 도출된다. \\[ \\mathbb{E}[e^{\\lambda(X+Y)}]=\\mathbb{E}[e^{\\lambda X}e^{\\lambda Y}]\\stackrel{(\\mathrm{i})}\\leq\\ \\left(\\mathbb{E}[e^{\\lambda p X}]\\right)^{1/p}\\left(\\mathbb{E}[e^{\\lambda q Y}]\\right)^{1/q} \\\\ \\stackrel{(ii)}{\\le}\\,e^{\\frac{\\lambda^{2}p^{2}\\sigma^{2}}{2}\\times\\frac{1}{p}+\\frac{\\lambda^{2}q^{2}\\tau^{2}}{2}\\times\\frac{1}{q}} \\\\= e^{\\frac{\\lambda^{2}}{2}(p\\sigma^{2}+q\\tau^{2})}\\ \\\\ \\stackrel{({iii})}{=}\\,e^{\\frac{\\lambda^{2}}{2}(\\sigma+\\tau)^{2}} \\] Holder 부등식 by condition \\(X\\in SG(\\sigma^2)\\), \\(Y\\in SG(\\tau^2)\\) by letting \\(p = \\frac{\\tau}{\\sigma}+1\\), \\(q = \\frac{\\sigma}{\\tau}+1\\). 를 증명하다. \\(X \\perp Y\\) 이므로, \\(E(X) = E(Y)=0\\) 을 가정하는 것으로, 이하에 의해 성립. \\[ \\mathbb{E}[e^{\\lambda(X+Y)}]=\\mathbb{E}[e^{\\lambda X}]\\times\\mathbb{E}[e^{\\lambda Y}]\\leq e^{\\frac{\\lambda^{2}(\\sigma^{2}+\\tau^{2})}{2}}, \\] Holder’s Inquality \\(\\forall p, q &gt;0\\) with \\(\\frac 1 p + \\frac 1 q = 1\\), it holds that \\[ \\mathbb{E}[|X Y|]\\leq||X||p||Y||_{q} = \\{\\mathbb{E}[|X|^{p}]\\}^{1/p} \\cdot \\{\\mathbb{E}[|Y|^{q}]\\}^{1/q} \\] Proof. Observe that \\(\\forall a, b \\ge 0: a b=e^{\\log(a b)}=e^{\\frac{1}{p}p\\log a+\\frac{1}{q}q\\log b}\\le\\frac{1}{p}e^{p\\log a}+\\frac{1}{q}e^{q\\log b}=\\frac{1}{p}a^{p}+\\frac{1}{q}b^{q}\\), 이는 Jensen 에 의해 성립. 이제 \\(a = \\frac{X}{||X||_p}\\), \\(b=\\frac{Y}{||Y||_q}\\) 로 하고 양쪽에 expectation 취하는 것으로 ineq 성립. 8.2.5 Equivalent definitions 이하는 \\(SG(\\sigma&gt;0)\\) 여부에 대해 equivalent. (up to multiplicative constants). Theorem 8.1 zero-mean 인 모든 랜덤변수 \\(X\\) 에 대해 이하의 성질은 equiv. $$ \\[\\begin{align} &amp;\\exists \\sigma &gt;0: &amp;&amp;\\forall \\lambda\\in\\mathbb{E}: &amp;&amp;\\mathbb{E}[e^{\\lambda X}]\\leq e^{\\frac{\\lambda^{2}\\sigma^{2}}{2}} \\\\ \\iff &amp;\\exists c \\ge 0, \\exists Z \\sim N(0, \\tau^2): &amp;&amp;\\forall s \\ge 0: &amp;&amp;\\mathbb{P}(|X|\\geq s) \\le c\\mathbb{P}(|Z|\\geq s) \\\\ \\iff &amp;\\exists \\theta \\ge 0: &amp;&amp;\\forall k = 1, 2, \\cdots: &amp;&amp;\\mathbb{E}[X^{2k}]\\leq{\\frac{(2k)!}{2^{k}k!}}\\theta^{2k} \\\\ \\iff &amp;\\exists \\sigma \\ge 0: &amp;&amp;\\forall \\lambda \\in [0, 1): &amp;&amp;\\mathbb{E}\\left[e^{\\frac{\\lambda X^{2}}{2\\sigma^{2}}}\\right]\\leq{\\frac{1}{\\sqrt{1-\\lambda}}} \\end{align}\\] $$ 8.2.6 Sub-Gaussian random vectors Definition 8.2 이하가 성립할 때 랜덤벡터 \\(X\\) 는 sub-Gaussian with variance proxy \\(\\sigma^2\\). 랜덤벡터 \\(\\mathbf X \\in \\mathbb R^d\\) 가 centered \\(\\forall u \\in \\mathbb R^d, \\; ||u||_2 = 1:\\) 랜덤변수 \\(u&#39;X \\in SG(\\sigma^2)\\). 8.2.7 Hoeffding’s inequality 독립인 SG 랜덤변수의 샘플 평균은 Hoeffding’s inequality 라는 exponential tail bound 를 갖는다. Theorem 8.2 (Hoeffding’s inequality) independent \\(X_i \\in SG(\\sigma_i^2)\\), \\(i = 1, \\cdots, n\\) 들이 각각 \\(E(X_i) = \\mu_i\\) 라고 하자. 그러면 이하가 성립한다. \\[ \\forall t \\ge 0: \\mathbb{P}\\left(\\left|{\\frac{1}{n}}\\sum_{i=1}^{n}X_{i}-\\mu_{i}\\right|\\geq t\\right)\\leq2\\exp\\left(-{\\frac{n^{2}t^{2}}{2\\sum_{i=1}^{n}\\sigma_{i}^{2}}}\\right) \\] 이는 Section 2.4의 property 3 과 inequality (2.4)에 의해 바로 구해진다. Hoeffding bound 는 보통 bounded 랜덤변수의 특별한 경우로서만 논해진다. \\(\\forall i = 1, \\cdots, n: X_i \\in [a,b]\\) 를 가정하자. 이 경우 Hoeffding’s lemma (property 2 in Section 2.4) 에 의해 \\(X_i \\in \\Bigg \\{ \\left( \\frac{b-a}{2}\\right )^2 \\Bigg \\}\\), i.e., \\(\\forall i = 1, \\cdots, n: \\sigma_i^2 = \\frac{(b-a)^2}{4}\\). 따라서 위의 thm에 의해 \\[ \\mathbb{P}\\biggl(\\left|{\\frac{1}{n}}\\sum_{i=1}^{n}X_{i}-\\mu_{i}\\right|\\geq t\\biggr)\\leq2\\exp\\biggl(-{\\frac{2n t^{2}}{(b-a)^{2}}}\\biggr) \\] 8.2.8 Maximal inequalities finite 숫자의 SG 랜덤변수의 maximum 에 대한 tail / expectation bound 를 구할 수 있다. Theorem 8.3 let independent \\(X_1 , \\cdots, X_n \\in SG(\\sigma^2)\\), \\(E(X_i) = 0\\). Then $$ \\[\\begin{align} \\mathbb{E}\\ \\Big[\\operatorname*{max}_{i=1,\\cdots,n}X_{i} \\Big] &amp;\\leq\\sigma{\\sqrt{2\\log n}} \\\\ \\forall t \\ge 0: \\mathbb{P}\\Bigl(\\max\\limits_{i=1,\\cdots, n}X_{i}\\geq t\\Bigr) &amp;\\leq n e^{-{\\frac{t^{2}}{2\\sigma^{2}}}} \\end{align}\\] $$ Proof: $$ $$ 8.2.9 $$ \\[\\begin{align} \\mathbb{E}{\\biggl[}\\operatorname*{max}_{i=1,\\dots,n}X_{i}{\\biggr]}\\ &amp;=\\ \\frac{1}{s}{\\bf E}{\\biggl[}\\log{\\biggl\\{}\\exp{\\biggl(}s{\\max_{i = 1, \\cdots, n}}\\,X_{i}{\\biggr)}\\Biggr\\} \\\\ &amp;\\stackrel{(i)]}{\\leq}~\\frac{1}{s}\\log\\left\\{\\mathbb{E}\\Big[\\exp\\left(s\\operatorname*{max}_{i=1\\ldots n}X_{i}\\right)\\Big]\\right\\} &amp;&amp;= \\frac{1}{s}\\log\\left\\{\\mathbb{E}\\Big[\\operatorname*{max}_{i=1\\ldots n}e^{sX_{i}}\\Big]\\right\\} \\tag{Jensen&#39;s inequality} \\\\ &amp;\\leq\\ {\\frac{1}{s}}\\log\\left\\{\\mathbb{E}\\left[\\sum_{i=1}^{n}e^{s X_{i}}\\right]\\right\\} \\\\ &amp;\\stackrel{\\mathrm{(ii)}}{\\leq}\\;\\frac{1}{s}\\log{\\Big\\{n e^{\\frac{s^{2}\\sigma^{2}}{2}}{\\Big\\}}} &amp;&amp;=\\frac{\\log n}{s}+\\frac{s\\sigma^{2}}{2} \\tag{2} \\end{align}\\] $$ (2): \\(\\forall i = 1, \\cdots, n: \\; E \\Big( e^{s X_i}\\Big) \\le e^{\\frac{s^2 \\sigma^2}{2}}\\) condition 을 사용하면, 1번째는 \\(s=\\sqrt{\\frac{2 \\log n}{\\sigma^2}}\\), 2번째는 union bound 로 성립. 위를 응용하면 이하로 이어짐. 이는 위의 thm에서 abs 씌우고 n 을 2n 으로 바꾼 형. Exercise 8.1 \\[ \\begin{align} \\mathbb{E} \\Big[\\max \\limits_{i=1\\ldots n}|X_{i}|{\\Big\\rbrack} &amp; \\leq\\sigma{\\sqrt{2\\log(2n)}} \\\\ \\forall t \\ge0: \\mathbb{P}{\\Biggl(}\\operatorname*{max}\\limits_{i=1, \\cdots, n}|X_{i}|\\geq t{\\Biggr)} &amp; \\leq2n e^{-{\\frac{t^{2}}{2\\sigma^{2}}}} \\end{align} \\] "],["concentration-inequalities-1.html", "8.3 Concentration inequalities", " 8.3 Concentration inequalities SG 는 꽤 빡빡한 strict 개념. mgf 여부에 기반한 sub-Exponential 은 좀 느슨함. 8.3.1 Sub-exponential random variables Definition 8.3 (sub-Exponential rv) \\(E(X) = \\mu\\), 패러미터 \\(\\exists (\\nu, \\alpha) \\ge 0\\) 에 대해 이하가 성립하면 랜덤변수 \\(X\\)는 sub-exponential. \\[ \\forall |\\lambda| &lt; \\frac{1}{\\alpha}: \\mathbb{E}[e^{\\lambda(X-\\mu)}]\\leq e^{\\frac{\\nu^{2}\\lambda^{2}}{2}} \\] SG 또한 \\(1/0 = +\\infty\\) 로 해석할 경우 \\(\\nu = \\sigma\\), \\(\\alpha=0\\) 인 sub-exponential. 8.3.2 Bernstein’s condition \\(X\\)의 polynomial moment를 조작하는 것으로 \\(X\\)의 sub-exponential 성질을 증명할 수 있다. Definition 8.4 (Bernstein’s Condition) let \\(E(X) = \\mu\\), \\(Var(X) = \\sigma^2 = E(X^2) - \\mu^2\\) 인 랜덤변수 \\(X\\). 이하의 경우와 패러미터 \\(b\\) 에 대해 Bernstein’s Condition 이 성립한다. $$ \\[\\begin{align} &amp;\\Bigg | E \\Big [ (X-\\mu)^k \\Big ] \\Bigg | \\le \\frac{1}{2} k! \\sigma^2 b^{k-2}, &amp;&amp; k = 2, 3, 4, \\cdots \\end{align}\\] $$ 이때, 모든 bounded 랜덤변수 (즉 \\(|X-\\mu| \\le b\\)) 에 대해 Bernstein’s Condition 이 성립한다는 것을 파악해라. \\(X\\)가 Bernstein’s condition 을 만족할 경우, X는 패러미터 \\((\\sqrt 2 \\sigma, 2b)\\) 인 sub-exponential. Theorem 8.4 (Bernstein-type inequality) Bernstein’s Condition 을 만족하는 모든 랜덤변수에 대해 이하가 성립한다. $$ \\[\\begin{align} &amp;\\forall |\\lambda|&lt; \\tfrac{1}{b}: &amp;&amp;E \\Big [ e^{\\lambda(X-\\mu)}\\Big ] &amp;&amp;\\le \\exp \\left( \\frac{\\frac{\\lambda^2 \\sigma^2}{2}}{1-b|\\lambda|}\\right) \\\\ &amp;\\forall t \\ge 0: &amp;&amp;P \\Big( |X-\\mu| \\ge t \\Big) &amp;&amp;\\le 2 \\exp \\left( - \\frac{t^2}{2(\\sigma^2 + bt)}\\right) \\end{align}\\] $$ 8.3.3 McDiarmid’s inequality Theorem 8.5 (McDiarmid’s inequality) 이하의 조건을 만족한다고 하자. 랜덤변수 \\(X_1 , \\cdots, X_n\\) 이 independent 함수 \\(f: \\mathbb R^n \\mapsto \\mathbb R\\): \\(\\Big | f(x_1, \\cdots, x_n) - f(x_1 , \\cdots, x_{k-1} , x_k &#39; , x_{k+1} , \\cdots, x_n) \\Big | \\le L_k\\) (bounded condition) 위 둘이 성립하면 이하가 성립한다. \\[ \\forall t \\ge 0: P \\left( \\Big | f(X_1 , \\cdots, X_n) - E \\big[ f(X_1 , \\cdots, X_n) \\big]\\Big | \\ge t \\right) \\le 2 \\exp \\left( - \\frac{2 t^2}{\\sum_{k=1}^n L_k^2}\\right) \\] 8.3.4 Levy’s inequality 충분히 smooth 한 Gaussian 랜덤변수에 대해, 유사한 concentration inequality 가 존재한다. 이때는 다른 가정이 필요. \\(X_1 , \\cdots, X_n \\overset{iid}{\\sim} N(0,1)\\) 에 대해 $$ \\[\\begin{align} &amp;\\forall x_{1},\\cdot\\cdot\\cdot,x_{n},y_{1},\\cdot\\cdot\\cdot,y_{n}\\in\\mathbb{R} &amp;&amp;: &amp;&amp;\\Bigg |f(x_{1},\\ldots,x_{n})-f(y_{1},\\ldots,y_{n}) \\Bigg | &amp;&amp;\\leq L{\\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^{2}}} \\\\ \\\\ \\Longrightarrow &amp;\\forall t \\ge 0 &amp;&amp;: &amp;&amp;\\mathbb{P} \\left( \\Bigg| f(X_{1},\\cdot\\cdot\\cdot,X_{n})-\\mathbb{E} \\Big [f(X_{1},\\cdot\\cdot\\cdot,X_{n}) \\Big] \\Bigg |\\geq t \\right ) &amp;&amp;\\leq2\\exp\\left(-{\\frac{t^{2}}{2L^{2}}}\\right) \\end{align}\\] $$ 8.3.5 Quadratic form \\(Q\\) 가 symmetric Matrix 일 때 이하를 정의할 수 있다. \\[ \\begin{align} \\|Q\\|_{\\mathrm{{op}}}&amp;=\\operatorname*{sup}_{\\|u\\|_{2}=1}\\|Q u\\|_{2} \\tag{l2-operator norm} \\\\ \\|Q\\|_{\\mathrm{{F}}}&amp;={\\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n}Q_{i j}^{2}}} \\tag{Frobenius norm} \\end{align} \\] 이하는 관련한 thm. Theorem 8.6 (Hanson–Wright inequality) independent, zero-mean, \\(X \\in SG(\\sigma^2)\\) 인 랜덤벡터 \\(X = (X_1 , \\cdots, X_n)&#39; \\in \\mathbb R^n\\) 를 정의하자. 그러면 Hanson–Wright inequality 에 의해 이하와 같이 quadratic form \\(X&#39;QX\\) 의 tail bound 가 정의된다. \\[ \\forall t \\ge 0: \\mathbb{P}\\left( \\Bigg|X^{\\top}Q X-\\mathbb{E}\\Big [X^{\\top}Q X \\Big] \\Bigg|\\geq t\\right)\\leq2\\exp\\left(-\\operatorname*{min}\\left\\{{\\frac{c_{1}t^{2}}{\\|Q\\|_{\\mathrm{F}}}},{\\frac{c_{2}t}{\\|Q\\|_{\\mathrm{op}}}}\\right\\}\\right) \\] 이때 \\(c_1 , c_2\\)는 SG 패러미터 \\(\\sigma\\) 에 의존하는 some constant. 증명 개어려움. decoupling 테크닉을 다수 쓰는데 궁금하면 Vershynin 책 찾아보던가. 8.3.6 The Johnson–Lindenstrauss Lemma Example 3.5 에서 확인한 \\(\\chi^2\\) 의 tail bound 의 응용으로서 유명한 것 중 하나는 “random projection”. \\(d\\) 가 충분히 큰 데이터셋 \\(X_1 , \\cdots, X_n \\in \\mathbb R^d\\) 을 가지고 있다고 치자. 이러한 데이터셋을 보관하는 것은 과한 비용을 요구하므로, 이를 해결하기 위해 우리는 이때 우리는 \\(m \\ll d\\) 인 map \\(F: \\mathbb R^d \\mapsto \\mathbb R^m\\) 을 만든 것이 목적인 “sketching”, 혹은 “random projection” 을 사용한다. 이를 적용한 이후에 우리는 앞서 말한 대용량 데이터셋을 저장하는 대신 \\(\\Big \\{ F(X_1) , \\cdots, F(X_n) \\Big \\}\\) 를 보관하게 된다. 여기서 관건은 오리지널 데이터셋의 본질을 해치지 않는 map \\(F\\) 를 만들어내는 것이다. 특히, 우리는 모든 pair \\((X_i , X_j)\\) 에 대해 이하가 성립하는 것을 목표로 하며, i.e., map 은 모든 pair-wise distance 를 \\((1 \\pm \\epsilon)\\) factor 에 bound 되게 보존한다. \\[ (1-\\epsilon)\\Vert X_{i}-X_{j}\\Vert_{2}^{2}\\le\\Vert F(X_{i})-F(X_{j})\\Vert_{2}^{2}\\le(1+\\epsilon)\\Vert X_{i}-X_{j}\\Vert_{2}^{2} \\] 이때 Johnson-Lindenstrauss lemma 은 실로 놀라운 결과를 보여준다. \\(m \\ge \\frac{16 \\log \\left(\\frac{n}{\\delta}\\right)}{\\epsilon^2}\\) 라는 조건이 주어져 있다면, simple randomized construction 만으로 그러한 map 을 with probability \\(\\max(c, 1-\\delta)\\) 로 생산할 수 있다는 것이다. 이 결과는 원본 데이터셋의 dim \\(d\\) 와는 완전히 독립이며 points 의 숫자 \\(n\\) 에만 logarithmical 하게 의존한다는 것에 notice. 이 map 은 핵심적으로 모든 pairwise 거리를 보존하면서도 보관 비용을 획기적으로 줄일 수 있다. map 그 자체의 방법론은 매우 간단하다. matrix \\(\\mathbf Z \\in \\mathbb R^{m \\times d}\\), \\(Z_{jk} \\overset{iid}{\\sim} N(0,1)\\) 이 되도록 설계하고, map 이 \\(F(X_i) = \\frac{\\mathbf Z X_i}{\\sqrt m}\\) 되도록 정의한다. 이제 pair 중에 \\((X_j , X_k)\\) 하나를 고르고 이하를 생각하자. \\[ \\frac {\\Bigg\\|F\\left(X_{j}\\right)-F\\left(X_{k}\\right)\\Bigg\\|^2_2} {\\Bigg\\|X_{j}-X_{k}\\Bigg \\|^2_2 } = \\left\\| \\frac {{\\mathbf Z}(X_{j}-X_{k})} {\\sqrt{m}\\Bigg\\|X_{j}-X_{k} \\Bigg\\|_{2}} \\right\\|^2_2 ={\\frac{1}{m}}\\sum_{i=1}^{m} \\underbrace{\\left \\langle \\mathbf Z_{i}, \\; \\; {\\frac{X_{j}-X_{k}}{\\Bigg\\|X_{j}-X_{k}\\Bigg\\|_{2}}}\\right\\rangle^{2}}_{T_i} \\] 이때 \\(\\mathbf Z_i\\) 는 \\(\\mathbf Z\\) 의 i-th row. 이제, for some fixed numbers \\(a_j\\) 에 대해, \\(\\sum\\limits_{j=1}^d a_j Z_{ij} \\sim N \\left(0, \\; \\sum\\limits_{j=1}^d a_j \\right)\\). 따라서 이에 의해 \\(T_i \\sim \\chi^2\\) 이며 각각은 independent. 이제 \\(\\chi^2\\) 의 tail bound 를 적용하는 것으로 우리는 이하를 얻는다. \\[ \\mathbb{P}\\left(\\left|{\\frac{\\bigg\\|F(X_{j})-F(X_{k})\\bigg\\|_{2}^{2}}{\\bigg\\|X_{j}-X_{k}\\bigg\\|_{2}^{2}}}-1\\right|\\geq\\epsilon\\right)\\leq2\\exp \\left (\\frac{-m\\epsilon^{2}}{8} \\right) \\] 따라서 fixed pair \\((X_i , X_j)\\) 에 대해, 우리의 map 이 distance 를 보존 (preserve) 하는데에 실패할 확률은 exponentially small, i.e., 최대로 해봐야 \\(2 \\exp \\left (\\frac{-m\\epsilon^{2}}{8} \\right)\\). 이제 우리의 map 이 \\(n \\choose 2\\) 중 무엇 하나라도 보전에 실패할 확률은 단순히 union bound 적용해보면 해결됨. 이 계산을 통하면 \\(P(\\text{Failure}) \\le 2 {n \\choose 2} \\exp \\left (\\frac{-m\\epsilon^{2}}{8} \\right)\\). 따라서 \\(m \\ge \\frac{16 \\log \\left(\\frac{n}{\\delta}\\right)}{\\epsilon^2}\\) 일때 위에서 이야기했던 확률이 최대로 해봐야 \\(\\delta\\) 임을 증명하는 건 쉽다. 여기서 note 해야 할 것은 \\(m\\) 을 그러한 작은 값으로 이끄는 exponential concentration 이라는 개념이다. (i.e., 이는 그냥 sample size 에 맞춰서 logarithmically 하게 grow 하기만 하면 된다) "],["metric-entropy-and-its-uses.html", "8.4 Metric entropy and its uses", " 8.4 Metric entropy and its uses set \\(\\mathcal I\\) 에 의해 index 된 랜덤변수의 collection \\(\\{X_i \\}_{i \\in \\mathcal I}\\) 를 생각해보자. 이 경우, 우리는 보통 \\(\\max\\limits_{i \\in \\mathcal I}X_i\\) 혹은 \\(E \\Big \\{ \\max\\limits_{i \\in \\mathcal I}X_i \\Big \\}\\) 를 제어하는 것이 목적이 된다. 예를 들어 \\(||X||_{2}=\\operatorname*{max}\\limits_{\\alpha\\in\\mathbb{R}^{d}:\\|\\alpha\\|_{2}=1}\\alpha^{\\top}X\\) 로 표기될 수 있는 랜덤벡터 \\(X \\in \\mathbb R^d\\) 의 \\(L_2\\) norm 을 제어하는데에 관심이 있다고 예를 들어보자. set \\(\\mathcal I\\) 의 크기가 무한하다면, uniform bound 를 구성하는 건 꽤나 빡센 일임. (e.g., Chapter 2 에서 논했던 maximal ineq. 등이 사용 불가.) 이를 해결하기 위해 \\(\\mathcal I\\) 의 finite subset \\(\\mathcal I_{sub}\\) 를 구하여 \\(\\mathcal I\\) 를 분절 (discrete) 하고 \\(\\max\\limits_{i \\in \\mathcal I}X_i\\) 를 \\(\\max\\limits_{i \\in \\mathcal I_{sub}}X_i\\) 로 모사 (approximate). 8.4.1 Metric space Definition 8.5 (Metric Space) 이하가 성립할 때, ordered pair \\((\\mathcal X, \\; d)\\) 는 metric space. set \\(\\mathcal X \\not = \\varnothing\\) \\(d\\) 는 \\(d: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) 을 따르는 \\(\\mathcal X\\) 에 대한 metric \\(x, y, z \\in \\mathcal X\\) 에 대해 이하가 성립: $$ \\[\\begin{align} d(x,y) &amp;\\ge 0 &amp;&amp;\\text{ and } d(x,y) = 0 \\iff x=y \\tag{non-negative} \\\\ d(x,y) &amp;= d(y,x) \\tag{symmetric} \\\\ d(x,z) &amp;\\le d(x,y) + d(y,z) \\tag{triangel ineq. holds} \\end{align}\\] $$ ::: ※ Remark: The \\(p\\)-norms (often denoteed by \\(l_p\\)-norm) are nested: for \\(1 \\le p_1 &lt; p_2\\), we have \\(||x||_{p_2} \\le ||x||_{p_1}\\). 마지막으로 \\(L_p\\) function space 를 살펴보자. \\(\\mathcal X = \\{ f: [0,1] \\rightarrow \\mathbb R \\}\\) 을 함수의 set 이라고 하자. \\([0,1]\\) 에 대한 \\(L_p\\) function space 는 \\(\\mathcal X\\) 의 함수들 중에서도 절대값의 \\(p\\)-th power 가 \\(\\mu\\)-integrable 한 함수들을 엄선하여 담고 있다. 즉 \\[ ||f||_p = \\left( \\int_0^1 |f|^p d \\mu \\right)^{\\frac{1}{p}} &lt; \\infty \\] 이때 \\(\\mu\\) 는 \\([0,1]\\) 에서의 측도 (measure) 이며 \\(p \\ge 1\\). (일반적으로 르베그 측도 사용 typically the Lebesgue measure) 보통 \\(p=2\\) 를 \\(p\\) 로 사용. 이 경우에 이론이 좀더 풍성하고 탄탄해짐. $$ \\[\\begin{align} || f-g||_p &amp;= \\left( \\int_0^1 \\Bigg | f(x) - g(x) \\Bigg |^p d \\mu \\right)^{\\frac{1}{p}} \\tag{L_p distance b/w f and g} \\\\ || f-g||_\\infty &amp;= \\sup_{x \\in [0,1]} \\Bigg | f(x) - g(x) \\Bigg | \\tag{when p = infty} \\end{align}\\] $$ 8.4.2 Covering numbers and metric entropy metric space \\(\\mathcal X\\) 가 있을 때 해당 space 의 크기가 궁금함. metric space 의 크기를 구하는 방법은 보통 space 를 덮는데 필요한 radius \\(\\delta\\) 인 구의 크기로 보통 구함. 이게 covering. :::{.definition “Covering number”} set \\(\\mathcal X\\) 의 metric \\(d\\) 에 비춘 \\(\\delta\\)-cover는 이하와 같다. \\(set \\{\\theta_1, \\cdots, \\theta_N\\} \\in \\mathcal X\\) s.t. \\(\\forall \\theta \\in \\mathcal X, \\exists i \\in \\{1 , \\cdots, N \\} : d(\\theta, \\theta_i) \\le \\delta\\). 이때 \\(\\delta\\)-covering number \\(N(\\delta; \\mathcal X , d)\\) 는 가장 작은 \\(delta\\)-cover 의 cardinality. ::: Definition 8.6 (Metric Entropy) \\((\\mathcal X, d)\\) 의 metric entropy 는 \\(\\log N(\\delta; \\; \\mathcal X, d)\\). 보통 \\(l_2\\)-norm \\(||\\cdot||_2\\) 을 가지는 p-차원 real space \\(\\mathbb R^p\\) 의 bounded subset 에 대해, metric entorpy 는 \\(C \\cdot p\\log\\left(\\frac{1}{\\delta} \\right)\\) 로 scale 됨. 보통 \\(\\mathbb R^p\\) 의 bounded subset 은 “small” space 로 간주됨. (metric entropy 가 \\(p\\) 에 대해 linearly 선형적으로 scale) non-Euclindean space 에 대해 (e.g., function space), metric entropy 는 다른 식으로 salce 됨. 이들은 보통 “large” space 로 간주됨. 8.4.3 Packing numbers :::{.def “Packing number”} set \\(\\mathcal X\\) 의 metric \\(d\\) 에 비춘 \\(\\delta\\)-packing은 이하와 같다. \\(set \\{\\theta_1, \\cdots, \\theta_M\\} \\in \\mathcal X\\) s.t. \\(\\forall i \\not = j \\in \\{1, 2, \\cdots, M\\}: d(\\theta_i, \\theta_j) \\ge \\delta\\) 이때 \\(\\delta\\)-packing number $M(; X , d) 는 가장 큰 \\(delta\\)-packing 의 cardinality. ::: 8.4.4 8.4.5 8.4.6 "],["covariance-estimation.html", "8.5 Covariance estimation", " 8.5 Covariance estimation 8.5.1 Matrix algebra review 8.5.2 Covariance matrix estimation in the operator norm Theorem 8.7 (Covariance estimation) \\(X_1 , \\cdots, X_n \\overset {iid}\\sim SG(\\sigma)\\) s.t. \\(E(X_1) = 0, Var(X_1) = \\Sigma_{d \\times d}\\). Let sample Cov matrix \\(\\hat \\Sigma = \\frac{1}{n} \\sum X_i X_i &#39;\\) based on \\(X_1 , \\cdots, X_n\\). Then there exists a universal constant \\(C &gt;0\\) s.t. below holds with probabilty at least \\(1-\\sigma\\). \\[ \\forall \\sigma \\in (0,1): \\frac{\\|\\hat \\Sigma - \\Sigma \\|_{op}}{\\sigma^2} \\le C \\max \\left \\{ \\sqrt{\\frac{d + \\log(\\frac{2}{\\sigma})}{n}}, \\; {\\frac{d + \\log(\\frac{2}{\\sigma})}{n}}\\right \\} \\] 이건 결국 \\(\\lim_{n \\rightarrow \\infty \\frac{d}{n} \\rightarrow 0}\\) 일 때 operator norm 안의 \\(\\Sigma\\)를 계속해서 estimate 할 수 있다는 것을 말함. 실제로 추가적인 가정 없이는 이 rate 이상으로 측정을 정밀화할 수 없음. 증명을 2단계로 분할. discretization argument 를 써서 문제를 finitely 많은 랜덤변수의 maximum 을 제어하는 문제로 변경. 이하의 정보와 함께 finite maximum 라는 사실 사용해서 \\(\\|\\hat \\Sigma - \\Sigma \\|_{op}\\) 에 대한 상한 생산. let \\(A = A&#39; \\in \\mathbb R^{d \\times d}\\) 로 하고, \\(N_\\epsilon = \\{ y_1 , \\cdots, y_N \\}\\) 을 \\(\\mathbb S^{d-1}\\) 의 \\(\\epsilon\\)-covering 으로 함. 이때 \\(\\| A \\|_{op} \\le \\frac{1}{1-2\\epsilon} \\cdot \\max_{y \\in N_\\epsilon} | y&#39; A y |\\). 이를 증명하자. \\(x \\in \\mathbb S^{d-1}\\) 에 대해 \\(\\| x-y \\|_2 \\le \\epsilon\\) 만족하는 \\(y \\in N_\\epsilon\\) 선택. 이때 \\(A\\)는 symmetric Matrix 이므로, 여기서 \\(\\hat \\Sigma - \\Sigma\\) 는 symmetric Matrix 이므로, \\(\\|\\hat \\Sigma - \\Sigma \\|_{op} \\le \\frac{1}{1-2\\epsilon} \\max_{y \\in N_\\epsilon} | y&#39; (\\hat \\Sigma - \\Sigma) y |\\). \\[ \\vert x^{\\textsf{T}}A x-y^{\\textsf{T}}A y\\vert\\ =\\ \\vert x^{\\textsf{T}}A(x-y)-y^{\\textsf{T}}A(y-x)\\vert \\leq\\;\\left|x^{\\textsf{T}}A(x-y)\\right|+\\left|y^{\\textsf{T}}A(y-x)\\right| \\tag{triangle ineq.} \\] \\[ |x^{\\textsf{T}}A(x-y)|\\ \\stackrel{(1)}{\\le}\\ ||A(x-y)||_{2}||x||_{2} \\tag{Cauchy–Schwarz inequality} \\\\ \\overset{(2)}{\\le} {{||A||_{\\mathrm{op}}||x-y||_{2}}} \\\\ \\overset{(3)}{\\le} \\epsilon\\|A\\|_{\\mathrm{op}} \\] \\(|| x||_2 = 1\\), and \\(\\forall v \\in \\mathbb R^d:||A v ||_2 \\le ||A||_{op} ||v||_2\\) \\(||x-y||_2 \\le \\epsilon\\) Applying the same argument to \\(|y^T A(y-x)|\\) then gives \\(|x^T Ax - y^T Ay| \\le 2 \\epsilon ||A||_{op}\\). To complete the proof of inequality (5.1), note that \\[ |x^{\\textsf{T}}A x|=|x^{\\textsf{T}}A x-y^{\\textsf{T}}A y+y^{\\textsf{T}}A y| \\leq\\ \\left|x^{\\top}A x-y^{\\top}A y\\right|+\\left|y^{\\top}A y\\right| \\leq 2\\epsilon || A||_{\\mathrm{op}}+|y^{T}A y \\rvert \\] This implies that \\(||A||_{op} \\le 2 \\epsilon ||A||_{op} + \\max_{y \\in N_\\epsilon} |y^T A y|\\) and rearranging the terms yields inequality (5.1). standard concentration inequality 사용. Step 2. By choosing  = 1/4, inequality (5.2) becomes \\[ ||\\hat{\\Sigma} - \\Sigma||_\\mathrm{op} \\leq 2 \\max_{y\\in{N_{1/4}}} |\\mathcal{y}^{\\top} \\big(\\hat{\\Sigma} - \\Sigma\\big)\\mathcal{y}| \\] Therefore by the union bound \\[ P(||\\hat{\\Sigma} - \\Sigma||_\\mathrm{op} \\ge t) \\leq P \\left ( 2 \\max_{y\\in{N_{1/4}}} |\\mathcal{y}^{\\top} \\big(\\hat{\\Sigma} - \\Sigma\\big)\\mathcal{y}| \\ge t \\right ) \\leq \\sum_{y\\in{ N_{1/4}}} P \\left( |\\mathcal{y}^{\\top} \\big(\\hat{\\Sigma} - \\Sigma\\big)\\mathcal{y}| \\ge \\frac{t}{2} \\right) \\] Note that we can write \\(y^{\\top}(\\hat{\\Sigma}-\\Sigma)y=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{(y^{\\top}X_{i})^{2}-\\mathbb{E}[(y^{\\top}X_{i})^{2}]\\right\\}\\) We saw earlier in Lemma 3.3 that the square of a sub-Gaussian random variable is sub-exponential with parameters \\((\\nu, \\alpha) = (16 \\sigma^2, 16 \\sigma^2)\\). This property implies that \\(\\left\\{(y^{\\top}X_{i})^{2}-\\mathbb{E}[(y^{\\top}X_{i})^{2}]\\right\\}\\) is sub-exponential with \\((16\\sigma^2 , 16\\sigma^2)\\). Applying the sub-exponential tail bound, especially inequality (3.2), yields \\[ \\mathbb{P}(\\|\\widehat\\Sigma-\\Sigma\\|_{\\mathrm{lop}}\\geq t)\\ \\leq\\ 2\\ \\underbrace{\\mathrm{l}N_{1/4}}_{\\mathrm{extrianitv}} \\times \\exp\\Biggl(-{\\frac{1}{2}}\\operatorname{min}\\Biggl\\{{\\frac{n t}{16\\sigma^{2}}},\\ {\\frac{n t^{2}}{16^{2}\\sigma^{4}}}\\Biggr\\}\\Biggr) \\\\ \\leq\\ 2\\times9^{d}\\times\\exp\\biggl(-\\frac{1}{2}\\operatorname*{min}\\Biggl\\{\\frac{n t}{16\\sigma^{2}},\\ \\frac{n t^{2}}{16^{2}\\sigma^{4}}\\Biggr\\}\\biggr) \\] where the last step uses the result in Lecture 4, which shows that the cardinality of \\(N_{1/4}\\) is bounded by \\(|N_{1/4}| ≤ 9\\). (here note that \\(\\mathbb{S}^{d-1}\\subset\\mathbb{B}=\\{\\theta\\in\\mathbb{R}^{d}:||\\theta||_{2}\\leq1\\}\\})\\)). Finally, inverting the bound gives the desired result. 8.5.3 Bounds for structured covariance matrices 우리의 주된 목적은 샘플 Cov를 경유하여 unstructured Cov Matrix를 estimate 하는 것. Cov Matrix 가 추가적인 structure 를 품고 있다면, 샘플 Cov 가 아니라 다른 estimator 를 사용해서 좀더 연산이 빠른 estimate 가 가능. Diagonal matrix Cov Matrix 가 diagonal이라는 정보를 가지고 있다고 해보자. 이때 \\(\\hat \\Sigma_{diag} = diag \\{ \\hat \\Sigma_{11}, \\cdots, \\hat \\Sigma_{dd}\\) 로 estimate 하는 것은 자연스럽다. 이 경우 sub-Gaussinianity 를 가정한다면 어떻게 될까? unstructured 케이스에서 order 가 \\(\\sqrt{\\frac{d}{n}}\\) rates (단, \\(d \\le n\\)) 였던 것과 대비되게 estimation error of the order $ 가 생산된다. 좀 더 자세히 살펴보자. diagonal 케이스에서, \\(\\hat \\Sigma_{diag} - \\Sigma\\) 의 operator norm 은 본질적으로 \\(d\\) 개의 entry값 \\(\\{| \\hat \\Sigma_{diag,11} - \\Sigma_{11} |, \\cdots, | \\hat \\Sigma_{diag,dd} - \\Sigma_{dd} |\\}\\) 중의 maximum 이다. 그렇다면 여기서 the union bound argument along with an exponential tail bound 를 통해 우리는 \\(\\sqrt{\\frac{\\log d}{n}} → 0\\) 일 때 operator norm 이 0로 decay 된다는 것을 파악할 수 있다. See Theorem 2.11 for a similar argument. Unknown sparsity and thresholding 좀더 일반적인 케이스를 생각해보자. Cov Matrix가 상대적으로 sparse 하다는 사실이 알려져 있지만, 어느 entry가 non-zero인지는 알려져있지 않다. 이때 estimator가 thresholding 에 기반하고 있다고 생가가흔 넉승 나젼스럽다. 이때 \\(\\lambda &gt;0\\) 라는 패러미터가 주어져 있다고 생각할 때, hard-thresholding 을 통해 얻어지는 Cov estimator 의 \\((i,j)\\) entry 는 \\([T_\\lambda (\\hat \\Sigma)]_{ij} = \\hat \\Sigma_{ij} \\cdot I(|\\hat \\Sigma_{ij}&gt;\\lambda)\\). let \\(\\Sigma\\)의 adjacency matrix \\(A \\in \\mathbb R^{d \\times d}\\), \\(A_{ij} = I(\\Sigma_{ij}) \\not = 0\\). adjacency matrix 의 operator norm \\(\\| A \\|_{op}\\) 는 sparsity 에 대한 natural measure 를 제공한다. 이때 우리는 \\(\\Sigma\\) 가 row 별로 \\(s\\) 개의 non-zero entry를 갖고 있다면 \\(\\| A \\|_{op} \\le s\\) 임을 보일 수 있다. 또한 thresholded 샘플 Cov Matrix 는 다음과 같은 concentration bound를 가짐. Theorem 8.8 (Thresholding-based covariance estimation) \\(X_1 , \\cdots, X_n \\overset {iid} \\sim\\), s.t. \\(E(X_1) = 0, Var(X_1) = \\Sigma_{d \\times d}\\), and suppose each component \\(X_{ij}\\) is sub-Gaussinian with 패러미터 at most \\(\\sigma\\). 만약 \\(n &gt; 16 \\log d\\) 라면, \\(\\forall \\delta&gt;0\\)에 대해, thresholded 샘플 Cov Matrix \\(T_{\\lambda_n} (\\hat \\Sigma)\\) with \\(\\frac{\\lambda_n}{\\sigma^2} = 8 \\sqrt{\\frac{\\log d}{n}} + \\delta\\) 는 이하를 만족한다. \\[ P \\Big ( \\| T_{\\lambda_n} ( \\hat \\Sigma ) - \\Sigma \\|_{op} \\ge 2 \\| A \\|_{op} \\cdot \\lambda_n \\Big) \\le 8 \\exp \\Big( -\\frac{n}{16} (\\delta \\wedge \\delta^2)\\Big) \\] 위의 부등식은 높은 확률로 \\(\\| T_{\\lambda_n} ( \\hat \\Sigma ) - \\Sigma \\|_{op} \\lesssim \\| A \\|_{op} \\sqrt{\\log d}{n}\\) 임을 보여줌. 이에 더해서 \\(\\sigma\\) 가 row 당 최대 \\(s\\) 개의 non-zero entry 를 가진다는 조건을 생각하자. 이는 곧 \\(\\|A\\|_2 \\le s\\) 라는 의미가 됨. 그렇다면 thresholded Cov Matrix는 \\(s\\sqrt{\\frac{\\log d}{n}}→0\\) 일 때 consistent 하며, 이는 곧 특히 \\(s\\) 가 작을 때 \\(\\sqrt{\\frac{d}{n}}\\) 보다 훨씬 빠르다. thresholding 패러미터는 sub-Gaussian 패러미터 \\(\\sigma\\)에 의존하는데, 이는 실전 상황에서는 대부분 unknown. Proof: Let us denote the elementwise infinity norm of the error matrix \\(\\hat \\Delta = \\hat \\Sigma - \\Sigma\\) by \\(|| \\hat \\Delta ||_\\infty = \\max_{1\\le j , \\; \\; j \\le d} |\\hat \\Delta_{ij}|\\). The proof of the theorem is based on two intermediate results: Under the assumptions of the theorem, \\(\\mathbb{P}(||\\widehat{\\Delta}||_{\\infty}/\\sigma^{2}\\geq t)\\leq8e^{-{\\frac{n}{16}}\\operatorname*{min}\\{t,t^{2}\\}+2\\log d}\\quad\\mathrm{for~all~}t\\gt 0 \\tag{5.3}\\) For any choice of \\(\\lambda_n\\) such that \\(|| \\hat \\Delta ||_\\infty \\le \\lambda_n\\) we are guaranteed that \\(||\\widehat{\\Sigma}-\\Sigma||_{\\mathrm{lop}}\\leq\\ 2||A||_{\\mathrm{op}}\\lambda_{n} \\tag{5.4}\\) Having these results in place, the theorem follows by taking \\(t = \\frac{\\lambda_n}{\\sigma^2} = 8 \\sqrt{\\frac{\\log d}{n}} + \\delta\\)in inequality (5.3) and see \\(8e^{-\\frac{n}{16}\\operatorname*{min}\\{t,t^{2}\\}+2\\log d}\\lt 8e^{-\\frac{n}{16}\\operatorname*{min}\\{\\delta,\\delta^{2}\\}},\\) , when \\(n &gt; 16 log d\\). Thus It remains to prove inequality (5.3) and inequality (5.4), which are left as exercises (see Section 6.5 of Martin’s book) \\[ |\\mathbb{P} \\Bigg (||T_{\\lambda_{n}}(\\hat{\\Sigma})\\to\\Sigma||_{\\mathrm{op}}\\geq\\ 2||{A}||_{\\mathrm{op}}\\lambda_{n} \\Bigg) \\le P (||\\widehat\\Delta||_{\\infty}\\ge\\,\\lambda_{n})\\le8e^{-\\frac n{16}\\it\\ m i n}\\{\\delta,\\delta^{2}\\}_{.} \\] It remains to prove inequality (5.3) and inequality (5.4), which are left as exercises (see Section 6.5 of Martin’s book) "],["matrix-concentration-inequalities.html", "8.6 Matrix concentration inequalities", " 8.6 Matrix concentration inequalities 이전 강의에서는 샘플 Cov Matrix의 tail bound를 discretization argument 를 통해 탐색했음. 여기선 Matrix Chernoff 테크닉을 통해 탐색한 후 랜덤 매트릭스에 Hoeffding bound 와 Bernstein bound 를 제시할거임. 8.6.1 Matrix calculus symmetric Matrix 의 set \\(\\mathcal S^{d \\times d} = \\{ X \\in \\mathbb R^{d \\times d} : X = X&#39; \\}\\) 와 ev 가 non-negative 인 PSD Matrix의 subset \\(\\mathcal S^{d \\times d}_+\\) 를 사용할 것. 8.6.2 Matrix Chernoff independent symmetric 랜덤 매트릭스의 collection \\(X_1 , \\cdots, X_n \\in \\mathcal S^{d \\times d}\\) 가 주어졌고 \\(E(X_1) = 0\\). 이때 \\(\\bar X\\)의 maximum ev를 $P(_{max} (X) T) 와 같이 bound하고 싶다. Chernoff argument 를 쓰는 것이 일반적. 적용하면: $$ \\[\\begin{align} \\forall s &gt;0 : P[\\lambda_{max}(\\bar X) \\ge t] &amp;= P[\\exp \\Big[ \\lambda_{max}(s \\bar X) \\Big] \\ge \\exp(st)] \\\\ &amp;= P[ \\lambda_{max}(\\exp \\Big[s \\bar X \\Big]) \\ge \\exp(st)] \\\\ &amp;le \\exp(-st) \\cdot E \\Big [ \\lambda_{max}(\\exp \\Big[s \\bar X \\Big]) \\Big ] \\\\ &amp;le \\exp(-st) \\cdot E \\Big [ \\tr (\\exp \\Big[s \\bar X \\Big]) \\Big ] \\end{align}\\] $$ 2번째 등식에선 exponential 함수의 spectral mapping property 과 monotonicity 사용 function standard Markov 부등식 \\(\\exp(s \\bar X)\\) 가 PSD Matrix 라는 사실 활용 trace 가 linear operator이며, it can commute with expectation 위의 전개에서 모든 \\(s&gt;0\\) 에 inf를 적용하면 Chernoff argument 완성. 이제 \\(tr(E[exp(sX)])\\) 를 bound 해야 함. 일반적인 스칼라 케이스에서 평균의 exponential 은 그냥 prod 로 쓰일 수 있음. 이를 연장하여 우리는 개별 랜덤변수들의 mgf 생산까지도 끌고갈 수 있음. 하지만 매트릭스 exponential 에서 \\(e^{X+Y} = e^X e^Y\\) 려면 \\(XY=YX\\) 여야 함. 따라서 우리는 \\(X_1 , \\cdots, X_n\\)의 임의의 실현값에 직접적으로 factorization 적용하는 건 불가능함. 이때 Lieb’s inequality 적용하면 이 난점 돌파 가능. 8.6.3 Sub-Gaussian and sub-exponential matrices 실값 랜덤변수의 케이스와 같이, 우리는 랜덤 매트릭스의 class 를 이들의 mgf 사용해서 특성을 드러내는 것이 가능. :::{..def “Sub-Gaussian random matrices”} symmetric Matrix \\(X \\in \\mathcal S^{d \\times d}\\), 이때 \\(E(X) = 0\\), 는 이하를 만족할 경우 matrix 패러미터 \\(V \\in \\mathcal S^{d \\times d}_+\\) 를 가지는 sub-Gaussian. \\[ \\forall t \\in \\mathbb R: E \\Big [\\exp(tX) \\Big ] \\le \\exp \\left( \\frac{t^2 V}{2} \\right) \\] ::: ※Remark: 패러미터 \\(V\\) 를 가지는 Sub-Gaussian 랜덤 매트릭스는 패러미터 \\((V , 0)\\)을 가지는 sub-exponential이기도 하다. 8.6.4 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds 8.6.4.1 Hoeffding bound :::{..def “Hoeffding bound for random matrices”} Let independent 한 zero-mean symmetric 랜덤 매트릭스의 sequence \\(\\{ X_i \\}^n_{i=1}\\), 이에 더해 이 랜덤 매트릭스들은 패러미터 \\(\\{ V_i \\}^n_{i=1} \\in S_+^{d \\times d}\\) 를 가지는 sub-Gaussian 조건을 만족한다. 이때 우리는 이하와 같은 upper tail bound 를 얻는다. $$ \\[\\begin{align} &amp;P \\Big [ \\lambda_{max}(\\bar X) \\ge t \\Big ] \\le d \\exp \\left( - \\frac{nt^2}{2\\sigma^2}\\right), &amp;&amp;\\sigma^2 = \\textstyle \\| \\tfrac{1}{n} \\sum_{i=1}^n V_i \\|_{op} \\end{align}\\] $$ ::: 8.6.4.2 Bernstein bound :::{..def “Variance of random matrices”} 랜덤 매트릭스 \\(X \\in S^{d \\times d}\\)에 대해, 이의 Var을 아래와 같이 정의한다. 이때 Var(X)는 자연스럽게 PSD. \\[ Var(X) = E(X^2 ) - \\left[ E(X) \\right]^2 \\] ::: :::{..def “Bernstein bound for random matrices”} bounded operator norm 을 가지는, zero-mean independent symmetric 랜덤 매트릭스의 sequence 를 \\(\\{ X_i \\}^n_{i=1}\\) 로 하자. 이에 더해 \\(\\exists b&gt;0, \\forall i : \\| X_i \\|_{op}\\). 이때 ::: Let independent 한 zero-mean symmetric 랜덤 매트릭스의 sequence \\(\\{ X_i \\}^n_{i=1}\\), 이에 더해 이 랜덤 매트릭스들은 패러미터 \\(\\{ V_i \\}^n_{i=1} \\in S^{d \\times d}\\) 를 가지는 sub-Gaussian 조건을 만족한다. 이때 우리는 이하와 같은 upper tail bound 를 얻는다. $$ \\[\\begin{align} &amp;P \\Big [ \\lambda_{max}(\\bar X) \\ge t \\Big ] \\le d \\exp \\left( - \\frac{nt^2}{2\\sigma^2}\\right), &amp;&amp;\\sigma^2 = \\textstyle \\| \\tfrac{1}{n} \\sum_{i=1}^n V_i \\|_{op} \\end{align}\\] $$ ::: $$ \\[\\begin{align} &amp;P \\Big [ \\lambda_{max}(\\bar X) \\ge t \\Big ] \\le 2d \\exp \\left( - \\frac{nt^2}{2(\\sigma^2 bt)}\\right), &amp;&amp;\\sigma^2 = \\textstyle \\| \\tfrac{1}{n} \\sum_{i=1}^n Var(X_i) \\|_{op} \\end{align}\\] $$ 8.6.4.3 Generalization to non-symmetric/rectangular matrices 이렇게 symmetric (그리고 당연히 square) 매트릭스의 concentration bounds 를 살펴봤음. 하지만 이 bounds는 non-symmetric 에도, 그리고 nonsquare 에도 적용할 수 있도록 확장 가능함. 바로 self-adjoint dilation 을 사용해서. 랜덤 매트릭스 \\(X_i \\in \\mathbb R^{d_1 \\times d_2}\\) 가 주어졌음. 이제 다음과 같은 매트릭스 생산: \\[ Y_i = \\begin{bmatrix} 0_{d_1 \\times d_1} &amp; X_i \\\\ X_i &#39; &amp; 0_{d_2 \\times d_2}\\end{bmatrix} \\in \\mathbb R^{(d_1 + d_2) \\times (d_1 + d_2)} \\] \\(Y_i\\) 가 symmetric 임을 보이는 건 쉬움. 더욱 중요한 것은, \\(\\|X_i \\|_{op} = \\|Y_i \\|_{op}\\) 임을 보이는 것도 가능.6 따라서 이하와 같으며, \\(Y_i\\)의 mgf 에 특정한 조건을 부여하는 것으로 위에서 진행해온 프로세스를 그대로 적용할 수 있다. \\[ P(\\| \\bar X \\|_{op} \\ge t)= P(\\| \\bar Y \\|_{op} \\ge t) \\] Tropp, Joel A. ”User-friendly tail bounds for sums of random matrices.” Foundations of computational mathematics 12.4 (2012): 389-434↩︎ "],["principal-component-analysis.html", "8.7 Principal Component Analysis", " 8.7 Principal Component Analysis Principal Component Analysis (PCA) 는 차원축소에 가장 유명한 방법론 중 하나. 데이터의 저차원 표현을 통해 데이터를 보여주고 또 해석하는 것이 가능. 이는 Var 의 대부분을 포착 (capture, 설명가능) 한 저차원 subspace 를 탐색해내거나, 혹은 equivalent 하게 분포의 maximal Var component 를 탐색해내는 것으로 성립. 샘플의 finite collection 이 주어졌을 때 PCA 의 empirical form 은 샘플 Cov Matrix 의 상위 evec 의 subset 을 계산해내는 것으로 작동함. 관심대상은 언제 이 evec 들이 population Cov Matrix 의 상위 evec 들에 의해 span 되는 subspace 를 잘 모사해내는가, 그 condition. 초기형 tool 들을 사용해 고차원 상황이랑 non-asymptotic framework 에서 해당 이슈를 살펴보자. 8.7.1 PCA let \\(E(X)=0\\), \\(Cov(X) = \\Sigma\\) 인 랜덤벡터 \\(X \\in \\mathbb R^d\\). ev Decompostion 을 고려하자. 즉 \\(\\Sigma = V \\Lambda V&#39;\\).78 PCA 에 던지는 질문은 결국 이거다. unit norm vector \\(v\\), 즉 \\(v \\in \\mathbb S^{d-1} = \\Big \\{ v \\in \\mathbb R^d : || v ||_2 =1 \\Big \\}\\) 에 대해, 어떤 \\(v\\) 를 골라야 랜덤변수 \\(v&#39;X\\) 의 Var 이 최대화되는가? 더 이론적인 이야기를 해보자. 우리는 \\(v_1 = \\arg \\max\\limits_{v \\in \\mathbb S^{d-1}} Var(v&#39;X) = \\arg \\max\\limits_{v \\in \\mathbb S^{d-1}} \\langle v, \\Sigma \\rangle\\) 를 만족하는 direction \\(v_1\\) 을 찾는 것에 목적을 둔다. 이를 first principal component 라고 부르자. 이를 일반화하면 \\(\\Sigma\\) 의 top \\(k\\) principal component \\(\\{v_1 , \\cdots, v_k \\}\\) 를 구성할 수 있다. 이때 각각은 for \\(2 \\le j \\le k: v_j = \\arg \\max\\limits_{\\substack{v \\in \\mathbb S^{d-1},\\\\ \\langle v, \\Sigma v_i \\rangle = 0,\\\\ 1 \\le \\forall i \\le j}} \\langle v, \\Sigma \\rangle\\) 를 만족해야 한다. 이 principal component 들은 단순히 \\(\\Sigma\\) 의 top \\(k\\) evec, 즉, \\(V\\)의 first \\(k\\) 개의 column 이 된다. PCA 는 보통 \\(k\\) 를 작게 잡고 노는 걸 좋아함. Best rank k approximation PCA 는 low-rank 근사 (approximation) 의 관점으로도 해석될 수 있다. 우리가 rank 가 커봐야 \\(k\\) 인 \\(Z^\\ast_{d \\times d} = \\arg \\min\\limits_{rank(Z) \\le k} ||\\Sigma Z||_F\\) 를 찾는다고 하자. 이에 대한 optimal solution 이 \\(Z^\\ast = \\sum\\limits^k_{i=1} \\lambda_1 v_i v_i&#39;\\) 이며 \\(\\Bigg\\|\\Sigma - Z^\\ast \\Bigg\\|^2_F = \\sum\\limits^d_{i=k+1}\\lambda_i^2\\) 임을 알 수 있다. 8.7.2 Matrix Perturbation 실전에서 \\(\\Sigma\\) 는 불명이며 PCA 가 적용되는건 언제나 샘플 Cov \\(\\hat \\Sigma\\) 이다. 이때 주된 질문은 샘플에서 얻은 ev 와 evec 들이 그들의 population Cov 를 얼마나 잘 근사하는지 하는 것이다. 이 질문에 답하기 위한 tool 들은 아래와 같다. 8.7.2.1 ev 의 estimation let \\(\\hat \\Sigma = \\Sigma + \\underbrace{E}_{\\text{noise matrix}}\\). 이때 maimum ev 의 정의에 의해 $$ \\[\\begin{align} \\lambda_{max} (\\hat \\Sigma) &amp;= \\max_{v \\in \\mathbb R^{d-1}} v&#39;(\\Sigma + E)v \\\\ &amp;\\le \\lambda_{max}(\\Sigma) + ||E||_{op} \\end{align}\\] $$ \\(\\hat \\Sigma\\) 와 \\(\\Sigma\\) 의 역할이 뒤바뀌었을 때도 동일한 argument 가 성립하므로 동시에 \\(\\lambda_{max} (\\Sigma) \\le \\lambda_{max}(\\hat \\Sigma) + ||E||_{op}\\) 이기도 하다. 이 둘을 합하면 결국 \\(\\Big | \\lambda_{max} (\\Sigma) - \\lambda_{max}(\\hat \\Sigma) \\Big |\\le ||E||_{op}\\). 이를 더 일반화시키면 이하와 같다. Theorem 8.9 (Weyl’s inequality) \\[ \\max\\limits_{i=1, \\cdots, d} \\Big | \\hat \\lambda_i - \\lambda_i \\Big | \\le ||E||_{op} = ||\\hat Sigma - \\Sigma ||_{op} \\] where \\(\\hat \\lambda_1, \\cdots, \\hat \\lambda_d\\) are the ordered ev of \\(\\hat \\Sigma\\). 이것이 의미하는 바는 명확함. \\(\\forall i = 1,\\cdots,d: ||\\hat \\lambda - \\lambda||_{op} \\Longrightarrow\\) \\(\\{\\hat \\lambda_i\\) 는 \\(\\lambda_i\\) 의 consistent estimator \\(\\}\\)라는 이야기. 실제로 SG assumption 하에서, \\(|| \\hat \\Sigma - \\Sigma || \\lesssim \\max \\left( \\sqrt{\\frac{d}{n}}, \\frac{d}{n} \\right)\\) with high probability. 따라서 개별 empirical ev 값은 이 경우에 \\(\\frac{d}{n} \\rightarrow 0\\) 일 경우 consistent. 8.7.2.2 evec 의 estimation ev 는 일반적으로 stable 하지만 evec 의 경우에는 그렇지 않음. 8.7.3 Spiked Cov Model Definition 8.7 (Spiked Cov model) Cov matrix \\(\\Sigma \\in \\mathbb R^{d \\times d}\\) 가 이하의 형을 만족하면 이는 Spiked Covariance Model 를 만족한다 고 불림. 이때 vector \\(v\\) 는 spike 라고 명명. \\[ \\exists \\theta &gt; 0, \\exists v \\in \\mathbb S^{d-1}: \\Sigma = \\theta v v&#39; + I_d \\] 이러한 spiked Cov model 에 있어, \\(\\max(ev) = \\theta + 1\\), corresponding evec (largest evec) \\(=v\\) 이라는 관점이 성립하는 것을 note. \\(v\\) 의 natural estimate 는 empirical Cov Matrix 의 largest evec \\(\\hat v\\). 우리의 목적은 고차원 setting에서 \\(\\hat v\\)와 \\(v\\) 가 얼마나 가까운지 보는 것. 이때 \\(u\\) 가 symmetric Matrix 의 evec 이라고 하면, \\(-u\\) 또한 같은 ev 에 묶인 evec. 따라서 우리가 \\(v\\) 를 estimate 해봐야 최대로 estimate 가능한 종착지는 참값의 sign flip 까지가 한계. This means that we can only estimate \\(v\\) up to a sign flip. 이 문제를 해결하기 위해 우리는 2개의 벡터 \\(u, v\\) 사이가 얼마나 가까운지 proximity 를 그들 각각의 linear span 사이의 principal angle 이라는 개념을 이용하여 설명한다. \\[ \\angle(u,v) = \\arccos \\left( \\Bigg | u&#39;v \\Bigg | \\right) \\] Davis–Kahan \\(\\sin(\\theta)\\) thm 은 eigenspace 들 사이의 principal angle 의 \\(\\sin\\) 에 대한 bound 를 생산함. 이하는 1차원 eigenspace 들 사이의 principal angle 에 대해 사용되는 Davis–Kahan \\(\\sin(\\theta)\\) thm 의 간단한 버전. Theorem 8.10 (Davis–Kahan sin(θ) theorem) let \\(A_{d \\times d}, B_{d \\times d} \\in PSD\\). \\(\\lambda_{1} \\ge {\\lambda}_{2} \\ge \\cdots: \\; (\\lambda_{1},u_{1}),\\cdots,(\\lambda_{d,}^{\\cdot}\\,u_{d})\\) is pairs of ev and evec of \\(A\\). \\(\\mu_{1} \\ge {\\mu}_{2} \\ge \\cdots: \\; \\; (\\mu_{1},v_{1}),\\cdots,(\\mu_{d,}^{\\cdot}\\,v_{d})\\) is pairs of ev and evec of \\(B\\). 이때 $$ \\[\\begin{align} &amp;\\sin \\Big (\\angle(u_{1},v_{1}) \\Big )\\leq{\\frac{2}{\\operatorname*{max} \\Big (\\lambda_{1}-\\lambda_{2},\\mu_{1}-\\mu_{2} \\Big )}}\\|A-B\\|_{\\mathrm{op}} \\\\ \\operatorname*{min}\\limits_{\\epsilon\\in\\{\\pm1\\}} \\Big \\|\\epsilon \\cdot u_{1}-v_{1} \\Big \\|_{2}^{2}\\le 2&amp;\\mathrm{sin}^{2} \\Big (\\angle(u_{1},v_{1}) \\Big ) \\end{align}\\] $$ Proof: 여기서 Matrix 에 대한 Holder ineq. 를 적용하자. 이때 \\(u_1 &#39; A u_1 = \\lambda_1\\), i.e. maxiumum ev. 여기서 $$ \\[\\begin{align} \\forall x \\in \\mathbb S^{d-1}: x^{\\textsf{T}}A x\\ =\\ x^{\\textsf{T}}\\!\\left(\\sum_{i=1}^{d}\\lambda_{i}u_{i}u_{i}^{\\top}\\right)\\!x &amp;=\\sum_{i=1}^{d}\\lambda_{i}(u_{i}^{\\top}x)^{2} \\\\ &amp;\\leq\\;\\ \\lambda_{1}(u_{1}^{\\top}x)^{2}+\\lambda_{2}\\sum_{i=2}^{d}(u_{i}^{\\top}x)^{2} \\\\ &amp;\\overset{(\\mathrm{i})}{=}~\\lambda_{1}\\big(u_{1}^{\\top}x\\big)^{2}\\,+\\,\\lambda_{2}\\big(1\\,-\\,\\big(u_{1}^{\\textsf{T}}x\\big)^{2}\\big) \\\\ &amp;\\overset{(\\mathrm{ii})}{=} \\lambda_{1}\\cos^{2}\\Big(\\angle(u_{1},x)\\Big)+\\lambda_{2}\\sin^{2}\\Big(\\angle(u_{1},x)\\Big), \\end{align}\\] $$ \\(x = \\sum\\limits_{i=1}^d u_u(u_i &#39; x)\\) 이며 \\(x&#39;x = \\sum\\limits_{i=1}^d(u_i &#39; x)^2 =1\\) 이라는 사실 사용 trigonometric identity \\(\\cos^2 + \\sin^2 = 1\\) 따라서 여기에 \\(x = v_1\\) 으로 잡는 것으로 $$ \\[\\begin{align} u_{1}^{\\top}A u_{1}-v_{1}^{\\top}A v_{1}\\ &amp;\\geq\\ \\lambda_{1}-\\lambda_{1}\\mathrm{cos}^{2}\\Big(\\angle(u_{1},x) \\Big)-\\lambda_{2}\\mathrm{sin}^{2} \\Big (\\angle(u_{1},x) \\Big ) \\\\ &amp;=\\;(\\lambda_{1}-\\lambda_{2}){\\sin}^{2} \\Big (\\angle(u_{1},x) \\Big ) \\end{align}\\] $$ On the other hand, $$ \\[\\begin{align} u_{1}^{\\textsf{T}}A u_{1}-v_{1}^{\\textsf{T}}A v_{1}\\ \\ &amp;=\\ \\ u_{1}^{\\textsf{T}}B u_{1}-v_{1}^{\\textsf{T}}A v_{1}+u_{1}^{\\textsf{T}}(A-B)u_{1} \\\\ &amp;\\overset{\\mathrm{(i)}}{\\leq}~v_{1}^{\\top}B v_{1}-v_{1}^{\\top}A v_{1}+u_{1}^{\\top}(A-B)u_{1} \\\\ &amp;=\\;\\Big\\langle A-B, \\; u_{1}u_{1}^{\\top}-v_{1}v_{1}^{\\top} \\Big \\rangle \\\\ &amp;\\overset{(ii)}\\le \\left|\\right|A-B||_{\\infty}||u_{1}u_{1}^{\\top}-v_{1}v_{1}^{\\top}||_{1} \\\\ &amp;\\overset{(iii)}\\le \\vert\\vert A-B\\vert\\vert_{\\mathrm{op}}\\sqrt{2}\\vert\\vert u_{1}u_{1}^{\\textsf{T}}-v_{1}v_{1}^{\\textsf{T}}\\vert\\vert_{2}, \\end{align}\\] $$ \\(v_1\\) 이 \\(B\\) 의 leading evec 이므로 Holder ineq. \\(||A-B||_\\infty = ||A-B||_{op}\\) 이며, \\(rank(u_1u_1&#39; - v_1v_1&#39; )\\le 2\\) 와 함께 CS ineq. 사용. 이하는 명확함. \\[ ||u_{1}u_{1}^{\\top}-v_{1}v_{1}^{\\top}||_{2}^{2}=2-2(u_{1}^{\\top}v_{1})^{2}=2\\mathrm{sin}^{2}(\\angle(u_{1},v_{1})) \\] 이제 모든 조각을 모으면 이하가 성립. \\[ \\left(\\lambda_{1}-\\lambda_{2}\\right)\\mathrm{sin}^{2} \\bigg(\\angle(u_{1},v_{1}) \\bigg)\\leq2\\|A-B\\|_{\\mathrm{op}} \\mathrm{sin}\\bigg(\\angle(u_{1},v_{1}) \\bigg) \\] 이는 곧 thm 의 첫번째 부분을 보여줌. \\(A\\)와 \\(B\\) 에 대해 결과가 완벽하게 symmetric 이므로 \\(\\lambda_1 - \\lambda_2\\) 를 \\(\\mu_1 - \\mu_2\\) 로 대체할 수 있음을 note. 이제 thm 의 2번째 부분만 보이면 됨. 이는 이하의 ineq. 를 통해 성립함이 분명. 이하의 ineq. 는 \\(|u_{1}^{\\top}v_{1}|\\leq\\|u_{1}\\|_{2}\\|v_{1}\\|_{2}=1.\\) 이므로 성립함. \\[ \\operatorname*{min}_{\\epsilon\\in\\{\\pm1\\}}||\\epsilon u_{1}-v_{1}||_{2}^{2}=2-2|u_{1}^{\\top}v_{1}|\\le2-2|u_{1}^{\\top}v_{1}|^{2}=\\sin^{2}(2(u_{1},v_{1}) \\] Theorem 8.11 (Holder’s inequality for matrices) let \\(A_{d \\times d}, B_{d \\times d} \\in PSD\\), 그리고 각각의 ev들을 \\(\\lambda_1 , \\cdots, \\lambda_d\\), \\(\\mu_1 , \\cdots, \\mu_d\\). 이를 이하와 같이 쓸 수 있다. \\[ ||A||_{p}=\\left(\\sum_{i=1}^{d} \\Big |\\lambda_{i} \\Big |^{p}\\right)^{\\frac1p} \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; ||B||_{q}=\\left(\\sum_{i=1}^{d} \\Big |\\mu_{i} \\Big |^{q}\\right)^{\\frac1q} \\] 이때 \\(\\forall p, q \\; \\; \\text{ s.t. } \\; \\; \\frac1p + \\frac1q = 1, p,q \\in [1, \\infty]: \\; \\; \\langle A, B \\rangle = tr(A&#39;B) = tr(B&#39;A) \\le ||A||_{p} ||B||_{q}\\). “Davis–Kahan sin(θ) theorem” 을 thm 5.1 과 조합하는 것으로 이하의 결과를 얻을 수 있음. Corollary 8.1 (Empirical principal component) \\(E(X_1) = 0\\), \\(Var(X_1) = \\Sigma_{d \\times d}\\) 인 랜덤벡터의 sequence \\(X_1 , \\cdots, X_n \\overset{iid}{\\sim} \\in SG(\\sigma^2)\\), i.e., sequnce of \\(\\sigma\\)-sub-Gaussian random vectors. let 샘플 Cov Matrix \\(\\hat \\Sigma = \\frac{1}{n} \\sum_{i=1}^n X_i X_i &#39;\\). assume \\(\\Sigma = \\theta v v&#39; + I_d\\) spiked Cov model 만족. 그렇다면 \\(\\hat \\Sigma\\) 의 largest evec \\(\\hat v\\) 는 이하를 with probability \\(1-\\delta\\) 로 만족. \\[ \\operatorname*{min}_{\\epsilon\\in\\{\\pm1\\}}\\left|\\right|\\epsilon \\cdot \\widehat{v}-v||_{2} \\lesssim \\frac{1}{\\theta}\\,\\mathrm{max}\\left\\{\\sqrt{\\frac{d+\\log(\\frac2\\delta)}{n}},\\;\\frac{d+\\log(\\frac2\\delta)}{n}\\right\\} \\] 이 결과를 통해 저차원 상황 (\\(d \\ll n\\)) 에서의 PCA 를 진행할 때 population Cov \\(\\Sigma\\) 를 샘플 Cov \\(\\hat \\Sigma\\) 로 대체하는 것이 정당화된다. 고차원 상황 (\\(d \\gg n\\)) 일 때는 \\(\\hat \\Sigma\\) 를 써서 PCA 를 진행하면 결과값이 구리다는 것이 증명되어 있다. 실제로 \\(\\frac dn\\) 이 0에서 bounded away 되어있는 한, population evec 에 대한 consistent estimator 를 생산할 수 있는 방법 자체가 아예 없다 는 것을 보이는 것이 가능하다. 하지만 evec 에 대해 certain structure 가 존재한다면 고차원에서도 population evec 을 consistently estimate 하는 것이 가능하긴 하다. 8.7.4 sparse PCA evec 에 sparsity 개념을 도입하자. leading evec \\(v\\) 가 \\(k\\)-sparse9, 즉 \\(||v||_0 = \\sum\\limits^d_{i=1}|v_i|^0 = k\\)10. 이 경우 \\(v\\) 를 추정하기 위한 natural candidate 는 \\(\\hat v_{sp} = \\arg \\max\\limits_{u \\in \\mathbb S^{d-1}, ||u||_0 = k} u&#39; \\hat \\Sigma u\\). 이 estimator 는 이하를 통해 타당화. Theorem 8.12 (Sparse PCA) Corollary 7.4 와 같은 setting 을 생각하자. 여기에 추가로 leading evec \\(v\\) 가 $k d 2: ||v||_0 k $ 를 만족한다고 assume. 이때 \\(\\hat \\Sigma\\) 의 k-sparse largest evec \\(\\hat v_{sp}\\) 는 with probability \\(1-\\delta\\) 로 이하를 만족. \\[ \\operatorname*{min}_{\\epsilon\\in\\{\\pm1\\}} \\|\\epsilon \\cdot \\widehat{v}_{\\mathrm{sp}}-v\\|_{2} \\lesssim \\frac{1}{\\theta}\\operatorname*{max}\\left\\{ \\sqrt{\\frac{k\\log(\\frac{e d}k)+\\log(\\frac2\\delta)}{n}},\\; \\frac{k\\log(\\frac{e d}k)+\\log(\\frac2\\delta)}{n}\\right\\}, \\] ※ REMARK. 일반적인 PCA 와 달리, k-sparcity 가 만족되었다면, \\(d \\gg n\\) 상황에서도 \\(\\hat v_{sp}\\) 는 consistent 가능. Detour: \\(1 \\le \\forall k \\in \\mathbb Z \\le n : {n \\choose k} \\le \\left( \\frac{en}{k}\\right)^k\\) Proof: thm 7.3 과 동일한 과정을 거쳐 \\(v^{\\top}\\Sigma v-\\widehat{v}_{\\mathrm{sp}}^{\\top}\\Sigma\\widehat{v}_{\\mathrm{sp}}\\leq \\Big \\langle\\widehat{\\Sigma}-\\Sigma, \\; \\widehat{v}_{\\mathrm{sp}}\\widehat{v}_{\\mathrm{sp}}^{\\top}-v v^{\\top} \\Big \\rangle\\) \\(v\\) 와 \\(\\hat v_{sp}\\) 양쪽 모두가 k-sparse 이므로, cardinality \\(|S| \\le 2k\\) 이며, \\((i,j) \\not = S \\times S\\) 일 때 \\(\\{\\hat v_{sp} \\hat v_{sp}&#39; - vv&#39;\\}_{ij}=0\\) 를 만족하는 랜덤 set \\(S \\subset \\{1, \\cdots, d\\}\\) 가 존재한다. 이는 곧 이하를 생산한다. \\[ \\Big \\langle\\widehat\\Sigma-\\Sigma,\\; \\widehat{v}_{\\mathrm{sp}}\\widehat{v}_{\\mathrm{sp}}^{\\intercal}-v v^{\\top} \\Big \\rangle= \\Big \\langle\\widehat{\\Sigma}(S)-\\Sigma(S), \\; \\widehat{v}_{\\mathrm{sp}}(S)\\widehat{v}_{\\mathrm{sp}}(S)^{\\intercal}-v(S)v(S)^{\\intercal} \\Big \\rangle \\] 이때 \\(\\forall M_{d \\times d}\\) 에 대해, 우리는 \\(S\\) 에 의해 row 와 col 이 index 되도록 구성된 \\(M\\) 의 submatrix \\(M(S)_{|S| \\times |S|}\\) 를 정의하자. 또 \\(\\forall \\in \\mathbb R^d\\) 에 대해, \\(S\\) 로 그것의 coordinate 가 index 된 x의 sub-vector \\(x(S) \\in \\mathbb R^{|S|}\\) 를 정의하자. 여기서 Matrix 에 대한 Holder ineq. 를 적용하는 것으로 이하가 생산된다. \\[ v^{\\top}\\Sigma v-\\widehat{v}_{\\mathrm{sp}}^{\\top}\\Sigma\\widehat{v}_{\\mathrm{sp}}\\leq \\Big \\|\\widehat{\\Sigma}(S)-\\Sigma(S) \\Big \\|_{\\mathrm{op}} \\Big \\|\\widehat{v}_{\\mathrm{sp}}(S)\\widehat{v}_{\\mathrm{sp}}(S)^{\\top}-v(S)v(S)^{\\top} \\Big \\|_{1} \\] 이제 thm 7.3 과 동일한 과정을 거치는 것으로 이하의 관계를 얻는다. \\[ \\sin \\bigg (\\angle(\\hat{v}_{\\mathrm{sp}},v) \\bigg)\\leq\\frac{2}{\\theta}\\operatorname*{sup}\\limits_{S:|S\\vert=2k} \\Big \\|\\hat{\\Sigma}(S)-\\Sigma(S) \\Big \\|_{\\mathrm{op}} \\] 증명을 마무리하기 위해 \\(\\sup_{S:|S|=2k} \\Bigg \\| \\hat \\Sigma(S) - \\Sigma(S) \\Bigg \\|_{op}\\) 를 control 하는 일이 남아있다. 이를 위해 이하를 보이자. $$ \\[\\begin{align} \\forall t\\ge0:\\mathbb{P}{\\Biggl(}\\operatorname*{sup}_{S:\\mathbf{|}S|=2k} \\bigg \\|{\\hat{\\Sigma}}(S)-\\Sigma(S) \\bigg \\|_{\\mathrm{op}}\\geq t{\\Biggr)} &amp;\\leq\\ \\sum_{S:|S|=2k}\\mathbb{P}{\\Bigg(} \\bigg \\|{\\hat{\\boldsymbol{\\Sigma}}}(S)-\\Sigma(S) \\bigg \\|\\log\\geq t{\\Bigg)} \\\\ &amp;\\stackrel{(i)}{\\leq}~{d \\choose {2k}} \\times2\\times9^{2k}\\times\\exp\\Biggl(-\\frac{1}{2}\\operatorname*{min}\\biggl\\{\\frac{n t}{16\\sigma^{2}},~\\frac{n t^{2}}{16^{2}\\sigma^{4}}\\biggr\\}\\Biggr) \\\\ &amp;\\stackrel{(ii)}{\\leq}~{d \\choose {2k}} 2 \\exp\\Biggl(-\\frac{1}{2}\\operatorname*{min}\\biggl\\{\\frac{n t}{16\\sigma^{2}},~\\frac{n t^{2}}{16^{2}\\sigma^{4}}\\biggr\\}+ 2k \\log 9 + k \\log\\left( \\frac{en}{k} \\right) \\Biggr ) \\end{align}\\] $$ thm 5.1. 의 증명을 사용. ineq. (7.2) 에 의해 증명. 이제 충분히 큰 \\(C&gt;0\\) 에 대해서, 이하의 식에 의해 \\(t\\) 에 대해 with probabilty at least \\(1-\\delta\\) 로 desired bound 가 성립하며, 그러한 \\(t\\) 를 고르면 된다. \\[ t\\geq C\\sigma^{2}\\operatorname*{max}\\left\\{\\sqrt{\\frac{k\\log(\\frac{ed}k)+\\log(\\frac2\\delta)}{n}},\\;\\frac{k\\log(\\frac{e d}k)+\\log(\\frac2\\delta)}{n}\\right\\} \\] Fin. \\(V_{d \\times d} V&#39; s= V&#39;V = I\\)↩︎ diagonal Matrix \\(\\lambda_{d \\times d}\\), entries with ev \\(\\lambda_1 \\ge \\cdots \\ge \\lambda_d \\ge 0\\)↩︎ vector 안에 들어있는 non-zero elements 의 갯수가 \\(k\\)↩︎ 이때 \\(0^0 = 0\\)이라고 정의↩︎ "],["linear-regression.html", "8.8 Linear Regression", " 8.8 Linear Regression 이것때문이라고 설마? 8.8.1 Problem formulation unknown vector 인 regression vector \\(\\theta^\\ast \\in \\mathbb R^d\\) 설정. 벡터 \\(Y = (Y_1 , \\cdots, Y_n)&#39; \\in \\mathbb R^n\\) 를 관측했으며, linear model \\(Y=X\\theta^{*}+\\epsilon\\) 를 통해 이와 관계되어 있는 \\(X \\in \\mathbb R^{n \\times d}\\) 를 가정하자. 이때 \\(\\epsilon\\;=\\;\\left(\\epsilon_{1},\\cdot\\cdot\\cdot,\\epsilon_{n}\\right)^{\\top}~\\in~\\mathbb{R}^{n}\\;\\) 이며, 각각은 independent zero-mean \\(\\epsilon_1 , \\cdots, \\epsilon_n \\in SG(\\sigma^2)\\). 이제 \\(\\hat \\theta\\) 를 \\(\\theta^\\ast\\) 의 estimator 로 잡는다. 이하의 2가지가 주된 관심사. Prediction \\(X \\theta^\\ast + \\tilde \\epsilon = \\tilde Y \\overset{iid} \\sim Y\\) 라고 설정하자. 우리의 목적은 \\(\\theta^\\ast\\) 에 대한 우리의 estimate \\(\\hat \\theta\\) 의 구현값을 사용해서 \\(\\tilde Y\\) 를 predict. performance 에 대한 natural measure 는 이하와 같다. 이때 unavoidable error 는 말 그대로 unavoidable 이므로, 우리는 후자인 MSE, 즉 \\(\\mathbb{E}\\left [\\Bigg \\|X(\\theta^{*}-{\\widehat{\\theta}})\\Bigg \\|_{2}^{2}\\right ]\\), 를 조사하고자 한다. \\[ \\frac{1}{n}\\mathbb{E}\\left [\\Bigg \\|{\\tilde{Y}}-X{\\widehat{\\theta}}\\Bigg \\|_{2}^{2}\\right ] = \\frac{1}{n}\\mathbb{E}\\left [\\Bigg \\|{\\tilde{\\epsilon}}+X(\\theta^{*}-{\\widehat{\\theta}})\\Bigg \\|_{2}^{2}\\right ] = \\underbrace{\\frac{1}{n}\\mathbb{E}\\left [\\Bigg \\|\\tilde \\epsilon\\Bigg \\|_{2}^{2}\\right ]}_{\\text{unavoidable error}} + \\underbrace{\\frac{1}{n}\\mathbb{E}\\left [\\Bigg \\|X(\\theta^{*}-{\\widehat{\\theta}})\\Bigg \\|_{2}^{2}\\right ]}_{\\text{Mean Squared Error}} \\] Parameter Estimation 위와는 다른 케이스로, 몇몇 경우에 우리는 regression vector \\(\\theta*\\ast\\) 를 조사하고 싶어하는 경우가 있으며, 이 경우에 관심사 (조사대상) 는 \\[ \\mathbb{E}\\left [\\Bigg \\|\\theta^{*}-{\\widehat{\\theta}}\\Bigg \\|_{2}^{2}\\right ] \\] 8.8.2 Least Squares Estimator in high dimensions 고전적인 LR 은 LS 문제를 풀어내는 것과 같다. 이하의 형을 구한다는 것과 equivalent 이며, 이는 보통 Ordinary Least Squares (OLS) estimator 로 통칭됨. \\({\\widehat{\\theta}}_{\\mathrm{LS}}=(X^{\\top}X)^{-1}X^{\\top}Y=\\arg \\min\\limits_{\\theta \\in \\mathbb R^d}\\|Y-X\\theta\\|_{2}^{2}\\) Gauss–Markov thm 에 의해 우리는 OLS Estimator 가 Best Linear Unbiased Estimator (BLUE) 임을 알고 있음. 왜냐고 특정 condition 하에서의 Linear Unbiased Estimator 들의 class 내에서 OLS Estimator 가 가장 작은 Var 을 가지고 있으니까. 하지만 이 estimator 는 \\(X&#39;X\\) 가 uninvertible 하면 존재할 수 없음. 다른 말로, \\(n \\ge d\\) 인 경우에만 존재할 수 있다는 거임. \\(d&gt;n\\) 라도, \\(\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\|Y-X\\theta\\|_{2}^{2}\\) 라는 문제에 대한 해를 찾아내는 건 가능함. 다음과 같이 매핑하는 function \\(\\theta\\mapsto\\|Y=X\\theta\\|_{2}^{2}\\) 는 convex 이므로, 1차 optimality condition 인 \\(\\nabla_{\\theta}\\|Y-X\\theta\\|_{2}^{2}=0\\quad\\Longleftrightarrow\\quad X^{\\top}X\\theta=X^{\\top}Y\\) 를 체크하는 것만으로 충분함. 이의 해는 이하에 제시된 MP-pseudo Inverse \\(X&#39;X\\) 의 형으로 서술될 수 있음. 8.8.2.1 Mean Squared Error of the Least Squares Estimator Theorem 8.13 (Least Squares Estimator) let linear model \\(Y=X\\theta^{*}+\\epsilon\\), 이때 \\(\\epsilon\\) 의 elements 각각은 independent zero-mean \\(\\epsilon_1 , \\cdots, \\epsilon_n \\in SG(\\sigma^2)\\). 이때 \\(\\hat \\theta_{LS}\\) 는 이하를 만족하며, 2번째 ineq는 with probability at least \\(1-\\delta\\) 로 만족. $$ \\[\\begin{alignat}{2} &amp; &amp;&amp;\\frac{1}{n}E \\Bigg ( &amp;&amp; \\Bigg \\| X\\bigl(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}\\bigr) \\Bigg \\|_{2}^{2} \\Bigg) &amp;&amp;\\lesssim \\sigma^{2}\\frac{r}{m} &amp;&amp; \\; \\; \\; \\; \\; \\; \\; \\; \\;\\;r= rank(X&#39;X) \\\\ &amp;\\forall \\delta&gt;0: &amp;&amp; {\\frac{1}{n}} &amp;&amp;\\|X(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*})\\|_{2}^{2} &amp;&amp;\\lesssim\\sigma^{2} \\left({\\frac{r+\\log(\\frac 1 \\delta)}{n}} \\right) \\end{alignat}\\] $$ Remark \\(d \\le n\\) 이며 \\(rank(B) = rank \\left( \\frac{X&#39;X}{n}\\right) =d\\) 일 때, 이하가 성립한다. 이때 \\(\\lambda_{\\mathrm{min}}(B)\\) 는 \\(B\\) 의 ev 중 최소인 값이며, 따라서 이에 \\(\\|\\hat \\theta_{LS} - \\theta^\\ast \\|^2_2\\) 를 탐색하기 위해 thm 8.2. 를 바로 적용하는 것이 가능하다. 그러나 고차원 케이스에서는 \\(d&gt;n\\) 일 때 \\(\\lambda_{\\mathrm{min}}(B) = 0\\) 이므로 이 방법론을 쓸 수 없다. 따라서 MSE 의 형으로 \\(\\lambda_{\\mathrm{min}}(B)\\) 의 bound 를 설정할 필요가 있기 때문에 가정이 추가적으로 필요하다. \\[ \\begin{alignat}{2} &amp; &amp;&amp; \\lambda_{\\mathrm{min}}(B) &amp;&amp;\\Bigg \\|\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}\\Bigg \\|_{2}^{2} &amp;&amp;\\leq\\left(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}\\right)^{\\top}B&amp;&amp;(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}) \\\\ &amp; &amp;&amp; \\lambda_{\\mathrm{min}}(B) &amp;&amp;\\Bigg \\|\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}\\Bigg \\|_{2}^{2} &amp;&amp;\\leq\\left(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}\\right)^{\\top}\\frac{X&#39;X}{n}&amp;&amp;(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*}) \\\\ &amp;\\iff &amp;&amp; &amp;&amp; \\Bigg \\|{\\widehat\\theta}_{\\mathrm{LS}}-\\theta^{*} \\Bigg \\|_{2}^{2} &amp;&amp;\\leq{\\frac{1}{\\lambda_{\\mathrm{min}}(B)}}\\cdot\\frac{1}{n} &amp;&amp;\\Bigg \\|X({\\widehat\\theta}_{\\mathrm{LS}}-\\theta^{*}) \\Bigg \\|_{2}^{2} \\end{alignat} \\] Proof 8.2 의 증명은 basic inequality 와 sup-out 테크닉에 의존. Basic ineq. 첫줄의 ineq. 는 의 정의에 의해 성립. 이때 \\(\\epsilon\\) 과 \\(\\hat \\theta_{LS}\\) 는 dependent 하기 때문에 \\(\\frac{\\epsilon^{\\top}X(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*})}{||X(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*})||_{2}}\\) 를 control 하기가 어렵다는 것을 note. dependence structure 가 complicated 하다면 더더욱 그럴 것. 이 dependence 를 지워 없애기 위해 sup-out tachnique 를 사용. 이의 maximal ineq. 를 적용하는 것이 해당 문제 해결의 열쇠가 된다. $$ \\[\\begin{array} &amp; &amp; &amp;\\|Y-X{\\widehat{\\theta}}_{\\mathrm{LS}}\\|_{2}^{2} &amp;\\leq &amp;\\|Y-X\\theta^{*}\\|_{2}^{2} &amp;= \\|\\epsilon\\|_{2}^{2} \\\\ &amp;= &amp; \\| \\epsilon + X(\\theta^\\ast - \\hat \\theta_{LS})\\| &amp; \\\\ &amp;= &amp;\\|\\epsilon\\|_{2}^{2}+2\\epsilon^{\\mathsf{T}}X(\\theta^{*}-{\\widehat{\\theta}}_{\\mathsf{L S}})+\\|X(\\theta^{*}-{\\widehat{\\theta}}_{\\mathsf{L S}})\\|_{2}^{2} &amp;&amp; \\\\ \\iff &amp; &amp;\\|X(\\theta^{*}-\\hat{\\theta}_{\\mathrm{LS}})\\|_{2}^{2}\\; &amp;\\leq &amp;\\;2\\epsilon^{\\top}X(\\hat{\\theta}_{\\mathrm{LS}}-\\theta^{*}) &amp; &amp;= &amp;2\\|X(\\hat{\\theta}_{\\mathrm{LS}}-\\theta^{*})\\|_{2}\\times\\frac{\\epsilon^{\\top}X(\\hat{\\theta}_{\\mathrm{LS}}-\\theta^{*})}{\\|X(\\hat{\\theta}_{\\mathrm{LS}}-\\theta^{*})\\|_{2}} \\end{array}\\] $$ Sup-Out \\(X\\)의 column span 의 orthonormal basis 를 \\(\\Psi = [\\psi_1, \\cdots, \\psi_r] \\in \\mathbb R^{n \\times r}\\) 라고 하자. (SVD 를 생각하자.) 특히, \\(\\exists \\nu \\in \\mathbb R^r : X(\\hat \\theta_{LS} - \\theta^\\ast) = \\Psi \\nu\\). 이때 이 \\(\\nu\\) 에 대해 $$ \\[\\begin{alignat}{2} &amp; &amp;&amp;\\frac{\\epsilon^{\\top}X(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*})}{||X(\\widehat{\\theta}_{\\mathrm{LS}}-\\theta^{*})||_{2}} =\\frac{\\epsilon^{\\top}\\Psi\\nu}{||\\Psi \\nu||_{2}} &amp;&amp;=\\frac{\\epsilon^{\\top}\\Psi\\nu}{||\\nu||_{2}} \\\\ &amp; &amp;&amp; &amp;&amp;=\\frac{\\tilde \\epsilon^{\\top}\\Psi}{||\\nu||_{2}} &amp;&amp;\\le&amp;&amp;\\sup\\limits_{u\\in\\mathbb{R}^{r}:||u||_{2}=1}\\tilde{\\epsilon}^{\\top}u \\\\ &amp; \\iff &amp;&amp; &amp;&amp;||X(\\theta^{*}-\\widehat{\\theta}_{\\mathrm{LS}})||_{2}^{2} &amp;&amp;\\leq4 \\cdot &amp;&amp;\\sup\\limits_{u\\in\\mathrm{R}^{r}:\\|u\\|_{2}=1}(\\tilde{\\epsilon}^{\\mathsf{T}}u)^{2} \\end{alignat}\\] $$ since \\(\\forall u \\in \\mathbb S^{r-1} : u^{\\textsf{T}}\\Psi^{\\textsf{T}}\\Psi u=u^{\\textsf{T}}u=1\\),11 \\(\\forall s \\in \\mathbb R: \\mathbb{E}[e^{s{\\tilde{\\epsilon}}^{\\top}}u]=\\mathbb{E}[e^{s\\epsilon^{\\top}\\Psi u}]\\leq e^{{\\frac{s^{2}\\sigma^2}{2}}}\\). 이는 곧 \\(\\tilde \\epsilon \\in SG(\\sigma^2)\\) 이라는 소리. 이때 \\(X\\sim SG(\\sigma^2) \\Longrightarrow Var(X) \\le \\sigma^2\\) (Lecture 2). 따라서 CS ineq. 에 의해 \\(\\mathbb{E}\\left[\\sup\\limits_{u\\in\\mathbb{R}^{r}:\\|u\\|_{2}=1} ({\\tilde{\\epsilon}}^{\\mathsf{T}}u)^{2}\\right] \\leq\\sum\\limits_{i=1}^{r}\\mathbb{E}\\left[{\\widetilde{\\epsilon}}_{i}^{2}\\right]\\leq r\\sigma^{2}\\). 이것이 thm 8.2. 의 첫번째 claim 을 증명. bound in Probability 를 보이기 위해, 우리는 standard discretization argument 를 사용. \\(N_{\\frac12}\\) 를 \\(\\mathbb S^{r-1}\\) 의 \\(\\frac12\\)-covering 이라고 하자. 그러면 Lecture 4 에서의 결과물을 사용하는 것으로 \\(\\sup\\limits_{u\\in\\mathbb{R}^{r}:\\|u\\|_{2}=1} \\widetilde{\\epsilon}^{\\top}u\\le2\\operatorname*{max}_{u\\in{N}_{\\frac12}}\\widetilde{\\epsilon}^{\\top}u\\) 가 얻어진다. 따라서 이하가 성립하며 이를 만족시키는 임의의 \\(\\delta\\) 를 잡았을때 이에서 파생되는 임의의 \\(t\\) 를 얻는다. $$ \\[\\begin{alignat}{2} \\mathbb{P}(\\|X(\\theta^{*}-{\\widehat{\\theta}}_{LS})\\|_{2}^{2}\\geq t)&amp;\\leq\\mathbb{P}{\\Big(}\\max\\limits_{u\\in{ N}_{\\frac12}}({\\tilde{\\epsilon}}^{T}u)^{2}\\geq \\frac t {16}{\\Big)} \\\\ &amp;\\leq \\Bigg|{ N}_{\\frac 12} \\Bigg|e^{-{\\frac{t}{32 \\sigma^2}}} \\\\ &amp;\\leq5^{r}e^{-{\\frac{t}{32 \\sigma^2}}} \\\\ \\exists \\delta: &amp;\\le \\delta &amp;&amp;\\iff \\exists t: t \\ge 32 \\sigma^2 \\left \\{ r \\log 5 + \\log \\left( \\frac {1} {\\delta} \\right) \\right \\} \\end{alignat}\\] $$ 따라서 with probability at least \\(1-\\delta\\), \\(||X(\\theta^{*}-\\hat{\\theta}_{\\mathrm{LS}})||_{2}^{2}\\lesssim \\sigma^{2}\\{r+\\log \\left (\\frac1 \\delta \\right)\\}\\). 8.8.3 Sparse linear regression 이상을 통해 우리는 \\(\\frac d n \\righarrow 0\\) 일 때 \\(\\hat \\theta_{LS}\\) 가 consistent 함을 확인. 따라서 \\(\\frac d n \\righarrow 0\\) 가 우리가 바랄 수 있는 최적의 condition. 특히 \\(\\frac d n\\) 이 0에서 bounded away 된 채로 남는다면 consistent estimator 를 획득하는 건 불가능함. (minimax point of view 에서는.) 이러한 이유로 \\(d &gt; n\\) 상황에서 작업을 할 때는 unknown regression vector \\(\\theta^\\ast\\) 에 추가적인 structure 을 얹는 게 필수가 됨. 이제 \\(\\theta^\\ast\\) 의 대다수가 0이라는 조건인 sparse condition 에 대해서 논해보자. 8.8.3.1 Lasso linear model (8.1)에서 \\(\\theta\\) 의 support set \\(S(\\theta^{*})=\\{j\\in\\{1,\\ldots, d\\}:\\theta_{j}^{*}\\not=0\\}\\) 이 cardinality \\(s=|S(\\theta^\\ast)| \\overset{substantially}{&lt;} d\\), 즉 s-sparse 인 상황 가정. regularized estimator 로서, lasso estimator \\(\\widehat{\\theta}_{\\mathrm{lasso}}=\\arg\\min\\limits_{\\theta\\in\\mathbb{R}^{d}}\\left\\{\\frac{1}{2n}\\|Y-X\\theta\\|_{2}^{2}+\\lambda_{n}\\|\\theta\\|_{1}\\right\\}\\) 는 이러한 sparse structural assumption 에 대한 설명력을 가진다. lasso esimator 는 for many \\(j \\in \\{1, \\cdots, d\\}:\\hat_{lasso, \\; j} = 0\\) 라는 sparcity property 를 가지며 이건 \\(\\lambda_n\\) 을 무엇으로 골랐는지에 의존한다. 위의 등식 (8.3)은 lasso problem 이라고 불리며, 이는 convex optimization problem 이고, computationally tractable. Lemma 8.1 (Basic inequality) lasso estimator 에 대한 basic ineq. \\[ {\\frac{1}{2n}}\\|X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})\\|_{2}^{2}\\ \\leq\\ {\\frac{\\epsilon^{\\top}X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})}{n}}+\\lambda_{n}(\\|\\theta^{*}\\|_{1}-\\|\\widehat{\\theta}_{\\mathrm{lasso}}\\|_{1}) \\] Proof. Lasso 의 정의에 의해, \\[ \\frac{1}{2n}\\Vert Y-X\\widehat{\\theta}_{\\mathrm{lasso}}\\Vert_{2}^{2}+\\lambda_{n}\\Vert\\widehat{\\theta}_{\\mathrm{lasso}}\\Vert_{1}\\ \\leq\\ \\frac{1}{2n}\\Vert Y-X\\theta^{*}\\Vert_{2}^{2}+\\lambda_{n}\\Vert\\theta^{*}\\Vert_{1} = \\frac{1}{2n} ||\\epsilon||_2^2 + \\lambda_n ||\\theta^\\ast ||_1 \\] 그리고 MSE 를 확장하는 것으로 \\[ {\\frac{1}{2n}}\\|Y-X{\\widehat{\\theta}}_{\\mathrm{lasso}}\\|_{2}^{2}\\ =\\ {\\frac{1}{2n}}\\|\\epsilon\\|_{2}^{2}+{\\frac{1}{2n}}\\|X({\\widehat{\\theta}}_{\\mathrm{lasso}}-\\theta^{*})\\|_{2}^{2}+{\\frac{\\epsilon^{\\top}X(\\theta^{*}-{\\widehat{\\theta}}_{\\mathrm{laaso}})}{n}} \\] 둘을 복합. 8.8.3.2 Slow convergence rate 위에서의 basic inequality (lemma 8.3) 이 주어졌을 때, 우리는 lasso estimator \\(\\hat \\theta_{lasso}\\) 의 consistency 를 보장하는 sufficient condition 생성 가능. 이는 deterministic bound 를 구하는 것부터 시작함. Theorem 8.14 (Slow Convergence Rate) \\(X_j\\) 를 \\(X\\) 의 j-th column 이라고 하고, \\(\\lambda_{n}\\,\\geq\\,\\left|\\left|\\frac{X^{\\top}\\epsilon}{n}\\right|\\right|_{\\infty}\\ = \\max_{i \\le j \\le d}\\left|\\frac{X_{j}^{\\textsf{T}}\\epsilon}{n}\\right|\\) 라고 가정하자. 이 때 lasso estimator 는 이하를 만족. \\[ \\frac{1}{n} \\Bigg\\| X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*}) \\Bigg\\| _{2}^{2}\\leq4\\lambda_{n} \\Bigg\\| \\theta^{*} \\Bigg\\| _{1} \\] Proof: $$ \\[\\begin{align} \\frac{1}{2n}||X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})||_{2}^{2}\\ &amp;\\leq\\ \\frac{\\epsilon^{\\top}X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})}{n} &amp;&amp;+\\lambda_{n}(||\\theta^{*}||_{1}-||\\widehat{\\theta}_{\\mathrm{lasso}}||_{1}) \\\\ &amp;\\overset{(i)}{\\le} {\\frac{1}{n}}\\|\\epsilon^{\\mathsf{T}}X\\|_{\\infty}\\|\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*}\\|_{1} &amp;&amp;+\\lambda_{n}(\\|\\theta^{*}\\|_{1}-\\|\\widehat{\\theta}_{\\mathrm{lasso}}\\|_{1}) \\\\ &amp;\\overset{(ii)}{\\le}\\frac{1}{n}||\\epsilon^{\\top}X||_{\\infty}(||\\widehat{\\theta}_{\\mathrm{lasso}}||_{1}+||\\theta^{*}||_{1}) &amp;&amp;+\\lambda_{n}(||\\theta^{*}||_{1}-||\\widehat{\\theta}_{\\mathrm{lasso}}||_{1}) \\\\ &amp;= \\|\\widehat\\theta_{\\mathrm{lasso}}\\|_{1}\\left(\\frac{1}{n}\\|\\epsilon^{\\mathsf{T}}X\\|_{\\infty}-\\lambda_{n}\\right) &amp;&amp;+\\|\\theta^{*}\\|_{1}\\left(\\frac{1}{n}\\|\\epsilon^{\\mathsf{T}}X\\|_{\\infty}+\\lambda_{n}\\right) \\\\ &amp;\\stackrel{(iii)}{\\le}2\\lambda_{n}\\vert\\vert\\theta^{*}\\|_{1} \\end{align}\\] $$ 휠더 부등식 triangle ineq. (8.4) 의 \\(\\lambda_n\\) 에 대해 걸었던 condition. th, 8.4. 의 error bound 는 (8.4) 에서 \\(\\lambda_n\\) 에 대해 걸었던 condition 에 의존. 이제 좀 더 자세히 살펴보자. \\(lambda_n\\) 의 good choice 는 무엇인가? 랜덤벡터 \\(\\epsilon \\in SG(\\sigma^2)\\) 이었음을 상기. 이제 \\(\\exists C&gt;0:\\max\\limits_{1\\le j\\le d} \\|X_i\\|_2 \\le C\\sqrt n\\) 라고 가정 assume 하자. 이때 \\(\\forall n, d:\\max\\limits_{ 1 \\le j \\le d}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}X_{i j}^{2}\\right\\}\\le C\\) 가 성립한다. 이를 통해 standard argument 를 만들수 있다: $$ \\[\\begin{alignat}{2} \\mathbb{P}\\!\\left(\\frac{1}{n}||\\epsilon^{\\textsf{T}}X||_{\\infty}\\geq t\\right)\\ &amp;= \\mathbb{P}\\!\\left(\\max\\limits_{1\\le j \\le d} |X_{j}^{\\textsf{T}}\\epsilon|\\geq\\,t n\\right) \\\\ &amp;\\leq\\mathrm{~}\\sum_{j=1}^{d}\\mathbb{P}(|X_{j}^{\\top}\\epsilon|\\geq\\,t n) \\\\ &amp;=\\;\\sum_{j=1}^{d}\\mathbb{P}\\left({\\frac{|X_{j}^{\\top}\\epsilon|}{||X_{j}||_{2}}}\\geq\\,{\\frac{t n}{||X_{j}||_{2}}}\\right) \\\\ &amp;\\leq\\ 2d\\exp\\left(-\\,\\frac{n^{2}t^{2}}{2\\sigma^{2}\\,\\max\\limits_{1\\le j\\le d}\\,||X_{j}||^{2}}\\right) \\\\ &amp;\\leq\\ 2d\\exp\\left(-\\,\\frac{n t^{2}}{2C^{2}\\sigma^{2}}\\right) &amp;&amp;=\\delta \\end{alignat}\\] $$ 마지막 ineq. 는 시작 전 더해둔 assumption \\(\\max\\limits_{1\\leq j\\leq d}\\|X_{j}\\|_{2}\\leq C{\\sqrt{n}}.\\) 에 의해서 성립. 이제 \\(t=\\lambda_{n}^{*}=\\sqrt{\\frac{2\\sigma^{2}C^{2}}{n} \\left \\{\\log(\\frac 1 \\delta)+\\log(\\frac 2d) \\right\\}}\\) 를 하나 고르는 것으로, 우리는 with probability \\(1-\\delta\\) 에 의해 \\({\\frac{1}{n}}\\|\\epsilon^{\\mathsf{T}}X\\|_{\\infty}\\leq\\lambda_{n}^{*}\\) 가 성립한다는 사실을 파악할 수 있다. 따라서 \\(\\delta = \\frac 1n\\) 으로 잡는 것으로, thm 8.4 를 적용하는 것으로 \\(\\lambda_n^\\ast\\) 가 주어진 lasso estimator 는 이하를 보장한다. \\[ {\\frac{1}{n}}\\|X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})\\|_{2}^{2}\\lesssim \\|\\theta^{*}\\|_{1} \\cdot \\sigma\\sqrt{\\frac{\\log(d)+\\log(n)}{n}} \\] 여기에 추가로 \\(\\max\\limits_{1\\leq j\\leq d}\\left|\\theta_{j}^{\\ast}\\right|\\) 가 uniformly bounded 되어 있다고 suppose 한다면? 그 경우 \\(\\theta^\\ast\\) 의 \\(s\\)-sparcity 하에서, \\(s \\sqrt{\\frac {\\log(d)} n} \\rightarrow 0\\) 이 1번이라도 발생한 순간 MSE 는 0으로 간다. Parameter Estimation 위에서 언급되었 듯이, \\(\\d \\le n\\) 이며 \\(\\lambda_{\\mathrm{min}} \\left(\\frac{X^{\\textsf{T}}X}n \\right)\\;\\geq\\;C_{\\mathrm{min}}\\;\\gt \\;0,\\) 일 경우, 앞의 결과는 이하를 보장한다. 안타깝게도 이 전략은 \\(d&gt;n\\) 인 경우에는 작동하지 않는다. \\(X&#39;X\\) 가 rank-deficient 가 되어 버리므로. \\[ ||{\\widehat\\theta}_{\\mathrm{lasso}}-\\theta^{*}||_{2}^{2} \\lesssim {\\frac{||\\theta^{*}||_1}{C_{\\mathrm{min}}}} \\cdot \\sigma\\sqrt{\\frac{\\log(d)+\\log(n)}{n}} \\] 8.8.3.3 Fast Convergence Rate 디자인 매트릭스 \\(X\\) 에 추가적인 assumption 을 붙이는 것으로 좀 더 빠른 convergence rate 를 얻는 것이 가능. 여기선 Restricted Ev (RE) condition 을 사용할 것. \\(\\{ 1, \\cdots, d\\}\\) 의 subset \\(S\\) 에 대해 \\(Z_S = (Z_{S,1}, \\cdots, Z_{S, d}) \\in \\mathbb R^d)\\) 이며 \\(\\forall j \\in S:Z_{S,j} = Z_j\\), o.w. \\(Z_{S, j} = 0\\) 으로 정의. \\(S^c\\) 를 \\(S\\) 의 complement 로 잡자. 즉슨 \\(Z_{S^c}\\) 도 \\(Z_S\\) 와 유사하게 정의. 이때 \\(\\forall \\alpha\\ge1:C(\\alpha,S)=\\{\\Delta\\in\\mathbb{R}^{d}:||\\Delta_{S^{c}}||_{1}\\leq\\alpha\\|\\Delta_{S}\\|_{1}\\}\\). 이 notation 들을 써서 이하 정의. Definition 8.8 (RE condition) 매트릭스 \\(X\\) 는 이하를 만족할 경우, 패러미터 \\((\\alpha, \\kappa)\\) 와 함께 \\(S\\) 에 대해 RE condition 을 만족한다. \\[ forall \\Delta\\in C(\\alpha,S):{\\frac{1}{n}}\\|X\\Delta\\|_{2}^{2}\\geq\\kappa\\|\\Delta\\|_{2}^{2} \\] Remark. RE condition 에 대한 직관을 좀 얻어보자. cost difference \\(\\mathcal{L}_{n}(\\widehat{\\theta}_{\\mathrm{lasso}})-\\mathcal{L}_{n}(\\theta^{*})=\\frac{1}{2n}\\vert\\vert Y-X\\widehat{\\theta}_{\\mathrm{lasso}}\\vert\\vert_{2}^{2}-\\frac{1}{2n}\\vert\\vert Y-X\\theta^{*}\\vert\\vert_{2}^{2}\\) 가 작다면, error vector \\(\\hat \\theta_{lasso} - \\theta^\\ast\\) 도 또한 작다는 것을 장담할 수 있을까? 일반적으로 이는 그렇다고 할 수 없다. 특히 cost function \\(\\mathcal L_n(\\theta)\\) 가 flat 할 경우에는 더더욱. 이 flat 상황을 피하기 위해 cost function \\(\\mathcal L_n (\\theta)\\) 로 하여금 이의 optimum \\(\\hat \\theta_{lasso}\\) 주위에서 높은 curvature 를 갖도록 하는 것이 요구됨. curvature 는 Hessian MAtrix \\(\\nabla^{2}{\\mathcal{L}}_{n}(\\theta)={\\frac{1}{n}}X^{\\top}X\\) 의 구조에 의해 결정됨. 만약 우리가 이 Hessian Matrix 의 ev 가 0 에서 bounded away 되었다고 장담할 수 있다면, 즉, \\(\\forall \\Delta\\in\\mathbb{R}^{d}\\setminus\\{0\\}:\\frac{1}{n}||X\\Delta||_{2}^{2}\\geq\\kappa||\\Delta||_{2}^{2}\\gt 0\\) 라면, 우리는 모든 지점에서 curvature 를 갖는다는 것을 확신할 수 있을 것. 하지만 고차원 상황 \\(d&gt;n\\) 에서는 Hessian Matrix 는 0 ev 를 가져야만 하고, condition (8.7) (바로 위 상황) 은 성립할 수 없다. 역으로 우리는 \\(C(\\alpha, S)\\) 로 정의된 특정한 지점에서 cost function 이 curved 한지를 고려한다. 이 직관을 써서 RE condition 하에서 lasso estimator 에 대한 deterministic bound 를 생산할 수 있다. 이것이 아래의 thm. Theorem 8.15 (Fast Convergence Rate) linear model (8.1) 을 살피고, \\(S=\\{i : \\theta^\\ast_i \\not = 0 \\}\\) 에 대해 패러미터 \\((3, \\kappa)\\) 를 통해 RE condition 을 만족하는 \\(X\\) 를 assume. 이때, \\(S\\) 의 cardinality 가 \\(s\\) 를 쓰자. 여기서 만약 \\(\\lambda_{n}\\geq2\\left\\| \\frac{X^{\\textsf{T}}\\epsilon}{n}\\right\\|_{\\infty}\\) 가 성립한다면 이하가 성립. \\[ {\\frac{1}{n}}||X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})||_{2}^{2}\\leq9{\\frac{s\\lambda_{n}^{2}}{\\kappa}}, \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; ||\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*}||_{2} \\le3{\\frac{\\sqrt{s}\\lambda_{n}}{n}} \\] Proof: 편의를 위해 \\(\\hat \\Delta = \\hat \\theta_{lasso} - \\theta^\\ast\\). 주어진 condition 하에서 \\(\\hat \\Delta \\in C(3, S)\\) 임을 먼저 보이자. basic ineq. (lemma 8.3) 을 사용하면 \\({\\frac{1}{2n}}||X\\hat{\\Delta}||_{2}^{2}\\; \\leq \\;{\\frac{\\epsilon^{\\top}X\\hat{\\Delta}}{n}}+\\lambda_{n}(||\\theta^{*}||_{1}-||\\hat{\\theta}_{\\mathrm{lasso}}||_{1})\\) 임을 보일 수 있다. \\(\\theta^\\ast\\) 가 s-sparse 이며 \\(\\hat \\Delta = \\hat \\theta_{lasso}-\\theta^\\ast\\) 이므로 이하가 성립. $$ \\[\\begin{alignat}{2} \\|\\theta^{*}\\|_{1}-\\|{\\widehat{\\theta}}_{\\mathrm{lasso}}\\|_{1} &amp;=\\|\\theta_{S}^{*}\\|_{1}-\\|\\widehat{\\theta}_{\\mathrm{lasso}}\\|_{1} \\\\ &amp;= \\| \\theta_{S}^{*}\\|_{1}-\\|{\\hat{\\Delta}}+\\theta^{*}\\|_{1} \\\\ &amp;=\\|\\theta_{S}^{*}\\|_{1} &amp;&amp;-\\|\\widehat{\\Delta}_{S}+\\theta_{S}^{*}\\|_{1} - \\| \\hat \\Delta_{S^c} \\|_1 \\\\ &amp;\\leq \\|\\hat{\\Delta}_{S}\\|_{1}+\\ \\|\\widehat{\\Delta}_{S}+\\theta_{S}^{*}\\|_{1} &amp;&amp;- \\|\\widehat{\\Delta}_{S}+\\theta_{S}^{*}\\|_{1}\\nonumber-\\|\\widehat{\\Delta}_{S^{c}}\\|_{1}\\nonumber \\tag{triangle ineq.} \\\\ &amp;= \\| \\hat \\Delta_S \\|_1 - \\| \\hat \\Delta_{S^c}\\|_1 \\end{alignat}\\] $$ $$ \\[\\begin{alignat}{2} 0\\;\\leq\\;\\frac{1}{n}\\Vert X\\hat{\\Delta}\\Vert_{2}^{2} &amp;\\stackrel{(i)}{\\le}\\;\\frac{2}{n}\\Vert\\epsilon^{\\top}X\\Vert_{\\infty}\\Vert\\hat{\\Delta}\\Vert_{1} &amp;&amp;+2\\lambda_{n}(\\Vert\\hat{\\Delta}_{S}\\Vert_{1}-\\Vert\\hat{\\Delta}_{S^{c}}\\Vert_{1}) \\tag{Holder} \\\\ &amp; {\\stackrel{\\mathrm{(ii)}}{\\leq}}\\ \\ \\lambda_{n}\\|{\\widehat\\Delta}\\|_{1} &amp;&amp;+2\\lambda_{n}(\\|{\\hat\\Delta}_{S}\\|_{1}-\\|{\\hat\\Delta}_{S^{c}}\\|_{1}) \\tag{condition (8.8) on λ_n} \\\\ &amp;= \\lambda_n (\\| \\hat \\Delta_S \\|_1 + \\| \\hat \\Delta_{S^c}\\|_1 ) &amp;&amp;+2\\lambda_n (\\| \\hat \\Delta_S \\|_1 - \\| \\hat \\Delta_{S^c}\\|_1 ) \\\\ &amp;=\\lambda_{n}(3||\\widehat\\Delta_{S}||_1-||\\widehat\\Delta_{S^{c}}||_1) \\end{alignat}\\] $$ 이를 통해 \\(\\hat \\Delta \\in C(3,S)\\) 라고 결론지을 수 있으며, 우리는 \\(X\\) 가 패러미터 \\((3, \\kappa)\\) 와 함께 RE condition 을 만족한다고 assume 했으므로, 이는 곧 \\({\\frac{1}{n}}\\|X{\\hat{\\Delta}}\\|_{2}^{2}\\geq\\kappa\\|{\\hat{\\Delta}}\\|_{2}^{2}\\) 라는 것을 보여준다. 앞의 과정을 다시 사용해서 이제 $$ \\[\\begin{align} {\\frac{1}{n}}\\|X{\\widehat{\\Delta}}\\|_{2}^{2}\\;&amp;\\leq\\;\\lambda_{n}(3\\|\\widehat{\\Delta}_{S}\\|_{1}-\\|\\widehat{\\Delta}_{S^{c}}\\|_{1})\\leq3\\lambda_{n}\\|\\widehat{\\Delta}_{S}\\|_{1} \\\\ &amp;\\overset{(i)}{\\le} 3 \\lambda_{n}\\sqrt{s}||\\hat{\\Delta}_{S}||_{2} \\tag{1} \\\\ &amp;\\leq\\ 3\\lambda_{n}\\sqrt{s}||\\hat{\\Delta}||_{2} \\\\ &amp;\\overset{(ii)}{\\le}\\frac{3\\lambda_{n}\\sqrt{s}}{\\sqrt{n\\kappa}}\\|X\\widehat{\\Delta}\\|_{2} \\tag{ineq. (8.9)} \\\\ \\iff {\\frac{1}{n}}\\|X(\\widehat{\\theta}_{\\mathrm{lasso}}-\\theta^{*})\\|_{2}^{2} &amp; \\leq9{\\frac{s\\lambda_{n}^{2}}{\\kappa}} \\end{align}\\] $$ \\(\\forall x\\in\\mathbb{R}^{d},\\,\\|x\\|_{1}\\leq{\\sqrt{d}}\\|x\\|_{2}\\) 따라서 위의 마지막 ineq. 와 ineq. (8.9) 를 다시 한번 적용하면 증명 완료. Remark. \\(\\lambda_{n} \\asymp \\sigma\\sqrt{\\frac{\\log(d)+\\log(n)}{n}}\\) 상황이라고 가정하자. 이 경우 design 매트릭스에 (8.5) 와 유사한 condition 을 두고 같은 argument 를 적용하면 \\(\\lambda_{n}\\geq2 \\left \\| \\frac{X^{\\top}\\epsilon}{n} \\right \\|_{\\infty}\\) with probability at least \\(1-\\frac {1} {n^c}\\) for some constant \\(c&gt;0\\) 임을 보일 수 있다. 이는 또한 (\\(d \\gg n\\) 일 때) \\({\\frac{1}{n}}\\|X({\\widehat{\\theta}}_{\\mathrm{lasso}}-\\theta^{*})\\|_{2}^{2}\\lesssim{\\frac{s\\log(d)}{n}}\\) 라는 것으로도 이어지며, 따라서 RE condition 하에서 once \\(\\frac{s \\log(d)}{n}\\rightarrow 0\\) 라면 lasso estimator 는 consistent 하다. 이와 별개로 \\(\\lim\\limits_{n \\rightarrow \\infty} \\lambda_n = 0\\) 를 가정하는 것으로, thm 8.6 의 결과가 8.4의 결과를 former의 upper bound 가 \\(\\lambda_n\\) 이 아니라 \\(\\labmda_n^2\\) 에 의존한다는 것으로 진화시킨다는 것을 발견할 수 있다. \\(\\mathbb S^{r-1} =\\{x\\in\\mathbb{R}^{r}: \\|x\\|_2 = 1\\}\\)↩︎ "],["uniform-laws-of-large-numbers.html", "8.9 Uniform laws of large numbers", " 8.9 Uniform laws of large numbers 랜덤변수의 fixed sequence 에만 적용되던 usual LLN 을 확장하여 uniform LLN 는 랜덤변수의 collection 들에 대해 uniform 하게 성립함. 이제 non-symptotic 결과물에 대해서도 설명할 것. 8.9.1 Motivation Detour: \\(\\mathbb{P}\\Big(\\Big\\{s\\in S:\\operatorname*{lim}_{n\\to\\infty}X_{n}(s)=X(s) \\Big\\}\\Big)=1\\) 일 경우, \\(X_n \\overset{a.s.}{\\rightarrow} X\\)12. 이때 \\(S\\) 는 underlying sample space. 8.9.1.1 Cumulative distribution functions 랜덤변수 \\(X\\) 가 cdf \\(F(t\\in \\mathbb R)\\) 을 보유한다. 우리가 collection \\(\\{X_i\\}^n_{i=1}\\) 를 보유하며 이는 \\(X\\) 의 \\(n\\) 개의 \\(iid\\) copy 들이라고 하자. \\(F\\) 에 대한 natural estimate 는 empirical cdf 이며, 이 empirical cdf 는 이하와 같다. 이 경우 population cdf 는 \\({{F}}(t)=\\mathbb E \\Big [{I}_{(-\\infty,t]}(X) \\Big ]\\) 로 표기될 수 있으므로, empirical cdf 는 UE. \\[ \\widehat{F}_{n}(t)=\\frac{1}{n}\\sum_{i=1}^{n}I_{(-\\infty,t]}(X_{i}) \\tag{empirical cdf} \\] \\(\\forall\\) fixed \\(t \\in \\R\\), 랜덤변수 \\(\\widehat F_n(t)\\) 는 \\(F(t)\\) 를 가지며, 모든 차수 (order) 의 moment 또한 가진다. 따라서 strong LLN 을 적용하는 것으로 \\({\\widehat{F}}_{n}(t)\\ {\\xrightarrow{\\mathrm{a.s.}}}\\ F(t)\\) 임을 획득할 수 있다. 우리의 목적은 이 pointwise convergence 를 확장시켜 uniform convergence 가 성립함을 보이는 것. 특히 이하를 보이는 것이 주된 목적. 이 uniform convergence 결과는 실제로 성립하며 이의 이름은 Glivenko–Cantelli theorem. \\[ \\|{\\hat{F}}_{n}-F\\|_{\\infty}=\\operatorname*{sup}_{t\\in\\R}|{\\hat{F}}_{n}(t)-F(t)|\\ {\\stackrel{\\mathrm{a.s.}}{\\longrightarrow}}\\ 0 \\tag{9.1.} \\] 이러한 uniform convergence 결과가 유의미한 이유는 무엇인가? 많은 estimation problem 이 cdf \\(F\\) 를 실수 \\(\\gamma(F)\\) 로 mapping 하는 functional 표기법으로 치환될 수 있으니까. plug-in principle 은 이 unknown cdf \\(F\\) 를 empirical cdf \\(\\hat F_n\\) 으로 대체하는 것으로 \\(\\gamma(\\hat F_n)\\) 을 $(F) 의 estimate 로 인식할 수 있다는 것을 제시하고 있음. 예시 몇가지: Remark (Continuity of a functional) 우리가 \\(\\|{\\hat{F}}_{n}-F\\|_{\\infty}\\to0\\) in probability 를 깨달았다면, 우리는 자연스럽게 functional \\(\\gamma\\) 가 sup-norm 에 비추었을 때 연속일 경우, \\(|\\gamma(\\hat{F_{n}})-\\gamma(F)|\\to0\\) in probability 라는 것 또한 알 수 있다. 우리는 functional \\(\\gamma\\) 의 continuity 를 sup-norm \\(\\|F-G\\|_\\infty=\\sup\\limits_{t\\in\\mathbb{R}}|F(t)-G(t)|\\) 에 기반하여 정의할 것이다. 더 정확하게 설명하자면, \\(\\forall \\epsilon&gt;0, \\exists \\delta&gt;0: \\|F-G\\|_\\infty &lt;\\delta \\; \\; \\; \\Rightarrow \\; \\; \\; |\\gamma(F) - \\gamma(G)|\\le \\epsilon\\) 가 성립할 경우, functional \\(\\gamma\\) 는, in sup-norm, \\(F\\) 에서 연속이다. 결과적으로 \\(\\forall \\epsilon&gt;0:\\mathbb{P}(|\\gamma({\\hat{F}}_{n})-\\gamma(F)|\\gt \\epsilon)\\leq\\mathbb{P}(||{\\hat{F}}_{n}-F||_{\\infty}\\gt \\delta)\\) 가 성립하며, 이를 통해, (9.1.) 의 Glivenko–Cantelli theorem 에 의해, \\(\\mathbb{P}(\\|{\\hat{F}}_{n}-F\\|_{\\infty}\\gt \\delta)\\to0\\) 이며, 이인즉 \\(|\\gamma(\\hat{F}_{n})-\\gamma(F)|\\to0\\) in probability. 8.9.1.2 Uniform laws for more general function classes unifrom LLN 의 좀 더 general 한 고려로 넘어가보자. - \\(\\mathcal F\\): domain \\(\\mathcal X\\) 를 가지는 적분가능한 (integrable) real-valued function 의 class - ${ X_i }^n_{i=1}: over \\(\\mathcal X\\) 인 어떤 distribution \\(\\mathbb P\\) 로부터의 iid 샘플 의 collection 이제 이하의 랜덤변수를 생각해보자. 이는 over class \\(\\mathcal F\\), 샘플평균과 population 평균 간의 absolute deviation 을 uniformly 측정한다. 이때 만약 \\(n \\to \\infty\\) 일 때 \\(\\|\\mathbb{P}_{n}-\\mathbb{P}\\|_{\\mathcal F}\\) 가 0 으로 in probability 하게 수렴하면, 우리는 \\(\\mathcal F\\) 를 Glivenko–Cantelli class 라고 명명한다. \\[ \\|\\mathbb{P}_{n}-\\mathbb{P}\\|_{\\mathcal F}=\\sup_{f\\in{\\mathcal{F}}}\\left|{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})-\\mathbb{E}\\Big [f(X) \\Big ]\\right| \\] 8.9.1.3 Empirical risk minimization empirical risk minimization 에서 quantity \\(\\|{\\vec{\\mathbf{p}}}_{n}-\\mathbb{P}\\|_{{\\mathcal{F}}}\\) 가 유의미하게 사용됨. probability distribution 의 indexed family \\(\\{\\mathbb P_\\theta : \\theta \\in \\Omega\\}\\) 를 생각해보자. fixed 되었지만 unknown 인 임의의 \\(\\theta^\\ast \\in \\Omega\\) 에 대해, distribution \\(\\mathbb P_{\\theta^\\ast}\\) 에서 iid 샘플을 추출한 상황을 생각하자. 여기서 \\(\\theta^\\ast\\) 를 estimate 하는 가장 보편적인 방법은 패러미터 \\(\\theta \\in \\Omega\\) 와 샘플 \\(X \\in \\mathcal X\\) 둘 사이의 fit 을 measure 하는 cost function \\(\\mathcal L_{\\theta}(X)\\) 을 최소하하는 것. 그후 우리는 empirical risk \\(\\widehat{R}_{n}(\\theta,\\theta^{*})={\\frac{1}{n}}\\sum_{i=1}^{n} \\mathcal L_{\\theta}(X_{i})\\) 를 최소로 하는 estimate \\(\\hat \\theta\\) 를 획득할 수 있다. 대조적으로 population risk \\(R(\\theta,\\theta^{*})=\\mathbb{E}_{\\theta^{*}} \\Big [{\\mathcal{L}}_{\\theta}(X) \\Big ]\\), 여기서 기댓값 \\(\\mathbb E_{\\theta^\\ast}\\)는 over 샘플 \\(X \\sim \\mathbb P_{\\theta^\\ast}\\) 에 대해서 구해졌다. 여기서 주로 논해지는 통계적 질문은 excess risk \\(E(\\widehat{\\theta},\\theta^{*})=R(\\widehat{\\theta},\\theta^{*})-\\inf\\limits_{\\theta\\in\\Omega}R(\\theta,\\theta^{*})\\) 를 어떻게 bound 할 것인가. 간단함을 위해 \\(R(\\theta_{0},\\theta^{*})=\\inf \\limits_{\\theta\\in\\Omega}R(\\theta,\\theta^{*})\\) 를 만족하는 \\(\\theta_0 \\in \\Omega\\) 가 존재한다고 가정하자. 이 notation 을 사용할 경우 excess risk 는 이하와 같이 표기 가능. \\[ E(\\widehat{\\theta},\\theta^{*})= \\underbrace{ \\Big \\{ R(\\widehat{\\theta},\\theta^{*})- \\hat R_n(\\widehat{\\theta},\\theta^{*}) \\Big \\}}_{T_1} + \\underbrace{\\Big \\{ \\hat R_n(\\widehat{\\theta},\\theta^{*}) -\\hat R_n(\\widehat{\\theta}_0,\\theta^{*}) \\Big \\}}_{T_2} +\\underbrace{ \\Big \\{\\hat R_n(\\widehat{\\theta}_0,\\theta^{*}) - R({\\theta}_0,\\theta^{*}) \\Big \\}}_{T3} \\tag{excess risk} \\] \\(\\hat R_n (\\hat \\theta , \\theta^\\ast)\\) 를 minimize 한다는 \\(\\hat \\theta\\) 의 정의 덕분에 우리는 \\(T_2 \\le 0\\) 임은 알 수 있다. \\(T_{3}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal L_{\\theta_{0}}(X_{i})-\\mathbb{E}_{X}[{\\mathcal{L}}_{\\theta_{0}}(X)]\\) 는 centered at 0 인 iid 랜덤변수들의 sample average 이다. 따라서 \\(T_3\\) 를 analyze 함에 있어 Hoeffding’s inequality 사용 가능. \\(T_{1}=\\mathbb{E}_{X}[{\\mathcal{L}}_{\\hat{\\theta}}(X)]-{\\frac{1}{n}}\\sum_{i=1}^{n}{\\mathcal{L}}_{\\hat{\\theta}}(X_{i})\\) 은 control 하기 좀 빡셈. 패러미터 \\(\\hat \\theta\\) 가 랜덤이고, 샘플들 \\(\\{X_i\\}^n_{i=1}\\) 에 의존하기 때문이다. 따라서 \\(T_1\\) 을 control 하는 건 강력한 result 를 필요로 하며, 바로 여기에서 cost function \\(\\mathcal{F}(\\Omega)=\\{x\\mapsto\\mathcal{L}_{\\theta}(x),\\theta\\in\\Omega_{0}\\}\\) 에 대해 (over) uniform LLN 를 적용한다. 이 notation 을 사용하면 이하와 같은 형식으로 \\(T_1\\) 이 정리됨. \\[ T_{1}\\leq\\operatorname*{sup}_{\\theta\\in\\Omega_{0}}\\left|{\\frac{1}{n}}\\sum_{i=1}^{n}{\\mathcal{L}}_{\\theta}(X_{i})-\\mathbb{E}_{X}[{\\mathcal{L}}_{\\theta}(X)]\\right|=\\|{\\mathbb{P}}_{n}-\\mathbb{P}\\|_{{\\mathcal{F}}(\\Omega)} \\] \\(T_3\\) 는 이 quantity 에 의해 dominated 되므로, 우리는 excess risk 가 \\(E(\\widehat{\\theta},\\theta^{*})\\leq2\\|\\mathbb{P}_{n}-\\mathbb{P}\\|_{{\\mathcal{F}}(\\Omega)}\\) 에 의해 bound 된다고 결론지을 수 있다. 이를 통해 우리는 empirical risk 최소화에 기반한 estimator analyzing 에 있어 가장 중요한 장애물은 loss class \\(\\mathcal F(\\Omega)\\) 에 대해 uniform LLN 을 구축하는 일이라는 것을 알 수 있다. 8.9.2 A uniform law via Rademacher complexity uniform law 의 구축에 필수불가결한 것은 function class \\(\\mathcal F\\) 의 Rademacher complexity. 아무 fixed class \\(x_{1}^{n}=(x_{1},\\cdots,x_{n})\\) 에 대해, \\(\\mathcal{F}(x_{1}^{n})=\\Bigg \\{\\Big (f(x_{1}),f(x_{2}),\\ldots,f(x_{n}) \\Big ):f\\in\\mathcal{F}\\Bigg \\}\\) 로서 주어지는 \\(\\R^n\\) 의 subset 을 생각하자. 이때 empirical Rademacher complexity 는 이하와 같다. \\[ \\mathcal R \\left(\\frac{{\\mathcal{F}}(x_{1}^{n})}n \\right) = \\mathbb{E}_{\\epsilon}{\\Bigg[}\\sup_{f\\in{\\mathcal{F}}}{\\Biggl|}{\\frac{1}{n}}\\sum_{i=1}^{n}\\epsilon_{i} \\cdot f(x_{i}){\\Biggr|}\\ \\Bigg] \\tag{empirical Rademacher complexity} \\] \\(\\epsilon_{1:n}\\): iid Rademacher 랜덤변수. 랜덤변수의 collection \\({X_{1}^{n}}=\\{{X_{i}}\\}_{i=1}^{n}\\) 가 주어졌다면, empirical Rademacher complexity \\(\\mathcal R \\left(\\frac{{\\mathcal{F}}(x_{1}^{n})}n \\right)\\) 또한 랜덤변수이다. \\(X_1^n\\) 에 대해 expectation 을 취하는 것으로 Rademacher complexity of the function class \\(\\mathcal F\\) 가 획득되며, 이는 주로 deterministic quantity. \\[ {\\cal R}_{n}({\\mathcal F})=\\mathbb{E}_{X}\\left[{\\cal R}\\left( \\frac{{\\mathcal F}(X_{1}^{n})}n\\right)\\right]=\\mathbb{E}_{X,\\epsilon}\\left[\\operatorname*{sup}_{f\\in{\\mathcal F}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}f(X_{i})\\right|\\right] \\] 여기서 Rademacher complexity 는 vector \\(\\Big(f(X_{1}),f(X_{2}),\\cdots,f(X_{n})\\Big)\\) 와 noise vector \\((\\epsilon_{1:n})\\) 사이의 maximum correlation 임을 note. function class 가 극도로 크다면 우리는 언제나 무작위로 drawn 된 noise vector 에 대해 높은 correlation 을 가지는 function 을 찾아내는 것이 가능하다. 역으로 function class 가 지나치게 작다면 무작위로 drawn 된 noise vector 에 대해 기댓값 적으로 강하게 correlate 된 function 을 찾아내는 것은 불가능할 지도 모른다. 이러한 관점에서 결국 Rademacher complexity 는 function class 의 size 를 측정하며, 이는 곧 uniform convergence result 를 구축함에 있어 핵심적인 역할을 한다. 이하는 \\(\\forall f \\in \\mathcal F \\in \\|f\\|_\\infty \\le b\\) 일 경우, function blass \\(\\mathcal F\\) 는 \\(b\\)-uniformly bounded 임을 보여준다. Theorem 8.16 (Glivenko–Cantelli property) \\(\\forall \\mathcal F\\), function 의 \\(b\\)-uniformly bounded class, 에 대해 \\(\\forall\\) positive integer \\(n \\ge 1\\) 과 \\(\\forall\\) scale \\(\\delta \\ge 0\\) 에 대해 with \\(\\mathbb P\\)-probability at least \\(1 - \\exp \\left ( - \\frac{n \\delta^2}{2b^2} \\right)\\) 에 대해 이하가 성립한다. \\[ \\|\\mathbb{P}_{n}-\\mathbb{P}\\|_{{\\mathcal{F}}}\\leq2\\mathcal R_{n}({\\mathcal{F}})+\\delta \\] 결과적으로 \\(\\mathcal R_n (\\mathcal F) = o(1)\\) 인 한, 우리는 \\(\\left\\|\\mathbb{P}_{n}-\\mathbb{P}\\right\\|_{{\\mathcal{F}}}\\overset{a.s.}{\\to}0\\) 임을 얻는다. 8.9.3 Upper bounds on the Rademacher complexity Glivenko–Cantelli property 가 유용하기 위해선 우리는 Rademacher complexity \\(\\mathcal R_n(\\mathcal F)\\) 에 대한 upper bound 를 획득할 필요가 있다. 이에 대해서는 다양한 방법론이 존재하며 simple union bound (finite funciton class 들에 적합한) 부터 metric entropy 의 개념부터 chaining argument 를 아우르는 고등한 방법까지 다양하다. 여기서는 polynomial discrimination 을 보유하는 function class 들에 적용할 수 있는 간단한 방법을 사용하겠다. Definition 8.9 (Polynomial discrimination) domain \\(\\mathcal X\\) 를 가지는 function 들의 class \\(\\mathcal F\\) 는 이하가 성립한다면 polynomial discrimination of order \\(\\nu \\ge 1\\) 를 가진다. 각각의 positive integer \\(n\\) 과, \\(\\mathcal X\\) 안의 \\(n\\)개의 point 들로 이루어진 collection \\(x_1^n = \\{x_{1:n}\\}\\) 에 대해, set \\({\\mathcal{F}}(x_{1}^{n})=\\Big \\{(f(x_{1}),\\ldots,f(x_{n})):f\\in{\\mathcal{F}}\\Big \\}\\) 은 \\(\\operatorname{card} \\Big ({\\mathcal{F}}(x_{1}^{n}) \\Big )\\leq(n+1)^{\\nu}\\) 로 upper bounded 된 cardinality 를 가진다. 이 성질은 empirical Rademacher complexity 를 control 하는데 있어 straightforward 한 접근법을 제공한다는 점에서 유의. Lemma 8.2 (Bound on the empirical Rademacher complexity) \\(\\mathcal F\\) 가 polynomial discrimination of order \\(\\nu\\) 를 가진다고 하자. 이때 \\[ \\forall n&gt;0\\in \\Z, \\forall \\text{collection of points }x^n_1 = (x_{1:n}): \\mathbb{E}_{\\epsilon}{\\Biggl[}\\sup_{f\\in{\\mathcal{F}}}{\\bigg|}{\\frac{1}{n}}\\sum_{i=1}^{n}\\epsilon_{i}f(x_{i}){\\biggr|} \\Bigg]\\leq2D(x_{1}^{n}){\\sqrt{\\frac{\\nu\\log(n+1)}{n}}} \\] \\(D(x_{1}^{n})=\\sup\\limits_{f\\in{\\mathcal{F}}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}f^{2}(x_{i})}}\\) 는 set \\(\\frac{\\mathcal F(x_1^n)}{\\sqrt n}\\) 의 \\(l_2\\)-raidus. almost surely.↩︎ "],["survival-analysis.html", "Chapter 9 Survival Analysis ", " Chapter 9 Survival Analysis "],["introduction-5.html", "9.1 Introduction", " 9.1 Introduction SA의 결과물은 보통 time-to-event, 즉슨 대부분의 경우에 nonnegative이며, 이는 곧 time domain을 한정함. time-to-event의 distribution은 보통 skewed. Survival data은 자주 right censored. 조사 대상자들은 조사 기간중에만 생존했음을 알며, 조사 기간 넘어서 죽으면 해당 시간이 정확히 기록되지 않음. tail probability. 충분한 후속연구 후에는, tail of survival curve에 해당하는 subject들이 보통 되게 적음. estimation of the tail of the survival curve can be quite difficult. tail에서 survival density는 엄청 적어짐. 따라서 총 표본 수가 많이 확보되어 있지 않으면 tail에 해당하는 분석결과는 확보하기가 어렵다. 모든 연구의 시간은 finite이므로 모든 subjects들에게서 발생한 event of interest 중 일부는 육안으로 관찰 못 할수도 있다. 장기적으로 발생은 했는데, 그게 우리 손닿는 곳에서 터지지 않았음. 일반적으로 관측안된 failure time 들이 포함되어 있으면 기존 통계 테크닉은 사용할 수 없음. failure time 이 관측되지 않은 subject들은 censored 되었다고 표현. censored observations를 포함한 자료에서 정보를 뽑아내는 것이 SA의 estimation methods의 목적. 9.1.0.1 Censoring Sources Adminisitrative censoring event 발생 전에 연구 종료 often independent of failure time Loss to follow-up subject들이 더이상 트랙 불과, 관찰 하에 있지 않음 (후속연구 개시했는데 예전에 살던 사람이 동네 떠났음) censoring may be related (indirectly) to the failure time withdrawl from study 너무 아프거나 증상이 낫던가 해서 연구에서 이탈 dependent censoring (informative drop-out), censoring이 failure time에 연관되어 있다는 점이 고민해야할 거리가 된다. 9.1.0.1.0.0.0.0.1 임시방편 censor된 시간을 failure time으로 인식. \\(\\bar X \\le E(X)\\) (underestimate). censor 관측치를 전부 삭제. loss of infomration. 9.1.0.1.0.1 notation \\(T_i\\): potential failure time for the i-th subject \\(C_i\\): potential censoring time for the i-th subject \\(X_i = \\min(T_i , C_i )\\) observed time \\(\\delta_i = \\begin{cases} 1, &amp; T_i \\le C_i &amp; \\text{(uncensored)} \\\\ 0, &amp; T_i &gt; C_i &amp; \\text{(censored)} \\end{cases}\\) 9.1.0.2 Right Censoring (most of the course) Fail이 확실하게 터진 경우에만 fail, 이외의 경우에는 censor. 조사기간 종료까지 발병하지 않았거나, 이외의 이유로 종료 이전에 연구 이탈하면 양쪽 모두 censored. 9.1.0.2.0.1 Type of Data to be analyzed in survival analysis Type Ⅰ Censoring: 특정 시점이 왔을 때 연구 종료. ex) 쥐한테 특정 영양소 먹이고 언제까지 생존하는지 Progressive Type Ⅰ Censoring: 대상들이 다른, 고정된 sacrifice time 보유 ex) 도즈 레벨 4개로 나누고 각 그룹에 다른 sacrifice 기간 적용, 비용 효율화 Generalized Type Ⅰ Censoring: subject들이 각각 다른 시기에 연구에 참여개시하고 정해진 시간에 연구 종료됨. subject가 참여할 때 censoring time 다 알려짐. Type Ⅱ Censoring: reliabilty 분석에서 흔함. 특정 횟수 failure 발생시 연구 종료. ※ Right Censoring: 개인의 정확한 survival time은 follow-up period의 우측에서는 incomplete해짐. Random Censoring: Censoring times are random. ※ let’s focus on right censoring. Suppose \\(T_1 , \\cdots, T_n \\sim f(t)\\) and \\(C_1 , \\cdots, C_n \\sim g(c)\\). Then, we observe \\(X_i = \\min(T_i , C_i )\\) for \\(i = 1, \\cdots, n\\). In type Ⅰ censoring, \\(C_i\\) is fixed (at \\(C_r\\) or \\(C_{r_i}\\)). In random censoring, \\(C_i\\) is random. 9.1.0.3 Left Censoring less common in practice $$ \\[\\begin{align} \\lambda(t) S(t)&amp;= f(t) \\\\ \\lambda(t) &amp;= \\dfrac{f(t)}{S(t)} \\\\ \\lambda(t) &amp;= \\dfrac{f(t)}{} \\dfrac{d}{dS(t)}\\log S(t) \\\\ \\end{align}\\] $$ $$ \\[\\begin{align} \\lambda(t) &amp;= - \\dfrac{d}{dt} \\log S(t) \\\\ \\lambda(t) &amp;= - \\dfrac {dS(t)}{dt} \\dfrac{d}{dS(t)} \\log S(t) \\\\ \\lambda(t) &amp;= - \\dfrac {d[1-F(t)]}{dt} \\dfrac{1}{S(t)} \\\\ \\lambda(t) &amp;= - (-f(t)) \\dfrac{1}{S(t)} \\\\ S(t)\\lambda(t) &amp;= f(t) \\end{align}\\] $$ \\[ A = A&#39;\\; \\; \\; \\Longrightarrow \\; \\; \\; \\exists \\text{basis for } C(A):\\text{constisting of evec of nonzero ev&#39;s.} \\] linear transformation, span, trace, nonsingular, null space \\[ tr(ABC) = tr(BCA)=tr(CAB) \\] \\[ r(A_{n \\times n})=r, \\; \\; \\; r[\\mathcal{N}(A)] = n-r \\] \\(\\lambda\\) is ev of \\(A\\), \\(v\\) is evec of \\(A\\). $$ \\[\\begin{alignat}{3} &amp;\\forall \\lambda_i \\not = 0 &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; \\forall v_i : &amp;&amp; span(v_i) \\subset \\mathcal{C}(A) \\\\ &amp;A = A&#39;, \\; \\lambda_i \\not = \\lambda_j &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; v_i \\perp v_j, &amp;&amp; &amp;&amp;span(v_i, v_j) \\subset \\mathcal{C}(A) \\\\ &amp;\\exists A^{-1} &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; \\prod \\lambda\\not = 0 &amp;&amp; &amp;&amp; \\\\ &amp;A = A&#39; &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; &amp;&amp;\\exists \\text{basis for } \\mathcal{C}(A) \\text{ consists of } v_i \\text{ of } \\lambda_i \\not = 0 \\\\ &amp;A_{n \\times n} = A&#39;, \\; \\prod \\lambda \\not = 0 &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; \\mathcal C (A)=\\mathbb R^n, &amp;&amp; &amp;&amp;span( v) = \\mathbb{R}^n \\\\ &amp;A_{n \\times n} = A&#39;, \\; \\forall \\lambda_i \\not = 0 &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; &amp;&amp;span(\\forall v_i) = \\mathcal{C}(A) \\subset \\mathbb{R}^n \\\\ &amp;A_{n \\times n} = A&#39;, \\; \\forall \\lambda_i = 0 &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; &amp;&amp;span(\\forall v_i) = \\mathcal{N}(A) \\\\ &amp;A = A&#39; &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; &amp;&amp;\\mathcal{N}(A) = \\mathcal C (A)^\\perp \\\\ &amp;A_{n \\times n} = A&#39; &amp;&amp;\\; \\; \\; \\Longrightarrow \\; \\; \\; &amp;&amp; &amp;&amp; &amp;&amp; \\exists v_i : span(v_i) = \\mathcal C (A), \\; v_i \\perp v_j \\; \\; \\tiny {\\bigoplus} \\; \\; \\normalsize \\exists A^{-1} : \\mathcal C(A) = \\mathbb{R}^n \\; \\; \\tiny {\\bigoplus} \\; \\; \\normalsize \\text{if normalized, orthonormal} \\end{alignat}\\] $$ "],["section-4.html", "9.2 ", " 9.2 "],["counting-processes-and-martingales.html", "9.3 Counting Processes and Martingales", " 9.3 Counting Processes and Martingales 1샘플 estimator 를 위해 counting process 사용했었음. 이에서 N-A estimator 의 asymptotic 성질을 확인했고. 하지만 아직 \\(n^{\\frac 12} \\{ \\hat \\Lambda(t) - \\Lambda(t)\\}\\) 의 limiting distribution 을 획득하진 않았고. \\(\\hat \\Lambda(t)\\) 의 성질을 얻는데 있어서는 conditioning 이 핵심. 모든 이론적 기반은 이 conditioning 에 있음. Martingales 포함. Martingale Central Limit Theorem (MCLT) 은 자동적으로 Normal 로의 convergence 가 보장되는 마일드한 condition 이하에서 성립되었음. Definition 9.1 (Probability space) 모든 가능한 결과의 abstract space Ω, σ-algebra \\(\\mathcal F\\), set function (measure) \\(P\\) 가 주어졌을 때 확률공간 (Probability space) \\((Ω, \\mathcal F, P)\\) 가 성립. \\(\\mathcal A\\) 가 \\(\\Omega\\) 로부터의 결과값의 subset 의 collection 이라고 하자. - 이하를 만족하면 \\(\\mathcal A\\) 는 algebra. 1. \\(E \\in \\mathcal A\\) 이 complement \\(\\bar E \\in \\mathcal A\\) 를 보장 2. \\(E_1 \\in \\mathcal A\\) 이며 \\(E_2 \\in \\mathcal A\\) 인 것이 \\(E_1 \\cup E_2 \\in \\mathcal A\\) 를 보장 - 이하를 만족하면 \\(\\mathcal A\\) 는 \\(\\sigma\\)-algebra. 1. \\(E \\in \\mathcal A\\) 이 complement \\(\\bar E \\in \\mathcal A\\) 를 보장 2. \\(\\forall j=1,2,\\cdots:E_j \\in \\mathcal A\\) 가 \\(E_1 \\cup E_2 \\cdots \\in \\mathcal A\\) 를 보장 즉 \\(\\sigma\\)-algebra 는 countable union 과 intersection 에 closed 인 collection of events. Definition 9.2 (Stochastic Process) 랜덤변수의 collection \\(X=\\{X(t) ; t \\in \\mathcal T\\}\\)^[흔히 \\(\\mathcal T = [0,\\infty)\\), 혹은 \\(\\mathcal T = (0,\\tau_\\ast)\\) where \\(\\forall n=1, \\cdots, n:P(X_i &gt; \\tau_\\ast)&gt;0\\).] 가 같은 확률공간 안에서 정의되어 있을때 이는 Stochastic Process. e.g., 주어진 확률공간 \\((Ω, \\mathcal F, P)\\) 과 measurable space \\((S, \\Sigma)\\) 에서, \\(S\\)-valued 랜덤변수들의 collection 을 stochastic process 라고 하며, 이는 \\(X=\\{X(t) ; t \\in \\mathcal T\\}\\). 이때 \\(S\\) 는 mathematical space 이며 이건 \\(\\sigma\\)-algebra 에 비추어 measurable 해야함. Stochastic Process 의 실현값 을 Path 라고 부른다. 여기 path 에 이하의 조건이 더해진다면 이는 추가로 counting process13. non-decreasing piece-wise constant cadlag step-function with increments of size 1 Definition 9.3 (Filtration) \\(\\sigma\\)-algebra 들의 increasing family, e.g., \\(\\{\\mathcal F_t : t \\ge 0\\}\\) increasing Filtration 이라는 것은 \\(s\\get: \\mathcal F_s \\subset \\mathcal F_t\\), e.g., \\(A \\in \\mathcal F_s \\Longrightarrow A \\in \\mathcal F_t\\). \\(\\forall t: X(t)\\) 가 \\(\\mathcal F_t\\)-measurable 일 경우, stochastic process \\(X\\) 는 adapted to \\(\\mathcal F_t\\). 특히, 변량에 대해 유의미한 probability statement 가 서술될 수 있다면 이 변량은 measurable. \\(X(t)\\) 가 \\(\\mathcal F_t\\) 에 adapted \\(\\iff\\) \\(E \\Big [X(t) \\Bigg | \\mathcal F_t \\Big] = X(t)\\). 모든 process 는 그 자신의 역사 (과거 실현값) 에 adapted. SA 에서는 \\(\\mathcal F_t\\) = own history, e.g., \\(\\mathcal F_t = \\sigma \\{X(s); 0 \\le s\\ le t\\}\\) 로 두는 것이 편리하고 쓸만함. 이때 \\(\\mathcal F_t\\) 는 \\((0, t]\\) 에 걸친 \\(X\\) 의 실현값, 즉 \\(X\\) 에 의해 생산된 모든 데이터를 담고 있음. 일례로 \\(\\mathcal F_t\\) 는 선풍기들이 돌기 시작한 시점부터 선풍기 전부를 관찰하고 있던 관찰자의 뇌속 기억. 언제 고장났는지 혹은 censoring 당했는지 다 암. 자주 쓰이는 filtration 은 \\(\\mathcal F_t = \\sigma \\{ N_i (s) , Y_i (s+) \\; ; \\; s\\in (0,t], i=1, \\cdots, n\\}\\). 9.3.1 Conditional Expectation 랜덤변수 \\(X\\) 가 \\(\\mathcal F\\)-measurable 이며 \\(\\mathcal G \\subset \\mathcal F\\) 이라면: \\[ \\begin{align} E(X|\\mathcal F) &amp;= X \\\\ E(aX|\\mathcal F) &amp;= aX \\\\ E(XY|\\mathcal F) &amp;= X \\cdot E(Y|\\mathcal F) \\\\ E(X | \\mathcal G) &amp;= \\mathcal G \\text{-measurable} \\\\ \\forall \\text{ events } B \\in \\mathcal G : E \\Big[X \\cdot I(B) \\Big] &amp;= E \\Big[E(X|\\mathcal G) \\cdot I(B) \\Big] \\end{align} \\] 이하의 조건이 만족된다면 Stochastic Process 는 tag 안의 property 가 성립. \\[ \\sup\\limits_{t\\in T}E[|X(t)|]\\lt \\infty\\ \\tag{integrable} \\\\ \\sup\\limits_{t\\in{\\mathcal{T}}}E[X(t)^{2}]\\lt \\infty \\tag{square integrable} \\\\ P\\left\\{\\operatorname*{sup}_{t\\in T}|X(t)|\\lt c\\right\\}=1 \\tag{uniformly bounded} \\] counting process \\(N(t)\\), filtration \\(\\mathcal F_t\\) 가 있을때, 이에 엮인 intensity process \\(A(t)\\) 는 다음과 같다. set \\(A(t)=\\int_0^t dA(s)\\). where $$ \\[\\begin{alignat}{2} d A(t) &amp;= E[d N(t)]\\mathcal{F}_{t^{-}}] &amp;&amp; &amp;&amp; =Y(t)\\lambda(t)d t \\\\ &amp;=\\lim\\limits_{d t\\uparrow0}E[N(t^{-}+d t)-N(t^{-}) &amp;&amp;| {\\mathcal{F}}_{t^{-}}] \\tag{1} \\\\ &amp;=\\lim\\limits_{d\\uparrow 0}\\{N(t^{-}+d t)-N(t^{-})=1 &amp;&amp;\\vert{\\mathcal F}_{t^{-}}\\} \\tag{2} \\end{alignat}\\] $$ \\(\\mathcal F_{t^-}\\) 는 (0, t) 에 대한 정보 보유. \\([t, t+dt)\\) 에서 event 발생이 1번을 초과할 가능성은 negligable 하다고 set. 즉, \\(\\lim\\limits_{d t\\downarrow0}P\\{N(t^{-}+d t)-N(t^{-})\\gt 1|\\mathcal{F}_{t^{-}}\\}\\;=\\;\\ o(d t^{2})\\). 9.3.2 Martingale Definition 9.4 (Martingale) 이하의 조건을 만족할 때, right-continuous 인 stochastic process \\(X=\\{X(t):t \\ge 0\\}\\) 는 filtration \\(\\{\\mathcal F_t : t \\ge 0\\}\\) 에 대해 martingale. X 가 \\(\\mathcal F_t\\) 에 대해 adapted. \\(\\forall t &lt; \\infty : E[ \\Big | X(t) \\Big | ] &lt; \\infty\\) \\(\\forall t, s \\ge 0: E[ X(t+s) | \\mathcal F_t] = X(t)\\) 3-(1). \\(\\forall t, s \\ge 0: E[ X(t+s) | \\mathcal F_t] \\ge X(t)\\), sub-martingale 3-(2). \\(\\forall t, s \\ge 0: E[ X(t+s) | \\mathcal F_t] \\le X(t)\\), super-martingale martingale 은 pure random noice process. 즉슨 history 가 주어졌을 때 조건부 평균이 0이며, conditional centered process 이고, \\(t\\) 에 걸쳐 mean 중심으로 랜덤하게 fluctuate. random walk, 페어 갬블링 등이 예시가 됨. \\(X\\) 의 matringale increment \\(dX(t)=X(t^- + dt) - X(t^-)\\) 를 정의. 앞의 성질을 통해 \\(E[dX(t)|\\mathcal F_{t^-}] = 0\\) 임이 보장되었다. 이제 \\(\\mathcal F_t\\) martingale 인 \\(X\\) 가 uncorrelated increment 를 가지고 있음을 보일 것.14 $$ \\[\\begin{align} s\\lt t,\\;E[X(s)\\{X(t)-X(s)\\}] &amp;= E\\, \\Big [E[X(s)\\{X(t)-X(s)\\}|\\mathcal{F}_{s} \\Big ] \\\\ &amp;= E\\, \\Big [X(s) \\cdot E[\\{X(t)-X(s)\\}|\\mathcal{F}_{s} \\Big ] \\\\ &amp;= {{E \\Big [ X(s)\\cdot \\Big \\{E[X(t)|\\mathcal{F}_{s}]-E[X(s)|\\mathcal{F}_{s}] \\Big \\} \\Big ]}} &amp;&amp;= 0 \\end{align}\\] $$ univariate survival 에 자주 사용되는 counting process 는 \\(N(t) = I(X \\le t , \\Delta = 1)\\). 이제 이하로 설정해보자. $$ \\[\\begin{align} M(t) &amp;= N(t) - A(t) \\\\ A(t) &amp;= \\int_0^t dA(s) \\\\ dA(t) &amp;= Y(t)\\lambda(t) dt \\\\ &amp;=E[dN(t) | \\mathcal F_{t^-}] \\end{align}\\] $$ 이제 intentisy process 의 integration 인 \\(A(t)\\) 을 \\(N(t)\\) 의 compensator 라고 명명한다. 이는 process 를 centerin, 즉 중앙쪽으로 보정한다는 의미. 9.3.2.1 Centering Increments \\(A(t)\\) 가 실제로 \\(N(t)\\) 의 compensator 임을 보이자. failure time 이 indenpendent (right) censoring 에 유관함을 suppose. 그렇다면 pertinent counting process 는 \\(N(t)\\) 로 설정되며, filtration \\(\\mathcal F_{t} = \\sigma \\{N_i(s) , Y_i (s+); i = 1, \\cdots, n;s \\in (0, t]\\}\\) 가 된다. 이때 compensator increment 는 이하로 주어진다. $$ \\[\\begin{alignat}{2} E[d N_{i}(t)]{\\mathcal{F}}_{t^{-}}]&amp;=&amp;&amp; P[d N_{i}(t)=1&amp;&amp;|{\\mathcal{F}}_{t^{-}}] \\\\ &amp;=&amp;&amp;P[d N_{i}(t)=1&amp;&amp;|Y(t)] \\\\ &amp;=Y_{i}(t) \\cdot &amp;&amp;P[t\\leq T_{i}\\lt t+d t&amp;&amp;|t\\leq T_{i},t\\leq C_{i}] \\\\ &amp;=Y_{i}(t) \\cdot &amp;&amp;P[t\\leq T_{i}\\lt t+d t&amp;&amp;|t\\leq T_{i}] \\\\ &amp;=Y_{i}(t) \\cdot &amp;&amp;d A(t) \\end{alignat}\\] $$ 이때, \\(M=N-A\\) 가 성립하는가? 다른 말로, \\(\\operatorname{E}[N_{i}(t)]=\\operatorname{E}[A(t)]\\) 인가? 이제 Predictable 에 대해 생각해보자. Predictable Process 란 무엇인가? \\(H(t)\\) 의 값이 \\(\\mathcal F_{t^-}\\) 의 함수, 혹은 특정된다면, stochastic process \\(H\\) 는 \\(\\forall t:\\) 의 filtration \\(\\mathcal F_t\\) 에 대해 predictable. 이는 곧 \\(H\\) \\(t\\) 시점의 값이 \\(t-\\) 까지의 정보로 인해 고정된다면, 즉 \\(H\\) 의 행위가 \\([0,t)\\) 까지 해왔던 행위로 인해 고정된다는 것과 동일. predictable 의 성질은 이하와 같다. - left-continuous process 는 predictable (e.g., \\(Y(t)\\)) - 모든 deterministic function 은 predictable (e.g., $S(t), (t)) - \\(E[H(t) | \\mathcal F_{t^-} = H(t)]\\) Definition 9.5 (Stochastic Integral) \\(M\\) 이 \\(\\mathcal F\\)-matringale 이라고 가정. 이때 process \\(Z(t)~=~\\int_{0}^{t}H(s)d M(s)\\) 는 \\(M(t)\\) 에 대한 Stochastic Integral. Theorem 9.1 이하의 조건이 만족될 때, \\(M(t)\\) 에 대한 Stochastic Integral \\(Z(t)~=~\\int_{0}^{t}H(s)d M(s)\\) 는 \\(\\mathcal F\\) matringale. \\(H\\) 가 filtration \\(\\mathcal F\\) 에 대해 predictable \\(M\\) 이 \\(\\mathcal F\\) matringale 증명을 위해서는 궁극적으로 \\(E[Z(t)-Z(s)|{\\mathcal{F}}_{s}]=0\\) 임을 보여야 한다. $$ \\[\\begin{align} E[Z(s)|\\mathcal{F}_{s}]\\;\\;&amp;=\\;\\;E\\left[\\int_{0}^{s}H(u)d M(u)|\\mathcal{F}_{s}\\right] \\\\ &amp;=\\ \\int_{0}^{s}E[H(u)d M(u) \\Bigg |\\mathcal{F}_{s}] \\\\ &amp;=\\ \\int_{0}^{s}H(u)d M(u) &amp;&amp;= Z(s) \\\\ \\\\ E[Z(t)|{\\mathcal{F}}_{s}]~&amp;=~E\\left[\\int_{0}^{t}H(u)d M(u)\\Bigg|{\\mathcal{F}}_{s}\\right] \\\\ &amp;=\\;\\int_{0}^{t}E[H(u)d M(u)|\\mathcal{F}_{s}] &amp;&amp;=\\;\\;Z(s)+\\int_{s}^{t}E[H(u)d M(u)|\\mathcal{F}_{s}] \\end{align}\\] $$ 이전과 같이 conditioning 을 적용. 단 이번에는 conditional quantity 쪽에. 먼저 conditional expectation 을 고려. 조건부 기댓값을 반복하는 것으로 이하가 발생. $$ \\[\\begin{align} E \\Big [H(u)d M(u) \\Big |\\mathcal{F}_{s}\\Big ] &amp;=\\;\\;E \\Bigg [E \\Big [H(u)d M(u) \\Big |\\mathcal{F}_{s},\\mathcal{F}_{u^{-}} \\Big ] \\Bigg |\\mathcal{F}_{s} \\Bigg ] \\\\ &amp;=\\;\\;E \\Bigg [E \\Big [H(u)d M(u) \\Big |\\mathcal{F}_{u^{-}} \\Big ] \\Bigg |\\mathcal{F}_{s} \\Bigg ] \\\\ &amp;=\\;\\;E \\Bigg [H(u) \\cdot E \\Big [d M(u) \\Big |\\mathcal{F}_{u^{-}} \\Big ] \\Bigg |\\mathcal{F}_{s} \\Bigg ] &amp;&amp;= 0 \\end{align}\\] $$ 따라서 \\(E[Z(t)]\\mathcal{F}_{s}]\\ \\ =\\ \\ Z(s)\\) 이며, 이인즉 \\(E[Z(t)-Z(s)|\\mathcal F_{s}]=0\\). 이를 통해 martingale 에 대해 적분한 stochastic integral 은 그자체로 martingale 임을 보일 수 있다. 9.3.3 Key Martingales Properties 위에서 martingale 의 핵심 성질이라고 말했던 (3) 을 increment 의 형식을 빌려 직접 표현하는 것이 가능. 천하쌍살단 살인시동 ㄱ 9.3.4 9.3.5 일반적인 counting process 에 대해, \\(N(0)\\) 이며 \\(\\forall t \\in \\mathcal T:N(t)&lt;0\\).↩︎ 증명에 \\(E[X(t)|\\mathcal{F}_{s}]=X(s)\\mathrm{~for~}s\\lt t\\) 가 사용되었다. 이는 위에서 보였다.↩︎ "],["section-7.html", "9.4 ", " 9.4 "],["cox-regression.html", "9.5 Cox Regression", " 9.5 Cox Regression 9.5.0.1 Proportional Hazards Model Proposed by Cox (1972, JRSS-B), primarily to model the relationship between hazard function and covariates. most cited paper in statistics ( 41; 000 as of April 2016), one of the most cited in science. Several extensions to more complex data structures, e.g., clustered failure time data, or recurrent event data, etc. ※ Data Structure Observed data: \\(\\Big \\{ X_i = T_i \\wedge C_i, \\; \\; \\; \\Delta_i = I(T_i &lt; C_i), \\; \\;\\; \\mathbf Z_i (\\cdot) \\Big \\} \\overset {iid} \\sim\\) 추가로 \\(N_i = I(X_i \\le t , \\; \\Delta_i = 1)\\), \\(Z_i(t)\\) = covariate vector (possibly time-dependent). 9.5.0.2 Cox PH Model \\[ \\lambda_i (t) = \\lambda (t \\vert Z_i ) = \\lambda_0 (t) \\exp (\\beta&#39; Z_i) \\tag{Cox Model} \\] semiparametric model: \\(\\exp(\\beta &#39; Z_i)\\), parametric assumption on covariate effects multiplicative model \\(\\beta\\) : \\(p \\times 1\\) vector, \\(p &lt; \\infty\\) \\(\\lambda_0(t)\\), nonparametric; is \\(\\infty\\) dimensional shape of hazard function is unspecified Due to nonparametric component, standard maximum likelihood theory does not apply Let \\(Z_{ij}\\) be the \\(j\\)-th element of \\(Z_i\\) - \\(\\beta_j\\) = difference in log hazards - \\(\\exp(\\beta_j)\\) = ratio of hazards; assumed constant for all \\(t\\) \\(\\lambda_0(t)\\): baseline hazard; common to all subjects, \\(\\lambda_0(t) = \\lambda_i(t \\big | Z_i = \\mathbf 0)\\) The hazard ratio, \\(\\exp(\\beta_j)\\), is sometimes referred to as a relative risk - risk = probability, not a rate - hazard is a rate, not a probability - in ratio of hazards, time dimension cancels out Direction of effect: $$ \\[\\begin{align} \\beta_j &gt; 0: &amp;&amp;\\uparrow\\lambda_i &amp;&amp;\\downarrow S_i(t) \\\\ \\beta_j &lt; 0: &amp;&amp;\\downarrow\\lambda_i &amp;&amp;\\uparrow S_i(t) \\end{align}\\] $$ Magnitude of effect is easy to interpret w.r.t. \\(\\lambda_i(t)\\) Cumulative hazard function: $$ \\[\\begin{align} \\lambda_i (t) &amp;= \\lambda_0(t) \\exp(\\beta Z_i) \\\\ \\Lambda_i (t) &amp;= \\int_0^t \\lambda_0(s) \\exp(\\beta Z_i) ds \\\\ &amp;= \\Lambda_0(t) \\exp(\\beta Z_i) \\end{align}\\] $$ Survival function: $$ \\[\\begin{align} S_i (t) &amp;= \\exp \\Big \\{ -\\Lambda_i (t) \\Big\\} \\\\ &amp;= \\exp \\Big \\{ -\\Lambda_0 (t) \\exp(\\beta &#39; Z_i)\\Big\\} \\\\ &amp;= S_0(t)^{\\exp \\Big \\{ \\beta&#39;Z_i \\Big\\}} \\end{align}\\] $$ By fitting a Cox model, one can readily interpret the multiplicative effect on the hazard: - ex) randomized trial: treatment (\\(Z_i=1\\)) versus placebo (\\(Z_i=0\\)); \\(\\hat \\beta = 0.405\\) (\\(\\exp(\\hat \\beta)=1.5\\)) - \\(\\lambda_i(t)\\) for treated patients is 50% more of that of the controls. - irrespective of \\(\\lambda_0(t)\\) Nevertheless, \\(\\Lambda_0(t)\\) is required in order to determine \\(Z_i\\)’s effect on \\(S_i(t)\\), e.g., $$ \\[\\begin{align} S(t \\Big | Z_i = 0) = 0.95 &amp;&amp; vs. &amp;&amp; S(t \\Big | Z_i = 1) = 0.93 \\\\ S(t \\Big | Z_i = 0) = 0.70 &amp;&amp; vs. &amp;&amp; S(t \\Big | Z_i = 1) = 0.59 \\end{align}\\] $$ 9.5.0.2.0.1 Cox Model: Independent Censoring Independent censoring assumption is less stringent than in nonparametric estimation. Assumption is often written as \\(T_i \\perp C_i \\Big \\vert Z_i\\): $$ \\[\\begin{alignat}{2} &amp;\\lim_{\\delta \\rightarrow 0} \\frac{1}{\\delta} P(t \\le T_i &lt; t+ \\delta \\Big | T_i \\ge t , \\; C_i \\ge t , &amp;&amp;\\; Z_i) \\\\ = &amp;\\lim_{\\delta \\rightarrow 0} \\frac{1}{\\delta} P(t \\le T_i &lt; t+ \\delta \\Big | T_i \\ge t , &amp;&amp;\\; Z_i) \\end{alignat}\\] $$ ※ Note: \\(C_i\\) is allowed to depend on \\(Z_i\\) 9.5.0.3 Semiparametric PH Model: General General expression for multiplicative proportional hazards model: \\[ \\lambda_i (t) = \\lambda_0 (t) g(\\beta &#39; Z_i ) \\] \\(g(x)\\) is link function, specified. \\(\\forall x: g(x) \\ge 0\\), \\(\\exists g&#39;&#39;(x)\\), and in special case, \\(g(x) = \\exp(x)\\). Other choices for link function (e.g., Self &amp; Prentice, 1983): \\(g(x) = 1+x = (1+x)^{-1} = \\log(1+x)\\) ※ Notes: - not all choices of \\(g(x)\\) lead to clear interpretation of \\(\\beta_j\\) - certain choices of \\(g(x)\\) lead to numerical issues; e.g., likelihood is flat; local maxima, etc. - \\(g(x) \\not = exp(x)\\) has received little attention in the literature 9.5.0.4 Multiplicative Model Cox model is a multiplicative model, i.e., covariates assumed to affect survival probability by multiplying the baseline hazard. Additive models also been proposed \\[ \\] 9.5.0.5 Proportional Hazards Regression and Multiplicative Intensity Model Recall Counting process: martingale representation $$ \\[\\begin{align} N(t) &amp;= I(X\\le t , \\; \\Delta = 1) \\\\ Y(t) &amp;= I(X \\ge t) \\\\ M(t) &amp;= N(t) - \\int_0^t Y(u)\\lambda_0(u) e^{\\beta &#39; Z } du \\tag{1} \\\\ \\mathcal F_t &amp;= \\sigma \\Big \\{ N(u) , Y(u+) , Z: \\; \\; 0 \\le u \\le t \\Big \\} \\end{align}\\] $$ intensity \\(l(u) = Y(u)\\lambda_0(u) e^{\\beta &#39; Z }\\), therefore integrated form is cumulative intensity \\(A(t)\\). Multiplicative Intensity Model: \\[ l(t) = Y(t)\\lambda_0(t) e^{\\beta &#39; Z(t) } \\] Counting process: \\(N(t)\\) = Number of events of a specified type that have occurred by time \\(t\\) \\(N(t)\\) may take more than one jump multiple infections, repeated breakdowns, hospital admissions \\(EN(t) &lt; \\infty\\) At-risk process: \\(Y(t)\\), left-continuous process, \\(1\\) if failure can be observed at time \\(t\\), otherwise \\(0\\). \\(Y(t)\\) can be used to represent situation in which a subject enter and exit risk sets several times \\(Y(t)\\) may be \\(1\\) even after an observed failure Covariate process: \\(Z(t)\\) = (bounded) predictable process time-dependent treatment, risk factors model checking and relaxing PH assumption Baseline hazard function: \\(\\lambda_0(\\cdot)\\) = an arbitrary deterministic function Filtration: \\(\\mathcal F_t = \\sigma \\Big \\{ N(u) , Y(u+) , Z(u): \\; \\; 0 \\le u \\le t \\Big \\}\\) Martingale: \\(M(t) = N(t) - \\int_0^t l(u) du\\) Intensity function: $ E { dN(t) | F_{t-} } = l(t) dt$ Data: \\(n\\) independent observations on $ { N(), ; Y(), ; Z() }$ 9.5.0.6 Likelihood; conditional, marginal and partial likelihoods \\(X =\\) vector of observations; \\(f_X(x, \\theta) =\\) density of \\(X\\) \\(\\theta =\\) vector parameter; \\(\\theta = (\\beta &#39; , \\phi&#39;)&#39;\\) \\(\\beta =\\) parameter of interest; \\(\\phi =\\) nuisance parameter likelihood: \\(f_X(x, \\theta) = f_{W|V} (w \\Big | v, \\theta )f_V (v, \\theta)\\) \\(X = (V&#39;, W&#39;)&#39;\\) infinite-dimensional \\(\\phi\\) \\(f_{W|V} (w \\Big | v, \\theta )\\) does not involve \\(\\phi\\) \\(\\Rightarrow\\) use \\(f_{W|V} (w \\Big | v, \\beta )\\) (conditional likelihood) \\(f_V (v, \\theta)\\) does not involve \\(\\phi\\) \\(\\Rightarrow\\) use \\(f_V (v, \\beta)\\) (marginal likelihood) \\[ X = (V_1 , W_1 , \\cdots, V_K , W_K) \\] $$ \\[\\begin{align} f_X(x, \\theta) &amp;= f_{V_1 , W_1 , \\cdots, V_K , W_K} (v_1 , w_1 , \\cdots, v_K , w_K\\; ;\\; \\theta) \\\\ &amp;= f_{V_1}(v_1 \\; ; \\; \\theta) f_{W_1 | V_1}(w_1 | v_1\\; ; \\; \\theta) f_{V_2 | V_1, W_1}(v_2 | v_1, w_1\\; ; \\; \\theta) \\times \\cdots \\\\ &amp;= \\left \\{ \\prod_{i=1}^K f_{W_i | Q_i } (w_i \\Big | q_i \\; ; \\theta) \\right \\} \\left \\{ \\prod_{i=1}^K f_{V_i | P_i } (v_i \\Big | p_i \\; ; \\theta) \\right \\} \\end{align}\\] $$ $$ \\[\\begin{align} P_1 = \\phi,&amp; &amp;&amp; P_i =(V_1 , W_1 , \\cdots, V_{i-1} , W_{i-1}) \\\\ Q_1 = V1,&amp; &amp;&amp; Q_i =(V_1 , W_1 , \\cdots , W_{i-1}, V_i) \\end{align}\\] $$ ${i=1}^K f{W_i | Q_i } (w_i | q_i ; ; ) $ is free of \\(\\phi\\) \\(\\Rightarrow\\) use $ {i=1}^K f{W_i | Q_i } (w_i | q_i ; ; ) $ (partial likelihood) 9.5.0.6.0.1 Partial &amp; Marginal Likelihoods Focus on Proportional Hazards Model: i.e., \\((X_i, \\; \\delta_i, \\; Z_i), \\; i = 1, \\cdots, n\\) (\\(n\\) independent triplets) $$ \\[\\begin{align} &amp;\\lambda(t \\Big | Z ) = \\lambda_0 (t) e^{\\beta &#39; Z} &amp;&amp;S(t \\Big | Z) = \\Big \\{ S_0(t) \\Big \\}^{e^{\\beta &#39; Z}} \\tag{1} \\end{align}\\] $$ 위에서 $ _0 (t)$는 unspecified. Partial Likelihood: assume no ties, absolutely continuous failure distribution Suppose there are L observed failures at \\(\\tau_1 &lt; \\cdots &lt; \\tau_L\\) (set \\(\\tau_0 \\equiv 0\\) &amp; \\(\\tau_{L+1} \\equiv \\infty\\)) 16.png Let (i) be the label for individual failing at \\(\\tau_i\\) (set \\((L + 1) \\equiv n + 1\\)). Note \\(t_{(i)} = \\tau_i\\) Covariates for \\(L\\) failures: \\((Z_{(1)}, \\cdots, Z_{(L)})\\). (Hereafter, condition on $ { Z_i : i = 1, , n }$) Censorship times in \\([\\tau_i; \\tau_{i+1})\\): \\((\\tau_{i1}, \\cdots, \\tau_{i, m_i})\\) with covariates \\((Z_{(i,1)}, \\cdots, Z_{(i,m_i)})\\), i.e., \\((i, j)\\) is label for item censored at \\(\\tau_{ij}\\) 17.png The data can be divided into sets \\[ (V_1 , W_1, \\cdots, V_{L+1} , W_{L+1}) \\] where, for \\(i = 1, \\cdots, L, L+1\\), $$ \\[\\begin{align} V_i &amp;= \\Big \\{ \\tau_i , \\tau_{i-1, j} \\; \\; ; \\; \\; (i-1, j):j = 1, \\cdots, m_{i-1} \\Big \\} \\\\ and \\; \\; \\; \\;W_i &amp;= \\Big \\{ (i) \\Big \\} \\end{align}\\] $$ 18.png 19.png GOAL: Build a likelihood on a subset of the full data set - carrying most of the information about \\(\\beta\\) - carrying no information on nuisance parameters \\(\\Big \\{ \\lambda_0 (t) : t \\ge 0 \\Big \\}\\) PROPOSAL: Generate likelihood of \\(\\Big \\{ W_1, \\cdots, W_L \\Big \\}\\) JUSTIFICATION, WHY?: - Timing of events \\(\\Big \\{ \\tau_1 , \\cdots, \\tau_L \\Big \\}\\) can be explained by \\(\\lambda_0(\\cdot)\\). - Censoring times and labels can be ignored if we assume non-informative censorship (independent censoring). So this is a partial likelihood in the sense that it is only part of the likelihood of the observed data. If \\(Q_i \\equiv (V_1, W_1 , \\cdots, V_{i-1}, W_{i-1}, V_i)\\) and \\(\\mathcal F_{\\tau_i} \\equiv (Q_i, Z)\\), the partial likelihood is \\(\\prod_{i=1}^L P \\Big ( W_i = (i) \\Big | \\mathcal F_{\\tau_i} \\Big)\\), i.e., given the risk set at \\(\\tau_i\\), and given event occurs at \\(\\tau_i\\). Denote \\(R_i \\equiv \\Big \\{ j : X_j \\ge \\tau_i \\Big \\}\\) as risk set at \\(\\tau_i\\). Then, by the assumption of independent censoring, $$ \\[\\begin{align} P \\Big ( W_i = (i) \\Big | \\mathcal F_{\\tau_i} \\Big) &amp;= \\frac{ P \\Bigg \\{ t_{(i)} \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\} \\cdot \\prod\\limits_{j \\in R_i - (i)} P \\Bigg \\{ t_{j} \\not \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\} }{ \\sum\\limits_{l \\in R_i} \\left[ P \\Bigg \\{ t_{l} \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\} \\cdot \\prod\\limits_{j \\in R_i - l} P \\Bigg \\{ t_{j} \\not \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\} \\right] } \\tag{a} \\\\ \\\\ \\\\ &amp;= \\frac{ d\\Lambda \\Big( \\tau_i \\Big | Z_{(i)} \\Big) \\prod\\limits_{j \\in R_i - (i)} \\bigg \\{ 1 - d\\Lambda \\Big( \\tau_i \\Big | Z_{j} \\Big) \\bigg \\} }{ \\sum\\limits_{l \\in R_i} \\left [ d\\Lambda \\Big( \\tau_i \\Big | Z_{l} \\Big) \\prod\\limits_{j \\in R_i - l} \\bigg \\{ 1 - d\\Lambda \\Big( \\tau_i \\Big | Z_{j} \\Big) \\bigg \\} \\right ] } \\; \\; \\; \\div \\; \\; \\; \\frac{d\\tau_i}{d\\tau_i} \\tag{2} \\\\ \\\\ \\\\ &amp;= \\frac{\\lambda\\Big(\\tau_i \\Big | Z_{(i)} \\Big)}{ \\frac{P \\Big\\{T\\in [t, t+dt) \\Big | T \\ge t , Z \\Big\\}}{dt}= \\sum\\limits_{l\\in R_i} \\left[ \\lambda\\Big(\\tau_i \\Big | Z_{l} \\Big) \\right]} \\; \\; \\; \\overset {(1)}{=} \\; \\; \\; \\frac{\\exp(\\beta &#39; Z_{(i)})}{\\sum\\limits_{l\\in R_i} \\exp(\\beta &#39; Z_{l})} \\end{align}\\] $$ - at (a), \\(P \\Bigg \\{ t_{j} \\not \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\} = 1 - P \\Bigg \\{ t_{j} \\in \\big[\\tau_i , \\tau_i + d\\tau \\big) \\Bigg |\\mathcal F_{\\tau_i} \\Bigg \\}\\) - at (2), $ d( i | Z{j} ) = 0$ Thus, the Partial Likelihood is \\[ \\prod^L_{i=1}\\frac{\\exp(\\beta &#39; Z_{(i)})}{\\sum\\limits_{l\\in R_i} \\exp(\\beta &#39; Z_{l})} = L(\\beta)\\tag{3} \\] Note: unspecified \\(\\lambda_0(\\cdot)\\) + noninformative censoring \\(\\Rightarrow\\) \\(\\prod\\limits_{i=1}^L f_{V_i \\big | P_i} (v_i \\Big | p_i ; \\theta)\\) contains little or no information about \\(\\beta\\). Counting process notation: $$ \\[\\begin{align} L(\\beta) = \\prod^n_{i=1}\\prod_{t\\ge0} \\left \\{ \\frac{\\exp(\\beta &#39; Z_{i})}{\\sum\\limits_{j=1}^n Y_j(t) \\exp(\\beta &#39; Z_{j})} \\right\\}^{dN_i(t)} , &amp;&amp; dN_i(t) = \\begin{cases} 1 &amp; N_i(t) - N_i {(t-)} =1\\\\0 &amp; o.w.\\end{cases} \\end{align}\\] $$ Maximum partial likelihood estimator (MPLE): \\(L( \\hat \\beta) = \\max_\\beta L(\\beta)\\) (using Newton-Raphson (NR) algorithm) Specifically, the log partial likelihood is then \\[ l(\\beta) = \\sum_{i=1}^n \\int_0^\\infty \\left[ Y_i (t) Z_i \\beta - \\log\\left( \\sum_{j=1}^n Y_j(t) \\exp(\\beta &#39; Z_j ) \\right) \\right]dN_i(t) \\] The score vector, \\(U(\\beta)\\), can be obtained by differentiating \\(l(\\beta)\\) w.r.t. \\(\\beta\\): $$ \\[\\begin{alignat}{2} U(\\beta) &amp;= \\sum_{i=1}^n \\int_0^\\infty \\Big \\{ Z_i - \\bar Z(\\beta, t) \\Big \\}&amp;&amp;dN_i (t) \\\\ &amp;= \\sum_{i=1}^n \\int_0^\\infty \\left \\{ Z_i - \\frac{\\sum_{i=1}^n Y_i (t) Z_i \\exp(\\beta &#39; Z_i)}{\\sum_{i=1}^n Y_i (t) \\exp(\\beta &#39; Z_i)} \\right \\}&amp;&amp;dN_i (t) \\end{alignat}\\] $$ where \\(\\bar Z(\\beta, t)\\) is a weighted mean of \\(Z\\) over those observations still at risk at time \\(t\\). The information matrix, \\(\\mathcal I(\\beta)\\), is the negative second derivative where $$ \\[\\begin{align} \\mathcal I(\\beta) &amp;= \\sum\\limits_{i=1}^n \\int_0^\\infty V(\\beta, t) dN_i(s) \\\\ \\\\ V(\\beta, t) &amp;= \\frac{\\sum\\limits_{i=1}^n Y_i(t) \\exp(\\beta &#39; Z_i ) \\Big \\{ Z_i - \\hat Z (\\beta, t)\\Big\\}&#39;\\Big \\{ Z_i - \\hat Z (\\beta, t)\\Big\\}}{\\sum\\limits_{i=1}^n Y_i(t) \\exp(\\beta &#39; Z_i )} \\end{align}\\] $$ and \\(V(\\beta, t)\\) is the weighted variance of \\(Z\\) at time \\(t\\). Then, the MPLE, \\(\\hat \\beta\\), is found by solving the partial likelihood equation: \\(U(\\hat \\beta) = 0\\). Under some regularity conditions, \\(\\hat \\beta\\) is consistent and asymptotically normally distributed with mean \\(\\beta\\) and variance \\(E \\Big \\{ \\mathcal I(\\beta) \\Big\\}^{-1}\\) (will be shown later.) The NR algorithm to solve the partial likelihood equation: Compute iteratively until convergence (requires an initial value \\(\\hat \\beta^{(0)}\\)). \\[ \\hat\\beta^{(n+1)} = \\hat\\beta^{(n)} + \\mathcal I ^{-1} \\Big( \\hat \\beta^{(n)}\\Big) \\cdot U \\Big( \\hat \\beta^{(n)}\\Big) \\] ※ Note: 1. (incredibly) Robust algorithm! 2. \\(\\hat \\beta^{(0)} = 0\\) usually works. 9.5.0.7 Cox Proportional Hazards Model Cox model: $$ \\[\\begin{align} \\lambda_i(t) = \\lambda(t \\Big | Z_i ) &amp;= \\lambda_0 (t) \\exp(\\beta &#39; Z_i) \\\\ &amp;= \\lambda_0(t) \\exp(\\beta_1 Z_{i1} + \\cdots + \\beta_k Z_{ik}) \\\\ &amp;\\Updownarrow \\\\ \\log \\lambda(t \\Big | Z_i ) &amp;= \\log \\Big[ \\lambda_0(t) \\Big] +\\beta_1 Z_{i1} + \\cdots + \\beta_k Z_{ik} \\\\ S(t \\Big | Z_i ) &amp;= \\Big[ S_0(t) \\Big]^{\\exp(\\beta_1 Z_{i1} + \\cdots + \\beta_k Z_{ik})} \\end{align}\\] $$ ※ Note: $$ \\[\\begin{align} \\lambda_0 (t) &amp;= \\lambda(t \\Big | Z_1 = \\cdots = Z_k = 0) \\\\ \\\\ \\exp(\\beta_1 Z_{1} + \\cdots + \\beta_k Z_{k}) &amp;= RR \\\\ &amp;= \\frac{\\lambda(t \\Big | Z_1 , \\cdots, Z_k)}{\\lambda(t \\Big | Z_1 = \\cdots = Z_k = 0)} \\tag{1} \\end{align}\\] $$ - (1) is relative risk of hazard of death comparing covariates values \\(Z_1,\\cdots, Z_k\\) to \\(Z_1 = \\cdots = Z_k = 0\\) Interpreting Cox Model Coeffcients: \\(\\beta_k\\) is the log RR (hazard ratio) for a unit change in \\(Z_k\\), given all other covariates remain constant, i.e., $$ \\[\\begin{align} \\frac {\\lambda\\Big[t \\Big | Z_1 , \\cdots, (Z_{k&#39;}+1), \\cdots, Z_k \\Big]} {\\lambda\\Big[t \\Big | Z_1 , \\cdots, Z_{k&#39;}, \\cdots, Z_k \\Big]} &amp;= \\exp \\Big (\\beta_1 \\cdot 0 + \\cdots + \\beta_{k&#39;} \\cdot (Z_{k&#39;} +1 - Z_{k&#39;}) + \\cdots + \\beta_k \\cdot 0 \\Big) \\\\ &amp;= \\exp(\\beta_{k&#39;}) \\end{align}\\] $$ The RR comparing 2 sets of values for the covariates \\((Z_1 , \\cdots, Z_k)\\) vs. \\((Z_1&#39; , \\cdots, Z_k&#39;)\\): \\[ RR = \\frac{\\lambda(t \\Big | Z_1 , \\cdots, Z_k)}{\\lambda(t \\Big | Z_1 &#39;, \\cdots, Z_k&#39;)} =\\exp \\Big \\{ \\beta_1(Z_1 - Z_1&#39;) + \\cdots + \\beta_k(Z_k - Z_k&#39;) \\Big \\} \\] 20.png 9.5.0.8 Comparison of Nested Models Nested Models: $$ \\[\\begin{align} \\lambda(t) &amp;= \\lambda_0(t) \\exp \\Big ( \\beta_1 Z_1 + \\cdots \\beta_p Z_p + \\beta_{p+1} Z_{p+1} +\\cdots + \\beta_{k} Z_{k}\\Big) \\tag{Full Model} \\\\ &amp;= \\lambda_0(t) \\exp \\Big ( \\beta_1 Z_1 + \\cdots \\beta_p Z_p \\Big) \\tag{Reduced Model} \\end{align}\\] $$ To test: Nested Models: $$ \\[\\begin{align} &amp;H_0: &amp;&amp;RM &amp;&amp; \\Leftrightarrow &amp;&amp; H_0: \\beta_{p+1} = \\cdots = \\beta_k = 0 \\\\ &amp;H_A: &amp;&amp;RM &amp;&amp; \\Leftrightarrow &amp;&amp; H_A: \\not = \\text{ somewhere} \\end{align}\\] $$ Use the partial likelihood ratio statistic, \\(X^2_{Cox} = -2 \\Big[ \\log PL(RM) - \\log PL(FM)\\Big]\\). Under \\(H_0\\): Reduced model, and when \\(n\\) is large: \\[ \\begin{align} X^2_{Cox} \\sim \\chi^2_{k-p} &amp;&amp; k-p \\text{ is the ## of parameters set to 0 by }H_0 \\end{align} \\] 20.png, 21.png 9.5.0.9 Stratification Two Ways to Stratify. Suppose a confounder \\(C\\) has 3 levels on which we would like to stratify when comparin g \\(\\lambda(t \\Big | E )\\) and \\(\\lambda ( t \\Big | \\bar E )\\). How? \\(X_E = \\begin{cases}1&amp;E&amp;\\text{(exposed)}\\\\0&amp;\\bar E&amp;\\text{(not exposed)}\\end{cases}\\) 22.png Which Way to Stratify? Under dummy variable stratification model, the proportional stratum-to-stratum hazards assumption may not be correct. If not, the con-founder \\(C\\) may be inadequately controlled. Proportionality assumption can be checked using time-dependent covariates. True stratification is a more thorough adjustment, as long as observations within each level are homogeneous. If \\(C\\) can be measured continuously and the strata were formed by grouping values of it, better control for \\(C\\) might be achieved with continuous (could be time-dependent) covariate adjustment. If \\(C\\) is controlled using the true stratification there is no way to estimate one summary relative risk comparing two levels of \\(C\\). However, we can estimate \\(\\lambda_{0i}(t)\\) for each stratum then we can estimate a RR function. True stratification generally requires more data to obtain the same precision in coefficient estimates. 23.png 24.png 9.5.0.10 Test statistics The standard asymptotic likelihood inference tests, Wald, score, and likelihood ratio (LR), still can be applied for the Cox partial likelihood. 25.png Their finite sample properties may differ; in general, the LRT is the most reliable, the Wald test is the least. 26.png When \\(p = 1\\) and the single covariate is categorical, the score test is identical to the log-rank test. 27.png 9.5.0.11 Handling ties Real data sets often contain tied event times. When do we have ties? Continuous event times are grouped into intervals. Event time scale is discrete. Four commonly used ways of handling ties: 1) Breslow approximation, 2) Efron approximation, 3) Exact partial likelihood, and 4) Averaged likelihood. When the underlying time is continuous but ties are generated due to a grouping, the contribution to the partial likelihood for the \\(i\\)-th event at time \\(t_i\\) is \\(\\frac{\\exp(\\beta &#39; Z_i)}{ \\sum\\limits_{j \\in R_i} Y_j(t_i) \\exp(\\beta &#39; Z_j)}\\) Two commonly used methods are 1. Breslow approximation 2. Efron approximation Example: Assume 5 subjects are at risk of dying at time \\(t\\) and two die at the same time \\(t\\) (because of grouping of time) If the time data had been more precise, then the first two terms in the likelihood would be either 28.png 29.png 30.png "],["filtration의-개념을-정복하자.html", "9.6 Filtration의 개념을 정복하자!", " 9.6 Filtration의 개념을 정복하자! 도대체 Filtration(Filtration event)을 정복해야 하는 이유가 무엇인가? 금융공학 특히 금융수학을 공부하는데 있어서 가장 핵심이 되는 것 중에 하나가 Stochastic Differencial Equation 이다. 금융수학책들을 보면 다들 처음에는 확률(이 바닥에서는 측도론을 건드리는 것이 일반적)을 다룬 다음에 Stochastic Process를 다루고 Ito’s lemma를 거쳐 Black Sholes PDE 한번 찍어주고 Girsanov를 돌아서 Feynmman-Kac Theory로 끝내곤 한다. 특히 표준 금융수학 전반에 걸쳐서 Stochastic Process가 들어가는데 개인적으로는 이거 처음에 개념잡기가 힘들었다.. 왜냐하면 확률을 측도론(Measure Theory)에서 접근을 해야 하니 측도론을 조금은 알아야 하고(덧붙여 약간의 함수해석학…) Random process, Stopping Time… 등등… 이것들의 성질이 Ft-measurable이므로 Ft-measurable이라는 의미를 잘 알아야 할 필요성까지 있기 때문이다. 뭐 Ft-measurable, 좀더 분해해서 Ft라는 filtration 과 measurable이라는 성질을 논리적으로 완벽하게 정의하는데 드는 노력은 A4용지에 반정도 끄적이면 충분하다. 그러니 어디서 누가 물으면 답변하는 정도로는 그냥 암기해 버리면 그만이 아닐까 생각한다. 그러나 정의를 주저리주저리 외우고 다닌다고 하는 것은 그 대상을 아는 것은 아니다.… 애매한 문제에 엄밀한 판결을 내릴 수 있도록 능력이 되려면 수학적 정의가 포함하는 세계가 머리속에 익숙해 져야 한다고 생각한다. 어째든지 Random process이던 Stopping Time 이건, Girsanov이건간에, 수학적인 내용이 나오면 적어도 머리속에는 간략화되고 가시적인 예제들이 Simulation되어야지 그 개념들이 ‘눈에’ 보이게 되는데 Filtration의 구체적인 모습들이 떠오르지 않으니까 나머지도 거기서 더 이상 이해가 되지 않았다. 이것이 본인이 Filtration을 한번 손봐야 겠다고 맘먹은 직접적인 동기이다. 9.6.1 Random Process를 이야기 하기까지의 긴 여정의 요약 확률공간 (Ω, F, P)의 정의에 대해서 이야기하자면 많은 사람들이 이미 알겠지만 측도론(Measure Theory)에 대한 고된 과정을 거쳐야지만 비로서 이야기할 수 있게 된다. Measure Theory는 미적분 함수해석학 등을 최신(?)관점에서 접근하도록 만들어진 이론체계인데 실해석학(Real Analysis)이라는 과목에서 주로 다루게 된다. 여기에서 Measure라는 개념 – 우리말로 하면 측도 라는 것은 말 그대로 측정한다는 것인데 이런 류로 쉽게 생각할 수 있는 것은 ‘길이’ 이다…. 혹은 넓이, 부피, 기타 등등. 여기에 쓰이는 측도의 개념을 확률에다가 접근 시킨 사람이 콜모고로프라는 라는 것은 이미 널리 알려진 바이다. 길이, 확률의 기초적 개념은 초딩때 이미 섭렵했는데 굳이 어렵게 이해할 필요가 있는가 할 수도 있겠지만 아무래도 무한대이고 연속의 세계에서는 별의별 해괴한 일이 벌어지기 때문에 유치한(유치하다는 것은 경멸적인 이야기가 아니라 덜 세련된… 정도로 생각을 해주면 된다.) 차원의 접근방법은 더 이상 통하지 않고, 집합론, 해석학의 고등 주제들을 총 동원해야 모호한 문제에 비로소 해답을 내릴 수 있다. 그러니 해석학을 하기 위해서는 집합론을 먼저 알아야 하는데 이것도 중고딩때 하던 겉핥기 식의 집합론이 아닌 무한차원을 다룬 진검승부의 집합론이 필요하다. 하여튼지 간에 이렇게 금융수학을 공부하기 위해서 선행적으로 해야 할 아주 높은 산들이 산적해 있는데.. 이걸 다 언급하는 것은 이 글의 주제를 넘어서는 것 같구… 차후에 ‘금융수학을 공부하기 위한 로드맵’에 좀더 자세히 언급하겠다. 여기서 이야기 하고 싶은 것은 확률과정(Random Process)를 실해석학 위에서 가지고 노는 것은 나름대로 다 이유가 있다는 말을 하고 싶은 것이다. 기존의 유치한 확률론으로 접근해서 나름대로 이론을 쌓을 수 있다면 아주 happy한 case이지만 그렇다 하더라도 표준적으로 쓰여있는 paper나 text가 마팅게일이니, Girsanov니.. 이런 식으로 쓰여져 있으니 뭐라고 하는지 이해는 할 수 있어야 할 것 아닌가, 더욱이 수학이 어렵다고 하는데 솔직히 공부하는 것은 아주 어려운 것 맞다. 하지만 어려운 것은 익숙지 않다는 것뿐이다. 익숙해지면 세상이 참 쉽게 보인다. 복잡한 것은 수학이 아니라 세상이다. 공부하기 어려운 수학에 익숙해지면 복잡하게만 보였던 세상이 수학에 의해서 간략하게 보이게 된다. 세상 만만하게 보인다는 이야기이다. 어렵게 공부한 사람만 볼 수 있으니 개인적 보람도 아주 크다. 9.6.1.1 Sigma-Algebra(Sigma-Field) Sigma-Algebra 라는 것은 measure를 다루기 위한 기본 개념인데 Sigma-Field와 같은 개념이다. 확률을 다루는 바닥에서는 Sigma-algebra 대신에 Sigma-Field라는 말을 쓴다. 참고로 Algebra, Field 라는 것은 Abstract Algebra(추상대수학)이라는 수학의 큰 줄기와 그 안의 체(Field)를 연상시키는데 연결고리에 대해서 약간의 심증은 가지고 있지만 물증은 찾지 못했다. 하여튼 간에 집합 A 위의 Sigma-Algebra(본인은 이 용어를 쓰겠다)의 정의는 다음의 말이 되는 A의 부분집합 E 들의 모임이다. 1. 공집합과 A는 Sigma-Algebra에 속한다. 2. E가 Sigma-Algebra에 속하면 E의 여집합도 Sigma-Algebra에 속한다. 3. E1과 E2가 Sigma-Algebra에 속하면 E1과 E2의 합집합도 Sigma-Algebra에 속한다. 뭐 여기까지는 상식적으로 이해가 된다. Ei(I는 집합의 첨수족이고 자연수N의 원소이다) 가 Sigma-Algebra에 속하면 모든 Ei 의 합집합도 Sigma-Algebra에 속한다. 과 (4) 는 같은 이야기 인 것이 아닌가? 하는 의문이 있을 수 있겠다. 즉 (3)을 반복적으로 적용하면 결국 (4)가 아닌가 하는 생각이 바로 그것이다. 답은 “같지 않다”이다. 이것이 다르다는 것을 이해하는데 에는 집합론을 깊이 아는 것이 필요하다. 뭐 그것은 그러려니 하자…. 그런데 문제는 이것뿐만이 아니다. (4) 의 내용을 이렇게 바꾸어 말할 수 있는가? 「Sigma-Algebra에 속하는 모든 E들을 무한히 합집합을 취한 것도 Sigma-Algebra에 속한다」 답의 yes, no 차원을 떠나서 위의 문제는 참으로 무책임한 질문이다. 왜냐하면 무한히 라는 것도 모호한 말이기 때문이다. 정확히는 가부번수만큼 합집합을 취한 것이 Sigma-algebra에 속한다. 가부번수라는 것은 자연수의 개수와 같으므로 위의 (4) 라는 표현을 쓴 것이다. 무한에는 가부번수라는 것도 있고 비가부번수가 있다. 짝수, 홀수, 자연수, 유리수등등 이것이 다 같은 개수를 가진다고 생각을 하고 그것을 가부번수라고 한다. 그리고 실수 무리수 같은 수는 비가부번 수이다. 그러므로 (4) 에서 I 가 N 의 집합이 아닌 실수R의 집합이나. 무리수Qc 의 집합이라고 적어 놓으면 더 이상 Sigma-algebra가 아닌 다른 무엇이 되고 마는 것이다. 참고로 (4)의 조건이 없어지면 Sigma-algebra가 아닌 Algebra가 된다. 참고로 컴퓨터 이론의 기초를 이루는 2진수 연산인 Bool algebra(불대수)도 따지고 보면 여기서 말하는 algebra이다. 그리고 (3) 의 조건을 비가부번수까지 포함해서의 합집합에 대해서 라는 조건으로 바꾸어주고 유한개의 교집합에 대해서 닫혀 있으면 위상(Topology)가 된다. 이렇듯 쉽게 보이는 것도 엄밀하게 따지지 않으면 안 되는 이유가 바로 이것이다. 무한이고 연속인 것을 머리 속에서 엄밀한 Simulation을 할 수가 없는데 이럴 경우 쉽게 볼 수 있는 간단한 discrete case를 가지고 머리 속에서 가지고 놀면 된다. 대상을 간단하게 한다고 하지만 그 대상에 대해서 무한과 연속을 다루는 규칙을 똑같이 적용하면 적어도 오류에 다다르지는 않는다. 그리고 때때로 그 결과는 그대로 연속일 때로 확장시켜 생각해도 된다. 물리학에서도 discrete로 모델을 만들고 모델을 미소변화량에 대한 연속모델로 만드는 것과 비슷하다. 게다가 Caratheodory Extension와 Pi-System 같은 도구들은 이론적으로 그러한 것이 타당하다는 보장까지 하니… 맘놓고 생각해도 된다. Caratheodory Extension 이니 Pi system이니 하는 것은 일단은 몰라도 된다. 이제 Sigma-Algebra에 대해서 조금 더 생각해 보자. A={w1,w2, w3} 인 A의 (2개의 원소를 가진) Sigma Algebra \\(F\\)들을 구해보면 F1={ Ø, A} 위 F1은 A위에서 Sigma Algebra의 1,2,3 을 만족시킨다. 유한집합이므로 3을 만족시키는 것은 4도 만족시킨다. 하지만 이것은 무한집합에서는 일반적인 것은 아니다. F2={ Ø, {w1}, {w2,w3}, {w1,w2, w3}} F3={ Ø, {w1}, {w2}, {w3}, {w1, w2}, {w2, w3}, {w1, w3}, {w1, w2, w3}} 어째든지 F1, F2, F3 셋 다 A위의 Sigma – Algebra이다. 그리고 F1⊆F2⊆F3이다. F1, F2, F3, 중에서 {w2, w3}을 포함하는 Sigma-Algebra 는 F2, F3이다. 그런데 F2와 F3중에서 크기가 작은 것은 F2이다. 즉 F2는 {w2, w3}을 포함하는 가장 작은 Sigma-Algebra이다. 이것은 {w2, w3}이 생성(generate)하는 Sigma-Algebra이다. 기호로는 Sigma({w2,w3}) 이다. 또한 Sigma({w1, w3})=F3이다. 그리고 짐작을 했겠지만 A의 모든 부분집합의 모임인 P(A)가 가장 큰 Sigma Field이다. 어떤 집합(A)의 모든 열린집합들이 Generate하는 Sigma algebra를 Borel Field 라고 하고 B(A) 라고 쓰고 그 원소를 Borel Set이라고 한다. 어떤 집합이 실수의 집합이면 B(A)는 실수의 모든 열린집합들이 Generate하는 Sigma algebra 가 된다. 이딴거 어따쓰나 생각할지 모르겠지만 이것이 바로 ‘길이’ 라고 부를 수 있는 대상들의 모임인 것이다. 길이measure도 어렵게 말하면 lebegue measure라고 한다. 머리에서 김 나겠지만 좀더 생각해 보자. **수직선 위의 어느 한 점으로 이뤄진 집합도, 그러한 점들이 자연수개수만큼씩 있는 집합도 다 __Borel set__이므로 길이를 생각할 수 있다. 그 길이는 무엇일까? 답은 0 이다. 0과 1 사이의 모든 유리수 집합도 그 길이는 0이다. 가부번 집합의 길이는 0이 된다. 연속적인 실수의 구간은 역시 Borel set이고 양끝의 값의 차이가 당근 길이이다. 그리고 그 구간에서 가부번 집합을 뺀 – 예를 들어 실수에서 유리수를 뺀 무리수의– 길이도 양끝 길이의 차이와 같다 왜냐하면 가부번 집합의 길이가 0이기 때문이다. 무리수구간의 길이는 같은 끝점을 가진 실수 구간의 길이에서 유리수 길이를 빼야 하는데 그 값, 즉 유리수의 길이,**이 0이므로 실수 구간의 길이와 같게 된다. 가부번 집합이 아닌 경우에 반드시 길이가 있는가? 답은 No 이다. 칸토르 집합의 경우 비가부번 집합이지만 전체 길이는 0 인 황당한 case 가 있기는 있다. 가부번, 비가부번 집합이란 둘 다 무한집합(원소의 개수가 무한개인 집합)을 의미한다. 참고로 측도가 0인 집합을 영집합(Null Set)이라고 하는데 해석학이건 확률론이건 확률미분방정식(Stochastic Differential Equation) 등에서 의외로 중요한 개념이므로 반드시 한번 더 보고 가자. 또한 실수에서 길이를 생각할 때 왜 실수집합의 모든 부분집합 위에서 길이를 생각하지 않고 꼭 Borel Set위에서 정의를 하냐고 생각할 수 있겠는데… 이것은 아마도 실수의 부분집합들 중에 길이의 대상이 되기에 부적합한 부분집합들이 존재하기 때문이라고 생각된다. 솔직하게 이쯤 되면 이제 머리 속에서 속속들이 Simulation을 하는 것은 포기해야 한다. 어떤 집합은 (보통) 무한집합인데 무한집합의 Sigma-algebra를 적는 것은 사실상 불가능 할뿐더러 열린집합들의 모임이라는 것도 그리 생각하는 것이 쉽지 않다. 그러니 그것이 generate하는 것을 머리에 어떻게 떠올린다는 것인가? 수학도들은 숱한 반복적인 증명연습과 연습문제 풀이로 장님 코끼리 만지듯이 감각을 키워나가지만 문제는 시간이다… 하루아침에 되는 것이 아니기 때문이다. 그렇지만 이것만 우선 먼저. 측도라는 것은 측정에 관한 이야기이다. 길이도 가장 간단한 측도이다. 길이의 대상이 되는 것들을 생각해 보자. 각각의 대상을 합해 놓아도 길이의 대상이 된다. 뿐만 아니라 두 측도 값의 합이 바로 합집합의 측도값이 된다. Sigma–Algebra의 특성인 ‘합집합에 대해서 닫혀 있음’이 바로 이것을 의미한다. 위의 A집합에 대해서 F2를 생각해 보자. F2={ Ø, {w1}, {w2,w3}, {w1,w2, w3}} Ø → 0 {w1} → 0.4 {w2, w3} → 0.6 {w1,w2,w3} → 1 이렇게 Sigma Algebra의 각 값에 대해서 측도 값을 부여하였다. 물리적인 의미는 없고, 측도의 논리에 틀리지 않게 구성이 되어 있다. 한번 보자 “어떠한 원소들의 여집합이 Sigma Algebra의 원소이다.” 확률의 경우를 생각해 보면 어떤 사건이 일어난 확률이 있다면 그것이 일어나지 않을 확률이 반드시 있다는 의미와 같다. 있다면 = \\(\\sigma\\)-field에 속한다면 있다 = \\(\\sigma\\)-field에 속한다 “어떠한 원소들의 합집합이 Sigma Algebra의 원소이다.” 또한 두 독립적 사건의 확률을 각각 계산할 수 있다면 반드시 두 사건 중 하나가 일어날 확률도 계산할 수 있다는 것을 의미한다. 또한 두 독립적 사건의 동시에 일어날 확률(합집합)은 각 확률(measure)의 합이다. 이러한 생각으로 위 측도값들을 보면 전혀 모순이 없다는 것을 알 수 있다. 물론 이러한 측도값은 하나만 있는 것이 아니다. Ø → 0 {w1} → 0.2 {w2, w3} → 0.8 {w1,w2,w3} → 1 이렇게 해도 부여한 측도값에는 전혀 문제 없다. 그런데 {w2}에 대해서 측도값(확률)을 부여할 수 있는가? 예를 들어 {w2}라는 사건이 존재한다면 뭐 억지로 어떤 값을 부여 했다고 하자. 그러면 {w2}가 일어나지 않는 경우에 대한 확률을 구할 수 있을까? 게다가 {w1} 또는 {w2}가 일어날 사건도 생각할 수 있는가? 대답할 수 없을 것이다. 이러한 예는 왜 확률을 생각할 때 Sigma Algebra를 생각해야 하는지를 설명해 준다. 즉, {2}는 가측(measurable)인가? 9.6.2 Ft-measurable 위의 내용은 초보자에게는 접근할 수 있는 흥미를, 이미 내용을 알고 있는 사람들에게는 일종의 쉬어가는 페이지가 되었을 것 같다. 이제는 좀더 엄밀하게 접근을 하려고 한다. Ft-measurable 이라는 것은 Ft라는 Filtration에 measurable 하다는 이야기이다. Filtration은 Sample Space Ω 의 Sigma Algebra 들의 모임인데 Ft1⊆Ft2⊆Ft3 (t1&lt;t2&lt;t3) 로 되어 있어서 시간이 지날수록 점점 Sigma Algebra들이 이전 \\(\\sigma\\)-Algebra를 포함하면서 커지도록 되어 있다. 이 절의 내용은 바로 이 Filtration의 몇 가지 가시적인 예제를 보여줄 것이고 그것이 엄밀한 수학적 정의와 어떻게 연결되는지를 따져 볼 것이다. 그렇게 해서 얻어진 가시적인 예제들은 장차 Stochastic Process에서 나오는 여러 가지 수학적인 개념들을 머리에서 Simulation 해서 각자의 이해의 영역으로 확보하는 과정에서 크나큰 역할을 하기만을 바랄 뿐이다. 9.6.2.1 가측(measurable)이란 먼저 measurable의 정의를 보자. Measurable의 의미를 따지기 전에 우선 임의의 하나의 함수를 생각해 보자. 그러한 함수는 집합A의 원소에서 실수로 간다고 하자. $f:A R 위와 같은 예가 될 수 있겠다. 그 다음은 역상에 대해서 생각해 보자. (역상도 집합이다.) A의 부분집합 Ω 위로의 역상은 \\(f^{-1} (E) = \\{ x | x \\in \\Omega, \\; f(x) \\in E \\subseteq B(R)\\}\\) 으로 정의된다. \\(f\\)의 정의역은 \\(A\\), 치역은 \\(E\\). 단, E는 보렐집합 (가측집합). 역함수, 역상과 \\(\\sigma\\)-algebra의 관계는? 조금만 생각해 보면 \\(f^{-1}\\)는 결국 다음과 같은 하나의 함수이다. \\[ f^{-1} : B(R) \\rightarrow \\sigma \\] Sigma(\\(\\sigma\\))는 (Ω 위의 = \\(\\Omega\\)가 생성하는) Sigma algebra이다. 즉 정의구역은 Borel Set(\\(B(R)\\)) 들이고 공변역(치역)은 Ω의 Sigma algebra이다. E는 실수의 Borel set 의 원소이다. 모든 E를 다 여기서 명세할 수는 없지만 몇가지 경우로 분류할 수는 있다. 1. r1을 원소로 포함한 E들 — 이것들의 무리를 E1이라고 하자 2. r1를 원소로 포함하지 않는 E들 — 이것들의 무리를 E1’이라고 하자 \\[ \\begin{align} E &amp;= E_1 \\cup E_2 E_1 \\cap E_2 &amp;= \\emptyset \\end{align} \\] E1에 속한 E의 경우 \\(f^{-1}(E)\\) 는 Ω가 된다. Check it! E1’에 속한 E의 경우 \\(f^{-1}(E)\\) 는 Ø가 된다. Check it! 이때 주의해야 할 점은 E의 원소로 r2, r3, r4, 가 있는 경우이다. 이 경우 이들에 대한 f의 역함수 값은 A에는 존재할 수도 있다. 하지만 Ω의 원소 중에는 그런 없기 때문에 Ø 되는 것이다. 다시 한번 정의를 살펴보자. \\(f^{-1} (E) = \\{ x | x \\in \\Omega, \\; f(x) \\in E \\subseteq B(R)\\}\\) 이다. x는 Ω의 원소이어야만 한다. 그런 원소가 아니면 공집합이 된다. Ω 와 Ø 의 모임인 {Ω , Ø} 는 Ω 위의 Sigma algebra 이다. 특별히 Ω 가 Generate하는 Sigma algebra 라고 한번 생각해 주고 넘어가자. 이제 measurable의 정의를 보자. 함수 f: Ω → R이 F – measurable 이면, \\(f^{-1} (E) = \\{ x | x \\in \\Omega, \\; f(x) \\in E \\subseteq B(R)\\}\\) 가 Ω 위에서 Sigma Algebra F의 원소이다. 그런데 어떤 책에서는 함수 f: Ω → R이 F – measurable 이면 \\(f^{-1} (E) = \\{ x | x \\in \\Omega, \\; f(x) \\in E \\subseteq B(R)\\}\\) 가 Ω 위에서 Sigma Algebra 를 이룬다. 라고 해서 사람을 헷갈리게 만든다. 이게 왜 사람 헷갈리게 만드는가 하면 F 안의 원소인 e1의 상이 B(R) 의 원소가 아닐 경우 위의 두 개의 정의는 동치가 되기 때문이다. 아직 이 문제는 개인적으로는 해결이 안되었고 학습에는 그다지 큰 문제는 아니므로 첫 번째 정의를 받아들이자. 9.6.2.2 Filtration 9.6.2.2.1 t =0 일때 Ft Ω={w1, w2, w3, w4} 라고 하자. B(R) 들의 원소 들 E 들은 r1 을 원소로 같는 E 들의 무리 E1 이 있고 그렇지 못한 무리 E2 가 있을 수 있다. 앞의 경우에서 해설한 바 몇 가지 점을 유의하면 다음과 같은 결과를 얻는다. 무리 E1 들의 역상은 Ω 이다. 무리 E2 들의 역상은 Ø 이다. 따라서 B(R)의 모든 역상들의 모임은 {Ω, Ø} 이다. Ft(t=0) = F0 을 {Ω, Ø}이라고 하면 함수 f 는 위 정의에 의하여 F0 measurable이다. 결과적으로 보니까. F0는 partition{Ω, Ø} 이 generate하는 Sigma Algebra로 볼 수 있다. 9.6.2.2.2 t =1 일 때 Ft 앞에서와 같이 B(R)을 다음과 같이 나눌 수 있다. 그리고 앞에서 한 분석을 다시 해보면 다음과 같은 결과가 나온다. E의 그룹 그룹의 성격 그룹내 모든 E들의 역상 E1 r1 을 원소로 갖는 E들 {w1} E2 r2 을 원소로 갖는 E들 {w2, w3, w4} E12 r1과 r2 모두를 원소로 갖는 E들 {w1, w2, w3, w4} En r1과 r2 모두를 원소로 갖지 않는 E들 Ø 마지막 열의 집합들은 partition{{w1}, {w2, w3, w4}} 이 generate하는 F1 내에 속하게 된다. 9.6.2.2.3 t =2 일때 Ft 앞의 분석을 여기서 다시 하면 다음과 같은 결과를 얻을 수 있다. E의 그룹 그룹의 성격 그룹내 모든 E 들의 역상 E1 r1 을 원소로 갖는 E들 {w1} E2 r2 을 원소로 갖는 E들 {w2} E3 r3 을 원소로 갖는 E들 {w3, w4} E12 r1과 r2를 동시에 원소로 갖는 E들 {w1, w2} E13 r1과 r3를 동시에 원소로 갖는 E들 {w1, w3, w4} E23 r2과 r3를 동시에 원소로 갖는 E들 {w2, w3, w4} E123 r1, r2, r3를 동시에 원소로 갖는 E {w1, w2, w3, w4} En r1, r2, r3를 원소로 갖지 않는 E Ø 확인해보면 마지막 컬럼의 집합들은 {{1},{2},{3, 4} }이 Generate하는 Sigma algebra의 원소들이다. 이제 어느 정도 요령이 생기고 규칙 같은 것들이 생길 것이다. 쉽게 말하면 Sample space를 partition 해서 Sigma Algebra Ft들을 generate하는 것이다. t가 증가함에 따라 partition을 좀더 세분화 하면 Ft1⊆Ft2⊆Ft3… 이런식으로 Filtration 을 만들 수 있다. 9.6.2.3 좀더 현실적인 예 좀더 많이 볼 수 있는 Random process 에 적합한 예를 만들어 보자. 동전을 던져서 앞이 나오면 1 뒤가 나오면 0을 점수에 더하는 것으로 하자. 점수는 random process(라고 보아도 되는 것)이다. 동전던지기는 (최대) 3회까지 하는 것으로 하자. Sample space Ω 는 다음과 같이 구성할 수 있다. 동전던지기 첫회 시행에 위, 그 다음 시행에 아래, 그 다음 시행에 위가 나오는 경우는 Wudu 이라고 보면 된다. T=0 시점은 동전던지기를 시행하기 전이다. 각 동전던지기 수행 시점에서 Random process인 점수를 다음 테이블에 표시하였다. 앞의 예제들로 요령이 생겼으니 각각의 시점에서 해당 Sigma – algebra를 얻을 수 있을 것이다. T=0 일 경우에는 { Ø, Ω } 가 generate하는 Sigma-algebra이고 t=1 인 경우에는 Ω 를 1인 값과 0인 값으로 partition을 하고 그것으로부터 Sigma-algebra를 생성(generate)할 수 있다. 각각에 대해서 증대하는 Sigma – algebra인 filtration을 볼 수 있으면 random process가 Ft에 measurable이라는 것이 어떠한 것인지 적어도 논리적으로 알 수 있을 것이다. 9.6.2.4 정보 집합으로서의 Filtration Stochastic process에서 나오는 이론적인 case에서는 위의 예제를 머리 속에서 굴려보면 충분히 이해가 될 듯 싶다. 그런데 간간히 들리는 이야기는 Filtration을 시장을 움직이는 정보들의 모임이라고 보는 견해가 있다. 이것을 알면 증대해 나가는 Sigma algebra에 대한 색다른 관점을 가지게 되는 것이고 또한 금융수학이 목적이면 우리가 사는 세상과 이 수학적 모델이 어떤 연결고리를 가지고 있는 것인지를 알아볼 수 있는 기회가 된다고 생각된다. 3개의 현실세계에서 주식가격을 움직이는 사건이 있다고 치자. A. 2000년 4월 금리인하조치 발표 – 발생될 경우 주식가격 10% 상승 B. 2001년 9월 11일 뉴욕 무역센터 건물 붕괴 – 발생될 경우 주식가격 30% 하락 C. 2002년 3월 예상을 웃도는 회계실적 발표 – 발생될 경우 주식가격 15% 상승 일어난다면 대문자 일어나지 않는다면 소문자로 표기하면 WABc는 A는 일어나고 B도 일어나고 c는 일어나지 않는다는 case이다. 위의 세 사건말고는 절대 주식가격이 움직이지 않는다고 가정한다. 1999.12 에는 Sigma Algebra가 { Ø, Ω } 이었다. 모든 사건들의 case는 아직 일어나지 않았다. 2000년 4월에 Sigma Algebra는 { {WABC, WABc, WAbC, WAbc}, {WaBC, WaBc, WabC, Wabc}}이라는 partition이 Generate하는 것이다. W의 첨자 중에서 첫번째 첨자가 가격을 결정한 것이다. 100원이 될지 110원이 될지는 첫 번째 첨자에 해당하는 사건의 정보가 나타났기 때문이다. 다시 말하면 2000년 4월에 100원 또는 110원을 결정하는 것은 2000년 4월에 일어난 ‘사건’이다. 이런 식으로 점점 잘게 partition 을 쪼개어 나가고 그것이 generate하는 것이 Filtration 이고 정보가 점점 증가된다. 위의 모델을 잘 살펴보면 특정시점의 주식가격의 결정은 그때까지의 모든 발생된 정보에 의해서 결정된다. 2001년 9월 직후의 주식가격은 첨자A와 첨자 B에 해당하는 사건발생여부의 ‘정보’가 가격을 결정한다. 9.6.2.5 Natural Filtration 실제로는 주식가격을 결정하는 정보는 무한이고 그에 따른 주식가격의 변화도 연속적으로 바뀐다. 하지만 모든 정보의 결과가 주식가격으로 나타나니까. t시점의 정보의 변화는 t시점의 가격으로 반영이 된다. Sigma algebra가 어떤 partition에 의해 generate되는 것이라면 그 partition들을 이루는 집합들은 마치 t시점의 주식가격을 index처럼 가지고 있는 것처럼 만들어야 한다. 다시 말하면 partition내의 집합마다 각각 다른 St값을 가지고 있어야 한다. 이를테면 { …{St=99, …}, {St=100…}, {St=101…}, …} 이런 식으로 partition을 하면 된다. t에서 dt만큼 지난 t’ 시점에서는 위의 각각의 partition 들을 다시 각각 쪼갠다. 쪼개는 방법은 각각의 partition의 집합들이 St와 St’ 2개의 index로 가지도록 하면 된다. 이렇게… {…{St=99, St’=99…}, {St=99, St’=100…}, {St=99, St’=101},… {St=100, St’=99…}, {St=100, St’=100…}, {St=100, St’=101},… ,{St=101, St’=99…}, {St=100, St’=100…}, {St=99, St’=101}…} 이런 식으로 말이다. 위의 식을 간략하게 말해보자. 첫 번째 t에서의 Partition(또는 책에 따라서는 Decomposition이라고 하기도 하더구먼)을 D(St)라고 표현해본다. St가 generate 하는 Partition이라고 생각하면 된다. 구체적인 방법은 위에 설명했다. T’에서는 어떻게 될까? D(St, St’) 두 개의 주식가격(또는 Random variable)에 의해서 generate되는 partition인 것이다. 일반적으로 보면 D(S0, S1, S2,….St)가 된다. T시점의 Sigma algebra Ft는 D(S0, S1,S2, …St)가 generate하는 Sigma-algebra이다. 이것을 σ(S0, S1, S2…St)라고 표시한다. 마지막 절을 다룬 것은 몇몇의 다른 책에 위와 같은 notation이 있기 때문이다. 9.6.3 EPILOGUE 하여튼지 수학책에는 St까지의 Random process가 generate하는 partition이니 Decomposition이니 또 그것이 Generate하는 Sigma-algebra니… 이러니까. 논리적으로는 깔끔해 보이지만 그게 눈으로 들어오기까지는 참으로 죽을 맛이더라. 하긴 이렇게 깔끔한 표현이 도움이 되긴 한다. 앞에서도 말했지만 앞에서 13페이지가 넘게 주저리주저리 써 놓은 것을 수학책에서는 몇 줄로 표현을 하니까.. 깔끔스럽기는 이보다 더하지는 못할 것이다. 많이 생각한 수학의 정의는 대부분 암기하게 되는데… 그게 외우려고 외워지는 것이 아니다. 애매한 예들을 적용하려고 Definition을 수십 번 들춰보게 되는데 그러면 자연스럽게 토씨 하나까지 암기 하게 된다. 이것은 토씨 하나에 따라 정말 많은 것이 좌우되기 때문인데 이때마다. 수학적 정의가 얼마나 정교하게 만들어진 것인지를 새삼 깨닫게 된다. 그렇다 하더라도 개인적으로는 수학자들에게 불만이 많다. 깔끔하게 써 놓은 것도 이해는 가지만, 좀 이해하기 좋게 주저리주저리 쓰는 개인교습교재를 함께 만들었으면 하는 바람 때문이다. 말이 나왔으니 말인데, 수학책들은 두께는 얇으면서도 내용은 많고 비싸기로 유명하다. 비싼 것은 대중적이지 않으니까 그렇다 쳐도, 내용이 많은 것은 그 많은 내용을 압축해서 썼기쓰여졌기 때문이다. 고딩 때 정석책도 그런 식으로 쓰면 아마 책 내용이 1/7로 줄을 것 같다. 반대로 중편소설책 같은 분량의 수학책도 고딩 때 정석책처럼 주저리주저리 설명을 하려면 아마도 백과사전 책의 몇 권이 될 것 같다. 실제로 수학책의 강의노트들은 두꺼운 파일로 정리해도 네댓 개는 나온다. 강의노트 없이 그리고 정식 수학과정을 밟지 않은 공돌이가 맨땅에 헤딩해보니까. 나오는 것은 머리에 피나는 일밖에 없더라. 방법은 그저 조금 더 생각해 보고 써보고 낮은 포복이라도 멈추지 않고 기어가는 길일 것이다. 그래도 이렇게 쓰는 일로 해서 다른 사람들이 좀더 빨리 이해하고 도움이 되면 그간의 고통이 보람이 될 것이라고 믿는다. 여기의 모든 예들은 본인이 직접 고안한 것이다. 뭐, 저작권을 이야기 하려는 것은 아니고 다만 공인된 내용이 아니라(그래도 곡학아세로 혹세무민하는 일은 결코 없다.)그래도 수학이니 엄밀성을 요하는 부분에 부족한 점이 있을 수 있다는 것을 말하려는 것이다. 해서 보다 고수님들이 이 부분을 지적해 주면 더할 나위 없이 고맙겠다. 또한 충분히 설명을 하려고 했으나, 생각지 못한 부분이 있을 수 있으니 보다가 모르는 부분이 있으면 같이 생각하는 것도 또한 쌍방의 유익이 될 것도 같다. 하나 알아둘 것은 본 내용은 Self-Contained이지 않다. 무슨 이야긴고 하면 Sigma-algebra이고 measure고 Random Process이고 생판 모르는 사람이 본 이 내용을 보고 예수가 장님 눈뜨게 하듯 번쩍 뜨일 것 같지는 않을 것 같다는 말이다. 적어도 한번은 수학책을 보고 고민해서 이 생각 저 생각쯤은 한번쯤 한 사람들에게 도움이 되지 않을까? 아직은 장마다. 주룩주룩 내리는 비속에서 스타벅스같은 데서 노트북이나 두드리는 한가함을 기대해보면서 이만 펜을 놓는다. "],["concepts.html", "9.7 Concepts", " 9.7 Concepts \\(S(t)\\): survival function. 시점 \\(t\\)까지는 살아있을 확률. \\(h(t) = \\lambda(t) = \\frac{f(t)}{S(t)}\\): hazard function. 시점 \\(t\\)에서 사망할 확률. "],["concepts-1.html", "A Concepts", " A Concepts 모먼트, MLE (2차까지 확인) MLE 불변성 MSE를 통해 통계량 성능 비교 가능함 bias MSE = precision + accuracy UMVUE 7.5 크래머-라오 부등식 : 최저 분산 뽑아내는 수단 피셔 정보 2차원 피셔 정보 라오-블랙웰 : uniform better UE 뽑아내는 수단 unique best UE best UE는 오직 하나뿐 (레만쉐페) CSS에 기반한 UE는 오직 유일함 W가 best UE면 W는 다른 모든 0에 대한 추정자들과 무연관 7.7 consistent (점근성) 충분통계량에 기반한 가설검정은 원본데이터 가설검정과 결과 동일 test으 unbaised 8.8 네이만 피어슨 카를린 루빈 8.3 빅 샘플 추정자들과 8.5 스코어 스탯 8.12 왈드 테스트 8.13 1-a confidence iterval = acceptance region of level 알파 test 뒤집은 테스트의 성질은 컨피던스 인터벌에도 전이됨 pivotal 주어진 X랑 모수로 다른 변량 만들었을 때 이것이 오리지널 모수와 무관한 분포 따름. CLT. MLE는 asymptotic 성질 갖음. MLE를 asymptotic 했을 때 이는 정규분포 따름. 따라서 MLE의 함수는 추축변량. cdf는 출신과 무관하게 U(0,1)을 따르므로 이를 추축변량으로 삼는게 가능. 이떄 자주 쓰이는건 알파/2. 감마와 포아송간 변환 유니모달 cdf가 이하의 조건을 지키면 shortest. 9.5. dog-tired Bubble Plot 3D Scatter Plot Star Plot Chernoff Faces Parallel Coordinate Plot 1.Q-Q Plot Shapiro-Wilks Test Kolmogorov-Smirnov Test Skewness Test ( ) Kurtosis Test: ( ) Lin and Mudholkar Scatter Plot Squared Generalized Distances Chi-Square Plot (Gamma Plot) nqplot contour plot cqplot (Python – assumption check) "],["autologistic.html", "A.1 Autologistics", " A.1 Autologistics Source: # Ising 에 추가적인 조건을 덧붙인 결과물. 2차원 lattice 를 가정. 이때 이 lattice 의 각 node 의 쌍 \\((i,j)\\) 는 이에 엮인 랜덤 변수 \\(X_{i,j}\\) 를 가짐. 이때 가장 가까이 위치하고 있는 최단거리 이웃 둘 사이의 관계를 서술하기 위해, 즉 상응하는 2개의 actor 사이의 상호작용에 해당하는 랜덤변수 \\(X_{i,j}\\) 를 서술하기 위해 사용될 수 있는 조건부 확률의 형태는 이하와 같다. 이때 변수들 \\(X_{i,j}\\) 들 사이의 상호관계를 서술하기 위해 적용될 수 있는 nearest-neighbor model 들 중 하나의 개념은 이하와 같이 조건부 확률의 형으로 제시된다. \\[ P(x_{i,j} | \\text{all other values}) \\equiv P(x_{i,j} | x_{i-1, j}, x_{i+1, j}, x_{i, j-1}, x_{i, j+1}) \\] 위의 식이 직관적인 표현을 제시하긴 하지만 여기에도 단점은 있음. 예를 들어 lattice 의 joint probability distribution 을 평가할 방법이 없으며, 또 이런 조건부 확률로 표현하는 함수의 형은 severe consistency condition 에 강하게 의존함. model 이 spatially homogenous 라고 가정하고 binary 데이터를 사용한다고 가정하자. 그러면 이하과 같이 식을 작성할 수 있다. \\[ \\exists \\alpha, \\beta_1, \\beta_2 \\in \\mathbb R: p(x_{i,j}|x_{i-1,j},x_{i+1,j},x_{i,j-1},x_{i,j+1}) \\\\ = \\frac{\\exp \\Bigg\\{x \\bigg[ \\alpha+\\beta_{1}(x_{i-1,j}+x_{i+1,j})+\\beta_{2}(x_{i,j-1}+x_{i,j+1})\\bigg]\\Bigg\\}} {1+\\exp\\Bigg\\{\\alpha+\\beta_{1}(x_{i-1,j}+x_{i+1,j})+\\beta_{2}(x_{i,j-1}+x_{i,j+1})\\Bigg\\}} \\] 위의 모델과 logistic regression model 간에 어느정도 유사점을 발견할 수 있을 것. 때문에 위의 모델을 auto-logistic model 이라고 명명하였다. boundary of zeros \\(\\mathbf x_B = 0\\) 이 lattice 의 inner array \\(\\mathbf x_I\\) 를 둘러싸고 있다고 한다면 이하가 성립한다. 이때 모든 \\((i,j)\\in I\\) 에 대한 summation 과 \\(C(\\alpha, \\beta_1, \\beta_2)\\) 는 normalizing function. 이의 실값 자체는 array 의 dimension 에 의존하여 정해짐. \\[ P\\{{\\bf x_{i}}|{\\bf x_{B}}=0\\}=\\frac{\\exp \\Bigg \\{\\sum \\Big (\\alpha+\\beta_{1}x_{i-1,j}+\\beta_{2}x_{i,j-1} \\Big )x_{i,j} \\Bigg \\}}{C(\\alpha,\\beta_{1,}\\beta_{2})} \\] 위의 결과는 closed boundary 만 가지고 있다면 그 closed boundary 가 뭔 형태든 성립함. 위의 결과를 가지면 문서의 가장 위에서 제시했던 조건부 확률의 식의 형과 일치하는 결과를 얻을 수 있지만, 이 시점까지도 아직 lattice 의 joint probability distribution 을 직접적으로 얻을 수는 없다는 점을 또 notice. "],["orderlogit.html", "A.2 Ordered Logit", " A.2 Ordered Logit "],["abstract-1.html", "B ABSTRACT", " B ABSTRACT Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy—using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16]. Our codes are publicly available at https://github.com/google-research/google-research/ tree/master/cluster_gcn 1 INTRODUCTION Graph convolutional network (GCN) [9] has become increasingly popular in addressing many graph-based applications, including semi-supervised node classification [9], link prediction [17] and recommender systems [15]. Given a graph, GCN uses a graph convolution operation to obtain node embeddings layer by layer—at each layer, the embedding of a node is obtained by gathering the embeddings of its neighbors, followed by one or a few layers of linear transformations and nonlinear activations. The final layer embedding is then used for some end tasks. For instance, in node classification problems, the final layer embedding is passed to a classifier to predict node labels, and thus the parameters of GCN can be trained in an end-to-end manner. Since the graph convolution operator in GCN needs to propagate embeddings using the interaction between nodes in the graph, this makes training quite challenging. Unlike other neural networks that the training loss can be perfectly decomposed into individual terms on each sample, the loss term in GCN (e.g., classification loss on a single node) depends on a huge number of other nodes, especially when GCN goes deep. Due to the node dependence, GCN’s training is very slow and requires lots of memory – backpropagation needs to store all the embeddings in the computation graph in GPU memory Previous GCN Training Algorithms: To demonstrate the need of developing a scalable GCN training algorithm, we first discuss the pros and cons of existing approaches, in terms of 1) memory requirement1 , 2) time per epoch2 and 3) convergence speed (loss reduction) per epoch. These three factors are crucial for evaluating a training algorithm. Note that memory requirement directly restricts the scalability of algorithm, and the later two factors combined together will determine the training speed. In the following discussion we denote N to be the number of nodes in the graph, F the embedding dimension, and L the number of layers to analyze classic GCN training algorithm. • Full-batch gradient descent is proposed in the first GCN paper [9]. To compute the full gradient, it requires storing all the intermediate embeddings, leading to O(N F L) memory requirement, which is not scalable. Furthermore, although the time per epoch is efficient, the convergence of gradient descent is slow since the parameters are updated only once per epoch. [memory: bad; time per epoch: good; convergence: bad] Mini-batch SGD is proposed in [5]. Since each update is only based on a mini-batch gradient, it can reduce the memory requirement and conduct many updates per epoch, leading to a faster convergence. However, mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. GraphSAGE [5] proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN [1] proposed importance sampling, but the overhead of these methods is still large and will become worse when GCN goes deep. [memory: good; time per epoch: bad; convergence: good] Mini-batch SGD is proposed in [5]. Since each update is only based on a mini-batch gradient, it can reduce the memory requirement and conduct many updates per epoch, leading to a faster convergence. However, mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. GraphSAGE [5] proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN [1] proposed importance sampling, but the overhead of these methods is still large and will become worse when GCN goes deep. [memory: good; time per epoch: bad; convergence: good] loiting the graph clustering structure. We find that the efficiency of a mini-batch algorithm can be characterized by the notion of “embedding utilization,” which is proportional to the number of links between nodes in one batch or within-batch links. This finding motivates us to design the batches using graph clustering algorithms that aims to construct partitions of nodes so that there are more graph links between nodes in the same partition than nodes in different partitions. Based on the graph clustering idea, we proposed Cluster-GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS [8]). We take this idea further by proposing a stochastic multi-clustering framework to improve the convergence of Cluster-GCN. Our strategy leads to huge memory and computational benefits. In terms of memory, we only need to store the node embeddings within the current batch, which is O(bF L) with the batch size b. This is significantly better than VR-GCN and full gradient decent, and slightly better than other SGD-based approaches. In terms of computational complexity, our algorithm achieves the same time cost per epoch with gradient descent and is much faster than neighborhood searching approaches. In terms of the convergence speed, our algorithm is competitive with other SGD-based approaches. Finally, our algorithm is simple to implement since we only compute matrix multiplication and no neighborhood sampling is needed. Therefore for Cluster-GCN, we have [memory: good; time per epoch: good; convergence: good]. We conducted comprehensive experiments on several large-scale graph datasets and made the following contributions: • Cluster-GCN achieves the best memory usage on large-scale graphs, especially on deep GCN. For example, Cluster-GCN uses 5x less memory than VRGCN in a 3-layer GCN model on Amazon2M. Amazon2M is a new graph dataset that we construct to demonstrate the scalablity of the GCN algorithms. This dataset contains a amazon product co-purchase graph with more than 2 millions nodes and 61 millions edges. • Cluster-GCN achieves a similar training speed with VR-GCN for shallow networks (e.g., 2 layers) but can be faster than VRGCN when the network goes deeper (e.g., 4 layers), since our complexity is linear to the number of layers L while VR-GCN’s complexity is exponential to L. • Cluster-GCN is able to train a very deep network that has a large embedding size. Although several previous works show that deep GCN does not give better performance, we found that with proper optimization, deeper GCN could help the accuracy. For example, with a 5-layer GCN, we obtain a new benchmark accuracy 99.36 for PPI dataset, comparing with the highest reported one 98.71 by [16]. Implementation of our proposed method is publicly available.3 BACKGROUND Suppose we are given a graph G = (V, E,A), which consists of N = |V | vertices and |E | edges such that an edge between any two vertices i and j represents their similarity. The corresponding adjacency matrix A is an N ×N sparse matrix with (i, j) entry equaling to 1 if there is an edge between i and j and 0 otherwise. Also, each node is associated with an F -dimensional feature vector and X ∈ R N ×F denotes the feature matrix for all N nodes. An L-layer GCN [9] consists of L graph convolution layers and each of them constructs embeddings for each node by mixing the embeddings of the node’s neighbors in the graph from the previous layer: where X (l) ∈ R N ×Fl is the embedding at the l-th layer for all the N nodes and X (0) = X; A ′ is the normalized and regularized adjacency matrix andW (l) ∈ R Fl ×Fl+1 is the feature transformation matrix which will be learnt for the downstream tasks. Note that for simplicity we assume the feature dimensions are the same for all layers (F1 = · · · = FL = F ). The activation function σ(·) is usually set to be the element-wise ReLU. Semi-supervised node classification is a popular application of GCN. When using GCN for this application, the goal is to learn weight matrices in (1) by minimizing the loss function where YL contains all the labels for the labeled nodes; z (L) i is the i-th row of Z (L) with the ground-truth label to be yi , indicating the final layer prediction of node i. In practice, a cross-entropy loss is commonly used for node classification in multi-class or multi-label problems. PROPOSED ALGORITHM We first discuss the bottleneck of previous training methods to motivate the proposed algorithm. In the original paper [9], full gradient descent is used for training GCN, but it suffers from high computational and memory cost. In terms of memory, computing the full gradient of (2) by backpropagation requires storing all the embedding matrices {Z (l) } L l=1 which needs O(N F L) space. In terms of convergence speed, since the model is only updated once per epoch, the training requires more epochs to converge. It has been shown that mini-batch SGD can improve the training speed and memory requirement of GCN in some recent works [1, 2, 5]. Instead of computing the full gradient, SGD only needs to calculate the gradient based on a mini-batch for each update. In this paper, we use B ⊆ [N] with size b = |B| to denote a batch of node indices, and each SGD step will compute the g to perform an update. Despite faster convergence in terms of epochs, SGD will introduce another computational overhead on GCN training (as explained in the following), which makes it having much slower per-epoch time compared with full gradient descent. Why does vanilla mini-batch SGD have slow per-epoch time?: We consider the computation of the gradient associated with one node i : ∇loss(yi , z (L) i ). Clearly, this requires the embedding of node i, which depends on its neighbors’ embeddings in the previous layer. To fetch each node i’s neighbor nodes’ embeddings, we need to further aggregate each neighbor node’s neighbor nodes’ embeddings as well. Suppose a GCN has L + 1 layers and each node has an average degree of d, to get the gradient for node i, we need to aggregate features fromO(d L ) nodes in the graph for one node. That is, we need to fetch information for a node’s hop-k (k = 1, · · · , L) neighbors in the graph to perform one update. Computing each embedding requires O(F 2 ) time due to the multiplication withW (l) , so in average computing the gradient associated with one node requires O(d L F 2 ) time. Embedding utilization can reflect computational efficiency.: If a batch has more than one node, the time complexity is less straightforward since different nodes can have overlapped hopk neighbors, and the number of embedding computation can be less than the worst case O(bdL ). To reflect the computational efficiency of mini-batch SGD, we define the concept of “embedding utilization” to characterize the computational efficiency. During the algorithm, if the node i’s embedding at l-th layer z (l) i is computed and is reused u times for the embedding computations at layer l + 1, then we say the embedding utilization of z (l) i is u. For mini-batch SGD with random sampling, u is very small since the graph is usually large and sparse. Assume u is a small constant (almost no overlaps between hop-k neighbors), then mini-batch SGD needs to compute O(bdL ) embeddings per batch, which leads to O(bdL F 2 ) time per update and O(NdL F 2 ) time per epoch. We illustrate the neighborhood expansion problem in the left panel of Fig. 1. In contrary, full-batch gradient descent has the maximal embedding utilization—each embedding will be reused d (average degree) times in the upper layer. As a consequence, the original full gradient descent [9] only needs to compute O(N L) embeddings per epoch, which means on average only O(L) embedding computation is needed to acquire the gradient of one node. To make mini-batch SGD work, previous approaches try to restrict the neighborhood expansion size, which however do not improve embedding utilization. GraphSAGE [5] uniformly samples a fixed-size set of neighbors, instead of using a full-neighborhood set. We denote the sample size as r. This leads to O(r L ) embedding computations for each loss term but also makes gradient estimation less accurate. FastGCN [1] proposed an important sampling strategy to improve the gradient estimation. VR-GCN [2] proposed a strategy to store the previous computed embeddings for all the N nodes and L layers and reuse them for unsampled neighbors. Despite the high memory usage for storing all the N L embeddings, we find their strategy very useful and in practice, even for a small r (e.g., 2) can lead to good convergence. We summarize the time and space complexity in Table 1. Clearly, all the SGD-based algorithms suffer from exponential complexity with respect to the number of layers, and for VR-GCN, even though r can be small, they incur huge space complexity that could go beyond a GPU’s memory capacity. In the following, we introduce our Cluster-GCN algorithm, which achieves the best of two worlds— the same time complexity per epoch with full gradient descent and the same memory complexity with vanilla SGD. 3.1 Vanilla Cluster-GCN Our Cluster-GCN technique is motivated by the following question: In mini-batch SGD updates, can we design a batch and the corresponding computation subgraph to maximize the embedding utilization? We answer this affirmative by connecting the concept of embedding utilization to a clustering objective. Consider the case that in each batch we compute the embeddings for a set of nodes B from layer 1 to L. Since the same subgraph AB, B (links within B) is used for each layer of computation, we can then see that embedding utilization is the number of edges within this batch ∥AB, B ∥0. Therefore, to maximize embedding utilization, we should design a batch B to maximize the within-batch edges, by which we connect the efficiency of SGD updates with graph clustering algorithms. Now we formally introduce Cluster-GCN. For a graph G, we partition its nodes into c groups: V = [V1, · · · Vc ] where Vt consists of the nodes in the t-th partition. Thus we have c subgraphs as where each Et only consists of the links between nodes in Vt . After reorganizing nodes, the adjacency matrix is partitioned into c 2 submatrices as where each diagonal block At t is a |Vt | × |Vt | adjacency matrix containing the links within Gt . A¯ is the adjacency matrix for graph G¯; Ast contains the links between two partitions Vs and Vt ; ∆ is the matrix consisting of all off-diagonal blocks of A. Similarly, we can partition the feature matrix X and training labels Y according to the partition [V1, · · · , Vc ] as [X1, · · · ,Xc ] and [Y1, · · · ,Yc ] where Xt and Yt consist of the features and labels for the nodes in Vt respectively. The benefit of this block-diagonal approximation G¯ is that the objective function of GCN becomes decomposible into different batches (clusters). Let A¯′ denotes the normalized version of A¯, the final embedding matrix becomes due to the block-diagonal form ofA¯(note thatA¯′ t t is the corresponding diagonal block of A¯′ ). The loss function can also be decomposed into The Cluster-GCN is then based on the decomposition form in (6) and (7). At each step, we sample a cluster Vt and then conduct SGD to update based on the gradient of LA¯′ t t , and this only requires the sub-graph At t , the Xt , Yt on the current batch and the models {W (l) } L l=1 . The implementation only requires forward and backward propagation of matrix products (one block of (6)) that is much easier to implement than the neighborhood search procedure used in previous SGD-based training methods. We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis [8] and Graclus [4] aim to construct the partitions over the vertices in the graph such that withinclusters links are much more than between-cluster links to better capture the clustering and community structure of the graph. These are exactly what we need because: 1) As mentioned before, the embedding utilization is equivalent to the within-cluster links for each batch. Intuitively, each node and its neighbors are usually located in the same cluster, therefore after a few hops, neighborhood nodes with a high chance are still in the same cluster. 2) Since we replace A by its block diagonal approximation A¯ and the error is proportional to between-cluster links ∆, we need to find a partition to minimize number of between-cluster links. In Figure 1, we illustrate the neighborhood expansion with full graph G and the graph with clustering partition G¯. We can see that cluster-GCN can avoid heavy neighborhood search and focus on the neighbors within each cluster. In Table 2, we show two different node partition strategies: random partition versus clustering partition. We partition the graph into 10 parts by using random partition and METIS. Then use one partition as a batch to perform a SGD update. We can see that with the same number of epochs, using clustering partition can achieve higher accuracy. This shows using graph clustering is important and partitions should not be formed randomly. Time and space complexity.: Since each node in Vt only links to nodes inside Vt , each node does not need to perform neighborhoods searching outside At t . The computation for each batch will purely be matrix products A¯′ t tX (l) t W (l) and some element-wise operations, so the overall time complexity per batch isO(∥At t ∥0F + bF 2 ). Thus the overall time complexity per epoch becomesO(∥A∥0F+ N F 2 ). In average, each batch only requires computingO(bL) embeddings, which is linear instead of exponential to L. In terms of space complexity, in each batch, we only need to load b samples and store their embeddings on each layer, resulting in O(bLF ) memory for storing embeddings. Therefore our algorithm is also more efficient than all the previous algorithms. Moreover, our algorithm only requires loading a subgraph into GPU memory instead of the full graph (though graph is usually not the memory bottleneck). The detailed time and memory complexity are summarized in Table 1. 3.2 Stochastic Multiple Partitions Although vanilla Cluster-GCN achieves good computational and memory complexity, there are still two potential issues: • After the graph is partitioned, some links (the ∆ part in Eq. (4)) are removed. Thus the performance could be affected. • Graph clustering algorithms tend to bring similar nodes together. Hence the distribution of a cluster could be different from the original data set, leading to a biased estimation of the full gradient while performing SGD updates. In Figure 2, we demonstrate an example of unbalanced label distribution by using the Reddit data with clusters formed by Metis. We calculate the entropy value of each cluster based on its label distribution. Comparing with random partitioning, we clearly see that entropy of most clusters are smaller, indicating that the label distributions of clusters are biased towards some specific labels. This increases the variance across different batches and may affect the convergence of SGD. To address the above issues, we propose a stochastic multiple clustering approach to incorporate between-cluster links and reduce variance across batches. We first partition the graph into p clusters V1, · · · , Vp with a relatively large p. When constructing a batch B for an SGD update, instead of considering only one cluster, we randomly choose q clusters, denoted as t1, . . . ,tq and include their nodes {Vt1 ∪ · · · ∪Vtq } into the batch. Furthermore, the links between the chosen clusters, are added back. In this way, those between-cluster links are reincorporated and the combinations of clusters make the variance across batches smaller. Figure 3 illustrates our algorithm—for each epochs, different combinations of clusters are chosen as a batch. We conduct an experiment on Reddit to demonstrate the effectiveness of the proposed approach. In Figure 4, we can observe that using multiple clusters as one batch could improve the convergence. Our final Cluster-GCN algorithm is presented in Algorithm 1. 3.3 Issues of training deeper GCNs Previous attempts of training deeper GCNs [9] seem to suggest that adding more layers is not helpful. However, the datasets used in the experiments may be too small to make a proper justification. For example, [9] considered a graph with only a few hundreds of training nodes for which overfitting can be an issue. Moreover, we observe that the optimization of deep GCN models becomes difficult as it may impede the information from the first few layers being passed through. In [9], they adopt a technique similar to residual connections [6] to enable the model to carry the information from a previous layer to a next layer. Specifically, they modify (1) to add the hidden representations of layer l into the next layer. Here we propose another simple technique to improve the training of deep GCNs. In the original GCN settings, each node aggregates the representation of its neighbors from the previous layer. However, under the setting of deep GCNs, the strategy may not be suitable as it does not take the number of layers into account. Intuitively, neighbors nearby should contribute more than distant nodes. We thus propose a technique to better address this issue. The idea is to amplify the diagonal parts of the adjacency matrix A used in each GCN layer. In this way, we are putting more weights on the representation from the previous layer in the aggregation of each GCN layer. An example is to add an identity to A¯ as follows. While (9) seems to be reasonable, using the same weight for all the nodes regardless of their numbers of neighbors may not be suitable. Moreover, it may suffer from numerical instability as values can grow exponentially when more layers are used. Hence we propose a modified version of (9) to better maintain the neighborhoods information and numerical ranges. We first add an identity to the original A and perform the normalization, (10) and then consider Experimental results of adopting the “diagonal enhancement” techniques are presented in Section 4.3 where we show that this new normalization strategy can help to build deep GCN and achieve SOTA performance. 4 EXPERIMENTS We evaluate our proposed method for training GCN on two tasks: multi-label and multi-class classification on four public datasets. The statistic of the data sets are shown in Table 3. Note that the Reddit dataset is the largest public dataset we have seen so far for GCN, and the Amazon2M dataset is collected by ourselves and is much larger than Reddit (see more details in Section 4.2). We include the following state-of-the-art GCN training algorithms in our comparisons: • Cluster-GCN (Our proposed algorithm): the proposed fast GCN training method. • VRGCN4 [2]: It maintains the historical embedding of all the nodes in the graph and expands to only a few neighbors to speedup training. The number of sampled neighbors is set to be 2 as suggested in [2]5 . • GraphSAGE6 [5]: It samples a fixed number of neighbors per node. We use the default settings of sampled sizes for each layer (S1 = 25, S2 = 10) in GraphSAGE. We implement our method in PyTorch [13]. For the other methods, we use all the original papers’ code from their github pages. Since [9] has difficulty to scale to large graphs, we do not compare with it here. Also as shown in [2] that VRGCN is faster than FastGCN, so we do not compare with FastGCN here. For all the methods we use the Adam optimizer with learning rate as 0.01, dropout rate as 20%, weight decay as zero. The mean aggregator proposed by [5] is adopted and the number of hidden units is the same for all methods. Note that techniques such as (11) is not considered here. In each experiment, we consider the same GCN architecture for all methods. For VRGCN and GraphSAGE, we follow the settings provided by the original papers and set the batch sizes as 512. For Cluster-GCN, the number of partitions and clusters per batch for each dataset are listed in Table 4. Note that clustering is seen as a preprocessing step and its running time is not taken into account in training. In Section 6, we show that graph clustering only takes a small portion of preprocessing time. All the experiments are conducted on a machine with a NVIDIA Tesla V100 GPU (16 GB memory), 20-core Intel Xeon CPU (2.20 GHz), and 192 GB of RAM. 4.1 Training Performance for median size datasets Training Time vs Accuracy: First we compare our proposed method with other methods in terms of training speed. In Figure 6, the x-axis shows the training time in seconds, and y-axis shows the accuracy (F1 score) on the validation sets. We plot the training time versus accuracy for three datasets with 2,3,4 layers of GCN. Since GraphSAGE is slower than VRGCN and our method, the curves for GraphSAGE only appear for PPI and Reddit datasets. We can see that our method is the fastest for both PPI and Reddit datasets for GCNs with different numbers of layers. For Amazon data, since nodes’ features are not available, an identity matrix is used as the feature matrix X. Under this setting, the shape of parameter matrix W (0) becomes 334863x128. Therefore, the computation is dominated by sparse matrix operations such as AW (0) . Our method is still faster than VRGCN for 3-layer case, but slower for 2-layer and 4-layer ones. The reason may come from the speed of sparse matrix operations from different frameworks. VRGCN is implemented in TensorFlow, while Cluster-GCN is implemented in PyTorch whose sparse tensor support are still in its very early stage. In Table 6, we show the time for TensorFlow and PyTorch to do forward/backward operations on Amazon data, and a simple two-layer network are used for benchmarking both frameworks. We can clearly see that TensorFlow is faster than PyTorch. The difference is more significant when the number of hidden units increases. This may explain why Cluster-GCN has longer training time in Amazon dataset. Memory usage comparison: For training large-scale GCNs, besides training time, memory usage needed for training is often more important and will directly restrict the scalability. The memory usage includes the memory needed for training the GCN for many epochs. As discussed in Section 3, to speedup training, VRGCN needs to save historical embeddings during training, so it needs much more memory for training than Cluster-GCN. GraphSAGE also has higher memory requirement than Cluster-GCN due to the exponential neighborhood growing problem. In Table 5, we compare our memory usage with VRGCN’s memory usage for GCN with different layers. When increasing the number of layers, Cluster-GCN’s memory usage does not increase a lot. The reason is that when increasing one layer, the extra variable introduced is the weight matrix W (L) , which is relatively small comparing to the sub-graph and node features. While VRGCN needs to save each layer’s history embeddings, and the embeddings are usually dense and will soon dominate the memory usage. We can see from Table 5 that Cluster-GCN is much more memory efficient than VRGCN. For instance, on Reddit data to train a 4-layer GCN with hidden dimension to be 512, VRGCN needs 2064MB memory, while Cluster-GCN only uses 308MB memory. 4.2 Experimental results on Amazon2M A new GCN dataset: Amazon2M. By far the largest public data for testing GCN is Reddit dataset with the statistics shown in Table 3, which contains about 200K nodes. As shown in Figure 6 GCN training on this data can be finished within a few hundreds seconds. To test the scalability of GCN training algorithms, we constructed a much larger graph with over 2 millions of nodes and 61 million edges based on Amazon co-purchasing networks [11, 12]. The raw co-purchase data is from Amazon-3M7 . In the graph, each node is a product, and the graph link represents whether two products are purchased together. Each node feature is generated by extracting bag-of-word features from the product descriptions followed by Principal Component Analysis [7] to reduce the dimension to be 100. In addition, we use the top-level categories as the labels for that product/node (see Table 7 for the most common categories). The detailed statistics of the data set are listed in Table 3. In Table 8, we compare with VRGCN for GCNs with a different number of layers in terms of training time, memory usage, and test accuracy (F1 score). As can be seen from the table that 1) VRGCN is faster than Cluster-GCN with 2-layer GCN but slower than ClusterGCN when increasing one layer while achieving similar accuracy. 2) In terms of memory usage, VRGCN is using much more memory than Cluster-GCN (5 times more for 3-layer case), and it is running out of memory when training 4-layer GCN, while Cluster-GCN does not need much additional memory when increasing the number of layers, and achieves the best accuracy for this data when training a 4-layer GCN. 4.3 Training Deeper GCN In this section we consider GCNs with more layers. We first show the timing comparisons of Cluster-GCN and VRGCN in Table 9. PPI is used for benchmarking and we run 200 epochs for both methods. We observe that the running time of VRGCN grows exponentially because of its expensive neighborhood finding, while the running time of Cluster-GCN only grows linearly. Next we investigate whether using deeper GCNs obtains better accuracy. In Section 4.3, we discuss different strategies of modifying the adjacency matrix A to facilitate the training of deep GCNs. We apply the diagonal enhancement techniques to deep GCNs and run experiments on PPI. Results are shown in Table 11. For the case of 2 to 5 layers, the accuracy of all methods increases with more layers added, suggesting that deeper GCNs may be useful. However, when 7 or 8 GCN layers are used, the first three methods fail to converge within 200 epochs and get a dramatic loss of accuracy. A possible reason is that the optimization for deeper GCNs becomes more difficult. We show a detailed convergence of a 8-layer GCN in Figure 5. With the proposed diagonal enhancement technique (11), the convergence can be improved significantly and similar accuracy can be achieved. State-of-the-art results by training deeper GCNs.: With the design of Cluster-GCN and the proposed normalization approach, we now have the ability for training much deeper GCNs to achieve better accuracy (F1 score). We compare the testing accuracy with other existing methods in Table 10. For PPI, Cluster-GCN can achieve the state-of-art result by training a 5-layer GCN with 2048 hidden units. For Reddit, a 4-layer GCN with 128 hidden units is used. 5 CONCLUSION We present ClusterGCN, a new GCN training algorithm that is fast and memory efficient. Experimental results show that this method can train very deep GCN on large-scale graph, for instance on a graph with over 2 million nodes, the training time is less than an hour using around 2G memory and achieves accuracy of 90.41 (F1 score). Using the proposed approach, we are able to successfully train much deeper GCNs, which achieve state-of-the-art test F1 score on PPI and Reddit datasets. Acknowledgement: CJH acknowledges the support of NSF via IIS-1719097, Intel faculty award, Google Cloud and Nvidia. "],["cnn.html", "C CNN", " C CNN Abstract We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1 Introduction Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the currentbest error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories. To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse. Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting. The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly1 . Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance. In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available. 2 The Dataset ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels. 3 The Architecture The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first. 3.1 ReLU Nonlinearity Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons. The standard way to model a neuron’s output f as a function of its input x is with f(x) = tanh(x) or f(x) = (1 + e −x ) −1 . In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0, x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models. We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al. [11] claim that the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets. 3.2 Training on Multiple GPUs A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation. The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cire¸san et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net2 . The one-GPU net actually has the same number of kernels as the two-GPU net in the final convolutional layer. This is because most of the net’s parameters are in the first fully-connected layer, which takes the last convolutional layer as input. So to make the two nets have approximately the same number of parameters, we did not halve the size of the final convolutional layer (nor the fully-conneced layers which follow). Therefore this comparison is biased in favor of the one-GPU net, since it is bigger than “half the size” of the two-GPU net. 3.3 Local Response Normalization ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by a i x,y the activity of a neuron computed by applying kernel i at position (x, y) and then applying the ReLU nonlinearity, the response-normalized activity b i x,y is given by the expression, where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 10−4 , and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5). This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization,” since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization3 . 3.4 Overlapping Pooling Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s &lt; z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2, z = 2, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit. 3.5 Overall Architecture Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution. The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fullyconnected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer. The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each. Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000. 4 Reducing Overfitting Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting. 4.1 Data Augmentation The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free. The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches4 . This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches. The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel Ixy = [I R xy, IG xy, IB xy] T we add the following quantity: where pi and λi are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and αi is the aforementioned random variable. Each αi is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%. 4.2 Dropout Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks. We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge. 5 Details of learning We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight w was B. We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0. We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs. 6 Results Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5 . The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24] We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7]. Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19]. 6.1 Qualitative Evaluations Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs). In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph. Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material. Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying autoencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar 7 Discussion Our results show that a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results. To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images. "],["cnn-1.html", "D CNN", " D CNN SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS ABSTRACT We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin 1 INTRODUCTION We consider the problem of classifying nodes (such as documents) in a graph (such as a citation network), where labels are only available for a small subset of nodes. This problem can be framed as graph-based semi-supervised learning, where label information is smoothed over the graph via some form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function: Here, L0 denotes the supervised loss w.r.t. the labeled part of the graph, f(·) can be a neural networklike differentiable function, λ is a weighing factor and X is a matrix of node feature vectors Xi . ∆ = D − A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with N nodes vi ∈ V, edges (vi , vj ) ∈ E, an adjacency matrix A ∈ R N×N (binary or weighted) and a degree matrix Dii = P j Aij . The formulation of Eq. 1 relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could contain additional information. In this work, we encode the graph structure directly using a neural network model f(X, A) and train on a supervised target L0 for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function. Conditioning f(·) on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss L0 and will enable it to learn representations of nodes both with and without labels. Our contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions (Hammond et al., 2011). Secondly, we demonstrate how this form of a graph-based neural network model can be used for fast and scalable semi-supervised classification of nodes in a graph. Experiments on a number of datasets demonstrate that our model compares favorably both in classification accuracy and efficiency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning 2 FAST APPROXIMATE CONVOLUTIONS ON GRAPHS In this section, we provide theoretical motivation for a specific graph-based neural network model f(X, A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule: Here, A˜ = A + IN is the adjacency matrix of the undirected graph G with added self-connections. IN is the identity matrix, D˜ ii = P j A˜ ij and W(l) is a layer-specific trainable weight matrix. σ(·) denotes an activation function, such as the ReLU(·) = max(0, ·). H(l) ∈ R N×D is the matrix of activations in the l th layer; H(0) = X. In the following, we show that the form of this propagation rule can be motivated1 via a first-order approximation of localized spectral filters on graphs (Hammond et al., 2011; Defferrard et al., 2016). 2.1 SPECTRAL GRAPH CONVOLUTIONS We consider spectral convolutions on graphs defined as the multiplication of a signal x ∈ R N (a scalar for every node) with a filter gθ = diag(θ) parameterized by θ ∈ R N in the Fourier domain, i.e.: where U is the matrix of eigenvectors of the normalized graph Laplacian L = IN − D− 1 2 AD− 1 2 = UΛU &gt;, with a diagonal matrix of its eigenvalues Λ and U &gt;x being the graph Fourier transform of x. We can understand gθ as a function of the eigenvalues of L, i.e. gθ(Λ). Evaluating Eq. 3 is computationally expensive, as multiplication with the eigenvector matrix U is O(N2 ). Furthermore, computing the eigendecomposition of L in the first place might be prohibitively expensive for large graphs. To circumvent this problem, it was suggested in Hammond et al. (2011) that gθ(Λ) can be well-approximated by a truncated expansion in terms of Chebyshev polynomials Tk(x) up to Kth order: with a rescaled Λ = ˜ 2 λmax Λ − IN . λmax denotes the largest eigenvalue of L. θ 0 ∈ R K is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as Tk(x) = 2xTk−1(x) − Tk−2(x), with T0(x) = 1 and T1(x) = x. The reader is referred to Hammond et al. (2011) for an in-depth discussion of this approximation. Going back to our definition of a convolution of a signal x with a filter gθ 0 , we now have: with L˜ = 2 λmax L − IN ; as can easily be verified by noticing that (UΛU &gt;) k = UΛ kU &gt;. Note that this expression is now K-localized since it is a Kth-order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (Kth-order neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al. (2016) use this K-localized convolution to define a convolutional neural network on graphs. 2.2 LAYER-WISE LINEAR MODEL A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now, imagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is linear w.r.t. L and therefore a linear function on the graph Laplacian spectrum. In this way, we can still recover a rich class of convolutional filter functions by stacking multiple such layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev polynomials. We intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, knowledge graphs and many other real-world graph datasets. Additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains (He et al., 2016). In this linear formulation of a GCN we further approximate λmax ≈ 2, as we can expect that neural network parameters will adapt to this change in scale during training. Under these approximations Eq. 5 simplifies to: with two free parameters θ 0 0 and θ 0 1 . The filter parameters can be shared over the whole graph. Successive application of filters of this form then effectively convolve the k th-order neighborhood of a node, where k is the number of successive filtering operations or convolutional layers in the neural network model. In practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. This leaves us with the following expression: with a single parameter θ = θ 0 0 = −θ 0 1 . Note that IN + D− 1 2 AD− 1 2 now has eigenvalues in the range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following renormalization trick: IN +D− 1 2 AD− 1 2 → D˜ − 1 2 A˜D˜ − 1 2 , with A˜ = A + IN and D˜ ii = P j A˜ ij . We can generalize this definition to a signal X ∈ R N×C with C input channels (i.e. a C-dimensional feature vector for every node) and F filters or feature maps as follows: where Θ ∈ R C×F is now a matrix of filter parameters and Z ∈ R N×F is the convolved signal matrix. This filtering operation has complexity O(|E|F C), as AX˜ can be efficiently implemented as a product of a sparse matrix with a dense matrix. 3 SEMI-SUPERVISED NODE CLASSIFICATION Having introduced a simple, yet flexible model f(X, A) for efficient information propagation on graphs, we can return to the problem of semi-supervised node classification. As outlined in the introduction, we can relax certain assumptions typically made in graph-based semi-supervised learning by conditioning our model f(X, A) both on the data X and on the adjacency matrix A of the underlying graph structure. We expect this setting to be especially powerful in scenarios where the adjacency matrix contains information not present in the data X, such as citation links between documents in a citation network or relations in a knowledge graph. The overall model, a multi-layer GCN for semi-supervised learning, is schematically depicted in Figure 1. 3.1 EXAMPLE In the following, we consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A (binary or weighted). We first calculate Aˆ = D˜ − 1 2 A˜D˜ − 1 2 in a pre-processing step. Our forward model then takes the simple form Here, W(0) ∈ R C×H is an input-to-hidden weight matrix for a hidden layer with H feature maps. W(1) ∈ R H×F is a hidden-to-output weight matrix. The softmax activation function, defined as softmax(xi) = 1 Z exp(xi) with Z = P i exp(xi), is applied row-wise. For semi-supervised multiclass classification, we then evaluate the cross-entropy error over all labeled examples: where YL is the set of node indices that have labels. The neural network weights W(0) and W(1) are trained using gradient descent. In this work, we perform batch gradient descent using the full dataset for every training iteration, which is a viable option as long as datasets fit in memory. Using a sparse representation for A, memory requirement is O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via dropout (Srivastava et al., 2014). We leave memory-efficient extensions with mini-batch stochastic gradient descent for future work. 3.2 IMPLEMENTATION In practice, we make use of TensorFlow (Abadi et al., 2015) for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of evaluating Eq. 9 is then O(|E|CHF), i.e. linear in the number of graph edges. 4 RELATED WORK Our model draws inspiration both from the field of graph-based semi-supervised learning and from recent work on neural networks that operate on graphs. In what follows, we provide a brief overview on related work in both fields. 4.1 GRAPH-BASED SEMI-SUPERVISED LEARNING A large number of approaches for semi-supervised learning using graph representations have been proposed in recent years, most of which fall into two broad categories: methods that use some form of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent examples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold regularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012). Recently, attention has shifted to models that learn graph embeddings with methods inspired by the skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings via the prediction of the local neighborhood of nodes, sampled from random walks on the graph. LINE (Tang et al., 2015) and node2vec (Grover &amp; Leskovec, 2016) extend DeepWalk with more sophisticated random walk or breadth-first search schemes. For all these methods, however, a multistep pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label information in the process of learning embeddings. 4.2 NEURAL NETWORKS ON GRAPHS Neural networks that operate on graphs have previously been introduced in Gori et al. (2005); Scarselli et al. (2009) as a form of recurrent neural network. Their framework requires the repeated application of contraction maps as propagation functions until node representations reach a stable fixed point. This restriction was later alleviated in Li et al. (2016) by introducing modern practices for recurrent neural network training to the original graph neural network framework. Duvenaud et al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level classification. Their approach requires to learn node degree-specific weight matrices which does not scale to large graphs with wide node degree distributions. Our model instead uses a single weight matrix per layer and deals with varying node degrees through an appropriate normalization of the adjacency matrix (see Section 3.1). A related approach to node classification with a graph-based neural network was recently introduced in Atwood &amp; Towsley (2016). They report O(N2 ) complexity, limiting the range of possible applications. In a different yet related model, Niepert et al. (2016) convert graphs locally into sequences that are fed into a conventional 1D convolutional neural network, which requires the definition of a node ordering in a pre-processing step. Our method is based on spectral graph convolutional neural networks, introduced in Bruna et al. (2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast to these works, we consider here the task of transductive node classification within networks of significantly larger scale. We show that in this setting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of Bruna et al. (2014) and Defferrard et al. (2016) that improve scalability and classification performance in large-scale networks. 5 EXPERIMENTS We test our model in a number of experiments: semi-supervised document classification in citation networks, semi-supervised entity classification in a bipartite graph extracted from a knowledge graph, an evaluation of various graph propagation models and a run-time analysis on random graphs. 5.1 DATASETS We closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized in Table 1. In the citation network datasets—Citeseer, Cora and Pubmed (Sen et al., 2008)—nodes are documents and edges are citation links. Label rate denotes the number of labeled nodes that are used for training divided by the total number of nodes in each dataset. NELL (Carlson et al., 2010; Yang et al., 2016) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9,891 entity nodes. Citation networks We consider three citation network datasets: Citeseer, Cora and Pubmed (Sen et al., 2008). The datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. We treat the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use 20 labels per class, but all feature vectors. NELL NELL is a dataset extracted from the knowledge graph introduced in (Carlson et al., 2010). A knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow the pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes r1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). Entity nodes are described by sparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot representation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per node. The semi-supervised task here considers the extreme case of only a single labeled example per class in the training set. We construct a binary, symmetric adjacency matrix from this graph by setting entries Aij = 1, if one or more edges are present between nodes i and j. Random graphs We simulate random graph datasets of various sizes for experiments where we measure training time per epoch. For a dataset with N nodes we create a random graph assigning 2N edges uniformly at random. We take the identity matrix IN as input feature matrix X, thereby implicitly taking a featureless approach where the model is only informed about the identity of each node, specified by a unique one-hot vector. We add dummy labels Yi = 1 for every node. 5.2 EXPERIMENTAL SET-UP Unless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate prediction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using deeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang et al. (2016) with an additional validation set of 500 labeled examples for hyperparameter optimization (dropout rate for all layers, L2 regularization factor for the first GCN layer and number of hidden units). We do not use the validation set labels for training. For the citation network datasets, we optimize hyperparameters on Cora only and use the same set of parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training iterations) using Adam (Kingma &amp; Ba, 2015) with a learning rate of 0.01 and early stopping with a window size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive epochs. We initialize weights using the initialization described in Glorot &amp; Bengio (2010) and accordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden layer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization). 5.3 BASELINES We compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk) (Perozzi et al., 2014). We omit TSVM (Joachims, 1999), as it does not scale to the large number of classes in one of our datasets. We further compare against the iterative classification algorithm (ICA) proposed in Lu &amp; Getoor (2003) in conjunction with two logistic regression classifiers, one for local node features alone and one for relational classification using local features and an aggregation operator as described in Sen et al. (2008). We first train the local classifier using all labeled training set nodes and use it to bootstrap class labels of unlabeled nodes for relational classifier training. We run iterative classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled nodes (bootstrapped using the local classifier). L2 regularization parameter and aggregation operator (count vs. prop, see Sen et al. (2008)) are chosen based on validation set performance for each dataset separately. Lastly, we compare against Planetoid (Yang et al., 2016), where we always choose their bestperforming model variant (transductive vs. inductive) as a baseline. 6 RESULTS 6.1 SEMI-SUPERVISED NODE CLASSIFICATION Results are summarized in Table 2. Reported numbers denote classification accuracy in percent. For ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other baseline methods are taken from the Planetoid paper (Yang et al., 2016). Planetoid* denotes the best model for the respective dataset out of the variants presented in their paper We further report wall-clock training time in seconds until convergence (in brackets) for our method (incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation provided by the authors3 and trained on the same hardware (with GPU) as our GCN model. We trained and tested our model on the same dataset splits as in Yang et al. (2016) and report mean accuracy of 100 runs with random weight initializations. We used the following sets of hyperparameters for Citeseer, Cora and Pubmed: 0.5 (dropout rate), 5 · 10−4 (L2 regularization) and 16 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 · 10−5 (L2 regularization) and 64 (number of hidden units). In addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard error of prediction accuracy on the test set split in percent. 6.2 EVALUATION OF PROPAGATION MODEL We compare different variants of our proposed per-layer propagation model on the citation network datasets. We follow the experimental set-up described in the previous section. Results are summarized in Table 3. The propagation model of our original GCN model is denoted by renormalization trick (in bold). In all other cases, the propagation model of both neural network layers is replaced with the model specified under propagation model. Reported numbers denote mean classification accuracy for 100 repeated runs with random weight matrix initializations. In case of multiple variables Θi per layer, we impose L2 regularization on all weight matrices of the first layer. 6.3 TRAINING TIME PER EPOCH Here, we report results for the mean training time per epoch (forward pass, cross-entropy calculation, backward pass) for 100 epochs on simulated random graphs, measured in seconds wall-clock time. See Section 5.1 for a detailed description of the random graph dataset used in these experiments. We compare results on a GPU and on a CPU-only implementation4 in TensorFlow (Abadi et al., 2015). Figure 2 summarizes the results 7 DISCUSSION 7.1 SEMI-SUPERVISED MODEL In the experiments demonstrated here, our method for semi-supervised node classification outperforms recent related methods by a significant margin. Methods based on graph-Laplacian regularization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their assumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize. Our proposed model can overcome both limitations, while still comparing favorably in terms of efficiency (measured in wall-clock time) to related methods. Propagation of feature information from neighboring nodes in every layer improves classification performance in comparison to methods like ICA (Lu &amp; Getoor, 2003), where only label information is aggregated. We have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both improved efficiency (fewer parameters and operations, such as multiplication or addition) and better predictive performance on a number of datasets compared to a na¨ıve 1 st-order model (Eq. 6) or higher-order graph convolutional models using Chebyshev polynomials (Eq. 5). 7.2 LIMITATIONS AND FUTURE WORK Here, we describe several limitations of our current model and outline how these might be overcome in future work. Memory requirement In the current setup with full-batch gradient descent, memory requirement grows linearly in the size of the dataset. We have shown that for large graphs that do not fit in GPU memory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can alleviate this issue. The procedure of generating mini-batches, however, should take into account the number of layers in the GCN model, as the Kth-order neighborhood for a GCN with K layers has to be stored in memory for an exact procedure. For very large and densely connected graph datasets, further approximations might be necessary. Directed edges and edge features Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted). Results on NELL however show that it is possible to handle both directed edges and edge features by representing the original directed graph as an undirected bipartite graph with additional nodes that represent edges in the original graph (see Section 5.1 for details). Limiting assumptions Through the approximations introduced in Section 2, we implicitly assume locality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be beneficial to introduce a trade-off parameter λ in the definition of A˜: This parameter now plays a similar role as the trade-off parameter between supervised and unsupervised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via gradient descent. 8 CONCLUSION We have introduced a novel approach for semi-supervised classification on graph-structured data. Our GCN model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. Experiments on a number of network datasets suggest that the proposed GCN model is capable of encoding both graph structure and node features in a way useful for semi-supervised classification. In this setting, our model outperforms several recently proposed methods by a significant margin, while being computationally efficient. "],["cnn-2.html", "E CNN", " E CNN The Anatomy of an Amazon 6-pager A deep dive into writing detailed planning docs from one of the most successful companies in the world Jesse Freeman Jesse Freeman Jul 17, 2020·16 min read Image by CurtisM media from Pixabay The Amazon 6-pager I worked at Amazon for five years, leaving days before receiving the coveted yellow badge. Most people don’t make it past the first year. It’s an intense work environment, and the company revolves around specific leadership principles that provide a strict framework for how you interact with your co-workers. Because of this distinct culture, Amazon is unlike any company I’ve ever seen. Perhaps the most challenging part of working at Amazon comes down to being able to write a 6-pager. While there are several types of documents you may need to write at Amazon, such as the backward press-release or the 2-pager, your ability to write a 6-pager will directly impact your status at the table and your ability to move up in the company. If you struggle to write detailed plans for your business, this may be a useful structure to emulate. My plan here is to walk you through the anatomy of an Amazon-style 6-page document in granular detail. I was surprised to see that the few articles out there on the process mostly get it wrong. Even stranger, there doesn’t appear to be a single real example of a 6-pager outside of Amazon. So, I’ve decided to create one from scratch and share it with you. I based it on a structure I used when I headed up Framework Partner Marketing for the Amazon Appstore. Each group in Amazon has its way of writing 6-pagers. While they may look or read a bit differently, they all share the same structure. Also, there are different approaches based on the goal of the document. The example I am going to share is from what we call an operational plan document. These are 6-month plans that outline the current state of the business, the historical data from the last period, the goals for this period, and how you plan on achieving them. There are two of these documents that cover the year called OP-1 and OP-2. Right about now, Amazon teams would be executing their 2020 OP-2 plan and beginning to draft the 2021 OP-1 plan. It’s rare to write a large plan like this on your own. We usually work as a team to contribute to a single OP doc. One owner is responsible for cleaning it up and making it sounds like it came from an individual author. The heads of each team will have a plan for their group, and highlights from there will roll up into a master OP document used by the entire organization. I’ve had to write several tactical plans myself in various groups at Amazon, and it’s an incredibly time-consuming process that usually involves dozens of revisions. It feels like writing a master’s thesis, and a lot of care goes into making sure it is ready before presenting to a group of superiors. How Amazon conducts meetings Amazon is well known for its lack of using PowerPoint. The way this works is that before a meeting, you print out enough copies for everyone in the room. You’re not allowed to read the document from your computer unless you are remote. Also, no one reads the document before the meeting. You are usually given 20–25 minutes at the beginning of a 60-minute meeting to read the doc from beginning to end. Most people write down questions or feedback directly on the printout since using a computer during this time is frowned upon. Image by websubs from Pixabay All print outs are handed back to you at the end of the meeting. The rest of the time consists of everyone in the room challenging your position, questioning your tactics, and digging through the data to make sure it is valid. It’s incredibly stressful, and when the meeting is over, it’s your responsibility to update and recirculate the document to everyone as a final version. There is no ideation or brainstorming during these kinds of meetings. You need to go into them with everything prepared ahead of time, which usually means you’ve had multiple people review it to ensure that you are ready. The last thing you should know, which is perhaps the most critical part of the entire process, is that your 6-pager needs to stand on its own. One of the things I admired most at Amazon was their ability to transfer knowledge between different groups. Any time I interacted with a new group, I could ask to see their OP doc and get caught up on everything I needed to know. For this process to work, it means you need to write your 6-pager in a way that allows anyone, even people not familiar with the subject, to know what is going on without additional research. I’ll get into some of the ways you can do that later. Since this may be your first time reading an Amazon-style 6-pager, the last bit of context I’ll give you is that I wrote this for a game development tool I have been building in my spare time called Pixel Vision 8. There are a few things I dived deeper into than I would in a normal 6-pager to help make things a bit clearer. Also, some sections are more generic than I’d typically write, seeing how this isn’t a real plan I can execute on for a hobby project. Still, it was a good exercise in focusing my activities over the next six months. Writing a plan like this is one of the most powerful ways you can organize your thoughts to share with others. After you finish reading the 6-pager, I’ll break apart each section and explain them in more detail. Don’t get too hung up on the actual contents if you’re not familiar with game development, marketing, or reading dense plans. I also won’t accept any feedback on this one, so there is no need to print it out and hand it back to me. So, with that out of the way, take the next 20–25 minutes to read my 2020 OP-2 plan for Pixel Vision 8. Structuring a 6-pager narrative How did that go, intense, right? I remember the first time I had to read one. It looked like a complete wall of text. I am going to assume you didn’t get through the entire thing so I’ll include some excerpts as we go. To give you perspective, that took me five days to write, and I did four revisions. Since no one else had to review it, I’d say the process was almost tolerable. I love to write, but even these are a challenge for me. While it may feel completely alien to you, I miss this detail and clarity these types of documents provide now that I no longer work at Amazon. And while 6-pagers are a chore to write, I can no longer sit through a PowerPoint presentation without questioning why it wasn’t put into a document for me to read through. The first thing you probably noticed is that my 6-pager is using 10 point font. These things are dense, and there is a strict rule around it being exactly six pages. The goal is to fill up all six pages without any filler. The other thing you may have noticed is that the document isn’t six pages, it’s much longer. The perceived length of a 6-pager is a bit of a misconception. I’ve written 6-pagers that were over 40 pages long. The reason for this is because of the appendix. The main goal of authoring this kind of document is to craft the entire thing as a narrative. That doesn’t mean it needs to be an entertaining story. It merely means there are no bullet-point lists, no graphics, and no fluff in the document’s core 6-pages. Since it’s difficult to sum up the contextual information like data, graphs, or examples in narrative form, you can add it to the end of the document in an appendix. This allows the reader to choose what to look up for additional information as needed. It also allows you to store bulky, complex data visualizations without breaking up the narrative’s flow. At this point, we are ready to break down the skeleton of the 6-pager. Again, this might change slightly based on the document’s goal, but for the most part, it works like this: Introduction — This needs to set up precisely what the material is going to cover and to inherently state the general direction of where the document plans on going. Goals — List right up front what the metrics for success are so we can use them as a lens to see the remaining document through. Tenets — This is a very Amazon thing where every action has some clearly define north star. There are a lot of ways to word these. Generally, they are inspirational pillars that the rest of the plan sits on top of (go with me on this one). State of the business — This section is another important one. You need to inform the reader of the current state of the business. There needs to be a lot of detail here, which sets up the points to compare against in the next section. Lessons learned — Amazon is big on data. This section will outline the current state of the business and its influence over creating the goals you need to achieve. It should be a detailed enough snapshot to give the reader all of the data they need to understand the positive and negatives activities in the prior period. Strategic priorities — This is the meat of the document and lays out the plan, how to execute it, and should match up to achieving the goals stated at the top of the document. Of course, each of these sections has a specific job in building the narrative of the 6-pager. To pull it all together requires a certain amount of finesse. Luckily, I’ve made it through meetings needing to make only minor changes, and I’ve been in meetings where someone’s entire document is ripped apart line by line. I don’t profess to have the experience to say mine were better than others, but I did have a few good mentors. So here is how I decided to write my sample 6-pager. The introduction I usually try to consolidate my introduction to two paragraphs. Since this 6-pager was probably the first you’ve ever read on a topic you probably have no background on, I indulged and added the 3rd paragraph to pack in some extra detail. Figure 1. The introduction section from my sample 6-pager I’m not going to spend a lot of time on this section; it should be self-explanatory. I wanted to point out the two references to the appendix right off the bat. There is a lot of data to process upfront, especially if you’re not familiar with the subject. So you will want to add as much contextual data as possible in the document itself. You’ll notice there are no links in the documents to any websites either. The expectation is that you are reading a printout. So, if you need an offline copy of the entire internet for reference, put it all in the appendix. The goals Next up are the goals. The goal section is one of the only two areas of the document where bullet points are allowed. I follow a predefined structure for writing these, and I’ve seen it consistently used across several other groups. Figure 2. The goals section from my sample 6-pager You start with the actual goal, bolded, in as few words as possible. Then you follow with a single sentence that adds context to the goal. Finally, there needs to be a historical data point and a projected data point, followed by an explicit calculation of the change between the two. Not all goals end up with a positive outcome; I made my first one a negative one to show off two different types of results. But you want to have at least three goals which appear to be the magic number considering you will probably have multiple other priorities throughout the year from other groups since these plans do not exist in a vacuum. I’ve seen all kinds of ways of writing goals at companies I’ve worked at, and this is honestly the only format that has ever made sense to me. It feels like OKRs are another popular one but I find it very difficult to adjust to them after spending years at Amazon. The tenets After the goals, you declare the tenets. I’ve never seen another company so cultish in the way it requires employees to think according to a framework of rules. On the first day of orientation, they walk you through the leadership principles, and this way of thinking helps reinforce the documents you write. Figure 3. The tenets from my sample 6-pager I rarely put much effort into these. While the leadership principles become rules for how you engage with your co-workers, these tenets add nothing to the document. The people that write excellent tenets can link them back to the leadership principles somehow. Since one of the principles I liked the most is “disagree and commit,” I kept my opinion to myself and always added tenets to my 6-pagers since the decision to use them happened long before I got there. Leaders are obligated to respectfully challenge decisions when they disagree, even when doing so is uncomfortable or exhausting. Leaders have conviction and are tenacious. They do not compromise for the sake of social cohesion. Once a decision is determined, they commit wholly. The state of the business After the tenets, things get serious fast. The document begins with the state of the business. For this example, 6-pager, I decided to make it a single page. I would hardly call my hobby game engine a “business,” but I did my best to take it seriously. Figure 5. Setting up the introduction for the state of the business section in my sample 6-pager Appendix references litter this section. In a real 6-pager, I would have cut out the first paragraph and jumped right into the numbers. This section is really about distilling all of the current activities and their intended goals. However, this is not a section to talk in the past tense. The state of the business should be a current snapshot of the data. Because of this, I usually keep all of the figures blank until right before the meeting. This way, my document is always as up to date as possible. I still look at this section through the lens of the Appstore, where I was in charge of tracking the game engines that supported Fire OS, the game submission numbers, and my budget. I had to report weekly on the business’s health, so it was natural to have these numbers ready at a moment’s notice. Had this been a real 6-pager, I would have included a full snapshot of this data, which could go on for pages, in the appendix. Amazon is big on data; those are real facts. You are building a narrative around those facts so that we are not just skimming an excel spreadsheet. The last thing I want to call out in this section was how I ended it. It’s a subtle detail, but I make sure to complete the section with a little summary of everything that has been done and try to tie it into the goals. Since my goals were core to the way I ran my business, they didn’t change much every six months. The only thing that change was the target numbers. This made it easier to keep a consistent theme on how I ran things and made sure my summary of the business reflected that. Lessons learned After summing up the state of the business, it’s time to reflect on what happened in the past. This is a snapshot from the last period until now and has to be even more factual with numbers, percentages of goal completion, and even additional references in the appendix. Figure 6. Setting up the introduction for the lessons learned section in my sample 6-pager This section is similar to the state of the business but with one exception; the tense here is always in the past. There shouldn’t be any forward projections or expectations. There is zero room for interpretation. The data will tell the real story. Part of working at Amazon is that you are encouraged to share your failures along with your successes. I’ve worked in many places where people try to hide what didn’t work. You can usually tell because they never have data to back up their claims. At Amazon, the culture understands that you will fail; in fact, there is a leadership principle called “deliver results” to address that reality. Leaders focus on the key inputs for their business and deliver them with the right quality and in a timely fashion. Despite setbacks, they rise to the occasion and never settle. Notice the “despite setbacks” part? That is very important because real leaders fail from time to time but learn from their mistakes. Those failures help inform the new set of goals and give you clarity as to why it failed, so you don’t repeat your mistake. That’s outlined in the “ownership” and “are right, a lot” leadership principles too but you can look those up on your own. Strategic priorities At this point, you should have a real clear picture of what is going on, what has happened leading up to this, and what we hope to achieve moving forward. It’s now time for the most detailed section of all, the strategic priorities. Figure 7. The beginning of my strategic priorities from my sample 6-pager There is a lot to unpack here, mostly because this is usually 50–70% of the 6-pager. This section outlines each activity you plan on doing and how it relates to one of the goals. I don’t believe the order makes a difference, you can structure the narrative how you like, but people in the meeting will calculate up each activity to make sure it matches up to the goal’s projected data point. This is also the section that is open for interpretation and will be the one you spend the majority of the meeting defending. Up until this point, everything you have written has been facts, except for the tenets. The only questions you will get on those sections are whether the data adds up or they perceive the goals as not challenging enough. The strategic priorities section is all speculation. And at Amazon, they don’t accept anecdotes for an answer. That is why to write this section, you need to pull numbers from the lessons learned and state of the business that help prove your proposed activities will work. I’ll admit, I didn’t fully back my assumptions up with data from the previous sections. Honestly, since I don’t have to report this to anyone, I don’t analyze my game engine data all that much. I just do it for fun. But what I did want you to learn from this section was how to structure these priorities. You’ll see they follow a particular pattern. A strategic priority consists of two parts. The first part is a prediction. Think of this as your thesis on how you will contribute back to a given goal. The second part, however, is where you attempt laying out the actual plan to achieve it. These can get very detailed, and a lot of the plans I’ve put together use these to summarize the approach and the appendix for tactical execution steps. I’ve placed entire content plans in the appendix since they didn’t contribute to the narrative, but people will want to see the details. The beauty of the appendix, and probably the most brilliant part of the Amazon 6-pager, is that it gives the reader the choice of how deep they want to go down the rabbit hole. What you read in the 6-pager narrative gives you a high-level overview. If you chose to follow an appendix, that reference goes right into the details. And since the meeting comprises of team leads from other groups, each one will have specific areas of interest to dig into and skim over parts that don’t pertain to them. Since everyone at the table needs to understand the entire scope, they chose to follow an appendix reference for more information as needed. Either way, they still walk away with an overall sense of that part of the plan. It’s incredibly brilliant, and probably the one structural detail most plans I read at other companies don’t do very well. Ending a 6-pager Congratulations on making it this far. Luckily, we are almost at the end. Let’s take a look at the last strategic priority in my 6-pager. Figure 8. Excerpt of the last strategic priority from my sample 6-pager There are two things you should notice. First, I wrote right up to the bottom of page 6 to stick the landing. Second, you’ll see there is no summary section or closing remarks. That would be fluff. I’ve never read a 6-pager that summed everything up. They may exist, but out of the hundreds I’ve read, I never felt like a summary was missing. I got a clear picture of the past, present, and future activities and what better way to end then on an actionable item. I always make sure that the last sentence ends on a positive, forward-facing note. One that doesn’t sum everything up but at least feels like a natural ending to the document. Also, I didn’t add many appendix items to this example strategic priority section. That was on purpose; it would be more work than this example warranted. Regardless, you should now have a clear sense of how this section is supposed to be structured. Some final advice I think the last few bits of advice I’ll leave you with are how important the leadership principles come into play when reviewing a document and receiving feedback without taking it personally. The leadership principles give you a sort of rules of engagement on how to be direct with a co-worker and walk away civil. While I don’t fully drink the kool-aid, it’s impossible to survive at Amazon without weaponizing these principles in your meetings. Sometimes it’s to attack and others times to defend, but most of all, you want to always “dive deep.” Leaders operate at all levels, stay connected to the details, audit frequently, and are skeptical when metrics and anecdote differ. No task is beneath them. "],["section-8.html", "F 01", " F 01 여기서 thm 5.1. 과 복합하는 것으로 이하를 얻을 수 있음. $$ \\[\\begin{align} \\lambda_{max} (\\hat \\Sigma) &amp;= \\max_{v \\in \\mathbb^{d-1}} v&#39;(\\Sigma + E)v \\\\ &amp;\\le \\lambda_{max}(\\Sigma) + ||E||_{op} \\end{align}\\] $$ \\(\\lambda_{max} (\\Sigma) &amp;\\le \\lambda_{max}(\\hat \\Sigma) + ||E||_{op}\\) v "],["section-9.html", "G 02", " G 02 Network (adjacency Matrix) -&gt; Network Statistics \\(S(y)\\), which capture characteristics of network -&gt; plug-in network statistics with their corresponding prameteres into the exponential term \\(\\underbrace{\\exp \\Big (\\sum_{i=1}^p \\theta_i S_i(y) \\Big )}_{\\text{unnormalized density}}\\) -&gt; \\(P_\\theta (Y=y) = \\frac{1}{\\kappa(\\theta)} \\exp \\Big( \\sum^p_{i=1}\\theta_i S_i(y) \\Big)\\), to make a valid prob. \\(\\kappa\\): normalizing constant \\(\\theta_i\\): parameters measuring the strength of effects of \\(S_i(y)\\) \\(S_i(y)\\): network statistics -&gt; but extremely difficult to extimate. (1) doubly-interatable normalizing constant -&gt; also, called as a Markov Network, 여기서 Markov 라는 단어는 status of edge depends on a status of other edge 하는 성질을 가리킴 \\[ \\theta_1 (\\underbrace{\\text{# of edge}}_{\\substack {\\text{homophily effect} \\\\ \\text{dyadic *in*dependent stat.}}}) + \\theta_2 (\\underbrace{\\text{# of triangle}}_{\\substack {\\text{transitivity effect} \\\\ \\text{dyadic dependent stat.}}}) \\] dyadic independent statistics: a status of dyad does not depends on the status of other dyads. \\(S(y) = \\sum_{i&lt;j} y_{ij}h(x_i , x_j)\\) \\(x\\): covariate information of node \\(x_i , x_j\\) how we define \\(h(x_i , x_j)\\) determines the network stats? \\[ \\begin{align} h(x_i , x_j) &amp; = \\cases{1 &amp; \\( x_i = x_j\\) \\\\ 0 &amp; o.w.} \\tag{Uniform homophily effects} \\\\ h(x_i , x_j, d) &amp; = \\cases{1 &amp; \\( x_i = x_j = d\\) \\\\ 0 &amp; o.w.} \\tag{Differential homophily effects} h(x_i , x_j, d) &amp; = \\cases{2 &amp; \\( x_i = x_j = d\\) \\\\ 1 &amp; \\( x_i = d or x_j = d and x_i \\not = x_j \\) (only one) \\\\ 0 &amp; o.w.} \\tag{Nodal Factor Effect} \\\\ h(x_i , x_j) &amp; = \\frac12 (x_i + x_j) \\tag{Main Effect (Continuous Case)} \\\\ h(x_i , x_j , C ) &amp; = \\cases{1 &amp; |x_i - x_j | = C \\\\ 0 &amp; o.w.} \\tag{Absolute Difference Effect} \\\\ h(x_i ,x_j) &amp; = |x_i - x_j| \\end{align} \\] 상기의 1번과 2번은 \\(x\\) 가 discrete 값일 때만 사용 가능. 이인즉 \\(x\\) 가 factor 인 경우에만. cause (2): Model-degeneracy we also need to consider high-order interactions. degree distribution: \\(D_k(y)\\), which means # of node with degree \\(k\\) \\(\\sum^{n-1}_{k=0} D_k(y) = n\\): High-order interaction based on (nodes / edges / dyads) -&gt; shared partnership distribution edgewise shared partnership distribution: \\(EP_k(y)\\), the number of unordered pairs \\(9i,j)\\) for which i and j share k common neighbors and y_{ij}=1 \\(\\sum^{n-2}_k=0 EP_k (y) = S_1(y_1)\\), \\(S_1(y_1)\\) is high-order transitivites. \\(\\sum^{n-2}_k=0 DP_k (y) = \\choose n2\\), \\(DP_k (y)\\) is high-order relationships of two-stars \\(DP_k (y) - EP_k(y) = NP_k(y)\\) dyadwise shared partnership distribution non-edgewise shared partnership distribution \\(D_k(y), EP_k(y), DP_k(y)\\) are distribution -&gt; not plausible to plug-in -&gt; geometrically weighted statistics -&gt; summarise dist. into one statistics by specifying decayed rate \\(\\tau\\), we can construct GW statistics. homogeneous ERGM \\[ P(Y=y) = \\frac{1}{\\kappa(\\theta)} \\exp \\Big(\\theta_1 S_1 (X) + \\theta_2 \\underbrace{T(X)}_{# of triangle} \\Big) \\] \\(\\theta_1\\): behaves similar to density \\(\\theta_2&gt;0\\): condition on edge, how likely it forms a triangle \\(\\theta_2&lt;0\\): ???????????????????????????? inhomogeneous ERGM : homogenous’s all \\(\\theta_{1ij} = \\theta_1\\), $_{2ijk} = \\(\\theta_2\\) \\[ P(Y=y) = \\frac{1}{\\kappa(\\theta)} \\exp \\Big(\\sum_{i&lt;j}\\theta_{1ij} y_ij} + \\sum_{i \\not = j \\no = k}\\theta_{2jk} y_{ij}y_{ik}y_{jk} \\Big) \\] only applicable in multiplex network, which is all respondents are same but they have different networks. G.0.0.1 Nomralizing constant of ERGM \\[ k(\\theta) = \\sum_{\\text{all possible} y} \\exp \\Big( \\sum^p_{i=1} \\theta_i S_i (y) \\Big) \\] 각 dyad 는 0 과 1 의 2가지 상태를 가짐. total possible \\(y\\) 는 \\(2^{\\choose n2}\\). 연산량 미침. MCMC -&gt; frequentist -&gt; Bayesian Bayesian Inference: we assume parameters follow a distribution. prior \\(\\pi(\\theta)\\) -&gt; prior belief of parameters likelihood \\(f(y | \\theta)\\) posterior \\(\\pi(\\theta | y)\\) -&gt; posterior belief of parameters after including data Frequentist: parameters are fixed \\[ \\begin{align} &amp; \\propto f(y|\\theta) \\pi(\\theta) \\\\ pi(\\theta|y) &amp;= \\frac{f(y|\\theta) \\pi(\\theta)}{\\underbrace{\\pi(y) = \\int f(y|\\theta)\\pi(\\theta)d \\theta}_{\\text{marginal density of }y\\text{ (normalizing constant)}}} \\end{align} \\] Generally, normalizing constant (denominator) does not depend on \\(\\theta\\) calculating normalizing contants involves integration, which is generally difficult G.0.0.2 Latent Space Model we assume that there exists a latent space and we embedded each node into this latent space based on a connection (edge). \\(z_i\\): latent position of node \\(i\\) Then, we try to explain a network based on a logistic regression framework. Main assumption: given the latent position, each connection is conditionally independent each other. latent positions can capture all dependence structures the prob of having an edge is a function of latent positions. main function what researchers use is a distance $ \\[\\begin{bmatrix} \\circ \\end{bmatrix}\\] $$ ANOVA -&gt; Additive Effect Model for network analysis \\(y_{ij}\\): weighted edges from node \\(i\\) to \\(j\\). \\(y_{ij} = \\mu + a_i + b_j + \\epsilon_{ij}\\) G.0.0.3 Social Relation Model -&gt; Goal: Capture sender and receiver correlations (dyadic corrlations) Assign Variance structures to \\(a_i\\) and \\(b_j\\): \\[ Var \\left[ \\begin{pmatrix} a_i \\\\ b_j \\end{pmatrix}\\right] \\equiv \\Sigma = \\begin{pmatrix} \\sigma_a^2 &amp; \\sigma_{ab} \\\\ \\sigma_{ab} &amp; \\sigma^2_b \\end{pmatrix} \\] \\(\\sigma_{ab}\\): Covariance term for both \\(i \\rightarrow j\\) and \\(j \\rightarrow i\\), there should be a correlation b/w sender and receiver. In a social relation model, we caputre this correlation. \\[ Var \\left[ \\begin{pmatrix} \\epsilon_{ij} \\\\ \\epsilon_{ji}\\end{pmatrix}\\right] = \\sigma^2 \\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix} \\] G.0.0.4 Social Relation Covariance Model You decompese the variance of \\(y_{ij}\\) into three parts: \\[ Var(y_{ij}) = \\underbrace{\\sigma^2_a }_{\\text{variance of sender}} + \\underbrace{\\sigma^2_b }_{\\text{variance of receiver}} + \\underbrace{\\sigma^2 }_{\\text{common variance}} \\] \\(Cov(y_{ij}, y_{ik}) = \\sigma_a^2\\): within-row covariance \\(Cov(y_{ij}, y_{kj}) = \\sigma_b^2\\) : within-col covariance \\(Cov(\\underbrace{y_{ij}, y_{jk}}_{i\\rightarrow j \\rightarrow k}) = \\sigma_{ab}\\): row-col covariance \\(Cov(\\underbrace{y_{ij}, y_{ji}}_{ i\\rightarrow j \\\\j \\rightarrow i}) = 2 \\sigma_{ab} + \\rho \\sigma^2\\): row-col covariance + reciprocity G.0.0.5 Social Relation Regression Model \\[ y_{ij} = \\beta &#39; x_{ij} + \\mu + a_i + b_j + \\epsilon_{ij} \\tag{SRRM} \\] Goal: measure the covariate effects on an adjacency Matrix as well as sender and receiver effects Limitation of series of social relation model -&gt; we cannot capture high-order dependence within networks Goal: measure high-order dependence in a network as well as sender and receivers effect G.0.0.6 Additive and Multiplicative Model (AME) \\[ \\begin{align} &amp;y_{ij} = \\beta&#39;x_{ij} + u_i &#39; v_j + a_i + b_j + \\epsilon_{ij} \\tag{AME} &amp;&amp; \\begin{pmatrix} \\epsilon_{ij} \\\\ \\epsilon_{ji} \\end{pmatrix} \\sim N \\left(0, \\; \\;\\sigma^2\\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix} \\right) \\end{align} \\] we decompose \\(\\epsilon_{ij} = \\underbrace{u_i &#39; v_j}_{\\substack{\\text{high-order}\\\\ \\text{dependence}}} + \\epsilon_{ij}\\) \\(u_i\\): latent vector that measure a high-order dependence for node \\(i\\) in aspect of sender effect \\(v_j\\): latent vector that measure a high-order dependence for node \\(j\\) in aspect of receiver effect AME: $y_{ij} = If we fit LSM seperatively, we cannot find the trajectory of each individuals. -&gt; wee need a model taht combines each latent spaces. G.0.0.7 Hidden Markov Model \\(Y_t = \\{Y_{ijt}\\}\\): adjacency Matrix of observed network at time \\(t\\) \\(z_{it}\\): \\(p\\)-dimensional vectors of \\(i\\)-th actor/s latent position at time \\(t\\) \\(z_t\\): \\(n \\times p\\) Matrix whose its row is \\(z_{it}\\). When we assign the prior distribution of \\(z_t\\), we use the hidden Markov Model. \\[ \\pi(z_1 \\Big | \\Psi_1) = \\prod_{i=1}^n N(X_{i1} \\Big | 0, \\tau^2 I_P)\\\\ \\pi(z_t \\Big | z_{t-1}, \\Psi_t) = \\prod_{i=1}^n N(X_{it} \\Big | X_{i(t-1)}, \\sigma^2 I_P) \\] G.0.0.8 Definition of Hidden Markov Model two process \\(Y\\) (observable) \\(Z\\) (unobsevable, assumed to be a Markov Process) \\(Y\\) depends on \\(Z\\). G.0.0.9 Construct the likelihood (directed Network) $$ P(Y_t | X_t , ) = {i = j} ^{y{ijt}} ^{1-y_{ijt}} $$ G.0.0.10 Logistic Regression Framework \\[ \\pi_{ijt} = \\beta_{IN} \\Bigg( 1-\\frac{d_{ijt}}{r_j}\\Bigg) + \\beta_{OUT} \\Bigg( 1-\\frac{d_{ijt}}{r_j}\\Bigg) \\] \\(\\beta_{IN}\\): how likely it tends to receive the connection (popularity) \\(\\beta_{OUT}\\): how likely it tends to senc the connection (social activity) \\(d_{ijt} = \\| z_{it} - z_{jt}\\|\\): euclidian distance \\(r_i\\): positive actor specific parameters, that represent each actor’s social reach (tendency to form and receive edges) constraint \\(\\sum_{i=1}^n r_i = !\\) geometric interpretation \\(r_i\\) forming a radius arount the \\(i\\)-th actor. If the distance b/w two actors are within each other’s radii, \\(d_i &lt; \\min(r_i , r_j)\\), then the prob of forming an edge is greater than \\(\\frac12\\). If the distance b/w two actors are outside each other’s radii, \\(d_i &lt; \\max(r_i , r_j)\\), then the prob of forming an edge is less than \\(\\frac12\\). If \\(\\d_{ijt}= r_i = r_j\\), then the prob of forming an edge is equal to \\(\\frac12\\). If \\(\\beta_{IN} &gt; \\beta_{OUT}\\), the prob of an edge from \\(i \\rightarrow j\\) is determined more by the radius of \\(j\\) than by the radius of \\(i\\), which means popularity &gt; social activity. If \\(\\beta_{IN} &lt; \\beta_{OUT}\\), popularity &lt; social activity. ※ Prior Distribution \\[ \\beta_{IN} \\sim N \\\\ \\beta_{OUT} \\sim N \\\\ \\sigma^2 \\sim Inv-Gamma \\\\ \\tau^2 \\sim Inv-Gamma \\\\ (r_1, \\cdots, r_n ) \\sim Dirichlet (\\alpha_1 , \\cdots, \\alpha_n) \\] ※ MCMC iteration For \\(t=1:T\\) and \\(i=1:n\\), draw \\(z_{it}\\) using MH conditional posterior for \\(z_{it}\\) is $$ (z_{it} | Y_{i:T}, ) \\[\\begin{cases} \\displaystyle \\prod_{i \\not = j} \\frac{\\exp(\\pi_{ijt} \\cdot y_{ijt})}{1+\\exp(\\pi_{ijt})} &amp; \\cdot N(z_{it} \\Big | 0, \\tau^2 I_p ) &amp; \\cdot N(z_{i2} \\Big | z_{i1}, \\sigma^2 I_p) &amp; &amp;&amp; t=1 \\\\ \\displaystyle \\prod_{i \\not = j} \\frac{\\exp(\\pi_{ijt} \\cdot y_{ijt})}{1+\\exp(\\pi_{ijt})} &amp; \\cdot N(z_{i(t+1)} \\Big | z_{it}, \\sigma^2 I_p ) &amp; \\cdot N(z_{it} \\Big | z_{i(t-1)}, \\sigma^2 I_p) &amp; &amp;&amp; 2&lt;t&lt;T \\\\ \\displaystyle &amp; \\prod_{i \\not = j} \\frac{\\exp(\\pi_{ijt} \\cdot y_{ijt})}{1+\\exp(\\pi_{ijt})} &amp; \\cdot N(z_{it} \\Big | z_{i(t-1)}, \\sigma^2 I_p) &amp; &amp;&amp; \\end{cases}\\] $$ draw \\(\\tau^2\\) from its full conditional IG. draw \\(\\sigma^2\\) from its full conditional IG. draw \\(\\beta_{IN}\\) and \\(\\beta_{OUT}\\) using MH algorithm. draw \\(r_{1:N}\\) via MH using a Dirichlet proposal. let \\(\\mathcal D\\) denote the sampling simple approach to find the \\(z_{t+1}\\), use \\(\\hat z_{t+1} = E(z_{t+1} \\Big | Y_{1:T}) \\approx \\frac1L \\sum^L_{l=1}z_t^{(l)}\\), where \\(l\\) indicates the \\(l\\)-th draw from the posterior. By plugging-in \\(\\hat z_{t+1}\\) along with the posterior means of the parameter, into the \\(\\pi_{ijt}\\), we can generate \\(\\hat Y_{t+1}\\). \\[ \\begin{align} &amp;z_{it} = z_{i(t-1)} + \\epsilon_{it} &amp;&amp;\\epsilon_{it} \\sim N(\\mu_t , \\sigma^2 I_p) \\end{align} \\] \\(\\theta_t\\) equals the angle \\(\\arctan \\Big \\{2(X_{jt} - X_{i(t-1)} ) \\Big \\}\\), which is two argument of arc tangent, the common variation of the arctangent function, which preserve the angle quadrant. $$ _t = R_t \\[\\begin{pmatrix} \\mu \\\\ 0 \\end{pmatrix}\\] = \\[\\begin{pmatrix} \\cos(\\theta_t) &amp; -\\sin(\\theta_t) \\\\ \\sin(\\theta_t) &amp; \\cos(\\theta_t) \\end{pmatrix} \\begin{pmatrix} \\mu \\\\ 0 \\end{pmatrix}\\] $$ where \\(\\mu_t\\) is edge attraction at time \\(t\\), and \\(\\mu = \\Bigg \\|E(X_{it} - X_{i(t-1)}) \\Bigg \\|\\). \\[ \\pi(\\mu) = \\begin{cases} p_0 &amp; &amp; \\mu = 0 \\\\ (1-p_0) f(\\mu) &amp; &amp; \\mu&gt;0\\end{cases} \\] We assume \\(f(\\mu) \\sim \\exp (\\lambda)\\). then the posterior density of \\(\\mu\\) \\[ \\pi (\\mu \\Big | Y_{1:T}) = \\frac{\\pi (Y_{1:T} \\Big | \\mu) \\pi (\\mu)}{\\pi (Y_{1:T})} = \\frac{\\pi (Y_{1:T} \\Big | \\mu) }{\\pi (Y_{1:T})} p_0 \\cdot I\\Big(\\mu=0 \\Big ) +\\frac{\\pi (Y_{1:T} \\Big | \\mu) }{\\pi (Y_{1:T})} (1-p_0) f(\\mu) \\cdot I\\Big(\\mu&gt;0 \\Big ) \\] let \\[ \\pi_0 (\\mu_0 = 0 \\Big | Y_{1:T}) = \\frac{f(Y_{1:T} | \\mu)}{\\pi(Y_{1:T})} p_0 \\\\ \\pi_t (\\mu \\Big | Y_{1:T}) = \\frac{f(Y_{1:T} | \\mu)}{\\pi(Y_{1:T})} (1-p_0) f(\\mu) \\] $$ 0 (0 = 0 | Y{1:T}) + 0^+ (| Y{1:p}) d= 1 \\ 0 (= 0 | Y{1:p}) = , K(U) = $$ "],["section-10.html", "G.1 10.", " G.1 10. G.1.1 Stochastic Block Model Latent Class Model for each actor (node), we assign group memberships two components in the block model, the vector of grop membership : \\(z\\) conditional on the group membership, the block matrix represents the edge probability of two nodes $$ \\[\\begin{align} &amp; \\begin{bmatrix} 0.8 &amp; 0.05 &amp; 0.02 \\\\ 0.05 &amp; 0.9 &amp; 0.03 \\\\ 0.02 &amp; 0.03 &amp; 0.7 \\end{bmatrix} &amp;&amp; \\begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} \\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{32} \\end{bmatrix} \\end{align}\\] $$ \\(b_{11}\\) 에 부여된 확률 0.8: the connection prob within group 1 \\(b_{22}\\) 에 부여된 확률 0.9: the connection prob within group 1 \\(b_{11}\\) 에 부여된 확률 0.05: the connection prob b/w group 1 and group 2 ※ notations: \\(Y\\): adjacency Matrix \\(K\\): the total membership groups (\\(&lt;n\\)) \\(Z_i\\): \\(k\\)-vector, all elements of which are \\(0\\), except exactly one that takes the value \\(1\\) and represents the group node \\(i\\) belongs to. \\(Z := (z_1 , \\cdots, z_n)&#39;\\) \\(C\\): \\(k \\times k\\) block matrix entry \\(C_{ij} \\in [0,1]\\): prob of occurence of a edge from a node in \\(g\\) the idea of block Matrix \\(C\\) means the edges are are conditionally independent, given the block membership. \\(\\Rightarrow Y_{ij} \\sim\\) Bernoulli distribution with success probability \\(z_i &#39; C z_j\\), and independent of \\(Y_{kl}\\) for \\((i,j) \\no = (k,l)\\), given \\(z_i\\) and \\(z_j\\). G.1.2 Likelihood function $$ P (Y | Z, C) = {i&lt;j} P (y{ij} | Z, C) = _{i&lt;j} $$ In real data, \\(z\\) and \\(C\\) are unknown. \\(\\Rightarrow\\) we need additional assumptions. \\(\\underbrace{1}_{2}\\) 1. \\(z_i\\) is independent of \\(z_j\\). 2. \\(P(z_{i \\underbrace{k}_{\\text group}} =1) = \\theta_k\\), \\(\\sum_{j=1}^k \\theta_k = 1\\) \\(\\Rightarrow\\) the latent group \\(z_p\\) follows multinomial distribution with \\(\\theta\\), i.e., \\[ \\pi(z \\Big | \\theta) = \\prod_{i=1}^n z_i &#39; \\theta = \\prod_{k=1}^K \\theta_k^{n_k} \\tag{2} \\] , where \\(n_k\\) is the total number of nodes that belongs to group \\(K\\). In a Bayesian inference, we can assign \\(Dirichlet\\) prior \\(\\alpha, \\cdots, \\alpha\\) \\(\\pi(\\alpha) \\sim Gamma (a,b)\\). G.1.2.1 Inference By combining equation (1) and (2), we can make an inference by the frequentist way using EM algorithm. G.1.2.2 Bayesian we need to assign prior for \\(C \\Sim BETA\\), \\(C_{ij} \\sim BETA(A_{ij}, B_{ij})\\), where A, B are \\(k \\times k\\) Matrix. G.1.2.3 Posterior Distribution $$ \\[\\begin{alignat}{2} \\pi(Z , \\theta, C , \\alpha \\Big | Y) &amp; \\propto f(Y \\Big | Z , \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(Z , \\theta, C , \\alpha) \\\\ &amp; = f(Y \\Big | Z , \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(Z \\Big | \\theta, C , \\alpha) &amp;&amp; \\cdot \\pi(\\theta \\Big | C , \\alpha) &amp;&amp; \\cdot \\pi(C, \\alpha) \\\\ &amp; = f(Y \\Big | Z , C) &amp;&amp; \\cdot \\pi(Z \\Big | \\theta) &amp;&amp; \\cdot \\pi(\\theta \\Big | \\alpha) &amp;&amp; \\cdot \\pi(C) \\cdot \\pi(\\alpha) \\\\ &amp; \\propto \\prod_{i&lt;j} \\Big[ (z_i&#39; C z_j)^{y_{ij}}(1- z_i&#39; C z_j)^{1-y_{ij}} \\Big] &amp;&amp; \\cdot \\prod_{k=1}^K \\theta_k^{n_k} \\Bigg[\\Gamma(k \\alpha) \\Big \\{ \\sum^k_{i=} \\theta_k = 1 \\prod_{i=1}^k \\frac{\\theta_z^{\\alpha-1}}{\\Gamma(\\alpha)}\\Big \\} \\Bigg] &amp;&amp; \\cdot\\prod^k_{i&lt;j} \\Big[ C_{ij}^{A_{ij}-1} (1-C_{ij})^{B_{ij}-1} \\Big] &amp;&amp; \\cdot \\alpha^{a-1} e^{-b \\alpha} \\end{alignat}\\] $$ \\(\\Rightarrow\\) Computation is veery straightforward via MCMC. G.1.3 Mixed Membership Block Model (MMBM) SBM each actor only has one membership to alleviate the assumption of SBM, MMBM has been proposed This model allows each node can have multiple membership instead of assigning \\(1\\) to a ceratin membership, we estimate the probability of all membership. \\[ \\begin{align} z_i &amp;= (1, 0, 0) \\tag{SBM} \\\\ \\theta_i &amp;= (0.7, 0.2, 0.1) \\tag{MMSB} \\end{align} \\] 이때 MMSB 의 vector 의 각 항은 각각 group 1, group 2, group 3 일 prob. \\(\\theta_i\\): weights or probabilities in the grous that have to be non-negative and sum to 1. For each dyad \\(y_{ij}\\), a latent membership \\(z_{ij}\\) is drawn from the multinomial distribution with \\(\\theta_p\\). \\(\\pi(z \\Big | \\Theta) = \\prod_{i \\not = j} (z_{ij}&#39;\\theta_i x z_{ji}&#39;\\theta_j\\), where \\(\\Theta := (\\theta_1 , \\cdots, \\theta_k)\\) is \\(n \\times k\\) Matrix of membership probability. The likeliood is \\(f(Y \\Big | Z, C) = \\prod_{i&lt;j} \\Big[ (z_{ij}&#39; C z_{ji})^{y_{ij}}(1- z_{ij}&#39; C z_{ji})^{1-y_{ij}} \\Big]\\). The goal of inference estimating not \\(z\\) but the mixed memberships \\(\\Theta\\). G.1.3.1 Degree-corrected SBM use Poisson Model. \\(y_{ij}\\): the number of edges from dyad (i,j) following a Poisson dist \\(C_{ij}\\): the expected number of edges from a node in group \\(i\\) to a node in group \\(j\\) The likelihood is \\[ \\pi(Y_{ij} \\Big | Z, C) = (Y_{ij}!)^{-1} \\exp \\Big \\{ (-z_i &#39; C z_j ) (z_i &#39; C z_j )^{y_{ij}} \\Big \\} \\tag{3} \\] In a large sparse graph, where the edge probability equals the expected number of edges, DC-SBM is asymptotically equivalient to the Bernoulli counterpart in equation (3). \\(\\phi_p\\): the ratio of node \\(i\\)’s degree to the sum of degrees in node \\(i\\)’s group. Under constraint \\(\\sum_{i=1}^n \\phi_i \\cdot I \\Big ( z_{ik} = 1 \\Big) =1\\), then equation (3) becomes \\[ f(y_{ij} \\Big | Z, C, \\phi) = (y_{ij}!)^{-1} \\exp \\Big \\{ (- \\phi_i \\phi_j z_i &#39; C z_j ) (\\phi_i \\phi_j z_i &#39; C z_j )^{y_{ij}} \\Big \\} \\] which is DC-SBM. \\(\\Rightarrow\\) SBM ignores the variation of node degrees in a real network. With DC-SBM, we can make correction better. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
