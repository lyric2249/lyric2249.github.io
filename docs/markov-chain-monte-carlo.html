<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Markov Chain Monte Carlo | Self-Study</title>
  <meta name="description" content="4.2 Markov Chain Monte Carlo | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Markov Chain Monte Carlo | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Markov Chain Monte Carlo | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="importance-sampling.html"/>
<link rel="next" href="advanced-mcmc-wk08.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li><a href="index.html#intro" id="toc-intro">Intro<span></span></a></li>
<li><a href="#part-20-02" id="toc-part-20-02">(PART) 20-02<span></span></a></li>
<li><a href="categorical.html#categorical" id="toc-categorical"><span class="toc-section-number">1</span> Categorical<span></span></a>
<ul>
<li><a href="overview.html#overview" id="toc-overview"><span class="toc-section-number">1.1</span> Overview<span></span></a>
<ul>
<li><a href="overview.html#data-type-and-statistical-analysis" id="toc-data-type-and-statistical-analysis"><span class="toc-section-number">1.1.1</span> Data Type and Statistical Analysis<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="bayesian.html#bayesian" id="toc-bayesian"><span class="toc-section-number">2</span> Bayesian<span></span></a>
<ul>
<li><a href="abstract.html#abstract" id="toc-abstract"><span class="toc-section-number">2.1</span> Abstract<span></span></a>
<ul>
<li><a href="abstract.html#변수의-독립성" id="toc-변수의-독립성"><span class="toc-section-number">2.1.1</span> 변수의 독립성<span></span></a></li>
<li><a href="abstract.html#교환가능성" id="toc-교환가능성"><span class="toc-section-number">2.1.2</span> 교환가능성<span></span></a></li>
</ul></li>
<li><a href="continual-aeassessment-method.html#continual-aeassessment-method" id="toc-continual-aeassessment-method"><span class="toc-section-number">2.2</span> Continual Aeassessment Method<span></span></a></li>
<li><a href="horseshoe-prior.html#horseshoe-prior" id="toc-horseshoe-prior"><span class="toc-section-number">2.3</span> Horseshoe Prior<span></span></a></li>
</ul></li>
<li><a href="#part-21-01" id="toc-part-21-01">(PART) 21-01<span></span></a></li>
<li><a href="mathematical-stats.html#mathematical-stats" id="toc-mathematical-stats"><span class="toc-section-number">3</span> Mathematical Stats<span></span></a>
<ul>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">3.1</span> Inference<span></span></a>
<ul>
<li><a href="inference.html#rao-blackwell-thm." id="toc-rao-blackwell-thm."><span class="toc-section-number">3.1.1</span> Rao-Blackwell thm.<span></span></a></li>
<li><a href="inference.html#completeness" id="toc-completeness"><span class="toc-section-number">3.1.2</span> Completeness<span></span></a></li>
<li><a href="inference.html#레만-쉐페-thm." id="toc-레만-쉐페-thm."><span class="toc-section-number">3.1.3</span> 레만-쉐페 thm.<span></span></a></li>
<li><a href="inference.html#raoblack" id="toc-raoblack"><span class="toc-section-number">3.1.4</span> Rao-Blackwell thm.<span></span></a></li>
</ul></li>
<li><a href="hypothesis-test.html#hypothesis-test" id="toc-hypothesis-test"><span class="toc-section-number">3.2</span> Hypothesis Test<span></span></a></li>
<li><a href="power-fucntion.html#power-fucntion" id="toc-power-fucntion"><span class="toc-section-number">3.3</span> Power Fucntion<span></span></a>
<ul>
<li><a href="power-fucntion.html#significance-probability-p-value" id="toc-significance-probability-p-value"><span class="toc-section-number">3.3.1</span> Significance Probability (p-value)<span></span></a></li>
</ul></li>
<li><a href="optimal-testing-method.html#optimal-testing-method" id="toc-optimal-testing-method"><span class="toc-section-number">3.4</span> Optimal Testing Method<span></span></a></li>
<li><a href="data-reduction.html#data-reduction" id="toc-data-reduction"><span class="toc-section-number">3.5</span> Data Reduction<span></span></a>
<ul>
<li><a href="data-reduction.html#sufficiency-principle" id="toc-sufficiency-principle"><span class="toc-section-number">3.5.1</span> Sufficiency Principle<span></span></a></li>
</ul></li>
<li><a href="borel-paradox.html#borel-paradox" id="toc-borel-paradox"><span class="toc-section-number">3.6</span> Borel Paradox<span></span></a></li>
<li><a href="neymanpearson-lemma.html#neymanpearson-lemma" id="toc-neymanpearson-lemma"><span class="toc-section-number">3.7</span> Neyman–Pearson lemma<span></span></a>
<ul>
<li><a href="neymanpearson-lemma.html#overview-1" id="toc-overview-1"><span class="toc-section-number">3.7.1</span> Overview<span></span></a></li>
<li><a href="neymanpearson-lemma.html#generalized-lrt" id="toc-generalized-lrt"><span class="toc-section-number">3.7.2</span> Generalized LRT<span></span></a></li>
</ul></li>
<li><a href="개념.html#개념" id="toc-개념"><span class="toc-section-number">3.8</span> 개념<span></span></a></li>
</ul></li>
<li><a href="mcmc.html#mcmc" id="toc-mcmc"><span class="toc-section-number">4</span> MCMC<span></span></a>
<ul>
<li><a href="importance-sampling.html#importance-sampling" id="toc-importance-sampling"><span class="toc-section-number">4.1</span> Importance Sampling<span></span></a>
<ul>
<li><a href="importance-sampling.html#independent-monte-carlo" id="toc-independent-monte-carlo"><span class="toc-section-number">4.1.1</span> Independent Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo"><span class="toc-section-number">4.2</span> Markov Chain Monte Carlo<span></span></a>
<ul>
<li><a href="markov-chain-monte-carlo.html#mh-algorithm" id="toc-mh-algorithm"><span class="toc-section-number">4.2.1</span> MH Algorithm<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used" id="toc-random-walk-chains-most-widely-used"><span class="toc-section-number">4.2.2</span> Random Walk Chains (Most Widely Used)<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler" id="toc-basic-gibbs-sampler"><span class="toc-section-number">4.2.3</span> Basic Gibbs Sampler<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#implementation" id="toc-implementation"><span class="toc-section-number">4.2.4</span> Implementation<span></span></a></li>
</ul></li>
<li><a href="advanced-mcmc-wk08.html#advanced-mcmc-wk08" id="toc-advanced-mcmc-wk08"><span class="toc-section-number">4.3</span> Advanced MCMC (wk08)<span></span></a>
<ul>
<li><a href="advanced-mcmc-wk08.html#data-augmentation" id="toc-data-augmentation"><span class="toc-section-number">4.3.1</span> Data Augmentation<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm" id="toc-hit-and-run-algorithm"><span class="toc-section-number">4.3.2</span> Hit-and-Run Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm" id="toc-metropolis-adjusted-langevin-algorithm"><span class="toc-section-number">4.3.3</span> Metropolis-Adjusted Langevin Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm" id="toc-multiple-try-metropolis-algorithm"><span class="toc-section-number">4.3.4</span> Multiple-Try Metropolis Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm" id="toc-reversible-jump-mcmc-algorithm"><span class="toc-section-number">4.3.5</span> Reversible Jump MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="auxiliary-variable-mcmc.html#auxiliary-variable-mcmc" id="toc-auxiliary-variable-mcmc"><span class="toc-section-number">4.4</span> Auxiliary Variable MCMC<span></span></a>
<ul>
<li><a href="auxiliary-variable-mcmc.html#introduction" id="toc-introduction"><span class="toc-section-number">4.4.1</span> Introduction<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution" id="toc-multimodal-target-distribution"><span class="toc-section-number">4.4.2</span> Multimodal Target Distribution<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants" id="toc-doubly-intractable-normalizing-constants"><span class="toc-section-number">4.4.3</span> Doubly-intractable Normalizing Constants<span></span></a></li>
</ul></li>
<li><a href="approximate-bayesian-computation.html#approximate-bayesian-computation" id="toc-approximate-bayesian-computation"><span class="toc-section-number">4.5</span> Approximate Bayesian Computation<span></span></a>
<ul>
<li><a href="approximate-bayesian-computation.html#simulator-based-models" id="toc-simulator-based-models"><span class="toc-section-number">4.5.1</span> Simulator-Based Models<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods" id="toc-abcifying-monte-carlo-methods"><span class="toc-section-number">4.5.2</span> ABCifying Monte Carlo Methods<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm" id="toc-abc-mcmc-algorithm"><span class="toc-section-number">4.5.3</span> ABC-MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="hamiltonian-monte-carlo.html#hamiltonian-monte-carlo" id="toc-hamiltonian-monte-carlo"><span class="toc-section-number">4.6</span> Hamiltonian Monte Carlo<span></span></a>
<ul>
<li><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo" id="toc-introduction-to-hamiltonian-monte-carlo"><span class="toc-section-number">4.6.1</span> Introduction to Hamiltonian Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="population-monte-carlo.html#population-monte-carlo" id="toc-population-monte-carlo"><span class="toc-section-number">4.7</span> Population Monte Carlo<span></span></a>
<ul>
<li><a href="population-monte-carlo.html#adaptive-direction-sampling" id="toc-adaptive-direction-sampling"><span class="toc-section-number">4.7.1</span> Adaptive Direction Sampling<span></span></a></li>
<li><a href="population-monte-carlo.html#conjugate-gradient-mc" id="toc-conjugate-gradient-mc"><span class="toc-section-number">4.7.2</span> Conjugate Gradient MC<span></span></a></li>
<li><a href="population-monte-carlo.html#parallel-tempering" id="toc-parallel-tempering"><span class="toc-section-number">4.7.3</span> Parallel Tempering<span></span></a></li>
<li><a href="population-monte-carlo.html#evolutionary-mc" id="toc-evolutionary-mc"><span class="toc-section-number">4.7.4</span> Evolutionary MC<span></span></a></li>
<li><a href="population-monte-carlo.html#sequential-parallel-tempering" id="toc-sequential-parallel-tempering"><span class="toc-section-number">4.7.5</span> Sequential Parallel Tempering<span></span></a></li>
</ul></li>
<li><a href="stochastic-approximation-monte-carlo.html#stochastic-approximation-monte-carlo" id="toc-stochastic-approximation-monte-carlo"><span class="toc-section-number">4.8</span> Stochastic Approximation Monte Carlo<span></span></a></li>
<li><a href="review.html#review" id="toc-review"><span class="toc-section-number">4.9</span> Review<span></span></a>
<ul>
<li><a href="review.html#wk01" id="toc-wk01"><span class="toc-section-number">4.9.1</span> Wk01<span></span></a></li>
<li><a href="review.html#wk03" id="toc-wk03"><span class="toc-section-number">4.9.2</span> wk03<span></span></a></li>
<li><a href="review.html#wk04-05" id="toc-wk04-05"><span class="toc-section-number">4.9.3</span> wk04, 05<span></span></a></li>
</ul></li>
<li><a href="else.html#else" id="toc-else"><span class="toc-section-number">4.10</span> Else<span></span></a>
<ul>
<li><a href="else.html#hw4.-rasch-model" id="toc-hw4.-rasch-model"><span class="toc-section-number">4.10.1</span> Hw4. Rasch Model<span></span></a></li>
<li><a href="else.html#da-example-mvn" id="toc-da-example-mvn"><span class="toc-section-number">4.10.2</span> DA) Example: MVN<span></span></a></li>
<li><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes" id="toc-bayesian-adaptive-clinical-trial-with-delayed-outcomes"><span class="toc-section-number">4.10.3</span> Bayesian adaptive clinical trial with delayed outcomes<span></span></a></li>
<li><a href="else.html#nmar의-종류" id="toc-nmar의-종류"><span class="toc-section-number">4.10.4</span> NMAR의 종류<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-selection" id="toc-wk10-bayesian-model-selection"><span class="toc-section-number">4.10.5</span> wk10) Bayesian Model Selection<span></span></a></li>
<li><a href="else.html#autologistic-model" id="toc-autologistic-model"><span class="toc-section-number">4.10.6</span> Autologistic model<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-averaging" id="toc-wk10-bayesian-model-averaging"><span class="toc-section-number">4.10.7</span> wk10) Bayesian Model Averaging<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="mva.html#mva" id="toc-mva"><span class="toc-section-number">5</span> MVA<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#overview-of-mva-not-ended" id="toc-overview-of-mva-not-ended"><span class="toc-section-number">5.1</span> Overview of mva (not ended)<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#notation" id="toc-notation"><span class="toc-section-number">5.1.1</span> Notation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">5.1.2</span> Summary Statistics<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation" id="toc-statistical-inference-on-correlation"><span class="toc-section-number">5.1.3</span> Statistical Inference on Correlation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#standardization" id="toc-standardization"><span class="toc-section-number">5.1.4</span> Standardization<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#missing-value-treatment" id="toc-missing-value-treatment"><span class="toc-section-number">5.1.5</span> Missing Value Treatment<span></span></a></li>
</ul></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-nomral-wk2" id="toc-multivariate-nomral-wk2"><span class="toc-section-number">5.2</span> Multivariate Nomral (wk2)<span></span></a>
<ul>
<li><a href="multivariate-nomral-wk2.html#overview-2" id="toc-overview-2"><span class="toc-section-number">5.2.1</span> Overview<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">5.2.2</span> Spectral Decomposition<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">5.2.3</span> Properties of MVN<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#chi2-distribution" id="toc-chi2-distribution"><span class="toc-section-number">5.2.4</span> <span class="math inline">\(\Chi^2\)</span> distribution<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors" id="toc-linear-combination-of-random-vectors"><span class="toc-section-number">5.2.5</span> Linear Combination of Random Vectors<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood" id="toc-multivariate-normal-likelihood"><span class="toc-section-number">5.2.6</span> Multivariate Normal Likelihood<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s" id="toc-sampling-distribtion-of-bar-pmb-y-s"><span class="toc-section-number">5.2.7</span> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span><span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#assessing-normality" id="toc-assessing-normality"><span class="toc-section-number">5.2.8</span> Assessing Normality<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#power-transformation" id="toc-power-transformation"><span class="toc-section-number">5.2.9</span> Power Transformation<span></span></a></li>
</ul></li>
<li><a href="inference-about-mean-vector-wk3.html#inference-about-mean-vector-wk3" id="toc-inference-about-mean-vector-wk3"><span class="toc-section-number">5.3</span> Inference about Mean Vector (wk3)<span></span></a>
<ul>
<li><a href="inference-about-mean-vector-wk3.html#overview-3" id="toc-overview-3"><span class="toc-section-number">5.3.1</span> Overview<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#confidence-region" id="toc-confidence-region"><span class="toc-section-number">5.3.2</span> 1. Confidence Region<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#simultaneous-ci" id="toc-simultaneous-ci"><span class="toc-section-number">5.3.3</span> 2. Simultaneous CI<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison" id="toc-note-bonferroni-multiple-comparison"><span class="toc-section-number">5.3.4</span> 3. Note: Bonferroni Multiple Comparison<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector" id="toc-large-sample-inferences-about-a-mean-vector"><span class="toc-section-number">5.3.5</span> 4. Large Sample Inferences about a Mean Vector<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5" id="toc-profile-analysis-wk4-5"><span class="toc-section-number">5.3.6</span> 1. Profile Analysis (wk4, 5)<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend" id="toc-test-for-linear-trend"><span class="toc-section-number">5.3.7</span> 2. Test for Linear Trend<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix" id="toc-inferences-about-a-covariance-matrix"><span class="toc-section-number">5.3.8</span> 3. Inferences about a Covariance Matrix<span></span></a></li>
</ul></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparison-of-several-mv-means-wk5" id="toc-comparison-of-several-mv-means-wk5"><span class="toc-section-number">5.4</span> Comparison of Several MV Means (wk5)<span></span></a>
<ul>
<li><a href="comparison-of-several-mv-means-wk5.html#paired-comparison" id="toc-paired-comparison"><span class="toc-section-number">5.4.1</span> Paired Comparison<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations" id="toc-comparing-mean-vectors-from-two-populations"><span class="toc-section-number">5.4.2</span> Comparing Mean Vectors from Two Populations<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2" id="toc-profile-analysis-for-g2"><span class="toc-section-number">5.4.3</span> Profile Analysis (for <span class="math inline">\(g=2\)</span>)<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means" id="toc-comparing-several-multivariate-population-means"><span class="toc-section-number">5.4.4</span> Comparing Several Multivariate Population Means<span></span></a></li>
</ul></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression-wk6" id="toc-multivariate-multiple-regression-wk6"><span class="toc-section-number">5.5</span> Multivariate Multiple Regression (wk6)<span></span></a>
<ul>
<li><a href="multivariate-multiple-regression-wk6.html#overview-4" id="toc-overview-4"><span class="toc-section-number">5.5.1</span> Overview<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression" id="toc-multivariate-multiple-regression"><span class="toc-section-number">5.5.2</span> Multivariate Multiple Regression<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#example" id="toc-example"><span class="toc-section-number">5.5.3</span> Example)<span></span></a></li>
</ul></li>
<li><a href="pca.html#pca" id="toc-pca"><span class="toc-section-number">5.6</span> PCA<span></span></a></li>
<li><a href="factor.html#factor" id="toc-factor"><span class="toc-section-number">5.7</span> Factor<span></span></a>
<ul>
<li><a href="factor.html#method-of-estimation" id="toc-method-of-estimation"><span class="toc-section-number">5.7.1</span> Method of Estimation<span></span></a></li>
<li><a href="factor.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">5.7.2</span> Factor Rotation<span></span></a></li>
<li><a href="factor.html#varimax-criterion" id="toc-varimax-criterion"><span class="toc-section-number">5.7.3</span> Varimax Criterion<span></span></a></li>
<li><a href="factor.html#factor-scores" id="toc-factor-scores"><span class="toc-section-number">5.7.4</span> Factor Scores<span></span></a></li>
</ul></li>
<li><a href="discrimination-and-classification.html#discrimination-and-classification" id="toc-discrimination-and-classification"><span class="toc-section-number">5.8</span> Discrimination and Classification<span></span></a>
<ul>
<li><a href="discrimination-and-classification.html#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">5.8.1</span> Bayes Rule<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations" id="toc-classification-with-two-mv-n-populations"><span class="toc-section-number">5.8.2</span> Classification with Two mv <span class="math inline">\(N\)</span> Populations<span></span></a></li>
<li><a href="discrimination-and-classification.html#evaluating-classification-functions" id="toc-evaluating-classification-functions"><span class="toc-section-number">5.8.3</span> Evaluating Classification Functions<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-several-populations-wk13" id="toc-classification-with-several-populations-wk13"><span class="toc-section-number">5.8.4</span> Classification with several Populations (wk13)<span></span></a></li>
<li><a href="discrimination-and-classification.html#other-discriminant-analysis-methods" id="toc-other-discriminant-analysis-methods"><span class="toc-section-number">5.8.5</span> Other Discriminant Analysis Methods<span></span></a></li>
</ul></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-distance-methods-and-ordination" id="toc-clustering-distance-methods-and-ordination"><span class="toc-section-number">5.9</span> Clustering, Distance Methods, and Ordination<span></span></a>
<ul>
<li><a href="clustering-distance-methods-and-ordination.html#overview-5" id="toc-overview-5"><span class="toc-section-number">5.9.1</span> Overview<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">5.9.2</span> Hierarchical Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">5.9.3</span> K-means Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법" id="toc-군집의-평가방법"><span class="toc-section-number">5.9.4</span> 군집의 평가방법<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14" id="toc-clustering-using-density-estimation-wk14"><span class="toc-section-number">5.9.5</span> Clustering using Density Estimation (wk14)<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds" id="toc-multidimensional-scaling-mds"><span class="toc-section-number">5.9.6</span> Multidimensional Scaling (MDS)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linear.html#linear" id="toc-linear"><span class="toc-section-number">6</span> Linear<span></span></a>
<ul>
<li><a href="overview-svd.html#overview-svd" id="toc-overview-svd"><span class="toc-section-number">6.1</span> Overview &amp; SVD<span></span></a>
<ul>
<li><a href="overview-svd.html#spectral-decomposition-1" id="toc-spectral-decomposition-1"><span class="toc-section-number">6.1.1</span> Spectral Decomposition<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-general-version" id="toc-singular-value-decomposition-general-version"><span class="toc-section-number">6.1.2</span> Singular value Decomposition: General-version<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-another-version" id="toc-singular-value-decomposition-another-version"><span class="toc-section-number">6.1.3</span> Singular value Decomposition: Another-version<span></span></a></li>
<li><a href="overview-svd.html#quadratic-forms" id="toc-quadratic-forms"><span class="toc-section-number">6.1.4</span> Quadratic Forms<span></span></a></li>
<li><a href="overview-svd.html#partitioned-matrices" id="toc-partitioned-matrices"><span class="toc-section-number">6.1.5</span> Partitioned Matrices<span></span></a></li>
<li><a href="overview-svd.html#geometrical-aspects" id="toc-geometrical-aspects"><span class="toc-section-number">6.1.6</span> Geometrical Aspects<span></span></a></li>
<li><a href="overview-svd.html#column-row-and-null-space" id="toc-column-row-and-null-space"><span class="toc-section-number">6.1.7</span> Column, Row and Null Space<span></span></a></li>
</ul></li>
<li><a href="introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">6.2</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-1.html#what" id="toc-what"><span class="toc-section-number">6.2.1</span> What<span></span></a></li>
<li><a href="introduction-1.html#random-vectors-and-matrices" id="toc-random-vectors-and-matrices"><span class="toc-section-number">6.2.2</span> Random Vectors and Matrices<span></span></a></li>
<li><a href="introduction-1.html#multivariate-normal-distributions" id="toc-multivariate-normal-distributions"><span class="toc-section-number">6.2.3</span> Multivariate Normal Distributions<span></span></a></li>
<li><a href="introduction-1.html#distributions-of-quadratic-forms" id="toc-distributions-of-quadratic-forms"><span class="toc-section-number">6.2.4</span> Distributions of Quadratic Forms<span></span></a></li>
</ul></li>
<li><a href="estimation.html#estimation" id="toc-estimation"><span class="toc-section-number">6.3</span> Estimation<span></span></a>
<ul>
<li><a href="estimation.html#identifiability-and-estimability" id="toc-identifiability-and-estimability"><span class="toc-section-number">6.3.1</span> Identifiability and Estimability<span></span></a></li>
<li><a href="estimation.html#estimation-least-squares" id="toc-estimation-least-squares"><span class="toc-section-number">6.3.2</span> Estimation: Least Squares<span></span></a></li>
<li><a href="estimation.html#estimation-best-linear-unbiased" id="toc-estimation-best-linear-unbiased"><span class="toc-section-number">6.3.3</span> Estimation: Best Linear Unbiased<span></span></a></li>
<li><a href="estimation.html#estimation-maximum-likelihood" id="toc-estimation-maximum-likelihood"><span class="toc-section-number">6.3.4</span> Estimation: Maximum Likelihood<span></span></a></li>
<li><a href="estimation.html#estimation-minimum-variance-unbiased" id="toc-estimation-minimum-variance-unbiased"><span class="toc-section-number">6.3.5</span> Estimation: Minimum Variance Unbiased<span></span></a></li>
<li><a href="estimation.html#sampling-distributions-of-estimates" id="toc-sampling-distributions-of-estimates"><span class="toc-section-number">6.3.6</span> Sampling Distributions of Estimates<span></span></a></li>
<li><a href="estimation.html#generalized-least-squaresgls" id="toc-generalized-least-squaresgls"><span class="toc-section-number">6.3.7</span> Generalized Least Squares(GLS)<span></span></a></li>
</ul></li>
<li><a href="one-way-anova.html#one-way-anova" id="toc-one-way-anova"><span class="toc-section-number">6.4</span> One-Way ANOVA<span></span></a>
<ul>
<li><a href="one-way-anova.html#one-way-anova-1" id="toc-one-way-anova-1"><span class="toc-section-number">6.4.1</span> One-Way ANOVA<span></span></a></li>
<li><a href="one-way-anova.html#more-about-models" id="toc-more-about-models"><span class="toc-section-number">6.4.2</span> More About Models<span></span></a></li>
<li><a href="one-way-anova.html#estimating-and-testing-contrasts" id="toc-estimating-and-testing-contrasts"><span class="toc-section-number">6.4.3</span> Estimating and Testing Contrasts<span></span></a></li>
<li><a href="one-way-anova.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">6.4.4</span> Cochran’s Theorem<span></span></a></li>
</ul></li>
<li><a href="testing.html#testing" id="toc-testing"><span class="toc-section-number">6.5</span> Testing<span></span></a>
<ul>
<li><a href="testing.html#more-about-models-two-approaches-for-linear-model" id="toc-more-about-models-two-approaches-for-linear-model"><span class="toc-section-number">6.5.1</span> More About Models: Two approaches for linear model<span></span></a></li>
<li><a href="testing.html#testing-models" id="toc-testing-models"><span class="toc-section-number">6.5.2</span> Testing Models<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure" id="toc-a-generalized-test-procedure"><span class="toc-section-number">6.5.3</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-linear-parametric-functions" id="toc-testing-linear-parametric-functions"><span class="toc-section-number">6.5.4</span> Testing Linear Parametric Functions<span></span></a></li>
<li><a href="testing.html#theoretical-complements" id="toc-theoretical-complements"><span class="toc-section-number">6.5.5</span> Theoretical Complements<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure-1" id="toc-a-generalized-test-procedure-1"><span class="toc-section-number">6.5.6</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace" id="toc-testing-single-degrees-of-freedom-in-a-given-subspace"><span class="toc-section-number">6.5.7</span> Testing Single Degrees of Freedom in a Given Subspace<span></span></a></li>
<li><a href="testing.html#breaking-ss-into-independent-components" id="toc-breaking-ss-into-independent-components"><span class="toc-section-number">6.5.8</span> Breaking SS into Independent Components<span></span></a></li>
<li><a href="testing.html#general-theory" id="toc-general-theory"><span class="toc-section-number">6.5.9</span> General Theory<span></span></a></li>
<li><a href="testing.html#two-way-anova" id="toc-two-way-anova"><span class="toc-section-number">6.5.10</span> Two-Way ANOVA<span></span></a></li>
<li><a href="testing.html#confidence-regions" id="toc-confidence-regions"><span class="toc-section-number">6.5.11</span> Confidence Regions<span></span></a></li>
<li><a href="testing.html#tests-for-generalized-least-squares-models" id="toc-tests-for-generalized-least-squares-models"><span class="toc-section-number">6.5.12</span> Tests for Generalized Least Squares Models<span></span></a></li>
</ul></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">6.6</span> Generalized Least Squares<span></span></a>
<ul>
<li><a href="generalized-least-squares.html#a-direct-solution-via-inner-products" id="toc-a-direct-solution-via-inner-products"><span class="toc-section-number">6.6.1</span> A direct solution via inner products<span></span></a></li>
</ul></li>
<li><a href="flat.html#flat" id="toc-flat"><span class="toc-section-number">6.7</span> Flat<span></span></a>
<ul>
<li><a href="flat.html#flat-1" id="toc-flat-1"><span class="toc-section-number">6.7.1</span> 1.Flat<span></span></a></li>
<li><a href="flat.html#solutions-to-systems-of-linear-equations" id="toc-solutions-to-systems-of-linear-equations"><span class="toc-section-number">6.7.2</span> 2. Solutions to systems of linear equations<span></span></a></li>
</ul></li>
<li><a href="unified-approach-to-balanced-anova-models.html#unified-approach-to-balanced-anova-models" id="toc-unified-approach-to-balanced-anova-models"><span class="toc-section-number">6.8</span> Unified Approach to Balanced ANOVA Models<span></span></a></li>
</ul></li>
<li><a href="#part-21-02" id="toc-part-21-02">(PART) 21-02<span></span></a></li>
<li><a href="network-stats.html#network-stats" id="toc-network-stats"><span class="toc-section-number">7</span> Network Stats<span></span></a>
<ul>
<li><a href="introduction-2.html#introduction-2" id="toc-introduction-2"><span class="toc-section-number">7.1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-2.html#types-of-network-analysis" id="toc-types-of-network-analysis"><span class="toc-section-number">7.1.1</span> Types of Network Analysis<span></span></a></li>
<li><a href="introduction-2.html#network-modeling-and-inference" id="toc-network-modeling-and-inference"><span class="toc-section-number">7.1.2</span> Network Modeling and Inference<span></span></a></li>
<li><a href="introduction-2.html#network-processes" id="toc-network-processes"><span class="toc-section-number">7.1.3</span> Network Processes<span></span></a></li>
</ul></li>
<li><a href="descriptive-statistics-of-networks.html#descriptive-statistics-of-networks" id="toc-descriptive-statistics-of-networks"><span class="toc-section-number">7.2</span> Descriptive Statistics of Networks<span></span></a>
<ul>
<li><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics" id="toc-vertex-and-edge-characteristics"><span class="toc-section-number">7.2.1</span> Vertex and Edge Characteristics<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion" id="toc-characterizing-network-cohesion"><span class="toc-section-number">7.2.2</span> Characterizing Network Cohesion<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#graph-partitioning" id="toc-graph-partitioning"><span class="toc-section-number">7.2.3</span> Graph Partitioning<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing" id="toc-assortativity-and-mixing"><span class="toc-section-number">7.2.4</span> Assortativity and Mixing<span></span></a></li>
</ul></li>
<li><a href="data-collection-and-sampling.html#data-collection-and-sampling" id="toc-data-collection-and-sampling"><span class="toc-section-number">7.3</span> Data Collection and Sampling<span></span></a>
<ul>
<li><a href="data-collection-and-sampling.html#sampling-designs" id="toc-sampling-designs"><span class="toc-section-number">7.3.1</span> Sampling Designs<span></span></a></li>
<li><a href="data-collection-and-sampling.html#coping-strategies" id="toc-coping-strategies"><span class="toc-section-number">7.3.2</span> Coping Strategies<span></span></a></li>
<li><a href="data-collection-and-sampling.html#big-data-solves-nothing" id="toc-big-data-solves-nothing"><span class="toc-section-number">7.3.3</span> Big Data Solves Nothing<span></span></a></li>
</ul></li>
<li><a href="mathematical-models-for-network-graphs.html#mathematical-models-for-network-graphs" id="toc-mathematical-models-for-network-graphs"><span class="toc-section-number">7.4</span> Mathematical Models for Network Graphs<span></span></a>
<ul>
<li><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models" id="toc-classical-random-graph-models"><span class="toc-section-number">7.4.1</span> Classical Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models" id="toc-generalized-random-graph-models"><span class="toc-section-number">7.4.2</span> Generalized Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms" id="toc-network-graph-models-based-on-mechanisms"><span class="toc-section-number">7.4.3</span> Network Graph Models Based on Mechanisms<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics" id="toc-assessing-significance-of-network-graph-characteristics"><span class="toc-section-number">7.4.4</span> Assessing Significance of Network Graph Characteristics<span></span></a></li>
</ul></li>
<li><a href="introduction-to-ergm.html#introduction-to-ergm" id="toc-introduction-to-ergm"><span class="toc-section-number">7.5</span> Introduction to ERGM<span></span></a>
<ul>
<li><a href="introduction-to-ergm.html#exponential-random-graph-models" id="toc-exponential-random-graph-models"><span class="toc-section-number">7.5.1</span> Exponential Random Graph Models<span></span></a></li>
<li><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation" id="toc-difficulty-in-parameter-estimation"><span class="toc-section-number">7.5.2</span> Difficulty in Parameter Estimation<span></span></a></li>
</ul></li>
<li><a href="parameter-estimation-of-ergm.html#parameter-estimation-of-ergm" id="toc-parameter-estimation-of-ergm"><span class="toc-section-number">7.6</span> Parameter Estimation of ERGM<span></span></a>
<ul>
<li><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm" id="toc-current-methods-for-ergm"><span class="toc-section-number">7.6.1</span> Current Methods for ERGM<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm" id="toc-approximation-based-algorithm"><span class="toc-section-number">7.6.2</span> Approximation-based Algorithm<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches" id="toc-auxiliary-variable-mcmc-based-approaches"><span class="toc-section-number">7.6.3</span> Auxiliary Variable MCMC-based Approaches<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc" id="toc-varying-trunction-stochastic-approximation-mcmc"><span class="toc-section-number">7.6.4</span> Varying Trunction Stochastic Approximation MCMC<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#conclusion" id="toc-conclusion"><span class="toc-section-number">7.6.5</span> Conclusion<span></span></a></li>
</ul></li>
<li><a href="ergm-for-dynamic-networks.html#ergm-for-dynamic-networks" id="toc-ergm-for-dynamic-networks"><span class="toc-section-number">7.7</span> ERGM for Dynamic Networks<span></span></a>
<ul>
<li><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm" id="toc-temporal-ergm-tergm-t-ergm"><span class="toc-section-number">7.7.1</span> Temporal ERGM (TERGM, T ERGM)<span></span></a></li>
<li><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm" id="toc-separable-temporal-ergm-stergm-st-ergm"><span class="toc-section-number">7.7.2</span> Separable Temporal ERGM (STERGM, ST ERGM)<span></span></a></li>
</ul></li>
<li><a href="latent-network-models.html#latent-network-models" id="toc-latent-network-models"><span class="toc-section-number">7.8</span> Latent Network Models<span></span></a>
<ul>
<li><a href="latent-network-models.html#latent-position-model" id="toc-latent-position-model"><span class="toc-section-number">7.8.1</span> Latent Position Model<span></span></a></li>
<li><a href="latent-network-models.html#latent-position-cluster-model" id="toc-latent-position-cluster-model"><span class="toc-section-number">7.8.2</span> Latent Position Cluster Model<span></span></a></li>
</ul></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#additive-and-multiplicative-effects-network-models" id="toc-additive-and-multiplicative-effects-network-models"><span class="toc-section-number">7.9</span> Additive and Multiplicative Effects Network Models<span></span></a>
<ul>
<li><a href="additive-and-multiplicative-effects-network-models.html#introduction-3" id="toc-introduction-3"><span class="toc-section-number">7.9.1</span> Introduction<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression" id="toc-social-relations-regression"><span class="toc-section-number">7.9.2</span> Social Relations Regression<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models" id="toc-multiplicative-effects-models"><span class="toc-section-number">7.9.3</span> Multiplicative Effects Models<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation" id="toc-inference-via-posterior-approximation"><span class="toc-section-number">7.9.4</span> Inference via Posterior Approximation<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r" id="toc-discussion-and-example-with-r"><span class="toc-section-number">7.9.5</span> Discussion and Example with R<span></span></a></li>
</ul></li>
<li><a href="stochastic-block-models.html#stochastic-block-models" id="toc-stochastic-block-models"><span class="toc-section-number">7.10</span> Stochastic Block Models<span></span></a>
<ul>
<li><a href="stochastic-block-models.html#stochastic-block-model" id="toc-stochastic-block-model"><span class="toc-section-number">7.10.1</span> Stochastic Block Model<span></span></a></li>
<li><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm" id="toc-mixed-membership-block-model-mmbm"><span class="toc-section-number">7.10.2</span> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="high-dimension.html#high-dimension" id="toc-high-dimension"><span class="toc-section-number">8</span> High Dimension<span></span></a>
<ul>
<li><a href="introduction-4.html#introduction-4" id="toc-introduction-4"><span class="toc-section-number">8.1</span> Introduction<span></span></a></li>
<li><a href="concentration-inequalities.html#concentration-inequalities" id="toc-concentration-inequalities"><span class="toc-section-number">8.2</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities.html#motivation" id="toc-motivation"><span class="toc-section-number">8.2.1</span> Motivation<span></span></a></li>
<li><a href="concentration-inequalities.html#from-markov-to-chernoff" id="toc-from-markov-to-chernoff"><span class="toc-section-number">8.2.2</span> From Markov to Chernoff<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-variables" id="toc-sub-gaussian-random-variables"><span class="toc-section-number">8.2.3</span> sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables" id="toc-properties-of-sub-gaussian-random-variables"><span class="toc-section-number">8.2.4</span> Properties of sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#equivalent-definitions" id="toc-equivalent-definitions"><span class="toc-section-number">8.2.5</span> Equivalent definitions<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-vectors" id="toc-sub-gaussian-random-vectors"><span class="toc-section-number">8.2.6</span> Sub-Gaussian random vectors<span></span></a></li>
<li><a href="concentration-inequalities.html#hoeffdings-inequality" id="toc-hoeffdings-inequality"><span class="toc-section-number">8.2.7</span> Hoeffding’s inequality<span></span></a></li>
<li><a href="concentration-inequalities.html#maximal-inequalities" id="toc-maximal-inequalities"><span class="toc-section-number">8.2.8</span> Maximal inequalities<span></span></a></li>
<li><a href="concentration-inequalities.html#section" id="toc-section"><span class="toc-section-number">8.2.9</span> </a></li>
</ul></li>
<li><a href="concentration-inequalities-1.html#concentration-inequalities-1" id="toc-concentration-inequalities-1"><span class="toc-section-number">8.3</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities-1.html#sub-exponential-random-variables" id="toc-sub-exponential-random-variables"><span class="toc-section-number">8.3.1</span> Sub-exponential random variables<span></span></a></li>
<li><a href="concentration-inequalities-1.html#bernsteins-condition" id="toc-bernsteins-condition"><span class="toc-section-number">8.3.2</span> Bernstein’s condition<span></span></a></li>
<li><a href="concentration-inequalities-1.html#mcdiarmids-inequality" id="toc-mcdiarmids-inequality"><span class="toc-section-number">8.3.3</span> McDiarmid’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#levys-inequality" id="toc-levys-inequality"><span class="toc-section-number">8.3.4</span> Levy’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#quadratic-form" id="toc-quadratic-form"><span class="toc-section-number">8.3.5</span> Quadratic form<span></span></a></li>
<li><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma" id="toc-the-johnsonlindenstrauss-lemma"><span class="toc-section-number">8.3.6</span> The Johnson–Lindenstrauss Lemma<span></span></a></li>
</ul></li>
<li><a href="metric-entropy-and-its-uses.html#metric-entropy-and-its-uses" id="toc-metric-entropy-and-its-uses"><span class="toc-section-number">8.4</span> Metric entropy and its uses<span></span></a>
<ul>
<li><a href="metric-entropy-and-its-uses.html#metric-space" id="toc-metric-space"><span class="toc-section-number">8.4.1</span> Metric space<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy" id="toc-covering-numbers-and-metric-entropy"><span class="toc-section-number">8.4.2</span> Covering numbers and metric entropy<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#packing-numbers" id="toc-packing-numbers"><span class="toc-section-number">8.4.3</span> Packing numbers<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#section-1" id="toc-section-1"><span class="toc-section-number">8.4.4</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-2" id="toc-section-2"><span class="toc-section-number">8.4.5</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-3" id="toc-section-3"><span class="toc-section-number">8.4.6</span> </a></li>
</ul></li>
<li><a href="covariance-estimation.html#covariance-estimation" id="toc-covariance-estimation"><span class="toc-section-number">8.5</span> Covariance estimation<span></span></a>
<ul>
<li><a href="covariance-estimation.html#matrix-algebra-review" id="toc-matrix-algebra-review"><span class="toc-section-number">8.5.1</span> Matrix algebra review<span></span></a></li>
<li><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm" id="toc-covariance-matrix-estimation-in-the-operator-norm"><span class="toc-section-number">8.5.2</span> Covariance matrix estimation in the operator norm<span></span></a></li>
<li><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices" id="toc-bounds-for-structured-covariance-matrices"><span class="toc-section-number">8.5.3</span> Bounds for structured covariance matrices<span></span></a></li>
</ul></li>
<li><a href="matrix-concentration-inequalities.html#matrix-concentration-inequalities" id="toc-matrix-concentration-inequalities"><span class="toc-section-number">8.6</span> Matrix concentration inequalities<span></span></a>
<ul>
<li><a href="matrix-concentration-inequalities.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">8.6.1</span> Matrix calculus<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#matrix-chernoff" id="toc-matrix-chernoff"><span class="toc-section-number">8.6.2</span> Matrix Chernoff<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices" id="toc-sub-gaussian-and-sub-exponential-matrices"><span class="toc-section-number">8.6.3</span> Sub-Gaussian and sub-exponential matrices<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds" id="toc-랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><span class="toc-section-number">8.6.4</span> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds<span></span></a></li>
</ul></li>
<li><a href="principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.7</span> Principal Component Analysis<span></span></a>
<ul>
<li><a href="principal-component-analysis.html#pca-1" id="toc-pca-1"><span class="toc-section-number">8.7.1</span> PCA<span></span></a></li>
<li><a href="principal-component-analysis.html#matrix-perturbation" id="toc-matrix-perturbation"><span class="toc-section-number">8.7.2</span> Matrix Perturbation<span></span></a></li>
<li><a href="principal-component-analysis.html#spiked-cov-model" id="toc-spiked-cov-model"><span class="toc-section-number">8.7.3</span> Spiked Cov Model<span></span></a></li>
<li><a href="principal-component-analysis.html#sparse-pca" id="toc-sparse-pca"><span class="toc-section-number">8.7.4</span> sparse PCA<span></span></a></li>
</ul></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">8.8</span> Linear Regression<span></span></a>
<ul>
<li><a href="linear-regression.html#problem-formulation" id="toc-problem-formulation"><span class="toc-section-number">8.8.1</span> Problem formulation<span></span></a></li>
<li><a href="linear-regression.html#least-squares-estimator-in-high-dimensions" id="toc-least-squares-estimator-in-high-dimensions"><span class="toc-section-number">8.8.2</span> Least Squares Estimator in high dimensions<span></span></a></li>
<li><a href="linear-regression.html#sparse-linear-regression" id="toc-sparse-linear-regression"><span class="toc-section-number">8.8.3</span> Sparse linear regression<span></span></a></li>
</ul></li>
<li><a href="uniform-laws-of-large-numbers.html#uniform-laws-of-large-numbers" id="toc-uniform-laws-of-large-numbers"><span class="toc-section-number">8.9</span> Uniform laws of large numbers<span></span></a>
<ul>
<li><a href="uniform-laws-of-large-numbers.html#motivation-1" id="toc-motivation-1"><span class="toc-section-number">8.9.1</span> Motivation<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity" id="toc-a-uniform-law-via-rademacher-complexity"><span class="toc-section-number">8.9.2</span> A uniform law via Rademacher complexity<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity" id="toc-upper-bounds-on-the-rademacher-complexity"><span class="toc-section-number">8.9.3</span> Upper bounds on the Rademacher complexity<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="survival-analysis.html#survival-analysis" id="toc-survival-analysis"><span class="toc-section-number">9</span> Survival Analysis<span></span></a>
<ul>
<li><a href="introduction-5.html#introduction-5" id="toc-introduction-5"><span class="toc-section-number">9.1</span> Introduction<span></span></a></li>
<li><a href="section-4.html#section-4" id="toc-section-4"><span class="toc-section-number">9.2</span> </a></li>
<li><a href="counting-processes-and-martingales.html#counting-processes-and-martingales" id="toc-counting-processes-and-martingales"><span class="toc-section-number">9.3</span> Counting Processes and Martingales<span></span></a>
<ul>
<li><a href="counting-processes-and-martingales.html#conditional-expectation" id="toc-conditional-expectation"><span class="toc-section-number">9.3.1</span> Conditional Expectation<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#martingale" id="toc-martingale"><span class="toc-section-number">9.3.2</span> Martingale<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#key-martingales-properties" id="toc-key-martingales-properties"><span class="toc-section-number">9.3.3</span> Key Martingales Properties<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#section-5" id="toc-section-5"><span class="toc-section-number">9.3.4</span> </a></li>
<li><a href="counting-processes-and-martingales.html#section-6" id="toc-section-6"><span class="toc-section-number">9.3.5</span> </a></li>
</ul></li>
<li><a href="section-7.html#section-7" id="toc-section-7"><span class="toc-section-number">9.4</span> </a></li>
<li><a href="cox-regression.html#cox-regression" id="toc-cox-regression"><span class="toc-section-number">9.5</span> Cox Regression<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#filtration의-개념을-정복하자" id="toc-filtration의-개념을-정복하자"><span class="toc-section-number">9.6</span> Filtration의 개념을 정복하자!<span></span></a>
<ul>
<li><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약" id="toc-random-process를-이야기-하기까지의-긴-여정의-요약"><span class="toc-section-number">9.6.1</span> Random Process를 이야기 하기까지의 긴 여정의 요약<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#ft-measurable" id="toc-ft-measurable"><span class="toc-section-number">9.6.2</span> Ft-measurable<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#epilogue" id="toc-epilogue"><span class="toc-section-number">9.6.3</span> EPILOGUE<span></span></a></li>
</ul></li>
<li><a href="concepts.html#concepts" id="toc-concepts"><span class="toc-section-number">9.7</span> Concepts<span></span></a></li>
</ul></li>
<li><a href="#part-22-01" id="toc-part-22-01">(PART) 22-01<span></span></a></li>
<li><a href="scikit.html#scikit" id="toc-scikit"><span class="toc-section-number">10</span> scikit<span></span></a>
<ul>
<li><a href="linear-models.html#linear-models" id="toc-linear-models"><span class="toc-section-number">10.1</span> Linear Models<span></span></a>
<ul>
<li><a href="linear-models.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">10.1.1</span> Ordinary Least Squares<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-00-00" id="toc-appendix-00-00">(APPENDIX) 00-00<span></span></a></li>
<li><a href="concepts-1.html#concepts-1" id="toc-concepts-1"><span class="toc-section-number">11</span> Concepts<span></span></a>
<ul>
<li><a href="autologistic.html#autologistic" id="toc-autologistic"><span class="toc-section-number">11.1</span> Autologistics<span></span></a></li>
<li><a href="orderlogit.html#orderlogit" id="toc-orderlogit"><span class="toc-section-number">11.2</span> Ordered Logit<span></span></a></li>
<li><a href="concepts-questions.html#concepts-questions" id="toc-concepts-questions"><span class="toc-section-number">11.3</span> Concepts Questions<span></span></a>
<ul>
<li><a href="concepts-questions.html#통계-및-수학" id="toc-통계-및-수학"><span class="toc-section-number">11.3.1</span> 통계 및 수학<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="about-cluster-gcn.html#about-cluster-gcn" id="toc-about-cluster-gcn"><span class="toc-section-number">12</span> About Cluster-GCN<span></span></a>
<ul>
<li><a href="about-cluster-gcn.html#ann" id="toc-ann"><span class="toc-section-number">12.0.1</span> ANN<span></span></a></li>
<li><a href="about-cluster-gcn.html#cnn" id="toc-cnn"><span class="toc-section-number">12.0.2</span> CNN<span></span></a></li>
<li><a href="about-cluster-gcn.html#graph-convolution-network" id="toc-graph-convolution-network"><span class="toc-section-number">12.0.3</span> Graph Convolution Network<span></span></a></li>
<li><a href="about-cluster-gcn.html#cluster-gcn" id="toc-cluster-gcn"><span class="toc-section-number">12.0.4</span> Cluster-GCN<span></span></a></li>
</ul></li>
<li><a href="cnn-1.html#cnn-1" id="toc-cnn-1"><span class="toc-section-number">13</span> CNN<span></span></a></li>
<li><a href="cnn-2.html#cnn-2" id="toc-cnn-2"><span class="toc-section-number">14</span> CNN<span></span></a></li>
<li><a href="cnn-3.html#cnn-3" id="toc-cnn-3"><span class="toc-section-number">15</span> CNN<span></span></a></li>
<li><a href="section-8.html#section-8" id="toc-section-8"><span class="toc-section-number">16</span> 01<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Markov Chain Monte Carlo<a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(f\)</span> 가 측정은 되는데 샘플화가 안되면, MC를 통해 유사한 샘플을 만들어낼 수 있었다. 이를 넘어서 MCMC는 $ f $ 의 모사함수에서 샘플링하는 게 가능하지만, 이 이상으로 이는 임의의 함수 <span class="math inline">\(p\)</span>에 대해 <span class="math inline">\(E[p(X)]\)</span>가 신뢰도 높게 측정되는 경우에만 샘플링 가능한 별개의 방법론으로 보는 게 정확하다.</p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">MC</th>
<th align="center">MCMC</th>
<th align="center">numerical integration approach</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center">iterative nature</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">can be customized to very diverse &amp; <br> difficult problem</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">무관하며 implementation이 complex하지도 않음</td>
<td align="center">문제가 고차원이면 수렴이 느려짐</td>
</tr>
</tbody>
</table>
<p>시퀀스 <span class="math inline">\(\{\textbf X^{(t)}\}\)</span>는 MC, <span class="math inline">\(t = 0, 1, 2, ….\)</span>. <span class="math inline">\(\textbf X^{(t)} = (X_1^{t} , \cdots, X_p^{(t)})\)</span> 와 <strong>state space</strong> 는 양쪽 모두 연속이거나 discrete.</p>
<p>For the types of Markov chains, <span class="math inline">\(\{ \textbf X^{(t)} \}\)</span>의 분포는 체인의 limiting stationary distribution으로 수렴한다. 체인이 irreducible, aperiodic 할 때.</p>
<p>MCMC의 샘플링 전략은 irreducible, aperiodic MC를 만드는 것. stationary distribution이 목표분포 <span class="math inline">\(f\)</span> 와 일치하는.</p>
<p>t가 충분히 크다면 이 체인으로부터의 <span class="math inline">\(\textbf X^{(t)}\)</span>의 실현값은 근사적으로 마지널 분포 <span class="math inline">\(f\)</span> 를 갖는다.</p>
<p>이런 MCMC의 특성은 베이지안 추론에 크게 도움이 되며 자주 쓰인다.</p>
<p><br />
<br />
<br /></p>
<p>Markov Chain 자체는 <strong>어떤 상태에서 다른 상태로 넘어갈 때, 바로 전 단계의 상태에만 영향을 받는</strong> (Markov Property) 확률 과정을 의미한다.</p>
<ul>
<li>보통 사람들은 전날 먹은 식사와 유사한 식사를 하지 않으려는 경향이 있다.</li>
<li>가령, 어제 짜장면을 먹었다면 오늘은 면종류의 음식을 먹으려고 하지 않는다.</li>
</ul>
<p><br />
<br />
<br />
<br />
<br /></p>
<div id="mh-algorithm" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> MH Algorithm<a href="markov-chain-monte-carlo.html#mh-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>MCMC 중 가장 유명한 적용법은 MH 알고리즘. <span class="math inline">\(t=0\)</span>에서 시작. 시작 distribution <span class="math inline">\(g\)</span>에서 추출한, <span class="math inline">\(f(\textbf x^{(0)} )&gt; 0\)</span> 을 만족하는 <span class="math inline">\(\textbf x^{(0)}\)</span>를 <span class="math inline">\(\textbf X^{(0)} = \textbf x^{(0)}\)</span> 로 잡고 개시한다.</p>
<p>이때 제안분포 <span class="math inline">\(g\)</span> 에서 후보 <span class="math inline">\(\textbf X^{( \ast )}\)</span> 를 하나 만들고, MH ratio <span class="math inline">\(R (\textbf {x}^{(t)}, \textbf X^{\ast} )\)</span> 는</p>
<p><span class="math display">\[
R (\textbf {x}^{(t)}, \textbf X^{\ast} )
= \dfrac
{f(\textbf X^{( \ast )}) g(\textbf x^{( t )} | \textbf X^{( \ast )})}
{f(\textbf x^{( t )}) g(\textbf X^{( \ast )} | \textbf x^{( t )}) }
=\dfrac
{\dfrac
{f(\textbf X^{( \ast )})}
{g(\textbf X^{( \ast )} | \textbf x^{( t )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf x^{( t )} | \textbf X^{( \ast )})}
}
=\dfrac
{\dfrac
{f(\textbf X^{( t+1 )})}
{g(\textbf X^{( t+1 )} | \textbf x^{( t )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf x^{( t )} | \textbf X^{( t+1 )})}
}
\]</span></p>
<p><mark>
<strong>warning</strong></p>
<p><mark>
여기서 단순 Metropolis 알고리즘은 단순히 $ {f(x_0)}$ &gt; $1 $ 이기만 하면 새로운 샘플을 수용한다. 이인즉 <span class="math inline">\(g\)</span>로 표준화해주는 것의 가장 주요한 요점은 둘의 시작 높이, 즉 쥐고 나온 수저가 다를 수 있으므로 이를 표준화해준다는 것이다. 아웃풋이 높은 <span class="math inline">\(x_i\)</span>를 선택하는 것은 MLE 관점에 기반한다.</p>
<p><mark>
단 언제나 그렇듯 이렇게 샘플을 다쳐내면 오히려 음질의 결과가 나온다. 따라서 샘플의 풀을 넓히기 위해 탈락할 녀석들도 확률적으로 살려서 합류시킨다. 이게 고정된 기준점으로 샘플을 쳐내는 것이 아닌, <span class="math inline">\(\dfrac {f(x_1)} {f(x_0)}\)</span> &gt; <span class="math inline">\(u \sim U {(0,1)}\)</span> 을 기준으로 삼아 샘플을 생존시키는 것이다. 이 조건을 실패하면 생산해두었던 샘플 <span class="math inline">\(\textbf X^{(t)}\)</span> 는 버려지고 새로운 샘플을 <span class="math inline">\(t+1\)</span>으로 설정해 재진행한다.
</mark></p>
<p>이후</p>
<p><span class="math display">\[
\textbf {X}^{(t+1)} = \left\{\begin{array}{@{}lr@{}}
    \textbf {X}^{\ast}, &amp; \text{with probability } min \left\{ R \left( \textbf {x}^{(t+1)}, \textbf {X}^{\ast} \right), 1 \right\} \\
    \textbf {x}^{(t)}, &amp; o.w.
    \end{array}\right\}
\]</span></p>
<p>이러한 MH 알고리즘에 의해 생성된 MC가 aperiodic &amp; irreducible 이라면, 해당 체인은 정적분포로 수렴.</p>
<p>우리는 이러한 MH 체인에 의해 생성된 정적분포의 실현값들을 평균함으로써 rv의 함수의 기댓값을 구할 수 있다.</p>
<p><span class="math inline">\(E \left[ h \left( \textbf {X} \right) \right] \approx \dfrac {1} {n} \sum_{i=1}^n {h \left( \textbf {x}^{(i)} \right)},\)</span></p>
<p><span class="math inline">\(E { \left\{ h \left( \textbf {X} \right) - E \left[ h \left( \textbf {X} \right) \right] \right\} }^2\)</span>,</p>
<p><span class="math inline">\(E \{ I_{h ( \textbf {X} \le q )} \}\)</span></p>
<p>시퀀스 <span class="math inline">\(\{ \textbf x^{(\inf)} \}\)</span> 는 state space의 몇몇 포인트들의 multiple copies를 가질 수 있다는 것을 명심. 이는 <span class="math inline">\(\textbf {X}^{(t+1)}\)</span>가 제안값 <span class="math inline">\(\textbf {x}^{(\ast )}\)</span>가 아니라 <span class="math inline">\(\textbf {x}^{(t)}\)</span>를 채택했을 때 발생.</p>
<ul>
<li>Burn-in Period: 체인의 초기값에 대한 persistent dependence는 이의 성능을 심각하게 낮출 수 있다. 이는 샘플 평균을 계산할 때 체인의 초기 실현값 일부를 제하는 것으로 보정될 수 있다.</li>
</ul>
<p>consistent 결과들을 관측하기 위해 MCMC를 여러 시작점에서 각각 돌려본다.</p>
<p>잘 골라진 제안분포 $ g $가 생산하는 후보값들은 <strong>stationary 분포</strong>의 서포트를 합리적인 반복 안에서 전부 커버하고, 내놓는 후보값들이 지나치게 여러번 accepted되거나 rejected 되지 않는다.</p>
<ul>
<li>proposal 분포 $ g $ 가 지나치게 퍼져있으면, 후보값들은 자주 reject되고 체인은 타겟분포 $ f $ 의 space를 탐색하기 위해 많은 반복을 요구하게 된다.</li>
<li>proposal 분포 $ g $ 가 지나치게 모여있으면, 체인은 다회의 반복동안 타겟분포 $ f $의 한 작은 구역에 모여있게 된다. 따라서 타겟분포의 다른 영역은 적절하게 탐색되지 못한다.</li>
</ul>
<p><br /><br />
<br /><br />
<br /></p>
<div id="independent-chains" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Independent Chains<a href="markov-chain-monte-carlo.html#independent-chains" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>acceptance 여부 결정시에 ratio 자체는 MH ratio를 사용한다. 이 MC ratio에는 과거 실현값(<span class="math inline">\(x^{(t)}\)</span>)이 들어있다. 따라서 이는 MCMC 방법론에 해당한다. 하지만 새로운 value <span class="math inline">\(g(x&#39;)\)</span>을 생산할 때, 이 자체는 <span class="math inline">\(g(x&#39;\rvert x^{(t)})=g(x&#39; \rvert \cdot )\)</span>을 따르게, 즉 <span class="math inline">\(g(x&#39;\rvert x^{(t)})=g(x&#39; )\)</span> 마냥 과거의 실현값에 dependent 하지 않게 새로운 값을 생산해내는 방법론. 즉 새로이 제시되는 candidate value가 과거의 실현값들과 independent 하므로 명칭이 저러한 것이다.</p>
<p>즉 <strong>MH ratio 자체는 과거의 샘플을 이용해서</strong> MCMC 범주에 들어가나 샘플 자체는 과거의 샘플과 independent하게 생산.</p>
<p>MH 알고리즘의 제안분포는 고정된 덴시티 <span class="math inline">\(g\)</span>에 대해서 $ g ( ^{} ^{(t)} )$ 따위로 생성. 이는 <strong>independent chain</strong> 이라고 불리며, 이에 사용되는 각 후보값들은 과거에서 독립적으로 추출되었다. MH ratio는</p>
<p><span class="math display">\[
R (\textbf {x}^{(t)}, \textbf X^{\ast} )
= \dfrac
{f(\textbf X^{( \ast )}) g(\textbf x^{( t )} | \textbf X^{( \ast )})}
{f(\textbf x^{( t )}) g(\textbf X^{( \ast )} | \textbf x^{( t )}) }
= \dfrac
{f(\textbf X^{( \ast )}) g(\textbf x^{( t )})}
{f(\textbf x^{( t )}) g(\textbf X^{( \ast )}) }
=\dfrac
{\dfrac
{f(\textbf X^{( \ast )})}
{g(\textbf x^{( t )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf X^{( \ast )})}
}
=\dfrac
{\dfrac
{f(\textbf X^{( \ast )})}
{g(\textbf X^{( \ast )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf x^{( t )})}
}
= \dfrac {w^{\ast}} {w^{(t)}}
\tag{1}
\]</span></p>
<ul>
<li>importance ratio of <span class="math inline">\(X&#39;\)</span>, importance ratio of <span class="math inline">\(X^{(t)}\)</span>. This can be seen as weight also.</li>
</ul>
<p>이때 가장 우측의 등식은 importance ratio (<span class="math inline">\(r\)</span>)의 등식으로 재표현된 것이며, 이때 <span class="math inline">\(f\)</span> 는 타겟분포, <span class="math inline">\(g\)</span> 는 그것의 envelope로 본 것이다.</p>
<p><br /><br />
<br /><br />
<br /></p>
<div id="example-bayesian-inference-mixture-distribution" class="section level5 hasAnchor" number="4.2.1.1.1">
<h5><span class="header-section-number">4.2.1.1.1</span> Example: Bayesian Inference, Mixture Distribution<a href="markov-chain-monte-carlo.html#example-bayesian-inference-mixture-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>MCMC 알고리즘은 <span class="math inline">\(p(\theta \rvert y ) = c \cdot p(\theta) L(\theta \rvert y)\)</span> 로 표현될 수 있는 베이지안 추론에서 특히 강력하다. 베이지안 추론에서 <span class="math inline">\(c\)</span>의 계산이 드럽게 어렵다는 것이 이것 이상의 다른 추론전략을 방해하기 때문이다.</p>
<p>보유하는 stationary 분포가 타겟 post인 MCMC에서 생산된 샘플들은 post 모먼트, tail 확률, 그리고 다른 유용한 quantity 계산에 쓰일 수 있다.</p>
<p>independent 체인에서 prior를 proposal 분포(<span class="math inline">\(g\)</span>)로 쓰자. 즉 <span class="math inline">\(f\)</span>가 post, <span class="math inline">\(g\)</span>가 prior다. 이런다면</p>
<p><span class="math display">\[
R ({\theta}^{(t)}, \theta^{\ast} )
=\dfrac
{\dfrac
{f(\theta^{( \ast )})}
{g(\theta^{( \ast )})}
}
{\dfrac
{f(\theta^{( t )})}
{g(\theta^{( t )})}
}
=\dfrac
{c \; \cdot \; \pi( \theta^{\ast} ) L( \theta^{\ast} \rvert y )}
{c \; \cdot \; \pi( \theta^{(t)} ) L( \theta^{(t)} \rvert y )}
\cdot
\dfrac { \pi (\theta^{(t)} )}{ \pi (\theta^{\ast})}
=\dfrac
{\dfrac
{\pi (\theta^{\ast} \rvert y)}
{\pi (\theta^{\ast})}
}
{\dfrac
{\pi (\theta^(t) \rvert y)}
{\pi (\theta^{(t)})}
}
= \dfrac
{L \left( \theta^{\ast} \rvert y \right)}
{L \left( \theta^{(t)} \rvert y \right)}
\]</span></p>
<ul>
<li>Mixing Properties:
<ul>
<li>Good Mixing: 첫번째 그림의 MC는 시작점에서 빠르게 멀어지며 <span class="math inline">\(\delta\)</span>에 대한 post에서의 모든 서포트에 해당하는 패러미터 space의 모든 부분을 훑으면서 샘플을 뽑아내는게 쉬워보인다.</li>
<li>Bad Mixing: 두번째 그림은 starting value에서 멀어지는 것도 느리고, posterior support의 영역을 탐색하는 게 시원찮아보인다.</li>
</ul></li>
<li>Burn-in 이후의 실현값들을 히스토그램으로 만들어서 살펴보면 <span class="math inline">\(BETA(1,1)\)</span> proposal 덴시티를 쓴 MCMC만이 <span class="math inline">\(\delta\)</span>의 참값을 잘 모사하는듯.</li>
</ul>
<p><br /><br />
<br /><br />
<br /><br />
<br /><br />
<br /></p>
</div>
</div>
</div>
<div id="random-walk-chains-most-widely-used" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Random Walk Chains (Most Widely Used)<a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>MH method 에 해당.</p>
<p>let <span class="math inline">\(\textbf {X}^{\ast} = \textbf {X}^{(t)} + \epsilon , \epsilon \sim h(\epsilon )\)</span>. 이때 <span class="math inline">\(h\)</span>는 임의의 덴시티.</p>
<ul>
<li><span class="math inline">\(h\)</span> 로 자주 선택되는건 <span class="math inline">\(U, \textbf{N}, \text {Student&#39;s } t\)</span>.</li>
</ul>
<p>이러면 proposal 덴시티 <span class="math inline">\(g\)</span>는 어떻게 되는가? <span class="math inline">\(x^{&#39;} g \sim N( \cdot \rvert x^{(t)}, \sigma^2 )\)</span>. 가장 빈번하게 쓰이는게 <span class="math inline">\(N\)</span>이므로 <span class="math inline">\(N\)</span>으로 설명. 여기서 <span class="math inline">\(x^{(t)}\)</span>는 평균으로 사용되었고, <span class="math inline">\(\sigma^2\)</span>은 <strong>Jumping Rule</strong>에 해당한다.</p>
<ul>
<li>proposal can be
<ul>
<li>too diffused: Jumping rule is too big</li>
<li>too focused: Jumping rule is too small</li>
</ul></li>
<li>Random Walk
<ol style="list-style-type: decimal">
<li>generate <span class="math inline">\(x^{&#39;} g \sim N( \cdot \rvert x^{(t)}, \sigma^2 )\)</span></li>
<li><span class="math inline">\(u \sim U(0,1)\)</span>.</li>
<li>calculate MH ratio: <span class="math inline">\(r = \dfrac {f(x&#39;)}{f(x^{(t)}} \dfrac {g(x&#39; \rightarrow x^{(t)}} {g(x^{(t)} \rightarrow x&#39;} = \dfrac {f(x&#39;)}{f(x^{(t)}}\)</span>, cause it’s <span class="math inline">\(N\)</span>.</li>
<li>if <span class="math inline">\(u&lt;r\)</span>, <span class="math inline">\(x^{(t+1)} = x&#39;\)</span>. o.w., <span class="math inline">\(x^{(t+1)} = x^(t)\)</span>.</li>
</ol></li>
</ul>
<p><br /><br />
<br /><br />
<br /></p>
<div id="example-mixture-distribution" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Example: Mixture Distribution<a href="markov-chain-monte-carlo.html#example-mixture-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>let proposal <span class="math inline">\(\delta^{(t+1)} = \delta^{(t)} + U(-a, a)\)</span><a href="section-8.html#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. 이인즉 몇몇 proposal들은 <span class="math inline">\([0, 1]\)</span> 이외에서 생산된다.</p>
<ul>
<li>note that <span class="math inline">\(\forall \delta \notin [0,1]\)</span>, post is zero,</li>
<li>Reparameterize the problem by letting <span class="math inline">\(U = \log{\dfrac {\delta}{1-\delta}}\)</span>(logit). 왜? Probabilty Space is bounded: <span class="math inline">\(0 \le P(\cdot) \le 1\)</span>.</li>
</ul>
<p>Run a random walk chain on <span class="math inline">\(U\)</span> by adding <span class="math inline">\(U(-b,b)\)</span>.</p>
<p>Two ways of Reparamaterization:</p>
<ul>
<li>Run MCMC in <span class="math inline">\(\delta\)</span>-space. <span class="math inline">\(u\)</span> 값을 다시 T해와서 <span class="math inline">\(\delta\)</span>-space에서 돌림.</li>
<li>Run MCMC in <span class="math inline">\(u\)</span>-space. <span class="math inline">\(u\)</span>-space에서 돌리고 마지막에 모델 샘플들을 전부 <span class="math inline">\(\delta\)</span>-space로 환원.</li>
</ul>
<p><br /></p>
<div id="delta-space에서-돌리는-방법" class="section level5 hasAnchor" number="4.2.2.1.1">
<h5><span class="header-section-number">4.2.2.1.1</span> <span class="math inline">\(\delta\)</span>-space에서 돌리는 방법<a href="markov-chain-monte-carlo.html#delta-space에서-돌리는-방법" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>개략적으로는 <span class="math inline">\(T^{-1}: \delta&#39; \leftarrow u&#39; \sim g( \cdot \rvert u^{(t)})\)</span> 와 같은 형을 띤다. 조건부 proposal <span class="math inline">\(g\)</span>에서 생산된 u를 하나하나마다 <span class="math inline">\(\delta\)</span>로 역변환해서 그 하나하나의 역변환 값으로 MH 알고리즘을 돌린다. proposal 덴시티 <span class="math inline">\(g( \cdot \rvert u^{(t)} )\)</span> 는 <span class="math inline">\(\delta\)</span>-space에서의 proposal 덴시티로 변환되어야 한다. 이경우 MH ratio는</p>
<p><span class="math display">\[
R (\textbf {x}^{(t)}, \textbf X^{\ast} )
=\dfrac
{\dfrac
{f(\delta^{\ast})}
{g \left( logit(\delta^{\ast}) \rvert logit(\delta^{(t)}) \right) \cdot \left| J(\delta^{\ast})\right|}
}
{\dfrac
{f(\delta^{(t)})}
{g \left( logit(\delta^{(t)})|logit(\delta^{\ast}) \right) \cdot \left| J(\delta^{(t)})\right|}
}
\]</span></p>
<p>$J(^{(t)}) $ 는 $T: u $에 대한 <span class="math inline">\(J\)</span>를 <span class="math inline">\(\delta^{(t)}\)</span>에서 측정한 값. 주의해야 할 것이 해당 방법론에서는 <span class="math inline">\(T\)</span>한 value를 사용하였으므로 <span class="math inline">\(g\)</span>에 대한 <span class="math inline">\(J\)</span>를 구해야 한다.</p>
<p><br /></p>
</div>
<div id="u-space에서-돌리는-방법" class="section level5 hasAnchor" number="4.2.2.1.2">
<h5><span class="header-section-number">4.2.2.1.2</span> <span class="math inline">\(u\)</span>-space에서 돌리는 방법<a href="markov-chain-monte-carlo.html#u-space에서-돌리는-방법" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>이 상황에서 쓰이는 proposal은 <span class="math inline">\(\dfrac {g(u^{(t)} \rightarrow u&#39;)} {g(u&#39; \rightarrow u^{(t)})}\)</span>. <span class="math inline">\(\delta\)</span>에 대한 타겟 덴시티는 <span class="math inline">\(u\)</span>에 대한 덴시티로 변형되어야 한다. 이때 <span class="math inline">\(\delta = logit^{-1}(U) = \dfrac{\exp(U)}{1+\exp(U)}\)</span> 였으므로, <span class="math inline">\(U^{\ast} = u^{\ast}\)</span> 로 두었을 때 생산되는 MH ratio는</p>
<p><br>
<br></p>
<p>$$
R (^{(t)}, ^{} )</p>
<p>=
{
{f(logit^{-1} {(u^{})})}
{g (u<sup>{}|u</sup>{(t)}) ) * | J(u^{(t)})|}
}
{
{f(logit^{-1} {(u^{(t)})})}
{g (u<sup>{(t)}|u</sup>{()}) ) * | J(u^{})|}
}
$$</p>
<p><br>
<br></p>
<p>우리가 transform value를 사용한데가 <span class="math inline">\(f\)</span> 덴시티이므로 <span class="math inline">\(f\)</span> 덴시티에 대한 야코비안을 붙여줘야 하는데 그 덴시티에 대한 야코비안은 <span class="math inline">\(u\)</span>에 대한 야코비안이므로 쓰인 야코비안은 위와 같다. 이때 <span class="math inline">\(\lvert J(u^{\ast})\ \rvert = \dfrac {1} {\lvert J(\delta^{\ast}) \rvert}\)</span> 이므로 위와 아래에서 만들어지는 <strong>MH ratio는 같다.</strong> 따라서 두 관점은 equivalent한 체인을 생산한다.</p>
<ul>
<li>Sample paths for <span class="math inline">\(\delta\)</span> from RW chains in Ex. 7.3, run in <span class="math inline">\(u\)</span>-space iwth b=1 (top) and b=0.01 (bottom).</li>
</ul>
</div>
</div>
<div id="example-autocorrelation-plot-acf" class="section level4 hasAnchor" number="4.2.2.2">
<h4><span class="header-section-number">4.2.2.2</span> Example: Autocorrelation Plot (ACF)<a href="markov-chain-monte-carlo.html#example-autocorrelation-plot-acf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>배우지 않은 MCMC 방법론 중 하나.</p>
<p><img src="2-1.png">
<img src="2-2.png"></p>
<p>reminder. thinning을 하더라도 거의 줄지 않아서 MCMC 샘플로서 거의 가치가 없는 케이스가 존재한다.</p>
<p>no</p>
<p><br /><br />
<br /><br />
<br /><br />
<br /><br />
<br /></p>
</div>
</div>
<div id="basic-gibbs-sampler" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Basic Gibbs Sampler<a href="markov-chain-monte-carlo.html#basic-gibbs-sampler" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>need to derive the conditional density for all, 모든 joint density에 대해 coefficient를 1개씩 제한 상황의 <strong>모든 conditional density를 구한 후</strong> GS 제작이 가능
<ul>
<li>if conditional densities are not available, we can use MH algorithm when updating <span class="math inline">\(x_i\)</span> -&gt; 이런식으로 접근할 경우 이를 MH-within-Gibbs라고 부른다.
<ul>
<li>1PL IRT HW</li>
</ul></li>
</ul></li>
</ul>
<p>let <span class="math inline">\(\textbf {X} = (X_1 , \cdots, X_p )^{&#39;}\)</span> , <span class="math inline">\(\textbf {X}_{-i} = (X_1 , \cdots, X_{i-1}, X_{i+1}, \cdots, X_p )^{&#39;}\)</span>.</p>
<p>시작값 $^{(0)} $를 잡고, <span class="math inline">\(t=0\)</span>으로 설정한다. 이후 각각을 <span class="math inline">\(t+1\)</span> 단계의 시퀀스의 구성요소 각각을 <span class="math inline">\(X_i^{(t+1)} \vert \; \; \cdot \sim f \left( x_1 \rvert x_2^{(t)}, \cdots, x_p^{(t)} \right)\)</span> 에 따라서 생산한다.</p>
<p>Gibbs Sampler의 stationary 분포는 <span class="math inline">\(f\)</span>.</p>
<p><span class="math inline">\(X_i^{(t)}\)</span>의 limiting 마지널 분포는 <span class="math inline">\(i\)</span>번째 coordinate에 따른 타겟분포의 단변량 마지널化와 같다.</p>
<p>MH 알고리즘과 마찬가지로, 우리는 <span class="math inline">\(X\)</span>의 임의의 함수 <span class="math inline">\(g(X)\)</span>에 대해 <span class="math inline">\(E \left[ g(X) \right]\)</span> 를 추정하기 위해 체인에서의 실현값을 사용할 수 있다.</p>
<p><br /><br />
<br /><br />
<br /></p>
<div id="example-fur-seal-pup-capture-recapture-model" class="section level4 hasAnchor" number="4.2.3.1">
<h4><span class="header-section-number">4.2.3.1</span> Example: Fur Seal Pup Capture-Recapture Model<a href="markov-chain-monte-carlo.html#example-fur-seal-pup-capture-recapture-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>1800년대 후반 (by the late 1800s) 뉴질랜드 물범은 거의 전멸했다가 요즘 들어 폭증함 (abundance). 물범의 고름 숫자를 capture-recapture 사용해서 해보자.</p>
<p>사이즈 불명인 모집단의 크기 파악 위에 반복 연구 실행. 각 연구마다 <strong><em>포획</em></strong>었던 개체는 표식 새기고 풀어줌. 후속 연구에서 또 포획되면 <strong><em>재포획</em></strong>으로 표기. 높은 재포획 비율은 참 모집단 사이즈값이 포획되었던 개체들의 총량을 크게 넘지 않을 것임을 암시.</p>
<ul>
<li><span class="math inline">\(N\)</span>: 불명인 모집단 사이즈. <span class="math inline">\(l\)</span>회의 조사 통해 얻어진 각 회의 총 <strong><em>포획</em></strong> 숫자는 각각 <span class="math inline">\(c=(c_1, \cdots, c_l)\)</span>로 저장. 모집단 사이즈는 샘플링 동안에는 변동 없다(죽음, 출생, 이주 없음 inconsequential)고 가정한다.</li>
<li><span class="math inline">\(r\)</span>: 연구 동안에 포획되었던 이질 동물들의 총 숫자.</li>
<li>각 연구 시도에서 상응하는 구분되고 알려지지 않은 <strong><em>포획 확률</em></strong>은 <span class="math inline">\(\alpha = (\alpha_1 , \cdots, \alpha_l )\)</span>. 이 모델은 모든 동물들이 각 1회의 포획 발생에서 잡힐 가능성 자체는 각각의 동물에 대해서 동일하나, 이 被포획 확률은 시간이 지남에 따라 변할 수 있다는 것을 말함.</li>
</ul>
<p>이 모델의 likelihood는</p>
<p><span class="math display">\[
L \left( N, \alpha \rvert c, r \right)
\propto
\dfrac {N!}{(n-r)!} \prod_{=1}^{l} \alpha_i^{c_i}
\]</span>
Fur Seal Data for Seven Studies in One Season on the Otago Peninsula가 주어졌으며, prior은 <span class="math inline">\(\pi(N) \propto 1\)</span>, <span class="math inline">\(\pi (\alpha_i ) = BETA(\theta_1 , \theta_2)\)</span> 이다. 계산하라.</p>
<p>해당 모델의 conditional posterior distribution에서 시뮬레이트하는 것으로 Gibbs Sampler를 제작할 수 있다.</p>
<p><span class="math display">\[
N^{(t+1)}-r \rvert \cdot \sim Negative Binomial \left( r+1, 1- \prod_{i=1}^7 \left( 1- \alpha_i^{(t)} \right) \right)
\]</span></p>
<p><span class="math display">\[
\alpha_i^{(t+1)} \rvert \cdot \sim BETA \left( c_i + \dfrac{1}{2}, N^{(t+1)} - c_i + \dfrac{1}{2} \right)
\]</span></p>
<p>for <span class="math inline">\(i= 1, \cdots, 7\)</span>, <span class="math inline">\(r = \sum_{i=1}^7 {m_i} =84\)</span>. 이는 unique fur seals were observed during the sampling period.</p>
<ul>
<li>Split boxplots of <span class="math inline">\(\bar {\alpha}^{(t)}\)</span> against $N^{(t)} $ for the seal pup example.</li>
<li>Estimated marginal posterior probabilities for <span class="math inline">\(N\)</span> for the seal pup example.</li>
</ul>
<p><br /><br />
<br /><br />
<br /></p>
</div>
<div id="mh-within-gibbs-sampler" class="section level4 hasAnchor" number="4.2.3.2">
<h4><span class="header-section-number">4.2.3.2</span> MH-within-Gibbs Sampler<a href="markov-chain-monte-carlo.html#mh-within-gibbs-sampler" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>실제 implementation에 무지막지 유용하다.</strong> 이는 각각의 사이클을 GS의 사이클로 만들어놓고, conditional density는 MH 알고리즘으로 획득하는 것이다. <strong>Gibbs Sampler는 MH Sampler의 특별한 경우라고 볼 수 있다.</strong> MH 알고리즘의 proposal 분포를 시간에 따라 변화하도록 함으로써, GS와 MH 알고리즘 사이에 연결고리가 생긴다. 각 Gibbs 사이클은 <span class="math inline">\(p\)</span> 개의 MH 스텝으로 구성되어 있다. 사이클 내에서의 <span class="math inline">\(i\)</span>번째 Gibbs 스텝은, 체인의 현 상태 <span class="math inline">\((x_{1}^{(t+1)}, \cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\underline{t})}, \cdots, x_{p}^{(\underline{t})})\)</span> 가 주어졌을 때, 효과적으로 후보 벡터 <span class="math inline">\((x_{1}^{(t+1)}, \cdots, x_{i-1}^{(t+1)}, {\underline{X_i^{*}}}, x_{i+1}^{(\underline{t})}, \cdots, x_{p}^{(\underline{t})})\)</span> 를 생산한다. 밑줄 차이점에 주목.</p>
<p><span class="math inline">\(i\)</span>번째 단변량 Gibbs 업데이트는 이하와 같이 MH 스텝 drawing으로 볼 수 있다.</p>
<p><span class="math display">\[
{\underline{X_i^{*}}} \rvert  (x_{1}^{(t+1)}, \cdots, x_{i-1}^{(t+1)}, x_{i+1}^{(\underline{t})}, \cdots, x_{p}^{(\underline{t})}) \sim g_i \left( \cdot \rvert (x_{1}^{(t+1)}, \cdots, x_{i-1}^{(t+1)},  x_{i+1}^{(\underline{t})}, \cdots, x_{p}^{(\underline{t})}) \right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
g_i \left( \cdot \rvert (x_{1}^{(t+1)}, \cdots, x_{i-1}^{(t+1)},  x_{i+1}^{(\underline{t})}, \cdots, x_{p}^{(\underline{t})})\right)
= \left\{
\begin{array}{@{}lr@{}}
f(x_i^{*} \rvert x_i^{(t)} ), &amp; \text{if } \; \; \; X_i^{*} = x_i^{(t)} \\
0 &amp;  o.w.
\end{array}\right\}
\]</span></p>
<p>이 경우, MH ratio는 1과 같아진다. 즉슨 모든 후보들은 언제나 accept 된다. 즉슨 GS는 MH 알고리즘에서 acceptance ratio가 항상 1인 경우에 해당한다. 따라서 <strong>conditional density</strong>을 구할 수만 있으면 GS를 사용하는게 좋다. 샘플을 버릴 필요가 없고, 버려지는 샘플이 없기 때문.</p>
<p><br /><br />
<br /><br />
<br /></p>
</div>
<div id="update-ordering" class="section level4 hasAnchor" number="4.2.3.3">
<h4><span class="header-section-number">4.2.3.3</span> Update Ordering<a href="markov-chain-monte-carlo.html#update-ordering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Random Scan Gibbs Sampling</strong>: 기본 GS에서 $ $ 에 가해지는 업데이트의 순서는 <strong><em>한 사이클에서 다음 사이클로 넘어갈 때마다 바뀔 수 있다. 패러미터들이 높은 수준에서 상호연관되어있을 경우, 각 사이클을 랜덤하게 순서배치하는 것은 효과적일 수 있다.<a href="section-8.html#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></em></strong> 특정 모델에 대한 전문화된 지식이 없다면, 한 이터레이션에서 다음으로 넘어갈 때 패러미터끼리 높이 상호연관되어있다면 deterministic 한 방법과 RSGS 양쪽 모두를 시도해보는 것이 권장된다.</p>
</div>
<div id="blocking" class="section level4 hasAnchor" number="4.2.3.4">
<h4><span class="header-section-number">4.2.3.4</span> Blocking<a href="markov-chain-monte-carlo.html#blocking" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>with <span class="math inline">\(p=4\)</span>, e.g., 각 사이클을 다음 절차를 따르면서 업데이트:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_1^{(t+1)}\rvert \cdot \sim f \left(x_1 \rvert x_2^{(t)}, x_3^{(t)}, x_4^{(t)} \right)\)</span>.</li>
<li><span class="math inline">\(X_2^{(t+1)}, X_3^{(t+1)}\rvert \cdot \sim f \left(x_2, x_3 \rvert x_1^{(t+1)}, x_4^{(t)} \right)\)</span>.</li>
<li><span class="math inline">\(X_4^{(t+1)}\rvert \cdot \sim f \left(x_4 \rvert x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)} \right)\)</span>.</li>
</ol>
<p><strong>Blocing</strong>은 <span class="math inline">\(X\)</span>의 구성요소들이 서로 상관관계가 있을 때 <strong><em><span class="math inline">\(X_i\)</span> 내부가 상관이라는 건가, 아니면 <span class="math inline">\(X_t, X_{t+1}\)</span> 이 상관이라는 건가?</em></strong> 유용함. 해당 알고리즘을 통해 더욱 상관된 구성요소끼리는 한 블럭 안에서 샘플링됨.</p>
</div>
<div id="hybrid-gibbs-sampling" class="section level4 hasAnchor" number="4.2.3.5">
<h4><span class="header-section-number">4.2.3.5</span> Hybrid Gibbs Sampling<a href="markov-chain-monte-carlo.html#hybrid-gibbs-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>하나 이상의 <span class="math inline">\(X\)</span>에 대한 조건부 분포는 대부분 closed form으로 만들 수 없음. 깁스 샘플러의 주어진 스텝에서, 적절한 조건부 분포에서 샘플링하기 위해 MH 알고리즘이 쓰인다면 <strong>Hybrid MCMC</strong> 알고리즘이 완성됨.</p>
<p>with <span class="math inline">\(p=5\)</span>, e.g., 하이브리드 MCMC 알고리즘은 다음 절차를 따르면서 업데이트:</p>
<ol style="list-style-type: decimal">
<li>Update <span class="math inline">\(X_1^{(t+1)} \rvert \left( x_2^{(t)}, x_3^{(t)}, x_4^{(t)}, x_5^{(t)} \right)\)</span> with 깁스 스텝.</li>
<li>Update ( x_2^{(t+1)}, x_3^{(t+1)} ) $ ( x_1^{(t+1)}, x_4^{(t)}, x_5^{(t)} )$ with MH 스텝.</li>
<li>Update <span class="math inline">\(X_4^{(t+1)} \rvert \left( x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)}, x_5^{(t)} \right)\)</span> with 랜덤워크.</li>
<li>Update <span class="math inline">\(X_5^{(t+1)} \rvert \left( x_1^{(t+1)}, x_2^{(t+1)}, x_3^{(t+1)}, x_4^{(t+1)} \right)\)</span> with 깁스 스텝.</li>
</ol>
<div id="example-fur-seal-pup-capture-recapture-study" class="section level5 hasAnchor" number="4.2.3.5.1">
<h5><span class="header-section-number">4.2.3.5.1</span> Example: Fur Seal Pup Capture-Recapture Study<a href="markov-chain-monte-carlo.html#example-fur-seal-pup-capture-recapture-study" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><br>
<br>
<br>
<br></p>
</div>
</div>
</div>
<div id="implementation" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Implementation<a href="markov-chain-monte-carlo.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>MCMC의 목적은 타겟분포 <span class="math inline">\(f\)</span>의 특징들을 알아내는 것. 모든 MCMC는 정답인 limiting 정적분포를 가지고 있음. 실전에는 체인을 얼마나 충분히 오래 돌릴지를 결정하는 게 중요함. <span class="math inline">\(X\)</span>의 dimensionality(차원)이 높다면 수렴이 엄청 느려서 엄청 긴 run을 요할 수도 있음. 이하의 요건들을 생각해서 long run을 결정해야 함.</p>
<ul>
<li>Has the chain run long enough?</li>
<li>Is the first portion of the chain highly influenced by the starting value?</li>
<li>Should the chain be run from several different starting values?</li>
<li>Has the chain traversed all portions of the region of support of <span class="math inline">\(g\)</span>?</li>
<li>Are the sampled values approximate draws from <span class="math inline">\(f\)</span>?</li>
<li>How shall the chain output be used to produce estimates and assess their precision?</li>
</ul>
<p><br>
<br></p>
<div id="ensuring-good-mixing-and-convergence" class="section level4 hasAnchor" number="4.2.4.1">
<h4><span class="header-section-number">4.2.4.1</span> Ensuring Good Mixing and Convergence<a href="markov-chain-monte-carlo.html#ensuring-good-mixing-and-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>MCMC 알고리즘이 대상 문제에 얼마나 쓸만한 정보를 주는지 고민해야 함. 이는 곧</p>
<ol style="list-style-type: decimal">
<li>체인이 얼마나 빠르게 체인의 starting value를 까먹는가</li>
<li>얼마나 빠르게 체인이 타겟분포 <span class="math inline">\(f\)</span>의 모든 서포트를 훑는가</li>
<li>체인이 그것의 정적분포에 근사적으로나마 닿는가를 고민.</li>
<li>There is substantial overlap between the goals of diagnosing convergence to the stationary distribution and investigating the mixing properties of the chain.</li>
</ol>
<p><br>
<br></p>
</div>
<div id="simple-graphical-diagnostics" class="section level4 hasAnchor" number="4.2.4.2">
<h4><span class="header-section-number">4.2.4.2</span> Simple Graphical Diagnostics<a href="markov-chain-monte-carlo.html#simple-graphical-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>트레이스 플롯 (sample path)은 이터레이션 횟수 <span class="math inline">\(t\)</span>와 <span class="math inline">\(X^{(t)}\)</span>의 실현값 간의 플롯이다.</p>
<ul>
<li>체인의 mixing이 구리면 이는 장기간의 이터레이션동안 동일값 근처에서 머무르게 됨.</li>
<li>체인의 mixing이 좋으면 시작값에서 빠르게 떠나서 <span class="math inline">\(f\)</span>의 서포트에 해당하는 영역을 열심히 훑음.</li>
</ul>
<p><br>
<br></p>
<p><strong>autocorrelation 플롯</strong>은 <span class="math inline">\(X^{(t)}\)</span>의 시퀀스에서 다른 <strong>이터레이션 래그</strong>에서의 상관관계를 서술한다. 래그 <span class="math inline">\(i\)</span>에서의 autocorrelation은 <span class="math inline">\(i\)</span> 이터레이션만큼 떨어진 이터레이트 간의 상관관계이다. 구린 mixing 프로퍼티를 가지는 체인은 이터레이션 간의 래그가 증가하더라도 autocorrelation의 부식(decay)이 느림.</p>
<p>MCMC 체인에서의 첫 <span class="math inline">\(D\)</span> 개의 값은 보통 <strong>burn-in period</strong>라고 해서 버려짐. 체인의 시작점에 대한 의존이 강하게 남아있을지도 모르기 때문. 어느정도가 적절한 번인 피리어드인가는 <strong>Gelman-Rubin diagnostics</strong>에 의해 결정된다. 결정된 번인 피리어드가 제대로 된 값을 못내면 <span class="math inline">\(D\)</span>가 늘어나던가 <span class="math inline">\(L\)</span>이 늘어나던가 둘다 늘리던가 해야함.
- Motivated by an Analysis of Variance
- 사슬간 분산 (between-chain variance)이 사슬내 분산 (within-chain variance)보다 유의하게 크면 번인 피리어드나 MCMC 길이가 늘어나야 함
- Difficulties
- multimodal <span class="math inline">\(f\)</span>에서 적절한 스타팅 밸류를 찾는건 어렵고, 체인이 로컬 region이나 mode에서 갇힐수 있음
- 이의 단일차원성 (uni-dimensionality) 때문에 타겟분포 <span class="math inline">\(f\)</span>가 멀티디멘션이면 타겟분포의 수렴에 대한 정보에 대한 잘못된 직관을 줘버릴 수 있음</p>
<p><br>
<br></p>
<p>proposal <span class="math inline">\(g\)</span> 선택할 때 고려해야할 요소는? mixing은 proposal 분포 <span class="math inline">\(g\)</span>의 특질에 큰 영향을 받으며 특히 이의 스프레드(spread)에 큰 영향을 받음. 타겟분포 <span class="math inline">\(f\)</span>와 <span class="math inline">\(g\)</span>간의 닮음에 있어서, 프로포절 <span class="math inline">\(g\)</span>의 tail behavior의 닮음은 high density의 닮음보다 훨씬 중요함. <span class="math inline">\(f/g\)</span>가 bounded 라면, MC의 정적분포로의 수렴은 <strong>대체로(overall)</strong> 빠름. 실전에선 정보를 줄 수 있는 이터레이티브 프로세스를 통해 proposal 분포의 분산이 택해질 수 있음. (In practice, the variance of the proposal distribution can be selected through an informal iterative process.) 20%~50% 사이의 acceptance rate가 선호되어야 함.</p>
<p><br>
<br></p>
<ul>
<li><strong>Reparameterization</strong></li>
</ul>
<p>모델의 reparameterization은 MCMC 알고리즘의 mixing behavior에 상당한 기능향상을 가져올 수 있음. reparameterization은 dependence를 낮추기 위해 가장 우선되어야 할 전략 중 하나임. 서로 다른 모델들은 서로 다른 reparameterization 전략을 적용해야 함. reparameterization 접근법은 보통 특정 모델에 대한 원오프로서 채택되므로 일반화된 조언을 하기에는 어려운 부분이 있음.</p>
<p><br>
<br></p>
<ul>
<li><strong>Comparing Chains</strong></li>
</ul>
<p>MCMC 실현값이 크게 상관관계있다면, MCMC의 각 이터레이션에서 주어지는 정보는 run length에서 주어지는 정보 대비 보잘것 없다(will be less than suggested by the run length). 여기서 reduced information은 <strong>effective sample size</strong>라고 칭해지는 더 작은 iid 샘플에 담겨있는 정보와 동등하다. 여기서 샘플의 총 숫자와 effective sample size 사이의 차이는 잃게 된 효율을 의미한다. 관심있는 변량을 확인하기 위해 우리가 MC 체인에서의 correlated 샘플들을 사용했을 때 잃게된 효율.</p>
<p><br>
<br></p>
<ul>
<li><strong>Number of Chains</strong></li>
</ul>
<p>모델 진단에 있어서 가장 진단을 어렵게 하는 부분은 체인이 타겟분포 <span class="math inline">\(f\)</span>의 1개 이상의 모드에 걸리냐 안걸리냐 하는 부분. 이 경우 모든 수렴진단은 체인이 수렴하긴 한다는 결론을 내리게 된다. 정작 체인이 타겟분포 <span class="math inline">\(f\)</span>의 모든 특성을 나타내지 못하는데도. 그래서 여러번 돌려보게 되는 것. 최소한 그 여러번의 run들 중 체인 1개에서라도 타겟분포 <span class="math inline">\(f\)</span>의 모든 흥미로운 특질들이 드러났으면 하니까. 이러한 특질을을 찾아내는데 실패한 개별 체인들의 경우에는 mixing을 더 좋게 만들기 위해 체인 길이가 늘어나거나 문제가 reparameterize 되어야 함.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="importance-sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-mcmc-wk08.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/211202_MCMC.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
