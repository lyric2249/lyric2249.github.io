<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.5 Estimation | Self-Study</title>
  <meta name="description" content="7.5 Estimation | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7.5 Estimation | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="https://github.com/lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.5 Estimation | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distributions-of-quadratic-forms.html"/>
<link rel="next" href="one-way-anova.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="part"><span><b>I 20-02</b></span></li>
<li class="chapter" data-level="1" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>1</b> Categorical</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Data Type and Statistical Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian</a>
<ul>
<li class="chapter" data-level="2.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>2.1</b> Abstract</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>2.1.1</b> 변수의 독립성</a></li>
<li class="chapter" data-level="2.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>2.1.2</b> 교환가능성</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>2.2</b> Continual Aeassessment Method</a></li>
<li class="chapter" data-level="2.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>2.3</b> Horseshoe Prior</a></li>
</ul></li>
<li class="part"><span><b>II 21-01</b></span></li>
<li class="chapter" data-level="3" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>3</b> Mathematical Stats</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3.1</b> Inference</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>3.1.1</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="completeness.html"><a href="completeness.html"><i class="fa fa-check"></i><b>3.2</b> Completeness</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="completeness.html"><a href="completeness.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>3.2.1</b> 레만-쉐페 thm.</a></li>
<li class="chapter" data-level="3.2.2" data-path="completeness.html"><a href="completeness.html#rao-blackwell-thm.-1"><i class="fa fa-check"></i><b>3.2.2</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Test</a></li>
<li class="chapter" data-level="3.4" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>3.4</b> Power Fucntion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>3.4.1</b> Significance Probability (p-value)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>3.5</b> Optimal Testing Method</a></li>
<li class="chapter" data-level="3.6" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>3.6</b> Data Reduction</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>3.6.1</b> Sufficiency Principle</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>3.7</b> Borel Paradox</a></li>
<li class="chapter" data-level="3.8" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>3.8</b> Neyman–Pearson lemma</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>3.8.1</b> Overview</a></li>
<li class="chapter" data-level="3.8.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>3.8.2</b> Generalized LRT</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>3.9</b> 개념</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>4</b> MCMC</a>
<ul>
<li class="chapter" data-level="4.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Importance Sampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Independent Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>4.2</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>4.2.1</b> MH Algorithm</a></li>
<li class="chapter" data-level="4.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>4.2.2</b> Random Walk Chains (Most Widely Used)</a></li>
<li class="chapter" data-level="4.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>4.2.3</b> Basic Gibbs Sampler</a></li>
<li class="chapter" data-level="4.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>4.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>4.3</b> Advanced MCMC (wk08)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>4.3.1</b> 1. Data Augmentation</a></li>
<li class="chapter" data-level="4.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>4.3.2</b> 2. Hit-and-Run Algorithm</a></li>
<li class="chapter" data-level="4.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>4.3.3</b> 3. Metropolis-Adjusted Langevin Algorithm</a></li>
<li class="chapter" data-level="4.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>4.3.4</b> 4. Multiple-Try Metropolis Algorithm</a></li>
<li class="chapter" data-level="4.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>4.3.5</b> 5. Reversible Jump MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>4.4</b> Auxiliary Variable MCMC</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Multimodal Target Distribution</a></li>
<li class="chapter" data-level="4.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>4.4.3</b> Doubly-intractable Normalizing Constants</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>4.5.1</b> Simulator-Based Models</a></li>
<li class="chapter" data-level="4.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>4.5.2</b> ABCifying Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>4.5.3</b> ABC-MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>4.6</b> Hamiltonian Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.6.1</b> Introduction to Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>4.7</b> Population Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>4.7.1</b> Adaptive Direction Sampling</a></li>
<li class="chapter" data-level="4.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>4.7.2</b> Conjugate Gradient MC</a></li>
<li class="chapter" data-level="4.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>4.7.3</b> Parallel Tempering</a></li>
<li class="chapter" data-level="4.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>4.7.4</b> Evolutionary MC</a></li>
<li class="chapter" data-level="4.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>4.7.5</b> Sequential Parallel Tempering</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>4.8</b> Stochastic Approximation Monte Carlo</a></li>
<li class="chapter" data-level="4.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>4.9</b> Review</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>4.9.1</b> Wk01</a></li>
<li class="chapter" data-level="4.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>4.9.2</b> wk03</a></li>
<li class="chapter" data-level="4.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>4.9.3</b> wk04, 05</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>4.10</b> Else</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>4.10.1</b> Hw4. Rasch Model</a></li>
<li class="chapter" data-level="4.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>4.10.2</b> DA) Example: MVN</a></li>
<li class="chapter" data-level="4.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>4.10.3</b> Bayesian adaptive clinical trial with delayed outcomes</a></li>
<li class="chapter" data-level="4.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>4.10.4</b> NMAR의 종류</a></li>
<li class="chapter" data-level="4.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>4.10.5</b> wk10) Bayesian Model Selection</a></li>
<li class="chapter" data-level="4.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>4.10.6</b> Autologistic model</a></li>
<li class="chapter" data-level="4.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>4.10.7</b> wk10) Bayesian Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>5</b> MVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>5.1</b> Overview of mva (not ended)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>5.1.1</b> Notation</a></li>
<li class="chapter" data-level="5.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>5.1.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Inference on Correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>5.1.4</b> Standardization</a></li>
<li class="chapter" data-level="5.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>5.1.5</b> Missing Value Treatment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>5.2</b> Multivariate Nomral (wk2)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>5.2.1</b> Overview</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>5.2.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="5.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>5.2.3</b> Properties of MVN</a></li>
<li class="chapter" data-level="5.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>5.2.5</b> Linear Combination of Random Vectors</a></li>
<li class="chapter" data-level="5.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Normal Likelihood</a></li>
<li class="chapter" data-level="5.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>5.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span></a></li>
<li class="chapter" data-level="5.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>5.2.8</b> Assessing Normality</a></li>
<li class="chapter" data-level="5.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>5.2.9</b> Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>5.3</b> Inference about Mean Vector (wk3)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>5.3.2</b> 1. Confidence Region</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>5.3.3</b> 2. Simultaneous CI</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>5.3.4</b> 3. Note: Bonferroni Multiple Comparison</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>5.3.5</b> 4. Large Sample Inferences about a Mean Vector</a></li>
<li class="chapter" data-level="5.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>5.3.6</b> 1. Profile Analysis (wk4, 5)</a></li>
<li class="chapter" data-level="5.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>5.3.7</b> 2. Test for Linear Trend</a></li>
<li class="chapter" data-level="5.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>5.3.8</b> 3. Inferences about a Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>5.4</b> Comparison of Several MV Means (wk5)</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>5.4.1</b> Paired Comparison</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>5.4.2</b> Comparing Mean Vectors from Two Populations</a></li>
<li class="chapter" data-level="5.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>5.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)</a></li>
<li class="chapter" data-level="5.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>5.4.4</b> Comparing Several Multivariate Population Means</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>5.5</b> Multivariate Multiple Regression (wk6)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>5.5.1</b> Overview</a></li>
<li class="chapter" data-level="5.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>5.5.2</b> Multivariate Multiple Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>5.6</b> PCA</a></li>
<li class="chapter" data-level="5.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>5.7</b> Factor</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>5.7.1</b> Method of Estimation</a></li>
<li class="chapter" data-level="5.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>5.7.2</b> Factor Rotation</a></li>
<li class="chapter" data-level="5.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>5.7.3</b> Varimax Criterion</a></li>
<li class="chapter" data-level="5.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>5.7.4</b> Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>5.8</b> Discrimination and Classification</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>5.8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="5.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>5.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations</a></li>
<li class="chapter" data-level="5.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>5.8.3</b> Evaluating Classification Functions</a></li>
<li class="chapter" data-level="5.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>5.8.4</b> Classification with several Populations (wk13)</a></li>
<li class="chapter" data-level="5.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>5.8.5</b> Other Discriminant Analysis Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>5.9</b> Clustering, Distance Methods, and Ordination</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>5.9.1</b> Overview</a></li>
<li class="chapter" data-level="5.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.9.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>5.9.3</b> K-means Clustering</a></li>
<li class="chapter" data-level="5.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>5.9.4</b> 군집의 평가방법</a></li>
<li class="chapter" data-level="5.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>5.9.5</b> Clustering using Density Estimation (wk14)</a></li>
<li class="chapter" data-level="5.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>5.9.6</b> Multidimensional Scaling (MDS)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>6.1</b> SVD</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="svd.html"><a href="svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>6.1.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="6.1.2" data-path="svd.html"><a href="svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>6.1.2</b> Singular value Decomposition: General-version</a></li>
<li class="chapter" data-level="6.1.3" data-path="svd.html"><a href="svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>6.1.3</b> Singular value Decomposition: Another-version</a></li>
<li class="chapter" data-level="6.1.4" data-path="svd.html"><a href="svd.html#quadratic-forms"><i class="fa fa-check"></i><b>6.1.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="6.1.5" data-path="svd.html"><a href="svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>6.1.5</b> Partitioned Matrices</a></li>
<li class="chapter" data-level="6.1.6" data-path="svd.html"><a href="svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>6.1.6</b> Geometrical Aspects</a></li>
<li class="chapter" data-level="6.1.7" data-path="svd.html"><a href="svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>6.1.7</b> Column, Row and Null Space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="what.html"><a href="what.html"><i class="fa fa-check"></i><b>7.1</b> What</a></li>
<li class="chapter" data-level="7.2" data-path="random-vectors-and-matrices.html"><a href="random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>7.2</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="7.3" data-path="multivariate-normal-distributions.html"><a href="multivariate-normal-distributions.html"><i class="fa fa-check"></i><b>7.3</b> Multivariate Normal Distributions</a></li>
<li class="chapter" data-level="7.4" data-path="distributions-of-quadratic-forms.html"><a href="distributions-of-quadratic-forms.html"><i class="fa fa-check"></i><b>7.4</b> Distributions of Quadratic Forms</a></li>
<li class="chapter" data-level="7.5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>7.5</b> Estimation</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>7.5.1</b> Identifiability and Estimability</a></li>
<li class="chapter" data-level="7.5.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>7.5.2</b> Estimation: Least Squares</a></li>
<li class="chapter" data-level="7.5.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>7.5.3</b> Estimation: Best Linear Unbiased</a></li>
<li class="chapter" data-level="7.5.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>7.5.4</b> Estimation: Maximum Likelihood</a></li>
<li class="chapter" data-level="7.5.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>7.5.5</b> Estimation: Minimum Variance Unbiased</a></li>
<li class="chapter" data-level="7.5.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>7.5.6</b> Sampling Distributions of Estimates</a></li>
<li class="chapter" data-level="7.5.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>7.5.7</b> Generalized Least Squares(GLS)</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>7.6</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>7.6.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="7.6.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>7.6.2</b> More About Models</a></li>
<li class="chapter" data-level="7.6.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>7.6.3</b> Estimating and Testing Contrasts</a></li>
<li class="chapter" data-level="7.6.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.6.4</b> Cochran’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>7.7</b> Testing</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>7.7.1</b> More About Models: Two approaches for linear model</a></li>
<li class="chapter" data-level="7.7.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>7.7.2</b> Testing Models</a></li>
<li class="chapter" data-level="7.7.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>7.7.3</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="7.7.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>7.7.4</b> Testing Linear Parametric Functions</a></li>
<li class="chapter" data-level="7.7.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>7.7.5</b> Theoretical Complements</a></li>
<li class="chapter" data-level="7.7.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>7.7.6</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="7.7.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>7.7.7</b> Testing Single Degrees of Freedom in a Given Subspace</a></li>
<li class="chapter" data-level="7.7.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>7.7.8</b> Breaking SS into Independent Components</a></li>
<li class="chapter" data-level="7.7.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>7.7.9</b> General Theory</a></li>
<li class="chapter" data-level="7.7.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>7.7.10</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="7.7.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>7.7.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="7.7.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>7.7.12</b> Tests for Generalized Least Squares Models</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>7.8</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>7.8.1</b> A direct solution via inner products</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>7.9</b> Flat</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>7.9.1</b> 1.Flat</a></li>
<li class="chapter" data-level="7.9.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>7.9.2</b> 2. Solutions to systems of linear equations</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>7.10</b> Unified Approach to Balanced ANOVA Models</a></li>
</ul></li>
<li class="part"><span><b>III 21-02</b></span></li>
<li class="chapter" data-level="8" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>8</b> Survival Analysis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>8.2</b> </a></li>
<li class="chapter" data-level="8.3" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>8.3</b> </a></li>
<li class="chapter" data-level="8.4" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>8.4</b> </a></li>
<li class="chapter" data-level="8.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>8.5</b> Cox Regression</a></li>
<li class="chapter" data-level="8.6" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>8.6</b> Concepts</a></li>
</ul></li>
<li class="appendix"><span><b>00-00</b></span></li>
<li class="chapter" data-level="A" data-path="r-bookdown.html"><a href="r-bookdown.html"><i class="fa fa-check"></i><b>A</b> R Bookdown</a>
<ul>
<li class="chapter" data-level="A.1" data-path="tutorial.html"><a href="tutorial.html"><i class="fa fa-check"></i><b>A.1</b> Tutorial</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="tutorial.html"><a href="tutorial.html#about"><i class="fa fa-check"></i><b>A.1.1</b> About</a></li>
<li class="chapter" data-level="A.1.2" data-path="tutorial.html"><a href="tutorial.html#hello-bookdown"><i class="fa fa-check"></i><b>A.1.2</b> Hello bookdown</a></li>
<li class="chapter" data-level="A.1.3" data-path="tutorial.html"><a href="tutorial.html#cross-references"><i class="fa fa-check"></i><b>A.1.3</b> Cross-references</a></li>
<li class="chapter" data-level="A.1.4" data-path="tutorial.html"><a href="tutorial.html#parts"><i class="fa fa-check"></i><b>A.1.4</b> Parts</a></li>
<li class="chapter" data-level="A.1.5" data-path="tutorial.html"><a href="tutorial.html#footnotes-and-citations"><i class="fa fa-check"></i><b>A.1.5</b> Footnotes and citations</a></li>
<li class="chapter" data-level="A.1.6" data-path="tutorial.html"><a href="tutorial.html#blocks"><i class="fa fa-check"></i><b>A.1.6</b> Blocks</a></li>
<li class="chapter" data-level="A.1.7" data-path="tutorial.html"><a href="tutorial.html#sharing-your-book"><i class="fa fa-check"></i><b>A.1.7</b> Sharing your book</a></li>
<li class="chapter" data-level="" data-path="tutorial.html"><a href="tutorial.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="noname.html"><a href="noname.html"><i class="fa fa-check"></i><b>B</b> NoName</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Estimation</h2>
<p>이하와 같은 linear model 고려. 이때 <span class="math inline">\(x_i &#39;\)</span>는 <span class="math inline">\(X\)</span>의 i번째 row vector이며, <span class="math inline">\(E(\epsilon)=0, \; Cov(\epsilon)=\sigma^2 I = \sigma^2 \Sigma\)</span>.</p>
$
Y_{n } = X_{n p} <em>{p } + </em>{n } =
<span class="math display">\[\begin{pmatrix} x_i &#39;  \beta \end{pmatrix}\]</span>
<ul>
<li>$</li>
</ul>
<p><br/>
<br/></p>
<div id="identifiability-and-estimability" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Identifiability and Estimability</h3>
<div id="identifiable" class="section level5" number="7.5.1.0.1">
<h5><span class="header-section-number">7.5.1.0.1</span> Identifiable</h5>
<p>모델에서의 무한한 갯수의 관측치를 보유한다면, 모델의 underlying 패러미터의 참값을 획득하는 것이 가능한 성질.</p>
<p>A general linear model is a parameterization</p>
<p>$
<span class="math display">\[\begin{align}
E(Y) &amp;= f(X) \\
&amp;= E(X\beta + \epsilon)\\
&amp;= X\beta + E(\epsilon) \\
&amp;= X\beta  + 0 \\
&amp;= X\beta  

\end{align}\]</span>
$</p>
<p>The parameter <span class="math inline">\(\beta\)</span> is <strong>identifiable</strong> if for any <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> <span class="math inline">\(f(\beta_1) = f(\beta_2)\)</span> implies <span class="math inline">\(\beta_1 = \beta_2\)</span>. If <span class="math inline">\(\beta\)</span> is identifiable, we say that the parameterization <span class="math inline">\(f(\beta)\)</span> is identifiable. (패러미터 <span class="math inline">\(\beta\)</span>가 identifiable하다면, 우리는 해당 패러미터의 parameterization <span class="math inline">\(f(\beta)\)</span> 또한 identifiable 하다) Moreover, a vector-valued function <span class="math inline">\(g(\beta)\)</span> is identifiable if <span class="math inline">\(f (\beta_1) = f(\beta_2)\)</span> implies <span class="math inline">\(g (\beta_1) = g(\beta_2)\)</span>.</p>
<p>For regression models for which <span class="math inline">\(r(X) = p\)</span>, the parameters are identifiable: <span class="math inline">\(X&#39;X\)</span> is nonsingular, so if <span class="math inline">\(X\beta_1 = X\beta_2\)</span>, then</p>
<p>$
_1 = (X’X)^{-1} X’X _1 = (X’X)^{-1} X’X _2 = _2
$</p>
<p>A function <span class="math inline">\(g(\beta)\)</span> is identifiable <span class="math inline">\(\iff\)</span> <span class="math inline">\(g(\beta)\)</span> is a function of <span class="math inline">\(f(\beta)\)</span>.</p>
<p><br/>
<br/>
<br/></p>
</div>
<div id="estimable" class="section level5" number="7.5.1.0.2">
<h5><span class="header-section-number">7.5.1.0.2</span> Estimable</h5>
<p>The results in the last section suggest that some linear combinations of <span class="math inline">\(\beta\)</span> in the less than full rank case will not be estimable.</p>
<p>The linear parametric function <span class="math inline">\(c&#39;β\)</span> is an <strong>estimable</strong> function if there exists a vector <span class="math inline">\(a \in \mathbb{R}^n\)</span> such that <span class="math inline">\(\forall \beta: E(a &#39; y ) = c &#39; \beta\)</span>.</p>
<p>A vector-valued linear function of <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\Lambda &#39; \beta\)</span> is <strong>estimable</strong> if <span class="math inline">\(\Lambda &#39; \beta = P &#39; X \beta\)</span> for some matrix P; In other words, <span class="math inline">\(\Lambda &#39; \beta\)</span> is estimable if <span class="math inline">\(\Lambda = X &#39; P \in \mathcal{C}(X&#39;)\)</span>.</p>
<p>Clearly, if <span class="math inline">\(\Lambda &#39; \beta\)</span> is estimable, it is identifiable and therefore it is a reasonable thing to estimate.</p>
<ul>
<li>estimable <span class="math inline">\(\rightarrow\)</span> identifiable</li>
</ul>
<p>For estimable functions <span class="math inline">\(\Lambda&#39; \beta = P &#39; X \beta\)</span>, although <span class="math inline">\(P\)</span> need not be unique, its perpendicular projection (columnwise) onto <span class="math inline">\(\mathcal{C}(X)\)</span> is unique: <br/>
let <span class="math inline">\(P_1 , \; P_2\)</span> be matrices with <span class="math inline">\(\Lambda &#39; = P_1 &#39; X = P_2 &#39; X\)</span>, then</p>
<p>$</p>
<p>MP_1 = X(X’X)^{-}X’P_1 = X(X’X)^{-}= X(X’X)^{-}X’P_2 = MP_2</p>
<p>$</p>
<p><br/>
<br/>
<br/></p>
<ul>
<li>Example 2.1.4 and 2.1.5</li>
</ul>
<p><span class="math inline">\(g(\beta)\)</span>’s estimate, <span class="math inline">\(f(Y)\)</span>, is <strong>unbiased</strong> if <span class="math inline">\(\forall \beta: \; E[f(Y)] = g(\beta)\)</span>.</p>
<p>if <span class="math inline">\(f (Y) = a_0 + a&#39; Y\)</span> for some scalar <span class="math inline">\(a_0\)</span> and vector <span class="math inline">\(a\)</span>, <span class="math inline">\(f(Y)\)</span> is a <strong>linear estimate</strong> of <span class="math inline">\(\Lambda &#39; \beta\)</span>.</p>
<p>if <span class="math inline">\(\Lambda &#39; \beta\)</span> <span class="math inline">\(\iff\)</span> <span class="math inline">\(a_0 = 0\)</span> and <span class="math inline">\(a &#39; X = \Lambda&#39;\)</span>; say, <span class="math inline">\(\Lambda = X &#39; a \in \mathcal{C}(X&#39;)\)</span>, then a <strong>linear estimate</strong> <span class="math inline">\(a_0 + a &#39; Y\)</span> is <strong>unbiased</strong></p>
<p><span class="math inline">\(\Lambda &#39; \beta\)</span> is <strong>estimable</strong> <span class="math inline">\(\iff\)</span> there exists <span class="math inline">\(\rho\)</span> such that <span class="math inline">\(E(\rho &#39; Y ) = \Lambda &#39; \beta\)</span> for any <span class="math inline">\(\beta\)</span>.</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
</div>
<div id="estimation-least-squares" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Estimation: Least Squares</h3>
<p>Estimating <span class="math inline">\(E(Y)\)</span> is to take a vector in <span class="math inline">\(\mathcal{C}(X)\)</span> closest to <span class="math inline">\(Y\)</span>;</p>
<p>$
<span class="math display">\[\begin{alignat}{2}

E(Y) &amp;= X\beta \; &amp;&amp;\in \; \mathcal{C}(X)\\

\\

\hat \beta &amp;= \min_\beta \left\{ (Y-X \beta) &#39; (Y-X \beta)  \right\} \\
&amp;= \min_\beta \left\{ \Vert Y-X \beta \Vert^2   \right\}

\tag{Least Squares Estimate of beta}


\end{alignat}\]</span>
$</p>
<p>for any Least Squares Estimate <span class="math inline">\(\hat \beta\)</span>, LSE of <span class="math inline">\(\Lambda &#39; \beta is \Lambda &#39; \hat \beta\)</span>, e.g., <span class="math inline">\(\hat {\Lambda &#39; \beta}_{LSE} = \Lambda &#39; \hat \beta\)</span>.</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.2.1</li>
</ul>
<p>where <span class="math inline">\(M\)</span> is the perpendicular projection operator onto <span class="math inline">\(\mathcal{C}(X)\)</span>, then</p>
<p>$
$ is a LSE of <span class="math inline">\(\beta\)</span> <span class="math inline">\(\iff\)</span> $X = M Y
$</p>
<p><br/>
<br/></p>
<ul>
<li>Corollary 2.2.2</li>
</ul>
<p><span class="math inline">\(\hat \beta_{LSE} = X(X&#39;X)^{-}X&#39; Y\)</span></p>
<p><br/>
<br/></p>
<ul>
<li>Corollary 2.2.3</li>
</ul>
<p>The unique LSE of <span class="math inline">\(\rho &#39; X \beta = \rho &#39; M Y\)</span>.</p>
<p>※ Note: the unique LSE of <span class="math inline">\(\Lambda &#39; \beta = \Lambda &#39; \hat \beta = P&#39; M Y\)</span>.</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.2.4</li>
</ul>
<p>the LSE of <span class="math inline">\(\Lambda &#39; \beta\)</span> is unique only if <span class="math inline">\(\Lambda &#39; \beta\)</span> is estimable: <span class="math inline">\(\Lambda = X&#39;\rho\)</span> if <span class="math inline">\(\Lambda &#39; \hat \beta_1 =\Lambda &#39; \hat \beta_2\)</span>, so that <span class="math inline">\(X \hat \beta_1 = X \hat \beta_2 = MY\)</span>.</p>
<p>※ Note: When <span class="math inline">\(\beta\)</span> is not identifiable, we need side conditions imposed on the parameters to estimate nonidentifiable parameters.</p>
<p>※ Note: With <span class="math inline">\(r = r (X) &lt; p\)</span> (overparameterized model), we need <span class="math inline">\(p - r\)</span> individual side conditions to identify and estimate the parameters.</p>
<p><br/>
<br/></p>
<ul>
<li>Proposition 2.2.5</li>
</ul>
<p>If <span class="math inline">\(\Lambda = X &#39; \rho\)</span>, then <span class="math inline">\(E(\rho &#39; MY) = \Lambda &#39; \beta\)</span>.</p>
<p>let’s decompose</p>
<p>$
<span class="math display">\[\begin{alignat}{2}

Y 

&amp;= X \hat \beta &amp;&amp;+ Y - X \hat \beta

\\


&amp;= MY &amp;&amp;+ (I-M)Y

\\



&amp;= \hat Y &amp;&amp;+ e 

\end{alignat}\]</span>
$</p>
<p>이때
$
<span class="math display">\[\begin{align}
\hat Y &amp;\in \mathcal{C}(X) \tag{fitted values of Y} \\
e &amp;\in \mathcal{C}(X)^{\perp} \tag{residuals}
\end{align}\]</span>
$</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.2.6</li>
</ul>
<p>Let <span class="math inline">\(r (X) = r\)</span> and <span class="math inline">\(Cov(\epsilon) = \sigma^2 I\)</span>. At below formula, denominator is <strong>degrees of freedom for error</strong>.</p>
<p>Then an <strong>UE</strong> of <span class="math inline">\(\sigma^2\)</span>, MSE, is as below.</p>
<p>$
^2 = = 
$</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
<div id="estimation-best-linear-unbiased" class="section level3" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> Estimation: Best Linear Unbiased</h3>
<ul>
<li>Definition 2.3.1</li>
</ul>
<p><span class="math inline">\(a&#39;Y\)</span> is a Best Linear Unbiased Estimate(BLUE) of <span class="math inline">\(\lambda &#39; \beta\)</span> if <span class="math inline">\(a &#39; Y\)</span> is unbiased.</p>
<p>e.g., <span class="math inline">\(E(a &#39; Y) = \lambda &#39; \beta\)</span> and if for any other linear unbiased estimate <span class="math inline">\(b &#39; Y\)</span>, <span class="math inline">\(Var(a &#39; Y) \le Var(b&#39;Y)\)</span>.</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.3.2: Gauss-Markov thm</li>
</ul>
<p>Consider <span class="math inline">\(Y = X \beta + \epsilon\)</span> with <span class="math inline">\(E(\epsilon) = 0\)</span>, <span class="math inline">\(Cov(\epsilon) = \sigma^2 I\)</span>. Let <span class="math inline">\(\lambda &#39; \beta\)</span> be estimable.</p>
<p>Then LSE of <span class="math inline">\(\lambda &#39; \beta=\)</span> BLUE of <span class="math inline">\(\lambda &#39; \beta\)</span>.</p>
<p><br/>
<br/></p>
<ul>
<li>Corollary 2.3.3</li>
</ul>
<p>Let <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Then there exists a unique BLUE for any estimable function <span class="math inline">\(\lambda &#39; \beta\)</span>.</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
<div id="estimation-maximum-likelihood" class="section level3" number="7.5.4">
<h3><span class="header-section-number">7.5.4</span> Estimation: Maximum Likelihood</h3>
<p>Assume that <span class="math inline">\(Y \sim N_n(X\beta , \; \sigma^2 I_n)\)</span>. Then the Maximum Likelihood Estimates (MLEs) of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are obtained by maximizing the log of the likelihood so that</p>
<p>$
<span class="math display">\[\begin{align}

\left( 

\hat \beta , \; \hat \sigma^2


\right)

&amp;= \text{ MLE of }

\left( 

\beta , \; \sigma^2


\right)


\\

&amp;=

\max_{\left( \beta , \; \sigma^2 \right)} \left\{ 

-\dfrac{n}{2}log(2 \pi) - \dfrac{1}{2} \log \left[ (\sigma^2 )^n\right] - \dfrac{(Y-X\beta)&#39;(Y-X\beta)}{2\sigma^2}




\right\}


\end{align}\]</span></p>
<p>$</p>
<p>$</p>
<p><span class="math display">\[\begin{align}

\hat \beta &amp;= \text{ LSE of } \beta \\

\\\

\hat \sigma^2 &amp;= \dfrac{1}{n} \left\{Y&#39;(I-M)Y \right\}


\end{align}\]</span>
$</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
<div id="estimation-minimum-variance-unbiased" class="section level3" number="7.5.5">
<h3><span class="header-section-number">7.5.5</span> Estimation: Minimum Variance Unbiased</h3>
<p>Assume that <span class="math inline">\(Y = X \beta + \epsilon\)</span> with <span class="math inline">\(\epsilon \sim N_n(0, \; \sigma^2 I_n)\)</span>.</p>
<p>if <span class="math inline">\(\forall \beta, \sigma^2: \; E \left \{h[T(Y)] \right\} = 0\)</span> implies that <span class="math inline">\(Pr[h(T(Y)) = 0] = 1\)</span>, A vector-valued sufficient statistic <span class="math inline">\(T(Y)\)</span> is said to be <strong>complete</strong></p>
<p>If <span class="math inline">\(T(Y)\)</span> is a complete sufficient statistic, then <span class="math inline">\(f(T(Y))\)</span> is a <strong>Minimum Variance Unbiased Estimate (MVUE)</strong> of <span class="math inline">\(E \Big [ f (T(Y)) \Big ]\)</span>.</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.5.3</li>
</ul>
<p>let <span class="math inline">\(\theta = (\theta_1 , \cdots, \theta_s)&#39;\)</span> and let <span class="math inline">\(Y\)</span> be a rvec with pdf as below. then <span class="math inline">\(T(Y) = \Big( T_1(Y), \cdots, T_s(Y) \Big)&#39;\)</span> is a <strong>complete sufficient statistics</strong> provided that neither <span class="math inline">\(\theta\)</span> nor <span class="math inline">\(T(Y)\)</span> satisfies any linear constraints.</p>
<p>$
f(Y) = c() h(Y)
$</p>
<p><br/>
<br/></p>
<ul>
<li>Theorem 2.5.4</li>
</ul>
<p>MSE is a $_{MVUE} $, and <span class="math inline">\(\hat { \rho &#39; X \beta }_{MVUE} = \rho &#39; M Y\)</span> whenever <span class="math inline">\(\epsilon \sim N(0, \; I)\)</span>.</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
<div id="sampling-distributions-of-estimates" class="section level3" number="7.5.6">
<h3><span class="header-section-number">7.5.6</span> Sampling Distributions of Estimates</h3>
<p>Assume that <span class="math inline">\(Y = X \beta + \epsilon\)</span> with <span class="math inline">\(\epsilon \sim N_n(0, \; \sigma^2 I_n)\)</span>. Then <span class="math inline">\(Y \sim N_n(X \beta, \; \sigma^2 I_n)\)</span>. then</p>
<p>$
<span class="math display">\[\begin{alignat}{4}
\Lambda &#39; \hat \beta &amp;= P&#39; M Y &amp;&amp;\sim N(\Lambda &#39; \beta , \; &amp;&amp;\sigma^2 P&#39;MP&amp;&amp;\; \; \; ) &amp;&amp; \; \; \; \; \; \; \; \; \; \;&amp;&amp; &amp;&amp; &amp;&amp;  \\


&amp; &amp;&amp;\sim N(\Lambda &#39; \beta , \; &amp;&amp;\sigma^2 \Lambda &#39; (X&#39;X)^{-} \Lambda&amp;&amp;\; \; \; ) &amp;&amp;    &amp;&amp; \because &amp;&amp; \;M &amp;&amp; =X(X&#39;X)^- X&#39; \\



&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; \; \hat Y &amp;&amp; = MY &amp;&amp;\sim N(X\beta, \sigma^2 M)


\\

\hat \beta &amp;= (X&#39;X)^- X&#39;Y &amp;&amp;\sim N(\beta , \; &amp;&amp;\sigma^2 (X&#39;X)^{-1}) &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; &amp;&amp; (\text{if X is of full rank})



\end{alignat}\]</span>
$</p>
<p><br/>
<br/></p>
<p>Do Exercise 2.1. Show that
$
 ^2 ( r(I-M), ;  )
$</p>
<hr />
<p><br/>
<br/>
<br/>
<br/>
<br/></p>
</div>
<div id="generalized-least-squaresgls" class="section level3" number="7.5.7">
<h3><span class="header-section-number">7.5.7</span> Generalized Least Squares(GLS)</h3>
<p>Assume that for some known positive definite <span class="math inline">\(\Sigma\)</span>,</p>
<p>$
Y = X + , ; ; ; ; ;
$</p>
<p>$
<span class="math display">\[\begin{alignat}{3}

Y &amp;= X \beta &amp;&amp;+ \epsilon &amp;&amp; \; \; \; \; \; \; \; \; \; \; 

&amp;&amp; E(\epsilon)&amp;&amp;=0, \; \; &amp;&amp;\; Cov(\epsilon) &amp;&amp;= \sigma^2 \Sigma \tag{1}\\








\Sigma^{-\tfrac{1}{2}}Y &amp;= \Sigma^{-\tfrac{1}{2}} X \beta &amp;&amp;+ \Sigma^{-\tfrac{1}{2}} \epsilon 


 &amp;&amp; \; \; \; \; \; \; \; \; \; \; &amp;&amp; E(\Sigma^{-\tfrac{1}{2}} \epsilon)&amp;&amp;=0, &amp;&amp;\; Cov(\Sigma^{-\tfrac{1}{2}} \epsilon) &amp;&amp;= \sigma^2 I \tag{2, by SVD}
\\

Y_\ast &amp;= X_\ast \beta &amp;&amp;+ \epsilon_\ast



 &amp;&amp; \; \; \; \; \; \; \; \; \; \; &amp;&amp; E( \epsilon_\ast)&amp;&amp;=0, &amp;&amp;\; Cov( \epsilon_\ast) &amp;&amp;= \sigma^2 I



\end{alignat}\]</span>
$</p>
<p>$
<span class="math display">\[\begin{alignat}{2}

\hat \beta_{GLS} &amp;= \min_\beta (Y_\ast - X_\ast \beta)&#39;(Y_\ast - X_\ast \beta) \\

&amp;= \min_\beta \Vert Y_\ast - X_\ast \beta \Vert^2 \\



&amp;= \min_\beta (Y - X \beta)&#39; \Sigma^{-1} (Y - X \beta) \tag{Generalized LSE (GLSE) of β}


\end{alignat}\]</span>
$</p>
<ul>
<li>Theorem 2.7.1</li>
</ul>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lambda &#39; \beta\)</span> estimable in model (1) <span class="math inline">\(\iff\)</span> if <span class="math inline">\(\lambda &#39; \beta\)</span> is estimable in model (2).</li>
<li>$$ is GLSE of <span class="math inline">\(\beta\)</span> <span class="math inline">\(\iff\)</span> <span class="math inline">\(X(X&#39; \Sigma^{-1} X)^{-}X&#39; \Sigma^{-1}Y = X \hat \beta\)</span>, which is Normal Equation of GLS.</li>
</ol>
<ul>
<li>For any estimable function, there exists a unique GLSE.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>GLSE estimate of estimable <span class="math inline">\(\lambda&#39; \beta\)</span>, is BLUE of $’ $.</li>
<li>let <span class="math inline">\(\epsilon \sim N(0, \; \Sigma^2 \Sigma)\)</span>. then, GLSE of estimable <span class="math inline">\(\lambda &#39; \beta\)</span>, is MVUE.</li>
<li>let <span class="math inline">\(\epsilon \sim N(0, \; \Sigma^2 \Sigma)\)</span>. then, <span class="math inline">\(\hat \beta_{GLS} = \hat \beta_{MLE}\)</span>.</li>
</ol>
<p><br/>
<br/>
<br/></p>
<p>Normal Equation of GLS can be rewritten as</p>
<p>$
<span class="math display">\[\begin{align}

X(X&#39; \Sigma^{-1} X)^{-}X&#39; \Sigma^{-1}Y &amp;= X \hat \beta \\
AY &amp;=
\end{align}\]</span>
$</p>
<p><span class="math inline">\(A\)</span> is a projection operator onto <span class="math inline">\(\mathcal{C}(X)\)</span>.</p>
<p><span class="math inline">\(Cov(X \hat \beta_{GLS}) = \sigma^2 \ast X(X&#39; \Sigma^{-1} X)^{-}X&#39;\)</span>
Let <span class="math inline">\(\lambda &#39; \beta\)</span> be estimable. Then <span class="math inline">\(Var(\lambda &#39; \hat \beta_{GLS}) = \sigma^2 \ast \lambda &#39; (X&#39; \Sigma^{-1} X)^- \lambda\)</span>.</p>
<ul>
<li>Note: <span class="math inline">\((I-A)Y\)</span> is residual vector of GLSE.</li>
</ul>
<p>$
<span class="math display">\[\begin{align}

SSE_{GLS} &amp;= (Y_\ast - \hat Y_\ast)&#39; (Y_\ast - \hat Y_\ast) \\

&amp;\; \; \vdots \\

&amp;= Y&#39;(I-A)&#39; \Sigma^{-1}(I-A)Y \\

\\\

MSE_{GLS} &amp;= \hat \sigma^2 \\
&amp; = \dfrac{1}{n-r(X)} \ast SSE_{GLS}\\

\\\

\dfrac{1}{\hat \sigma^2}

\dfrac{\lambda&#39; \Big(\hat \beta_{GLS} - \beta_{GLS} \Big)}{ \lambda &#39; (X&#39; \Sigma^{-1} X)^- \lambda} &amp;\sim t\Big( n-r(x) \Big)





\end{align}\]</span>
$</p>
<p>denominator는 <span class="math inline">\(Var(\lambda &#39; \hat \beta_{GLS}) = \sigma^2 \ast \lambda &#39; (X&#39; \Sigma^{-1} X)^- \lambda\)</span>.</p>
<p>Let <span class="math inline">\(\Sigma\)</span> be nonsingular and <span class="math inline">\(\mathcal{C}(\Sigma X) \subset \mathcal{C}(X)\)</span>. Then least squares estimates are BLUEs.</p>
<ul>
<li>Note: for diagonal <span class="math inline">\(\Sigma\)</span>, GLS is referred to as <strong>Weighted Least Squares (WLS)</strong>.</li>
</ul>
<p><br/>
<br/></p>
<ul>
<li>Exercise 2.5.</li>
</ul>
<p>Show that <span class="math inline">\(A\)</span> is the perpendicular projection operator onto <span class="math inline">\(\mathcal{C}(X)\)</span> when the inner product between two vectors <span class="math inline">\(\pmb x\)</span> and <span class="math inline">\(\pmb y\)</span> is defined as <span class="math inline">\((\pmb x, \pmb y)_\Sigma \equiv \pmb x&#39; \Sigma^{-1} \pmb y\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distributions-of-quadratic-forms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="one-way-anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/211403_Estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
