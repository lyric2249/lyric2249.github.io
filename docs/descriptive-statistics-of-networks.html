<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Descriptive Statistics of Networks | Self-Study</title>
  <meta name="description" content="7.2 Descriptive Statistics of Networks | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Descriptive Statistics of Networks | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Descriptive Statistics of Networks | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-2.html"/>
<link rel="next" href="data-collection-and-sampling.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li><a href="index.html#intro" id="toc-intro">Intro<span></span></a></li>
<li><a href="#part-20-02" id="toc-part-20-02">(PART) 20-02<span></span></a></li>
<li><a href="categorical.html#categorical" id="toc-categorical"><span class="toc-section-number">1</span> Categorical<span></span></a>
<ul>
<li><a href="overview.html#overview" id="toc-overview"><span class="toc-section-number">1.1</span> Overview<span></span></a>
<ul>
<li><a href="overview.html#data-type-and-statistical-analysis" id="toc-data-type-and-statistical-analysis"><span class="toc-section-number">1.1.1</span> Data Type and Statistical Analysis<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="bayesian.html#bayesian" id="toc-bayesian"><span class="toc-section-number">2</span> Bayesian<span></span></a>
<ul>
<li><a href="abstract.html#abstract" id="toc-abstract"><span class="toc-section-number">2.1</span> Abstract<span></span></a>
<ul>
<li><a href="abstract.html#변수의-독립성" id="toc-변수의-독립성"><span class="toc-section-number">2.1.1</span> 변수의 독립성<span></span></a></li>
<li><a href="abstract.html#교환가능성" id="toc-교환가능성"><span class="toc-section-number">2.1.2</span> 교환가능성<span></span></a></li>
</ul></li>
<li><a href="continual-aeassessment-method.html#continual-aeassessment-method" id="toc-continual-aeassessment-method"><span class="toc-section-number">2.2</span> Continual Aeassessment Method<span></span></a></li>
<li><a href="horseshoe-prior.html#horseshoe-prior" id="toc-horseshoe-prior"><span class="toc-section-number">2.3</span> Horseshoe Prior<span></span></a></li>
</ul></li>
<li><a href="#part-21-01" id="toc-part-21-01">(PART) 21-01<span></span></a></li>
<li><a href="mathematical-stats.html#mathematical-stats" id="toc-mathematical-stats"><span class="toc-section-number">3</span> Mathematical Stats<span></span></a>
<ul>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">3.1</span> Inference<span></span></a>
<ul>
<li><a href="inference.html#rao-blackwell-thm." id="toc-rao-blackwell-thm."><span class="toc-section-number">3.1.1</span> Rao-Blackwell thm.<span></span></a></li>
<li><a href="inference.html#completeness" id="toc-completeness"><span class="toc-section-number">3.1.2</span> Completeness<span></span></a></li>
<li><a href="inference.html#레만-쉐페-thm." id="toc-레만-쉐페-thm."><span class="toc-section-number">3.1.3</span> 레만-쉐페 thm.<span></span></a></li>
<li><a href="inference.html#raoblack" id="toc-raoblack"><span class="toc-section-number">3.1.4</span> Rao-Blackwell thm.<span></span></a></li>
</ul></li>
<li><a href="hypothesis-test.html#hypothesis-test" id="toc-hypothesis-test"><span class="toc-section-number">3.2</span> Hypothesis Test<span></span></a></li>
<li><a href="power-fucntion.html#power-fucntion" id="toc-power-fucntion"><span class="toc-section-number">3.3</span> Power Fucntion<span></span></a>
<ul>
<li><a href="power-fucntion.html#significance-probability-p-value" id="toc-significance-probability-p-value"><span class="toc-section-number">3.3.1</span> Significance Probability (p-value)<span></span></a></li>
</ul></li>
<li><a href="optimal-testing-method.html#optimal-testing-method" id="toc-optimal-testing-method"><span class="toc-section-number">3.4</span> Optimal Testing Method<span></span></a></li>
<li><a href="data-reduction.html#data-reduction" id="toc-data-reduction"><span class="toc-section-number">3.5</span> Data Reduction<span></span></a>
<ul>
<li><a href="data-reduction.html#sufficiency-principle" id="toc-sufficiency-principle"><span class="toc-section-number">3.5.1</span> Sufficiency Principle<span></span></a></li>
</ul></li>
<li><a href="borel-paradox.html#borel-paradox" id="toc-borel-paradox"><span class="toc-section-number">3.6</span> Borel Paradox<span></span></a></li>
<li><a href="neymanpearson-lemma.html#neymanpearson-lemma" id="toc-neymanpearson-lemma"><span class="toc-section-number">3.7</span> Neyman–Pearson lemma<span></span></a>
<ul>
<li><a href="neymanpearson-lemma.html#overview-1" id="toc-overview-1"><span class="toc-section-number">3.7.1</span> Overview<span></span></a></li>
<li><a href="neymanpearson-lemma.html#generalized-lrt" id="toc-generalized-lrt"><span class="toc-section-number">3.7.2</span> Generalized LRT<span></span></a></li>
</ul></li>
<li><a href="개념.html#개념" id="toc-개념"><span class="toc-section-number">3.8</span> 개념<span></span></a></li>
</ul></li>
<li><a href="mcmc.html#mcmc" id="toc-mcmc"><span class="toc-section-number">4</span> MCMC<span></span></a>
<ul>
<li><a href="importance-sampling.html#importance-sampling" id="toc-importance-sampling"><span class="toc-section-number">4.1</span> Importance Sampling<span></span></a>
<ul>
<li><a href="importance-sampling.html#independent-monte-carlo" id="toc-independent-monte-carlo"><span class="toc-section-number">4.1.1</span> Independent Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo"><span class="toc-section-number">4.2</span> Markov Chain Monte Carlo<span></span></a>
<ul>
<li><a href="markov-chain-monte-carlo.html#mh-algorithm" id="toc-mh-algorithm"><span class="toc-section-number">4.2.1</span> MH Algorithm<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used" id="toc-random-walk-chains-most-widely-used"><span class="toc-section-number">4.2.2</span> Random Walk Chains (Most Widely Used)<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler" id="toc-basic-gibbs-sampler"><span class="toc-section-number">4.2.3</span> Basic Gibbs Sampler<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#implementation" id="toc-implementation"><span class="toc-section-number">4.2.4</span> Implementation<span></span></a></li>
</ul></li>
<li><a href="advanced-mcmc-wk08.html#advanced-mcmc-wk08" id="toc-advanced-mcmc-wk08"><span class="toc-section-number">4.3</span> Advanced MCMC (wk08)<span></span></a>
<ul>
<li><a href="advanced-mcmc-wk08.html#data-augmentation" id="toc-data-augmentation"><span class="toc-section-number">4.3.1</span> Data Augmentation<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm" id="toc-hit-and-run-algorithm"><span class="toc-section-number">4.3.2</span> Hit-and-Run Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm" id="toc-metropolis-adjusted-langevin-algorithm"><span class="toc-section-number">4.3.3</span> Metropolis-Adjusted Langevin Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm" id="toc-multiple-try-metropolis-algorithm"><span class="toc-section-number">4.3.4</span> Multiple-Try Metropolis Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm" id="toc-reversible-jump-mcmc-algorithm"><span class="toc-section-number">4.3.5</span> Reversible Jump MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="auxiliary-variable-mcmc.html#auxiliary-variable-mcmc" id="toc-auxiliary-variable-mcmc"><span class="toc-section-number">4.4</span> Auxiliary Variable MCMC<span></span></a>
<ul>
<li><a href="auxiliary-variable-mcmc.html#introduction" id="toc-introduction"><span class="toc-section-number">4.4.1</span> Introduction<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution" id="toc-multimodal-target-distribution"><span class="toc-section-number">4.4.2</span> Multimodal Target Distribution<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants" id="toc-doubly-intractable-normalizing-constants"><span class="toc-section-number">4.4.3</span> Doubly-intractable Normalizing Constants<span></span></a></li>
</ul></li>
<li><a href="approximate-bayesian-computation.html#approximate-bayesian-computation" id="toc-approximate-bayesian-computation"><span class="toc-section-number">4.5</span> Approximate Bayesian Computation<span></span></a>
<ul>
<li><a href="approximate-bayesian-computation.html#simulator-based-models" id="toc-simulator-based-models"><span class="toc-section-number">4.5.1</span> Simulator-Based Models<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods" id="toc-abcifying-monte-carlo-methods"><span class="toc-section-number">4.5.2</span> ABCifying Monte Carlo Methods<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm" id="toc-abc-mcmc-algorithm"><span class="toc-section-number">4.5.3</span> ABC-MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="hamiltonian-monte-carlo.html#hamiltonian-monte-carlo" id="toc-hamiltonian-monte-carlo"><span class="toc-section-number">4.6</span> Hamiltonian Monte Carlo<span></span></a>
<ul>
<li><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo" id="toc-introduction-to-hamiltonian-monte-carlo"><span class="toc-section-number">4.6.1</span> Introduction to Hamiltonian Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="population-monte-carlo.html#population-monte-carlo" id="toc-population-monte-carlo"><span class="toc-section-number">4.7</span> Population Monte Carlo<span></span></a>
<ul>
<li><a href="population-monte-carlo.html#adaptive-direction-sampling" id="toc-adaptive-direction-sampling"><span class="toc-section-number">4.7.1</span> Adaptive Direction Sampling<span></span></a></li>
<li><a href="population-monte-carlo.html#conjugate-gradient-mc" id="toc-conjugate-gradient-mc"><span class="toc-section-number">4.7.2</span> Conjugate Gradient MC<span></span></a></li>
<li><a href="population-monte-carlo.html#parallel-tempering" id="toc-parallel-tempering"><span class="toc-section-number">4.7.3</span> Parallel Tempering<span></span></a></li>
<li><a href="population-monte-carlo.html#evolutionary-mc" id="toc-evolutionary-mc"><span class="toc-section-number">4.7.4</span> Evolutionary MC<span></span></a></li>
<li><a href="population-monte-carlo.html#sequential-parallel-tempering" id="toc-sequential-parallel-tempering"><span class="toc-section-number">4.7.5</span> Sequential Parallel Tempering<span></span></a></li>
</ul></li>
<li><a href="stochastic-approximation-monte-carlo.html#stochastic-approximation-monte-carlo" id="toc-stochastic-approximation-monte-carlo"><span class="toc-section-number">4.8</span> Stochastic Approximation Monte Carlo<span></span></a></li>
<li><a href="review.html#review" id="toc-review"><span class="toc-section-number">4.9</span> Review<span></span></a>
<ul>
<li><a href="review.html#wk01" id="toc-wk01"><span class="toc-section-number">4.9.1</span> Wk01<span></span></a></li>
<li><a href="review.html#wk03" id="toc-wk03"><span class="toc-section-number">4.9.2</span> wk03<span></span></a></li>
<li><a href="review.html#wk04-05" id="toc-wk04-05"><span class="toc-section-number">4.9.3</span> wk04, 05<span></span></a></li>
</ul></li>
<li><a href="else.html#else" id="toc-else"><span class="toc-section-number">4.10</span> Else<span></span></a>
<ul>
<li><a href="else.html#hw4.-rasch-model" id="toc-hw4.-rasch-model"><span class="toc-section-number">4.10.1</span> Hw4. Rasch Model<span></span></a></li>
<li><a href="else.html#da-example-mvn" id="toc-da-example-mvn"><span class="toc-section-number">4.10.2</span> DA) Example: MVN<span></span></a></li>
<li><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes" id="toc-bayesian-adaptive-clinical-trial-with-delayed-outcomes"><span class="toc-section-number">4.10.3</span> Bayesian adaptive clinical trial with delayed outcomes<span></span></a></li>
<li><a href="else.html#nmar의-종류" id="toc-nmar의-종류"><span class="toc-section-number">4.10.4</span> NMAR의 종류<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-selection" id="toc-wk10-bayesian-model-selection"><span class="toc-section-number">4.10.5</span> wk10) Bayesian Model Selection<span></span></a></li>
<li><a href="else.html#autologistic-model" id="toc-autologistic-model"><span class="toc-section-number">4.10.6</span> Autologistic model<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-averaging" id="toc-wk10-bayesian-model-averaging"><span class="toc-section-number">4.10.7</span> wk10) Bayesian Model Averaging<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="mva.html#mva" id="toc-mva"><span class="toc-section-number">5</span> MVA<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#overview-of-mva-not-ended" id="toc-overview-of-mva-not-ended"><span class="toc-section-number">5.1</span> Overview of mva (not ended)<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#notation" id="toc-notation"><span class="toc-section-number">5.1.1</span> Notation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">5.1.2</span> Summary Statistics<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation" id="toc-statistical-inference-on-correlation"><span class="toc-section-number">5.1.3</span> Statistical Inference on Correlation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#standardization" id="toc-standardization"><span class="toc-section-number">5.1.4</span> Standardization<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#missing-value-treatment" id="toc-missing-value-treatment"><span class="toc-section-number">5.1.5</span> Missing Value Treatment<span></span></a></li>
</ul></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-nomral-wk2" id="toc-multivariate-nomral-wk2"><span class="toc-section-number">5.2</span> Multivariate Nomral (wk2)<span></span></a>
<ul>
<li><a href="multivariate-nomral-wk2.html#overview-2" id="toc-overview-2"><span class="toc-section-number">5.2.1</span> Overview<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">5.2.2</span> Spectral Decomposition<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">5.2.3</span> Properties of MVN<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#chi2-distribution" id="toc-chi2-distribution"><span class="toc-section-number">5.2.4</span> <span class="math inline">\(\Chi^2\)</span> distribution<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors" id="toc-linear-combination-of-random-vectors"><span class="toc-section-number">5.2.5</span> Linear Combination of Random Vectors<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood" id="toc-multivariate-normal-likelihood"><span class="toc-section-number">5.2.6</span> Multivariate Normal Likelihood<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s" id="toc-sampling-distribtion-of-bar-pmb-y-s"><span class="toc-section-number">5.2.7</span> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span><span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#assessing-normality" id="toc-assessing-normality"><span class="toc-section-number">5.2.8</span> Assessing Normality<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#power-transformation" id="toc-power-transformation"><span class="toc-section-number">5.2.9</span> Power Transformation<span></span></a></li>
</ul></li>
<li><a href="inference-about-mean-vector-wk3.html#inference-about-mean-vector-wk3" id="toc-inference-about-mean-vector-wk3"><span class="toc-section-number">5.3</span> Inference about Mean Vector (wk3)<span></span></a>
<ul>
<li><a href="inference-about-mean-vector-wk3.html#overview-3" id="toc-overview-3"><span class="toc-section-number">5.3.1</span> Overview<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#confidence-region" id="toc-confidence-region"><span class="toc-section-number">5.3.2</span> 1. Confidence Region<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#simultaneous-ci" id="toc-simultaneous-ci"><span class="toc-section-number">5.3.3</span> 2. Simultaneous CI<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison" id="toc-note-bonferroni-multiple-comparison"><span class="toc-section-number">5.3.4</span> 3. Note: Bonferroni Multiple Comparison<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector" id="toc-large-sample-inferences-about-a-mean-vector"><span class="toc-section-number">5.3.5</span> 4. Large Sample Inferences about a Mean Vector<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5" id="toc-profile-analysis-wk4-5"><span class="toc-section-number">5.3.6</span> 1. Profile Analysis (wk4, 5)<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend" id="toc-test-for-linear-trend"><span class="toc-section-number">5.3.7</span> 2. Test for Linear Trend<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix" id="toc-inferences-about-a-covariance-matrix"><span class="toc-section-number">5.3.8</span> 3. Inferences about a Covariance Matrix<span></span></a></li>
</ul></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparison-of-several-mv-means-wk5" id="toc-comparison-of-several-mv-means-wk5"><span class="toc-section-number">5.4</span> Comparison of Several MV Means (wk5)<span></span></a>
<ul>
<li><a href="comparison-of-several-mv-means-wk5.html#paired-comparison" id="toc-paired-comparison"><span class="toc-section-number">5.4.1</span> Paired Comparison<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations" id="toc-comparing-mean-vectors-from-two-populations"><span class="toc-section-number">5.4.2</span> Comparing Mean Vectors from Two Populations<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2" id="toc-profile-analysis-for-g2"><span class="toc-section-number">5.4.3</span> Profile Analysis (for <span class="math inline">\(g=2\)</span>)<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means" id="toc-comparing-several-multivariate-population-means"><span class="toc-section-number">5.4.4</span> Comparing Several Multivariate Population Means<span></span></a></li>
</ul></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression-wk6" id="toc-multivariate-multiple-regression-wk6"><span class="toc-section-number">5.5</span> Multivariate Multiple Regression (wk6)<span></span></a>
<ul>
<li><a href="multivariate-multiple-regression-wk6.html#overview-4" id="toc-overview-4"><span class="toc-section-number">5.5.1</span> Overview<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression" id="toc-multivariate-multiple-regression"><span class="toc-section-number">5.5.2</span> Multivariate Multiple Regression<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#example" id="toc-example"><span class="toc-section-number">5.5.3</span> Example)<span></span></a></li>
</ul></li>
<li><a href="pca.html#pca" id="toc-pca"><span class="toc-section-number">5.6</span> PCA<span></span></a></li>
<li><a href="factor.html#factor" id="toc-factor"><span class="toc-section-number">5.7</span> Factor<span></span></a>
<ul>
<li><a href="factor.html#method-of-estimation" id="toc-method-of-estimation"><span class="toc-section-number">5.7.1</span> Method of Estimation<span></span></a></li>
<li><a href="factor.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">5.7.2</span> Factor Rotation<span></span></a></li>
<li><a href="factor.html#varimax-criterion" id="toc-varimax-criterion"><span class="toc-section-number">5.7.3</span> Varimax Criterion<span></span></a></li>
<li><a href="factor.html#factor-scores" id="toc-factor-scores"><span class="toc-section-number">5.7.4</span> Factor Scores<span></span></a></li>
</ul></li>
<li><a href="discrimination-and-classification.html#discrimination-and-classification" id="toc-discrimination-and-classification"><span class="toc-section-number">5.8</span> Discrimination and Classification<span></span></a>
<ul>
<li><a href="discrimination-and-classification.html#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">5.8.1</span> Bayes Rule<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations" id="toc-classification-with-two-mv-n-populations"><span class="toc-section-number">5.8.2</span> Classification with Two mv <span class="math inline">\(N\)</span> Populations<span></span></a></li>
<li><a href="discrimination-and-classification.html#evaluating-classification-functions" id="toc-evaluating-classification-functions"><span class="toc-section-number">5.8.3</span> Evaluating Classification Functions<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-several-populations-wk13" id="toc-classification-with-several-populations-wk13"><span class="toc-section-number">5.8.4</span> Classification with several Populations (wk13)<span></span></a></li>
<li><a href="discrimination-and-classification.html#other-discriminant-analysis-methods" id="toc-other-discriminant-analysis-methods"><span class="toc-section-number">5.8.5</span> Other Discriminant Analysis Methods<span></span></a></li>
</ul></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-distance-methods-and-ordination" id="toc-clustering-distance-methods-and-ordination"><span class="toc-section-number">5.9</span> Clustering, Distance Methods, and Ordination<span></span></a>
<ul>
<li><a href="clustering-distance-methods-and-ordination.html#overview-5" id="toc-overview-5"><span class="toc-section-number">5.9.1</span> Overview<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">5.9.2</span> Hierarchical Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">5.9.3</span> K-means Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법" id="toc-군집의-평가방법"><span class="toc-section-number">5.9.4</span> 군집의 평가방법<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14" id="toc-clustering-using-density-estimation-wk14"><span class="toc-section-number">5.9.5</span> Clustering using Density Estimation (wk14)<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds" id="toc-multidimensional-scaling-mds"><span class="toc-section-number">5.9.6</span> Multidimensional Scaling (MDS)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linear.html#linear" id="toc-linear"><span class="toc-section-number">6</span> Linear<span></span></a>
<ul>
<li><a href="overview-svd.html#overview-svd" id="toc-overview-svd"><span class="toc-section-number">6.1</span> Overview &amp; SVD<span></span></a>
<ul>
<li><a href="overview-svd.html#spectral-decomposition-1" id="toc-spectral-decomposition-1"><span class="toc-section-number">6.1.1</span> Spectral Decomposition<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-general-version" id="toc-singular-value-decomposition-general-version"><span class="toc-section-number">6.1.2</span> Singular value Decomposition: General-version<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-another-version" id="toc-singular-value-decomposition-another-version"><span class="toc-section-number">6.1.3</span> Singular value Decomposition: Another-version<span></span></a></li>
<li><a href="overview-svd.html#quadratic-forms" id="toc-quadratic-forms"><span class="toc-section-number">6.1.4</span> Quadratic Forms<span></span></a></li>
<li><a href="overview-svd.html#partitioned-matrices" id="toc-partitioned-matrices"><span class="toc-section-number">6.1.5</span> Partitioned Matrices<span></span></a></li>
<li><a href="overview-svd.html#geometrical-aspects" id="toc-geometrical-aspects"><span class="toc-section-number">6.1.6</span> Geometrical Aspects<span></span></a></li>
<li><a href="overview-svd.html#column-row-and-null-space" id="toc-column-row-and-null-space"><span class="toc-section-number">6.1.7</span> Column, Row and Null Space<span></span></a></li>
</ul></li>
<li><a href="introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">6.2</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-1.html#what" id="toc-what"><span class="toc-section-number">6.2.1</span> What<span></span></a></li>
<li><a href="introduction-1.html#random-vectors-and-matrices" id="toc-random-vectors-and-matrices"><span class="toc-section-number">6.2.2</span> Random Vectors and Matrices<span></span></a></li>
<li><a href="introduction-1.html#multivariate-normal-distributions" id="toc-multivariate-normal-distributions"><span class="toc-section-number">6.2.3</span> Multivariate Normal Distributions<span></span></a></li>
<li><a href="introduction-1.html#distributions-of-quadratic-forms" id="toc-distributions-of-quadratic-forms"><span class="toc-section-number">6.2.4</span> Distributions of Quadratic Forms<span></span></a></li>
</ul></li>
<li><a href="estimation.html#estimation" id="toc-estimation"><span class="toc-section-number">6.3</span> Estimation<span></span></a>
<ul>
<li><a href="estimation.html#identifiability-and-estimability" id="toc-identifiability-and-estimability"><span class="toc-section-number">6.3.1</span> Identifiability and Estimability<span></span></a></li>
<li><a href="estimation.html#estimation-least-squares" id="toc-estimation-least-squares"><span class="toc-section-number">6.3.2</span> Estimation: Least Squares<span></span></a></li>
<li><a href="estimation.html#estimation-best-linear-unbiased" id="toc-estimation-best-linear-unbiased"><span class="toc-section-number">6.3.3</span> Estimation: Best Linear Unbiased<span></span></a></li>
<li><a href="estimation.html#estimation-maximum-likelihood" id="toc-estimation-maximum-likelihood"><span class="toc-section-number">6.3.4</span> Estimation: Maximum Likelihood<span></span></a></li>
<li><a href="estimation.html#estimation-minimum-variance-unbiased" id="toc-estimation-minimum-variance-unbiased"><span class="toc-section-number">6.3.5</span> Estimation: Minimum Variance Unbiased<span></span></a></li>
<li><a href="estimation.html#sampling-distributions-of-estimates" id="toc-sampling-distributions-of-estimates"><span class="toc-section-number">6.3.6</span> Sampling Distributions of Estimates<span></span></a></li>
<li><a href="estimation.html#generalized-least-squaresgls" id="toc-generalized-least-squaresgls"><span class="toc-section-number">6.3.7</span> Generalized Least Squares(GLS)<span></span></a></li>
</ul></li>
<li><a href="one-way-anova.html#one-way-anova" id="toc-one-way-anova"><span class="toc-section-number">6.4</span> One-Way ANOVA<span></span></a>
<ul>
<li><a href="one-way-anova.html#one-way-anova-1" id="toc-one-way-anova-1"><span class="toc-section-number">6.4.1</span> One-Way ANOVA<span></span></a></li>
<li><a href="one-way-anova.html#more-about-models" id="toc-more-about-models"><span class="toc-section-number">6.4.2</span> More About Models<span></span></a></li>
<li><a href="one-way-anova.html#estimating-and-testing-contrasts" id="toc-estimating-and-testing-contrasts"><span class="toc-section-number">6.4.3</span> Estimating and Testing Contrasts<span></span></a></li>
<li><a href="one-way-anova.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">6.4.4</span> Cochran’s Theorem<span></span></a></li>
</ul></li>
<li><a href="testing.html#testing" id="toc-testing"><span class="toc-section-number">6.5</span> Testing<span></span></a>
<ul>
<li><a href="testing.html#more-about-models-two-approaches-for-linear-model" id="toc-more-about-models-two-approaches-for-linear-model"><span class="toc-section-number">6.5.1</span> More About Models: Two approaches for linear model<span></span></a></li>
<li><a href="testing.html#testing-models" id="toc-testing-models"><span class="toc-section-number">6.5.2</span> Testing Models<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure" id="toc-a-generalized-test-procedure"><span class="toc-section-number">6.5.3</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-linear-parametric-functions" id="toc-testing-linear-parametric-functions"><span class="toc-section-number">6.5.4</span> Testing Linear Parametric Functions<span></span></a></li>
<li><a href="testing.html#theoretical-complements" id="toc-theoretical-complements"><span class="toc-section-number">6.5.5</span> Theoretical Complements<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure-1" id="toc-a-generalized-test-procedure-1"><span class="toc-section-number">6.5.6</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace" id="toc-testing-single-degrees-of-freedom-in-a-given-subspace"><span class="toc-section-number">6.5.7</span> Testing Single Degrees of Freedom in a Given Subspace<span></span></a></li>
<li><a href="testing.html#breaking-ss-into-independent-components" id="toc-breaking-ss-into-independent-components"><span class="toc-section-number">6.5.8</span> Breaking SS into Independent Components<span></span></a></li>
<li><a href="testing.html#general-theory" id="toc-general-theory"><span class="toc-section-number">6.5.9</span> General Theory<span></span></a></li>
<li><a href="testing.html#two-way-anova" id="toc-two-way-anova"><span class="toc-section-number">6.5.10</span> Two-Way ANOVA<span></span></a></li>
<li><a href="testing.html#confidence-regions" id="toc-confidence-regions"><span class="toc-section-number">6.5.11</span> Confidence Regions<span></span></a></li>
<li><a href="testing.html#tests-for-generalized-least-squares-models" id="toc-tests-for-generalized-least-squares-models"><span class="toc-section-number">6.5.12</span> Tests for Generalized Least Squares Models<span></span></a></li>
</ul></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">6.6</span> Generalized Least Squares<span></span></a>
<ul>
<li><a href="generalized-least-squares.html#a-direct-solution-via-inner-products" id="toc-a-direct-solution-via-inner-products"><span class="toc-section-number">6.6.1</span> A direct solution via inner products<span></span></a></li>
</ul></li>
<li><a href="flat.html#flat" id="toc-flat"><span class="toc-section-number">6.7</span> Flat<span></span></a>
<ul>
<li><a href="flat.html#flat-1" id="toc-flat-1"><span class="toc-section-number">6.7.1</span> 1.Flat<span></span></a></li>
<li><a href="flat.html#solutions-to-systems-of-linear-equations" id="toc-solutions-to-systems-of-linear-equations"><span class="toc-section-number">6.7.2</span> 2. Solutions to systems of linear equations<span></span></a></li>
</ul></li>
<li><a href="unified-approach-to-balanced-anova-models.html#unified-approach-to-balanced-anova-models" id="toc-unified-approach-to-balanced-anova-models"><span class="toc-section-number">6.8</span> Unified Approach to Balanced ANOVA Models<span></span></a></li>
</ul></li>
<li><a href="#part-21-02" id="toc-part-21-02">(PART) 21-02<span></span></a></li>
<li><a href="network-stats.html#network-stats" id="toc-network-stats"><span class="toc-section-number">7</span> Network Stats<span></span></a>
<ul>
<li><a href="introduction-2.html#introduction-2" id="toc-introduction-2"><span class="toc-section-number">7.1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-2.html#types-of-network-analysis" id="toc-types-of-network-analysis"><span class="toc-section-number">7.1.1</span> Types of Network Analysis<span></span></a></li>
<li><a href="introduction-2.html#network-modeling-and-inference" id="toc-network-modeling-and-inference"><span class="toc-section-number">7.1.2</span> Network Modeling and Inference<span></span></a></li>
<li><a href="introduction-2.html#network-processes" id="toc-network-processes"><span class="toc-section-number">7.1.3</span> Network Processes<span></span></a></li>
</ul></li>
<li><a href="descriptive-statistics-of-networks.html#descriptive-statistics-of-networks" id="toc-descriptive-statistics-of-networks"><span class="toc-section-number">7.2</span> Descriptive Statistics of Networks<span></span></a>
<ul>
<li><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics" id="toc-vertex-and-edge-characteristics"><span class="toc-section-number">7.2.1</span> Vertex and Edge Characteristics<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion" id="toc-characterizing-network-cohesion"><span class="toc-section-number">7.2.2</span> Characterizing Network Cohesion<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#graph-partitioning" id="toc-graph-partitioning"><span class="toc-section-number">7.2.3</span> Graph Partitioning<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing" id="toc-assortativity-and-mixing"><span class="toc-section-number">7.2.4</span> Assortativity and Mixing<span></span></a></li>
</ul></li>
<li><a href="data-collection-and-sampling.html#data-collection-and-sampling" id="toc-data-collection-and-sampling"><span class="toc-section-number">7.3</span> Data Collection and Sampling<span></span></a>
<ul>
<li><a href="data-collection-and-sampling.html#sampling-designs" id="toc-sampling-designs"><span class="toc-section-number">7.3.1</span> Sampling Designs<span></span></a></li>
<li><a href="data-collection-and-sampling.html#coping-strategies" id="toc-coping-strategies"><span class="toc-section-number">7.3.2</span> Coping Strategies<span></span></a></li>
<li><a href="data-collection-and-sampling.html#big-data-solves-nothing" id="toc-big-data-solves-nothing"><span class="toc-section-number">7.3.3</span> Big Data Solves Nothing<span></span></a></li>
</ul></li>
<li><a href="mathematical-models-for-network-graphs.html#mathematical-models-for-network-graphs" id="toc-mathematical-models-for-network-graphs"><span class="toc-section-number">7.4</span> Mathematical Models for Network Graphs<span></span></a>
<ul>
<li><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models" id="toc-classical-random-graph-models"><span class="toc-section-number">7.4.1</span> Classical Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models" id="toc-generalized-random-graph-models"><span class="toc-section-number">7.4.2</span> Generalized Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms" id="toc-network-graph-models-based-on-mechanisms"><span class="toc-section-number">7.4.3</span> Network Graph Models Based on Mechanisms<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics" id="toc-assessing-significance-of-network-graph-characteristics"><span class="toc-section-number">7.4.4</span> Assessing Significance of Network Graph Characteristics<span></span></a></li>
</ul></li>
<li><a href="introduction-to-ergm.html#introduction-to-ergm" id="toc-introduction-to-ergm"><span class="toc-section-number">7.5</span> Introduction to ERGM<span></span></a>
<ul>
<li><a href="introduction-to-ergm.html#exponential-random-graph-models" id="toc-exponential-random-graph-models"><span class="toc-section-number">7.5.1</span> Exponential Random Graph Models<span></span></a></li>
<li><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation" id="toc-difficulty-in-parameter-estimation"><span class="toc-section-number">7.5.2</span> Difficulty in Parameter Estimation<span></span></a></li>
</ul></li>
<li><a href="parameter-estimation-of-ergm.html#parameter-estimation-of-ergm" id="toc-parameter-estimation-of-ergm"><span class="toc-section-number">7.6</span> Parameter Estimation of ERGM<span></span></a>
<ul>
<li><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm" id="toc-current-methods-for-ergm"><span class="toc-section-number">7.6.1</span> Current Methods for ERGM<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm" id="toc-approximation-based-algorithm"><span class="toc-section-number">7.6.2</span> Approximation-based Algorithm<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches" id="toc-auxiliary-variable-mcmc-based-approaches"><span class="toc-section-number">7.6.3</span> Auxiliary Variable MCMC-based Approaches<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc" id="toc-varying-trunction-stochastic-approximation-mcmc"><span class="toc-section-number">7.6.4</span> Varying Trunction Stochastic Approximation MCMC<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#conclusion" id="toc-conclusion"><span class="toc-section-number">7.6.5</span> Conclusion<span></span></a></li>
</ul></li>
<li><a href="ergm-for-dynamic-networks.html#ergm-for-dynamic-networks" id="toc-ergm-for-dynamic-networks"><span class="toc-section-number">7.7</span> ERGM for Dynamic Networks<span></span></a>
<ul>
<li><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm" id="toc-temporal-ergm-tergm-t-ergm"><span class="toc-section-number">7.7.1</span> Temporal ERGM (TERGM, T ERGM)<span></span></a></li>
<li><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm" id="toc-separable-temporal-ergm-stergm-st-ergm"><span class="toc-section-number">7.7.2</span> Separable Temporal ERGM (STERGM, ST ERGM)<span></span></a></li>
</ul></li>
<li><a href="latent-network-models.html#latent-network-models" id="toc-latent-network-models"><span class="toc-section-number">7.8</span> Latent Network Models<span></span></a>
<ul>
<li><a href="latent-network-models.html#latent-position-model" id="toc-latent-position-model"><span class="toc-section-number">7.8.1</span> Latent Position Model<span></span></a></li>
<li><a href="latent-network-models.html#latent-position-cluster-model" id="toc-latent-position-cluster-model"><span class="toc-section-number">7.8.2</span> Latent Position Cluster Model<span></span></a></li>
</ul></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#additive-and-multiplicative-effects-network-models" id="toc-additive-and-multiplicative-effects-network-models"><span class="toc-section-number">7.9</span> Additive and Multiplicative Effects Network Models<span></span></a>
<ul>
<li><a href="additive-and-multiplicative-effects-network-models.html#introduction-3" id="toc-introduction-3"><span class="toc-section-number">7.9.1</span> Introduction<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression" id="toc-social-relations-regression"><span class="toc-section-number">7.9.2</span> Social Relations Regression<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models" id="toc-multiplicative-effects-models"><span class="toc-section-number">7.9.3</span> Multiplicative Effects Models<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation" id="toc-inference-via-posterior-approximation"><span class="toc-section-number">7.9.4</span> Inference via Posterior Approximation<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r" id="toc-discussion-and-example-with-r"><span class="toc-section-number">7.9.5</span> Discussion and Example with R<span></span></a></li>
</ul></li>
<li><a href="stochastic-block-models.html#stochastic-block-models" id="toc-stochastic-block-models"><span class="toc-section-number">7.10</span> Stochastic Block Models<span></span></a>
<ul>
<li><a href="stochastic-block-models.html#stochastic-block-model" id="toc-stochastic-block-model"><span class="toc-section-number">7.10.1</span> Stochastic Block Model<span></span></a></li>
<li><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm" id="toc-mixed-membership-block-model-mmbm"><span class="toc-section-number">7.10.2</span> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="high-dimension.html#high-dimension" id="toc-high-dimension"><span class="toc-section-number">8</span> High Dimension<span></span></a>
<ul>
<li><a href="introduction-4.html#introduction-4" id="toc-introduction-4"><span class="toc-section-number">8.1</span> Introduction<span></span></a></li>
<li><a href="concentration-inequalities.html#concentration-inequalities" id="toc-concentration-inequalities"><span class="toc-section-number">8.2</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities.html#motivation" id="toc-motivation"><span class="toc-section-number">8.2.1</span> Motivation<span></span></a></li>
<li><a href="concentration-inequalities.html#from-markov-to-chernoff" id="toc-from-markov-to-chernoff"><span class="toc-section-number">8.2.2</span> From Markov to Chernoff<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-variables" id="toc-sub-gaussian-random-variables"><span class="toc-section-number">8.2.3</span> sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables" id="toc-properties-of-sub-gaussian-random-variables"><span class="toc-section-number">8.2.4</span> Properties of sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#equivalent-definitions" id="toc-equivalent-definitions"><span class="toc-section-number">8.2.5</span> Equivalent definitions<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-vectors" id="toc-sub-gaussian-random-vectors"><span class="toc-section-number">8.2.6</span> Sub-Gaussian random vectors<span></span></a></li>
<li><a href="concentration-inequalities.html#hoeffdings-inequality" id="toc-hoeffdings-inequality"><span class="toc-section-number">8.2.7</span> Hoeffding’s inequality<span></span></a></li>
<li><a href="concentration-inequalities.html#maximal-inequalities" id="toc-maximal-inequalities"><span class="toc-section-number">8.2.8</span> Maximal inequalities<span></span></a></li>
<li><a href="concentration-inequalities.html#section" id="toc-section"><span class="toc-section-number">8.2.9</span> </a></li>
</ul></li>
<li><a href="concentration-inequalities-1.html#concentration-inequalities-1" id="toc-concentration-inequalities-1"><span class="toc-section-number">8.3</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities-1.html#sub-exponential-random-variables" id="toc-sub-exponential-random-variables"><span class="toc-section-number">8.3.1</span> Sub-exponential random variables<span></span></a></li>
<li><a href="concentration-inequalities-1.html#bernsteins-condition" id="toc-bernsteins-condition"><span class="toc-section-number">8.3.2</span> Bernstein’s condition<span></span></a></li>
<li><a href="concentration-inequalities-1.html#mcdiarmids-inequality" id="toc-mcdiarmids-inequality"><span class="toc-section-number">8.3.3</span> McDiarmid’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#levys-inequality" id="toc-levys-inequality"><span class="toc-section-number">8.3.4</span> Levy’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#quadratic-form" id="toc-quadratic-form"><span class="toc-section-number">8.3.5</span> Quadratic form<span></span></a></li>
<li><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma" id="toc-the-johnsonlindenstrauss-lemma"><span class="toc-section-number">8.3.6</span> The Johnson–Lindenstrauss Lemma<span></span></a></li>
</ul></li>
<li><a href="metric-entropy-and-its-uses.html#metric-entropy-and-its-uses" id="toc-metric-entropy-and-its-uses"><span class="toc-section-number">8.4</span> Metric entropy and its uses<span></span></a>
<ul>
<li><a href="metric-entropy-and-its-uses.html#metric-space" id="toc-metric-space"><span class="toc-section-number">8.4.1</span> Metric space<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy" id="toc-covering-numbers-and-metric-entropy"><span class="toc-section-number">8.4.2</span> Covering numbers and metric entropy<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#packing-numbers" id="toc-packing-numbers"><span class="toc-section-number">8.4.3</span> Packing numbers<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#section-1" id="toc-section-1"><span class="toc-section-number">8.4.4</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-2" id="toc-section-2"><span class="toc-section-number">8.4.5</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-3" id="toc-section-3"><span class="toc-section-number">8.4.6</span> </a></li>
</ul></li>
<li><a href="covariance-estimation.html#covariance-estimation" id="toc-covariance-estimation"><span class="toc-section-number">8.5</span> Covariance estimation<span></span></a>
<ul>
<li><a href="covariance-estimation.html#matrix-algebra-review" id="toc-matrix-algebra-review"><span class="toc-section-number">8.5.1</span> Matrix algebra review<span></span></a></li>
<li><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm" id="toc-covariance-matrix-estimation-in-the-operator-norm"><span class="toc-section-number">8.5.2</span> Covariance matrix estimation in the operator norm<span></span></a></li>
<li><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices" id="toc-bounds-for-structured-covariance-matrices"><span class="toc-section-number">8.5.3</span> Bounds for structured covariance matrices<span></span></a></li>
</ul></li>
<li><a href="matrix-concentration-inequalities.html#matrix-concentration-inequalities" id="toc-matrix-concentration-inequalities"><span class="toc-section-number">8.6</span> Matrix concentration inequalities<span></span></a>
<ul>
<li><a href="matrix-concentration-inequalities.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">8.6.1</span> Matrix calculus<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#matrix-chernoff" id="toc-matrix-chernoff"><span class="toc-section-number">8.6.2</span> Matrix Chernoff<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices" id="toc-sub-gaussian-and-sub-exponential-matrices"><span class="toc-section-number">8.6.3</span> Sub-Gaussian and sub-exponential matrices<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds" id="toc-랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><span class="toc-section-number">8.6.4</span> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds<span></span></a></li>
</ul></li>
<li><a href="principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.7</span> Principal Component Analysis<span></span></a>
<ul>
<li><a href="principal-component-analysis.html#pca-1" id="toc-pca-1"><span class="toc-section-number">8.7.1</span> PCA<span></span></a></li>
<li><a href="principal-component-analysis.html#matrix-perturbation" id="toc-matrix-perturbation"><span class="toc-section-number">8.7.2</span> Matrix Perturbation<span></span></a></li>
<li><a href="principal-component-analysis.html#spiked-cov-model" id="toc-spiked-cov-model"><span class="toc-section-number">8.7.3</span> Spiked Cov Model<span></span></a></li>
<li><a href="principal-component-analysis.html#sparse-pca" id="toc-sparse-pca"><span class="toc-section-number">8.7.4</span> sparse PCA<span></span></a></li>
</ul></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">8.8</span> Linear Regression<span></span></a>
<ul>
<li><a href="linear-regression.html#problem-formulation" id="toc-problem-formulation"><span class="toc-section-number">8.8.1</span> Problem formulation<span></span></a></li>
<li><a href="linear-regression.html#least-squares-estimator-in-high-dimensions" id="toc-least-squares-estimator-in-high-dimensions"><span class="toc-section-number">8.8.2</span> Least Squares Estimator in high dimensions<span></span></a></li>
<li><a href="linear-regression.html#sparse-linear-regression" id="toc-sparse-linear-regression"><span class="toc-section-number">8.8.3</span> Sparse linear regression<span></span></a></li>
</ul></li>
<li><a href="uniform-laws-of-large-numbers.html#uniform-laws-of-large-numbers" id="toc-uniform-laws-of-large-numbers"><span class="toc-section-number">8.9</span> Uniform laws of large numbers<span></span></a>
<ul>
<li><a href="uniform-laws-of-large-numbers.html#motivation-1" id="toc-motivation-1"><span class="toc-section-number">8.9.1</span> Motivation<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity" id="toc-a-uniform-law-via-rademacher-complexity"><span class="toc-section-number">8.9.2</span> A uniform law via Rademacher complexity<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity" id="toc-upper-bounds-on-the-rademacher-complexity"><span class="toc-section-number">8.9.3</span> Upper bounds on the Rademacher complexity<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="survival-analysis.html#survival-analysis" id="toc-survival-analysis"><span class="toc-section-number">9</span> Survival Analysis<span></span></a>
<ul>
<li><a href="introduction-5.html#introduction-5" id="toc-introduction-5"><span class="toc-section-number">9.1</span> Introduction<span></span></a></li>
<li><a href="section-4.html#section-4" id="toc-section-4"><span class="toc-section-number">9.2</span> </a></li>
<li><a href="counting-processes-and-martingales.html#counting-processes-and-martingales" id="toc-counting-processes-and-martingales"><span class="toc-section-number">9.3</span> Counting Processes and Martingales<span></span></a>
<ul>
<li><a href="counting-processes-and-martingales.html#conditional-expectation" id="toc-conditional-expectation"><span class="toc-section-number">9.3.1</span> Conditional Expectation<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#martingale" id="toc-martingale"><span class="toc-section-number">9.3.2</span> Martingale<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#key-martingales-properties" id="toc-key-martingales-properties"><span class="toc-section-number">9.3.3</span> Key Martingales Properties<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#section-5" id="toc-section-5"><span class="toc-section-number">9.3.4</span> </a></li>
<li><a href="counting-processes-and-martingales.html#section-6" id="toc-section-6"><span class="toc-section-number">9.3.5</span> </a></li>
</ul></li>
<li><a href="section-7.html#section-7" id="toc-section-7"><span class="toc-section-number">9.4</span> </a></li>
<li><a href="cox-regression.html#cox-regression" id="toc-cox-regression"><span class="toc-section-number">9.5</span> Cox Regression<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#filtration의-개념을-정복하자" id="toc-filtration의-개념을-정복하자"><span class="toc-section-number">9.6</span> Filtration의 개념을 정복하자!<span></span></a>
<ul>
<li><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약" id="toc-random-process를-이야기-하기까지의-긴-여정의-요약"><span class="toc-section-number">9.6.1</span> Random Process를 이야기 하기까지의 긴 여정의 요약<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#ft-measurable" id="toc-ft-measurable"><span class="toc-section-number">9.6.2</span> Ft-measurable<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#epilogue" id="toc-epilogue"><span class="toc-section-number">9.6.3</span> EPILOGUE<span></span></a></li>
</ul></li>
<li><a href="concepts.html#concepts" id="toc-concepts"><span class="toc-section-number">9.7</span> Concepts<span></span></a></li>
</ul></li>
<li><a href="#part-22-01" id="toc-part-22-01">(PART) 22-01<span></span></a></li>
<li><a href="scikit.html#scikit" id="toc-scikit"><span class="toc-section-number">10</span> scikit<span></span></a>
<ul>
<li><a href="linear-models.html#linear-models" id="toc-linear-models"><span class="toc-section-number">10.1</span> Linear Models<span></span></a>
<ul>
<li><a href="linear-models.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">10.1.1</span> Ordinary Least Squares<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-00-00" id="toc-appendix-00-00">(APPENDIX) 00-00<span></span></a></li>
<li><a href="concepts-1.html#concepts-1" id="toc-concepts-1"><span class="toc-section-number">11</span> Concepts<span></span></a>
<ul>
<li><a href="autologistic.html#autologistic" id="toc-autologistic"><span class="toc-section-number">11.1</span> Autologistics<span></span></a></li>
<li><a href="orderlogit.html#orderlogit" id="toc-orderlogit"><span class="toc-section-number">11.2</span> Ordered Logit<span></span></a></li>
<li><a href="concepts-questions.html#concepts-questions" id="toc-concepts-questions"><span class="toc-section-number">11.3</span> Concepts Questions<span></span></a>
<ul>
<li><a href="concepts-questions.html#통계-및-수학" id="toc-통계-및-수학"><span class="toc-section-number">11.3.1</span> 통계 및 수학<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="about-cluster-gcn.html#about-cluster-gcn" id="toc-about-cluster-gcn"><span class="toc-section-number">12</span> About Cluster-GCN<span></span></a>
<ul>
<li><a href="about-cluster-gcn.html#ann" id="toc-ann"><span class="toc-section-number">12.0.1</span> ANN<span></span></a></li>
<li><a href="about-cluster-gcn.html#cnn" id="toc-cnn"><span class="toc-section-number">12.0.2</span> CNN<span></span></a></li>
<li><a href="about-cluster-gcn.html#graph-convolution-network" id="toc-graph-convolution-network"><span class="toc-section-number">12.0.3</span> Graph Convolution Network<span></span></a></li>
<li><a href="about-cluster-gcn.html#cluster-gcn" id="toc-cluster-gcn"><span class="toc-section-number">12.0.4</span> Cluster-GCN<span></span></a></li>
</ul></li>
<li><a href="cnn-1.html#cnn-1" id="toc-cnn-1"><span class="toc-section-number">13</span> CNN<span></span></a></li>
<li><a href="cnn-2.html#cnn-2" id="toc-cnn-2"><span class="toc-section-number">14</span> CNN<span></span></a></li>
<li><a href="cnn-3.html#cnn-3" id="toc-cnn-3"><span class="toc-section-number">15</span> CNN<span></span></a></li>
<li><a href="section-8.html#section-8" id="toc-section-8"><span class="toc-section-number">16</span> 01<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="descriptive-statistics-of-networks" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Descriptive Statistics of Networks<a href="descriptive-statistics-of-networks.html#descriptive-statistics-of-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>complex system 에 대한 연구에서 연구하는 문제는 대응하는 네트워크 그래프의 구조, 혹은 특성을 분석하는 문제로 동치될 수 있음. 3개의 vertex 들을 묶어 특정 형태의 triplet 을 만들어 triplet 의 특성을 분석. 상품이나 정보의 흐름 분석은 네트워크 분석에서의 path 발생 혹은 비발생 확인 문제와 동치. 각각의 시스템에서 해당 element 의 중요도를 체크하는건 vertex 의 centrality 확인과 동치. 동계통의 community 혹은 그룹을 찾는 문제는 그래프 partitioning 문제와 동치. 이런 네트워크 분석은 순혈 통계와는 살짝 차이가 있음. 보통 수학과 컴퓨터과학, 사회구조 분석사회학, 물리학 등에 의존함.</p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
<div id="vertex-and-edge-characteristics" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Vertex and Edge Characteristics<a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>네트워크의 기본 요소는 <strong>edge</strong> 와 <strong>vertex</strong>. 이들을 characterization 하고자 하는 작업은 <strong>vertex degree</strong> 에 기반하며, 이는 각 vertex 가 얼마나 중요한지를 판단하기 위한 측도를 획득하기 위함.</p>
<p><br>
<br>
<br></p>
<div id="vertex-degree" class="section level4 hasAnchor" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Vertex Degree<a href="descriptive-statistics-of-networks.html#vertex-degree" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>네트워크 그래프 <span class="math inline">\(G=(V, E)\)</span> 에서 vertex <span class="math inline">\(v\)</span> 의 <strong>degree</strong> <span class="math inline">\(d_v\)</span> 는 <span class="math inline">\(v\)</span> 에 엮인 edge 의 갯수. <span class="math inline">\(\forall v \in V, d_v = d: f_d \coloneqq \frac{v}{d = d_v}\)</span>. collection <span class="math inline">\(\{f_d\}_{d \ge 0}\)</span> 는 <span class="math inline">\(G\)</span> 의 <strong>degree distribution</strong> 이라고 부른다. 이는 결국 원본 degree sequence 를 rescaling 한 것.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="descriptive-statistics-of-networks.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sand)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="descriptive-statistics-of-networks.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(karate)</span>
<span id="cb5-2"><a href="descriptive-statistics-of-networks.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb5-3"><a href="descriptive-statistics-of-networks.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">degree</span>(karate), <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">xlab =</span> <span class="st">&quot;Vertex Degree&quot;</span>,</span>
<span id="cb5-4"><a href="descriptive-statistics-of-networks.html#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb5-5"><a href="descriptive-statistics-of-networks.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(<span class="fu">graph.strength</span>(karate), <span class="at">col =</span> <span class="st">&quot;pink&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Vertex Strength&quot;</span>,</span>
<span id="cb5-6"><a href="descriptive-statistics-of-networks.html#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:1"></span>
<img src="_main_files/figure-html/1-1.png" alt="Karate Network" width="672" />
<p class="caption">
FIGURE 7.1: Karate Network
</p>
</div>
<p>weighted network 케이스에서 유용하게 자주 쓰이는 degree 의 일반화는 <strong>vertex strength</strong>. 이는 해당 vertex 에 연결된 edge 의 weight 를 전부 합한 것. 이러한 strength 의 distribution 은 <strong>weighted degree distribution</strong> 이라고 불리며, 이는 일반적인 degree distribution 과 입지가 같음.</p>
<p><em>Figure 2: The vertex strength distribution for the Karate club network - A Network of Interactions among Protein Pairs in Yeast</em></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="descriptive-statistics-of-networks.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(igraphdata)</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="descriptive-statistics-of-networks.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(yeast)</span>
<span id="cb7-2"><a href="descriptive-statistics-of-networks.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ecount</span>(yeast)</span>
<span id="cb7-3"><a href="descriptive-statistics-of-networks.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 11855</span></span>
<span id="cb7-4"><a href="descriptive-statistics-of-networks.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">vcount</span>(yeast)</span>
<span id="cb7-5"><a href="descriptive-statistics-of-networks.html#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2617</span></span></code></pre></div>
<p>히스토그램을 확인해보자. degree 가 낮은 substantial fraction of vertex 들이 존재한다. 이들의 magnitude 는 karate network 의 그것이랑 유사하지만, 이와 동시에 연속적으로 higher order of magnitude 를 가지는 vertex 들의 숫자가 non-trivial 하게 관측된다. 로그화 시킨 degree 의 경우, log frequency 에서 상당한 linear decay 관측 가능.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="descriptive-statistics-of-networks.html#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="descriptive-statistics-of-networks.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb8-3"><a href="descriptive-statistics-of-networks.html#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="descriptive-statistics-of-networks.html#cb8-4" aria-hidden="true" tabindex="-1"></a>d.yeast <span class="ot">=</span> <span class="fu">degree</span>(yeast)</span>
<span id="cb8-5"><a href="descriptive-statistics-of-networks.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(d.yeast, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Degree&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>,</span>
<span id="cb8-6"><a href="descriptive-statistics-of-networks.html#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">main =</span> <span class="st">&quot;Degree Distribution&quot;</span>)</span>
<span id="cb8-7"><a href="descriptive-statistics-of-networks.html#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="descriptive-statistics-of-networks.html#cb8-8" aria-hidden="true" tabindex="-1"></a>dd.yeast <span class="ot">=</span> <span class="fu">degree.distribution</span>(yeast)</span>
<span id="cb8-9"><a href="descriptive-statistics-of-networks.html#cb8-9" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">max</span>(d.yeast) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb8-10"><a href="descriptive-statistics-of-networks.html#cb8-10" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">=</span> (dd.yeast <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb8-11"><a href="descriptive-statistics-of-networks.html#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d[ind], dd.yeast[ind], <span class="at">log =</span> <span class="st">&quot;xy&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">xlab =</span> <span class="fu">c</span>(<span class="st">&quot;Log-Degree&quot;</span>),</span>
<span id="cb8-12"><a href="descriptive-statistics-of-networks.html#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="fu">c</span>(<span class="st">&quot;Log-Intensity&quot;</span>), <span class="at">main =</span> <span class="st">&quot;Log-Log Degree Distribution&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="_main_files/figure-html/unnamed-chunk-4-1.png" alt="The degree distribution for protein interactions in Yeast" width="672" />
<p class="caption">
FIGURE 7.2: The degree distribution for protein interactions in Yeast
</p>
</div>
<p>Figure 3: The degree distribution for protein interactions in Yeast</p>
<p>서로 다른 degree 의 vertex 들이 서로 연결되어 있다면 그 기저에 깔린 메커니즘은 무엇인가? 이 또한 흥미로운 부분. 이 문제의 해결에 주효하게 작용하는 개념은 주어진 vertex 의 <strong>average degree of the neighbors</strong>. 높은 degree 의 vertex 는 높은 degree 랑만 붙는 경향이 있는 반면 lower degree 들은 높든 낮든 들러붙는 경향.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="descriptive-statistics-of-networks.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># par(mfrow=c(1,1))</span></span>
<span id="cb9-2"><a href="descriptive-statistics-of-networks.html#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="descriptive-statistics-of-networks.html#cb9-3" aria-hidden="true" tabindex="-1"></a>a.nn.deg.yeast <span class="ot">=</span> <span class="fu">graph.knn</span>(yeast, <span class="fu">V</span>(yeast))<span class="sc">$</span>knn</span>
<span id="cb9-4"><a href="descriptive-statistics-of-networks.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d.yeast, a.nn.deg.yeast, <span class="at">log =</span> <span class="st">&quot;xy&quot;</span>, <span class="at">col =</span> <span class="st">&quot;goldenrod&quot;</span>,</span>
<span id="cb9-5"><a href="descriptive-statistics-of-networks.html#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="fu">c</span>(<span class="st">&quot;Log Vertex Degree&quot;</span>), <span class="at">ylab =</span> <span class="fu">c</span>(<span class="st">&quot;Log Average Neighbor Degree&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="_main_files/figure-html/unnamed-chunk-5-1.png" alt="The degree distribution for protein interactions in Yeast" width="672" />
<p class="caption">
FIGURE 7.3: The degree distribution for protein interactions in Yeast
</p>
</div>
<p><br>
<br>
<br></p>
</div>
<div id="vertex-centrality" class="section level4 hasAnchor" number="7.2.1.2">
<h4><span class="header-section-number">7.2.1.2</span> Vertex Centrality<a href="descriptive-statistics-of-networks.html#vertex-centrality" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>vertex 에 대해 갖는 많은 의문은 결국 해당 vertex 가 주어진 네트워크에서 얼마나 <strong>중요한가</strong> 를 알기 위한 것.</p>
<p><strong>centrality</strong> 에 대한 많은 측도 (measure) 들은 이러한 중요성을 측정하기 위해 개발되었음. <strong>vertex centrality</strong> 를 확인하기 위해 가장 자주 쓰이는 measure 는 <strong>vertex degree</strong>. 이외에 vertex centrality measure 로서 사용되는 건 <strong>Closeness</strong>, <strong>Betweenness</strong>, and <strong>Eigenvector</strong> 등이 존재. 이 셋이 좀 메이저, 마이저.</p>
<p>vertex centrality 를 나타내는 가장 직관적인 방법은 radial layout 을 쓰는 것. 이는 곧 central vertex 를 중앙에 가깝게 배치하는 것. 물론 네트워크가 너무너무 커버리면 표시불가. 작거나 중간크기 네트워크에만 사용가능.</p>
<p><br>
<br></p>
<ul>
<li><strong>Closeness centrality</strong></li>
</ul>
<p>vertex 가 다른 다수의 vertex 와 가깝다면 이를 central 이라고 판정. 일반적으로 사용되는 기준값은:</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}

&amp;C_{CL} &amp;&amp;=\frac{1}{\sum\limits_{u \in V} dist(u,v)}  \tag{closeness}
\\
0 \le &amp; &amp;&amp;=\frac{1}{\sum\limits_{u \in V} dist(u,v)} \cdot (N_v-1) \le 1 \tag{normalized}

\end{alignat}\]</span>
$$</p>
<ul>
<li>denominator <span class="math inline">\(dist(v, u)\)</span> 는 <span class="math inline">\(u, v \in V\)</span> 인 vertex <span class="math inline">\(u, v\)</span> 사이의 geodesic distance
<ul>
<li>이의 sum 은 결국 total distance of a vertex from all others.</li>
</ul></li>
</ul>
<p>각각 다른 그래프에서 산출된 centrality measure 를 비교하기 위해 normalize 하는 상황 있으며 위의 factor 곱하면 <span class="math inline">\([0,1]\)</span> 로 normalize.</p>
<p><mark>large centrality value 는 곧 small total distance 로 이어짐.</mark></p>
<p><br>
<br></p>
<ul>
<li><strong>Betweenness centrality</strong></li>
</ul>
<p>어떤 vertex 가 다른 vertex 쌍 사이에 위치하고 있는지를 확인. 이건 vertex 가 네트워크 그래프의 path 에 비추어서 어디에 위치하고 있는지가 중요하다는 관점에 기반. 현실세계에 비추어도 이러한 path 가 인간관계라고 생각한다면 path 가 다수 지나가는 vertex, 즉 인싸는 중요한 사람일 것. 일반적으로 사용되는 값은:</p>
<p><span class="math display">\[
C_B (v) = \sum\limits_{s \not = t \not = v \in V} \frac{\sigma(s,t | v)}{\sigma(s, t)}
\]</span></p>
<ul>
<li>numerator 는 <span class="math inline">\(v\)</span> 를 통과하면서 <span class="math inline">\(s,t\)</span> 사이가 최단거리인 path 의 총 숫자,</li>
<li>denominator 는 이런 조건 없이 <span class="math inline">\(s,t\)</span> 사이가 최단거리인 path 의 총 숫자.</li>
</ul>
<p>이를 unit interval 로 scale 할 때는 <span class="math inline">\(\frac{(N_v - 1) (N_v - 2)}{2} = \choose {N_v - 1}2\)</span> 로 나누면 됨.</p>
<p><br>
<br></p>
<ul>
<li><strong>eigenvector centrality</strong></li>
</ul>
<p>‘status’, ‘prestige’, ‘rank’ 등에 기반한 사고방식. vertex 의 이웃이 central 하다면, 본인 vertex 자체도 central 하리라는 관점. 이는 central 의 정의만 생각해보더라도 꽤 합리적인 추론임. 이는 적절하게 정의된 방정식의 linear system 의 eigenvalue solution 으로 표현가능함. 이러한 eigenvalue centrality 관점에 쓰이는 값은 꽤 많은데 가장 대표적인 건:</p>
<p><span class="math display">\[
C_{E_i} (v) = \alpha \sum\limits_{\{u,v\}\in E} C_{E_i}(u)
\]</span></p>
<ul>
<li><mark>What is this?</mark>
<ol style="list-style-type: decimal">
<li>we implement eigen-analysis for an adjacency matrix
- <span class="math inline">\(\alpha\)</span>: precision</li>
<li>find the largest eigenvalue and its corresponding eigenvector
- eigenvalue: the importance capture largest variance
- eigenvector: dependency information</li>
</ol></li>
<li>vector <span class="math inline">\(C_{E_i} = \left ( C_{E_i}(1), \cdots, C_{E_i}(N_v) \right)&#39;\)</span> 는 eigenvalue problem <span class="math inline">\(A_{C_{E_i}}\)</span> 의 solution.</li>
<li><span class="math inline">\(A\)</span> 는 네트워크 그래프 <span class="math inline">\(G\)</span> 의 adjacency 매트릭스.</li>
</ul>
<p><span class="math inline">\(\alpha^{-1}\)</span> 의 optimal choice 는 <span class="math inline">\(A\)</span> 의 가장 큰 eigenvalue. 따라서 <span class="math inline">\(h_{E_i}\)</span> 는 상응하는 eigenvector. <span class="math inline">\(G\)</span> 가 undirected 이며 connected 라면, <span class="math inline">\(A\)</span> 의 largest eigenvalue 는 간단하며 이의 eigenvector는 모두 nonzero entry 이며 부호 같음. 관례 (convention) 적으로는 이 entry 들의 abs 를 보고함. 이러면 eigenvector 의 orthonormality 에 의해 얘들은 자동적으로 <span class="math inline">\([0,1]\)</span> 사이로 scaled.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="descriptive-statistics-of-networks.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(network)</span>
<span id="cb10-2"><a href="descriptive-statistics-of-networks.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sna)</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="descriptive-statistics-of-networks.html#cb11-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">get.adjacency</span>(karate, <span class="at">sparse =</span> <span class="cn">FALSE</span>)</span>
<span id="cb11-2"><a href="descriptive-statistics-of-networks.html#cb11-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">as.network.matrix</span>(A)</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="descriptive-statistics-of-networks.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb12-2"><a href="descriptive-statistics-of-networks.html#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="descriptive-statistics-of-networks.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">gplot.target</span>(g, <span class="fu">degree</span>(g), <span class="at">main =</span> <span class="st">&quot;Degree&quot;</span>, <span class="at">circ.lab =</span> <span class="cn">FALSE</span>,</span>
<span id="cb12-4"><a href="descriptive-statistics-of-networks.html#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">circ.col =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">usearrows =</span> <span class="cn">FALSE</span>, <span class="at">vertex.col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,</span>
<span id="cb12-5"><a href="descriptive-statistics-of-networks.html#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">32</span>), <span class="st">&quot;yellow&quot;</span>), <span class="at">edge.col =</span> <span class="st">&quot;darkgray&quot;</span>)</span>
<span id="cb12-6"><a href="descriptive-statistics-of-networks.html#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">gplot.target</span>(g, <span class="fu">closeness</span>(g), <span class="at">main =</span> <span class="st">&quot;Closeness&quot;</span>, <span class="at">circ.lab =</span> <span class="cn">FALSE</span>,</span>
<span id="cb12-7"><a href="descriptive-statistics-of-networks.html#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">circ.col =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">usearrows =</span> <span class="cn">FALSE</span>, <span class="at">vertex.col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,</span>
<span id="cb12-8"><a href="descriptive-statistics-of-networks.html#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">32</span>), <span class="st">&quot;yellow&quot;</span>), <span class="at">edge.col =</span> <span class="st">&quot;darkgray&quot;</span>)</span>
<span id="cb12-9"><a href="descriptive-statistics-of-networks.html#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">gplot.target</span>(g, <span class="fu">betweenness</span>(g), <span class="at">main =</span> <span class="st">&quot;Betweenness&quot;</span>, <span class="at">circ.lab =</span> <span class="cn">FALSE</span>,</span>
<span id="cb12-10"><a href="descriptive-statistics-of-networks.html#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">circ.col =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">usearrows =</span> <span class="cn">FALSE</span>, <span class="at">vertex.col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,</span>
<span id="cb12-11"><a href="descriptive-statistics-of-networks.html#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">32</span>), <span class="st">&quot;yellow&quot;</span>), <span class="at">edge.col =</span> <span class="st">&quot;darkgray&quot;</span>)</span>
<span id="cb12-12"><a href="descriptive-statistics-of-networks.html#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="fu">gplot.target</span>(g, <span class="fu">eigenvaluecent</span>(g), <span class="at">main =</span> <span class="st">&quot;Eigenvalue&quot;</span>, <span class="at">circ.lab =</span> <span class="cn">FALSE</span>,</span>
<span id="cb12-13"><a href="descriptive-statistics-of-networks.html#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">circ.col =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">usearrows =</span> <span class="cn">FALSE</span>, <span class="at">vertex.col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,</span>
<span id="cb12-14"><a href="descriptive-statistics-of-networks.html#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">32</span>), <span class="st">&quot;yellow&quot;</span>), <span class="at">edge.col =</span> <span class="st">&quot;darkgray&quot;</span>)</span></code></pre></div>
<p>이러한 centrality measure 를 undirected 에만 적용해왔음. directed 에도 적용못할 이유는 없지.</p>
<p>소위 <strong>hub vertex</strong> 의 중요성을 정의해보자. 얼마나 많은 authority vertex 를 그들이 향하는지, 그리고 얼마나 많은 authority vertex 들이 해당 vertex 를 향하는지를 통해 판단. directed graph <span class="math inline">\(A\)</span> 가 주어졌을 때 <strong>hub</strong> 는 <span class="math inline">\(M_{hub} = AA&#39;\)</span> 의 eigenvector centrality 에 의해 결정됨. authority 는 <span class="math inline">\(M_{auth} = A&#39;A\)</span> 에 의해 결정.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="descriptive-statistics-of-networks.html#cb13-1" aria-hidden="true" tabindex="-1"></a>l <span class="ot">=</span> <span class="fu">layout.kamada.kawai</span>(aidsblog)</span>
<span id="cb13-2"><a href="descriptive-statistics-of-networks.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb13-3"><a href="descriptive-statistics-of-networks.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(aidsblog, <span class="at">layout =</span> l, <span class="at">main =</span> <span class="st">&quot;Hubs&quot;</span>, <span class="at">vertex.label =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb13-4"><a href="descriptive-statistics-of-networks.html#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">vertex.size =</span> <span class="dv">10</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">hub.score</span>(aidsblog)<span class="sc">$</span>vector))</span>
<span id="cb13-5"><a href="descriptive-statistics-of-networks.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(aidsblog, <span class="at">layout =</span> l, <span class="at">main =</span> <span class="st">&quot;Authorities&quot;</span>, <span class="at">vertex.label =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb13-6"><a href="descriptive-statistics-of-networks.html#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">vertex.size =</span> <span class="dv">10</span> <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">authority.score</span>(aidsblog)<span class="sc">$</span>vector))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="_main_files/figure-html/unnamed-chunk-9-1.png" alt="AIDS blog network with vertex area proportional to hubs and authority centrality measures" width="672" />
<p class="caption">
FIGURE 7.4: AIDS blog network with vertex area proportional to hubs and authority centrality measures
</p>
</div>
<p><br>
<br>
<br></p>
</div>
<div id="characterizing-edges" class="section level4 hasAnchor" number="7.2.1.3">
<h4><span class="header-section-number">7.2.1.3</span> Characterizing Edges<a href="descriptive-statistics-of-networks.html#characterizing-edges" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>vertex betweenness centrality 에서 <strong>edge betweenness centrality </strong> 는 직관적. 각 edge 에 해당 edge 가 최단거리 path 로 삼아지는 횟수를 고르면 됨.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="descriptive-statistics-of-networks.html#cb14-1" aria-hidden="true" tabindex="-1"></a>eb <span class="ot">=</span> <span class="fu">edge.betweenness</span>(karate)</span>
<span id="cb14-2"><a href="descriptive-statistics-of-networks.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">E</span>(karate)[<span class="fu">order</span>(eb, <span class="at">decreasing =</span> T)[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]]</span>
<span id="cb14-3"><a href="descriptive-statistics-of-networks.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="do">## + 3/78 edges from 4b458a1 (vertex names):</span></span>
<span id="cb14-4"><a href="descriptive-statistics-of-networks.html#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] Actor 20--John A   Mr Hi   --Actor 20 Mr Hi   --Actor 32</span></span></code></pre></div>
<p>하지만 이외의 vertex centrality measures 들은 edge 에 적용하려면 그렇게 쉽진 않음. 해결책 중 하나는 네트워크 그래프 <span class="math inline">\(G\)</span> 의 line graph 의 vertex 에 vertex centrality measures 를 적용하는 것. <span class="math inline">\(G\)</span> 의 라인그래프 <span class="math inline">\(G&#39;=(V&#39;, E&#39;)\)</span> 는 <span class="math inline">\(G\)</span> 의 vertex 를 edge 로, edge 를 vertex 로 바꾸는 것으로 획득됨. vertex <span class="math inline">\(v&#39; \in V&#39;\)</span> 는 원본 그래프의 edge <span class="math inline">\(e \in E&#39;\)</span> 를 의미하며, edge <span class="math inline">\(e&#39; \in E&#39;\)</span> 는 <span class="math inline">\(G\)</span> 에서의 대응하는 원본 vertex 1개의 세트를 의미함.</p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="characterizing-network-cohesion" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Characterizing Network Cohesion<a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>• 네트워크 그래프 상에서 edge 로서 정의된 관계에 비추었을 때, vertex 들의 어느 subset 이 어느 정도로 응집되는가, 혹은 같이 들러붙는가 하는 문제에 대해 생각해보는 것이 <strong>network cohesion</strong>. SNS 에서 친구의 친구끼리는 친구가 되기 쉬운가? 세포에서 어느 protein 들끼리 협업할 가능성이 높은가? 인터넷 토폴로지에서 어느 부분이 “backbone” 을 구성하는가? 다루는 문제가 무엇이냐에 따라 <strong>network cohesion</strong> 그 자체가 무엇인지 또한 달라짐. local 에서 global (ex. giant component), 어느 정도로 확실하게 정의되는가 (ex. clique) 혹은 두루뭉술한가 (ex. cluster, community) 등 스탯이 다양함.</p>
<p><br>
<br>
<br></p>
<div id="subgraphs-and-censuses" class="section level4 hasAnchor" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> Subgraphs and Censuses<a href="descriptive-statistics-of-networks.html#subgraphs-and-censuses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Cliques</strong></li>
</ul>
<p>complete 서브그래프, 즉 fully cohesive 한 vertex 들의 subset 을 <strong>Cliques</strong> 라고 통칭. 이인즉 소속된 모든 vertex 들이 edge 로 연결되어 있다는 것. 모든 사이즈의 clique 들에 대한 census 는 그래프가 어떻게 structure 되어 있는지에 대한 ‘snapshot’ 을 제공함.</p>
<p>더 큰 사이즈의 clique 는 필연적으로 작은 사이즈의 cluque 들을 포함함. <strong>maximal clique</strong> 는 더 큰 clique 의 subset 이 아닌 clique 를 일컫음. 큰 clique 는 본질적으로 드뭄. 큰 clique 의 존재는 결국 원본 그래프 <span class="math inline">\(G\)</span> 가 일정 이상으로 dense 할 것을 요구하니까. 하지만 현실세계 네트워크는 보통 sparse 하거든.</p>
<ul>
<li><mark>clique size: 1 (node), 2 (edge), 3 (triangle), 4 (안이 크로스된 사각형)</mark></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="descriptive-statistics-of-networks.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sapply</span>(<span class="fu">cliques</span>(karate), length))</span>
<span id="cb15-2"><a href="descriptive-statistics-of-networks.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb15-3"><a href="descriptive-statistics-of-networks.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  1  2  3  4  5 </span></span>
<span id="cb15-4"><a href="descriptive-statistics-of-networks.html#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 34 78 45 11  2</span></span>
<span id="cb15-5"><a href="descriptive-statistics-of-networks.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cliques</span>(karate)[<span class="fu">sapply</span>(<span class="fu">cliques</span>(karate), length) <span class="sc">==</span> <span class="dv">5</span>]</span>
<span id="cb15-6"><a href="descriptive-statistics-of-networks.html#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [[1]]</span></span>
<span id="cb15-7"><a href="descriptive-statistics-of-networks.html#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="do">## + 5/34 vertices, named, from 4b458a1:</span></span>
<span id="cb15-8"><a href="descriptive-statistics-of-networks.html#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] Mr Hi    Actor 2  Actor 3  Actor 4  Actor 14</span></span>
<span id="cb15-9"><a href="descriptive-statistics-of-networks.html#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb15-10"><a href="descriptive-statistics-of-networks.html#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [[2]]</span></span>
<span id="cb15-11"><a href="descriptive-statistics-of-networks.html#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="do">## + 5/34 vertices, named, from 4b458a1:</span></span>
<span id="cb15-12"><a href="descriptive-statistics-of-networks.html#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] Mr Hi   Actor 2 Actor 3 Actor 4 Actor 8</span></span>
<span id="cb15-13"><a href="descriptive-statistics-of-networks.html#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sapply</span>(<span class="fu">maximal.cliques</span>(karate), length))</span>
<span id="cb15-14"><a href="descriptive-statistics-of-networks.html#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb15-15"><a href="descriptive-statistics-of-networks.html#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  2  3  4  5 </span></span>
<span id="cb15-16"><a href="descriptive-statistics-of-networks.html#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 11 21  2  2</span></span>
<span id="cb15-17"><a href="descriptive-statistics-of-networks.html#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="fu">clique.number</span>(yeast)</span>
<span id="cb15-18"><a href="descriptive-statistics-of-networks.html#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 23</span></span></code></pre></div>
<ul>
<li><strong><span class="math inline">\(k\)</span>-core</strong></li>
</ul>
<p>clique 를 약화시킨 개념. 그래프 <span class="math inline">\(G\)</span> 의 <span class="math inline">\(k\)</span>-core 는 <span class="math inline">\(G\)</span> 의 서브그래프 중 모든 vertex degree 가 최소한 <span class="math inline">\(k\)</span> 는 되는 것 서브그래프들 중에서도, 다른 서브그래프들이 <span class="math inline">\(k\)</span>-core 와 동일한 condition 을 따르지 않는 것을 <strong><span class="math inline">\(k\)</span>-core</strong> 라고 말함. 즉, 해당 성질을 보유한 서브그래프들 중 maximal 한 놈. core 라는 개념은 특히 visualization 쪽에서 핫함. 네트워크를 ‘layer’ 로 decomposition 할 방법론을 제공하기 때문.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="descriptive-statistics-of-networks.html#cb16-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-2"><a href="descriptive-statistics-of-networks.html#cb16-2" aria-hidden="true" tabindex="-1"></a>cores <span class="ot">=</span> <span class="fu">graph.coreness</span>(karate)</span>
<span id="cb16-3"><a href="descriptive-statistics-of-networks.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">gplot.target</span>(g, cores, <span class="at">circ.lab =</span> <span class="cn">FALSE</span>, <span class="at">circ.col =</span> <span class="st">&quot;skyblue&quot;</span>,</span>
<span id="cb16-4"><a href="descriptive-statistics-of-networks.html#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">usearrows =</span> <span class="cn">FALSE</span>, <span class="at">vertex.col =</span> cores, <span class="at">edge.col =</span> <span class="st">&quot;darkgray&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="_main_files/figure-html/unnamed-chunk-12-1.png" alt="Visual representation of the k-core decomposition of the karate network" width="672" />
<p class="caption">
FIGURE 7.5: Visual representation of the k-core decomposition of the karate network
</p>
</div>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="descriptive-statistics-of-networks.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:sna&quot;</span>)</span>
<span id="cb17-2"><a href="descriptive-statistics-of-networks.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:network&quot;</span>)</span></code></pre></div>
<p>core 에 포함된 vertex 들은 center 에서 크게 떨어지지 않았으며, 각 core 에서 일정한 거리를 유지하고 있음이 시각적으로 확인 가능.</p>
<ul>
<li>Network Cohesion 을 정의함에 있어서 쓰이는 다른 서브그래프들의 class</li>
</ul>
<p>vertex 2개를 뽑아 만든 쌍 (pair) 를 <strong>dyad</strong> 라고 부름. directed 그래프에서 dyad 는 3개의 상태를 가질 수 있다.</p>
<ol style="list-style-type: decimal">
<li>null (no directed edges)</li>
<li>asymmetric (one directed edge)</li>
<li>mutual (two directed edges)</li>
</ol>
<p>대부분의 dyad 는 보통 null 이며, non-null 중에서도 대부분은 aymmetric 이다. 후자의 경우는 블로그에서 서로이웃이 아니라 한쪽만 팔로잉하는 경우겠지.</p>
<p>vertex 3개를 뽑아 만든 모음은 <strong>Triad</strong>. 이는 16개의 상태를 가질 수 있음. null 서브그래프부터, triad 에 속한 모든 vertex 들이 mutual directed edge 를 보유하는 서브그래프 까지.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="descriptive-statistics-of-networks.html#cb18-1" aria-hidden="true" tabindex="-1"></a>aidsblog <span class="ot">=</span> <span class="fu">simplify</span>(aidsblog)</span>
<span id="cb18-2"><a href="descriptive-statistics-of-networks.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dyad.census</span>(aidsblog)</span>
<span id="cb18-3"><a href="descriptive-statistics-of-networks.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="do">## $mut</span></span>
<span id="cb18-4"><a href="descriptive-statistics-of-networks.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3</span></span>
<span id="cb18-5"><a href="descriptive-statistics-of-networks.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb18-6"><a href="descriptive-statistics-of-networks.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="do">## $asym</span></span>
<span id="cb18-7"><a href="descriptive-statistics-of-networks.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 177</span></span>
<span id="cb18-8"><a href="descriptive-statistics-of-networks.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb18-9"><a href="descriptive-statistics-of-networks.html#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="do">## $null</span></span>
<span id="cb18-10"><a href="descriptive-statistics-of-networks.html#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 10405</span></span></code></pre></div>
<p>해당 데이터를 살펴보면 hub 와 authority 에 대한 기존 지식과 궤를 같이함을 확인 가능. <mark>Small connected subgraphs of interest are commonly termed motifs.</mark> <strong>motif</strong> 라는 개념은 생물 네트워크에서 두드러지게 유명한 개념, 생태계 substructure 를 biological function 과 연결지을때 자주 쓰임.</p>
<p><br>
<br>
<br></p>
</div>
<div id="density-and-related-notions-of-relative-frequency" class="section level4 hasAnchor" number="7.2.2.2">
<h4><span class="header-section-number">7.2.2.2</span> Density and Related Notions of Relative Frequency<a href="descriptive-statistics-of-networks.html#density-and-related-notions-of-relative-frequency" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Density</strong></li>
</ul>
<p>그래프의 <strong>Density</strong> 는 potential, 즉 잠재적으로 발생할 edge 대비 실제로 발생한 edge 들의 빈도. 예를 들어 (undirected) 그래프 <span class="math inline">\(G\)</span> 가 self-loop 가 없고 multiple edge 도 없다고 할 때, 서브그래프 <span class="math inline">\(H=(V_H, E_H)\)</span> 의 density 는</p>
<p><span class="math display">\[
\begin{align}
0 \le den(H) &amp;= \frac{|E_H|}{\frac{|V_H|(|V_H|-1)}{2}}= \frac{\text{# of edges}}{\text{# of dyads}} \le 1
\tag{undirected}
\\
&amp;= \frac{\phantom{|E_H|}}{|V_H|(|V_H|-1)}
\tag{directed}
\begin{alignat}
\]</span></p>
<p>해당 값은 <span class="math inline">\([0,1]\)</span> 에 존재하며 <span class="math inline">\(H\)</span> 가 clique 가 되기까지의 역치에 얼마나 가까운지에 대한 측도 (measure) 를 제공함.</p>
<p><span class="math inline">\(H=G\)</span> 인 상황이라면 전체 그래프 <span class="math inline">\(G\)</span> 에 대한 density 를 생산. 반대로 vertex <span class="math inline">\(v \in V\)</span> 의 neighbor 의 set <span class="math inline">\(H_v=H\)</span> 가 되게 한다면, 이들 사이의 edge 는 <span class="math inline">\(v\)</span> 의 immediate 이웃의 density 의 측도 (measure) 을 생산함. immediate 이웃의 합집합으로만 생산한 <strong>ego-centric 네트워크</strong> 는 원본의 overall 네트워크보다 명백히 dense 함.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="descriptive-statistics-of-networks.html#cb19-1" aria-hidden="true" tabindex="-1"></a>ego.instr <span class="ot">=</span> <span class="fu">induced.subgraph</span>(karate, <span class="fu">neighborhood</span>(karate, <span class="dv">1</span>,</span>
<span id="cb19-2"><a href="descriptive-statistics-of-networks.html#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span>)[[<span class="dv">1</span>]])</span>
<span id="cb19-3"><a href="descriptive-statistics-of-networks.html#cb19-3" aria-hidden="true" tabindex="-1"></a>ego.admin <span class="ot">=</span> <span class="fu">induced.subgraph</span>(karate, <span class="fu">neighborhood</span>(karate, <span class="dv">1</span>,</span>
<span id="cb19-4"><a href="descriptive-statistics-of-networks.html#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">34</span>)[[<span class="dv">1</span>]])</span>
<span id="cb19-5"><a href="descriptive-statistics-of-networks.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">graph.density</span>(karate)</span>
<span id="cb19-6"><a href="descriptive-statistics-of-networks.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">graph.density</span>(ego.instr)</span>
<span id="cb19-7"><a href="descriptive-statistics-of-networks.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="fu">graph.density</span>(ego.admin)</span></code></pre></div>
<ul>
<li><strong>Clustering Coefficients (transitivity)</strong></li>
</ul>
<p>일반적으로 이하를 일컫는다.</p>
<p><span class="math display">\[
cl_T (G) = \frac{3\tau_\Delta (G)}{\tau_3 (G)}
\]</span></p>
<ul>
<li><span class="math inline">\(\tau_\Delta (G)\)</span>: 그래프 <span class="math inline">\(G\)</span> 안에 있는 모든 triangle 의 숫자</li>
<li><span class="math inline">\(\tau_3 (G)\)</span>: connected triple, 즉 3개의 vertex 에 2개의 edge 가 놓여있는 (i.e., 2-star) vertex 들로 만든 서브그래프의 숫자.</li>
</ul>
<p>이 <strong>Clustering Coefficients</strong> 인 <span class="math inline">\(cl_T(g)\)</span> 는 그래프의 <strong>transitivity</strong> 라고 불리기도 함. 이는 소셜 네트워크 문헌에서 일반적으로 관심을 갖는 변량 중 하나임. 다른 말로 <strong>fraction of transitive triples</strong> 라고도 불림. <span class="math inline">\(cl_T(g)\)</span> 는 global clustering 의 measure 이며, <strong>connected triple</strong> 이 triangle 을 형성하기까지에 얼마나 가까운지에 대한 상대적 빈도를 서술함.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="descriptive-statistics-of-networks.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">transitivity</span>(karate)</span>
<span id="cb20-2"><a href="descriptive-statistics-of-networks.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.2556818</span></span>
<span id="cb20-3"><a href="descriptive-statistics-of-networks.html#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">transitivity</span>(karate, <span class="st">&quot;local&quot;</span>, <span class="at">vids =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">34</span>))</span>
<span id="cb20-4"><a href="descriptive-statistics-of-networks.html#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.1500000 0.1102941</span></span></code></pre></div>
<ul>
<li><strong>Reciprocity</strong></li>
</ul>
<p><strong>directed graph 에 한정된 개념</strong>. reciprocated (mutual) 한 edge 의 숫자를 총 edge 의 숫자로 나눈 것. 이는 single, unreciprocated 한 dyad 대비 reciprocated dyad 의 비중을 나타냄.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="descriptive-statistics-of-networks.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">reciprocity</span>(aidsblog, <span class="at">mode =</span> <span class="st">&quot;default&quot;</span>)</span>
<span id="cb21-2"><a href="descriptive-statistics-of-networks.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.03278689</span></span>
<span id="cb21-3"><a href="descriptive-statistics-of-networks.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">reciprocity</span>(aidsblog, <span class="at">mode =</span> <span class="st">&quot;ratio&quot;</span>)</span>
<span id="cb21-4"><a href="descriptive-statistics-of-networks.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.01666667</span></span></code></pre></div>
<p><br>
<br>
<br></p>
</div>
<div id="connectivity-cuts-and-flows" class="section level4 hasAnchor" number="7.2.2.3">
<h4><span class="header-section-number">7.2.2.3</span> Connectivity, Cuts, and Flows<a href="descriptive-statistics-of-networks.html#connectivity-cuts-and-flows" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>기본적으로 궁금한 건 주어진 그래프가 서로 다른 서브그래프로 쪼개질 수 있나 하는 것. 불가능하다면 해당 그래프가 이 쪼개질 수 있는 성질의 역치에 얼마나 가까운지를 체크하는 것이 목적이 된다.</p>
<p>만약 모든 vertex가 다른 모든 vertex에서 접근 가능하다면, 즉 adjacency Matrix가 diag 제외하고 모두 1이면, 그래프 <span class="math inline">\(G\)</span>는 <strong>connected</strong>라고 칭해진다.</p>
<p>그리고 그래프의 <strong>connected component</strong>는 maximally connected 서브그래프이다.</p>
<p>그래프 <span class="math inline">\(G\)</span>의 connected component 중 하나가 다른 모두를 위력에서 압도한다면, 이는 곧 해당 connected component가 <span class="math inline">\(G\)</span>의 대부분의 vertex를 포함하고 있다는 이야기. 이러한 component는 <strong>giant component</strong>라고 불리며 이는 random graph theory 출신 용어.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="descriptive-statistics-of-networks.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">is.connected</span>(yeast)</span>
<span id="cb22-2"><a href="descriptive-statistics-of-networks.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] FALSE</span></span>
<span id="cb22-3"><a href="descriptive-statistics-of-networks.html#cb22-3" aria-hidden="true" tabindex="-1"></a>comps <span class="ot">=</span> <span class="fu">decompose.graph</span>(yeast)</span>
<span id="cb22-4"><a href="descriptive-statistics-of-networks.html#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sapply</span>(comps, vcount))</span>
<span id="cb22-5"><a href="descriptive-statistics-of-networks.html#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb22-6"><a href="descriptive-statistics-of-networks.html#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="do">##    2    3    4    5    6    7 2375 </span></span>
<span id="cb22-7"><a href="descriptive-statistics-of-networks.html#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   63   13    5    6    1    3    1</span></span></code></pre></div>
<p>결과는 false로 나오지만 이에 대해 census 돌리면 giant component의 존재 확인 가능. 아래 예시의 경우 component 1개가 2375/2617로 90퍼 vertex랑 연결중임. 이는 현실 네트워크에서의 <strong>small world property</strong>와 연결. vertex 쌍들 collection에서의 minimum path는 보통 되게 작음. 대비되게 clustring은 상대적으로 높음. (ex) protein?</p>
<ul>
<li><mark><strong>small world property</strong>: high connectivity b/w pairs of nodes</mark>
<ul>
<li>small shortest-path distance</li>
<li>high clustering coefficient</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="descriptive-statistics-of-networks.html#cb23-1" aria-hidden="true" tabindex="-1"></a>yeast.gc <span class="ot">=</span> <span class="fu">decompose.graph</span>(yeast)[[<span class="dv">1</span>]]</span>
<span id="cb23-2"><a href="descriptive-statistics-of-networks.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">average.path.length</span>(yeast.gc)</span>
<span id="cb23-3"><a href="descriptive-statistics-of-networks.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 5.09597</span></span>
<span id="cb23-4"><a href="descriptive-statistics-of-networks.html#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="fu">diameter</span>(yeast.gc)</span>
<span id="cb23-5"><a href="descriptive-statistics-of-networks.html#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 15</span></span>
<span id="cb23-6"><a href="descriptive-statistics-of-networks.html#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">transitivity</span>(yeast.gc)</span>
<span id="cb23-7"><a href="descriptive-statistics-of-networks.html#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4686663</span></span></code></pre></div>
<p>해당 네트워크에서의 shortest path는 <span class="math inline">\(N_v\)</span>보다 <span class="math inline">\(\log N_v\)</span>로 표현되는게 정확할 정도로 짧음. scales more like, thus considered small. 동시에 해당 네트워크에서의 clustering은 상대적으로 large, 이는 transitivity로 확인 가능.</p>
<ul>
<li><strong>Connectivity</strong>
<ol style="list-style-type: decimal">
<li><strong><span class="math inline">\(k\)</span>-vertex-connected</strong>
<ul>
<li>the number of vertices <span class="math inline">\(N_v &gt; k\)</span></li>
<li>cardinality <span class="math inline">\(|X|&lt;k\)</span> 이며 <span class="math inline">\(X \subseteq V\)</span> 인 vertex의 subset <span class="math inline">\(X\)</span> 을 지우면 connected subgraph가 아니게 됨.</li>
</ul></li>
<li><strong><span class="math inline">\(k\)</span>-edge-connected</strong>
<ul>
<li><span class="math inline">\(N_v ≥ 2\)</span></li>
<li>cardinality <span class="math inline">\(|Y|&lt;k\)</span>이며 <span class="math inline">\(Y \subseteq E\)</span>인 edge의 subset <span class="math inline">\(Y\)</span>을 지우면 connected subgraph가 아니게 됨.</li>
</ul></li>
</ol></li>
</ul>
<p>위의 조건에 따라 그래프 <span class="math inline">\(G\)</span> 는 <strong><span class="math inline">\(k\)</span>-vertex-connected</strong> 혹은 <strong><span class="math inline">\(k\)</span>-edge-connected</strong>.</p>
<p>즉 <span class="math inline">\(G\)</span>의 vertex (edge) connectivity는 <span class="math inline">\(G\)</span>의 k-vertex(k-edge-) connected가 유지되는 가장 큰 integer. <mark> 이때 vertex connectivity <span class="math inline">\(\le\)</span> edge connectivity <span class="math inline">\(\le\)</span> minimum degree among vertex in <span class="math inline">\(G\)</span> (dmin).</mark> 따라서 이 서브그래프를 추가적인 component로 분해하기 위해서는 단 1개의 엄선된 vertex나 edge를 제거하는 것으로 충분하다.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="descriptive-statistics-of-networks.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vertex.connectivity</span>(yeast.gc)</span>
<span id="cb24-2"><a href="descriptive-statistics-of-networks.html#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb24-3"><a href="descriptive-statistics-of-networks.html#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">edge.connectivity</span>(yeast.gc)</span>
<span id="cb24-4"><a href="descriptive-statistics-of-networks.html#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<ul>
<li><strong>Cut</strong></li>
</ul>
<p>vertex (edge)의 subset <span class="math inline">\(S\)</span>를 제거하는 것으로 해당 그래프가 서브그래프로 조각난다면, <span class="math inline">\(S\)</span>는 vertex-cut (edge-cut). 여기서 vertex <span class="math inline">\(S\)</span>의 원소가 1개라면, 즉 vertex 1개만을 제거한 것으로 그래프가 조각났다면, 이는 cut vertex, 혹은 <strong>articulation point</strong>. 이러한 vertex의 여부를 식별하는 건 해당 네트워크가 외부 공격에 취약하는지를 파악하는데 도움이 됨. 해당 포인트 끊기면 네트워크 정상작동이 안되니까.</p>
<ul>
<li>Identification of such vertices can provide a sense of where a network is vulnerable (e.g., in the sense of an attack, where disconnecting produces undesired consequences, such as a power outage in an energy network).</li>
<li>In the giant component of the yeast network, almost 15% of the vertices are cut vertices.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="descriptive-statistics-of-networks.html#cb25-1" aria-hidden="true" tabindex="-1"></a>yeast.cut.vertices <span class="ot">=</span> <span class="fu">articulation.points</span>(yeast.gc)</span>
<span id="cb25-2"><a href="descriptive-statistics-of-networks.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(yeast.cut.vertices)</span>
<span id="cb25-3"><a href="descriptive-statistics-of-networks.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 350</span></span></code></pre></div>
<p>nontrivial 그래프 <span class="math inline">\(G\)</span>는 <span class="math inline">\(k\)</span>-vertex (k-edge) connected <span class="math inline">\(\iff\)</span> 서로다른 vertex의 쌍 <span class="math inline">\(u, v \in V\)</span>가 <span class="math inline">\(k\)</span> vertex-<strong>disjoint</strong> (edge-<strong>disjoint</strong>) paths에 의해 connected 가능.</p>
<p>이 결과는 그래프에서 특정 vertex (edge)가 제거된 상황에서도 그래프 내부에서 만들어지는 서로 다른 path 들이 얼마나 많은지를 통해 평가되는 그래프의 robust함과 연결되어 있다. 낮은 vertex (edge) connectivity 를 가지는 그래프는 따라서 path들을 가질 수 있으며, 이에 의해 그 path들을 통과했던 “information”들은 작은 숫자의 vertex (edge)를 없애는 것만으로 쉽게 방해되고 만다.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="descriptive-statistics-of-networks.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shortest.paths</span>()</span>
<span id="cb26-2"><a href="descriptive-statistics-of-networks.html#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">graph.maxfow</span>()</span>
<span id="cb26-3"><a href="descriptive-statistics-of-networks.html#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">graph.mincu</span>()</span></code></pre></div>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="graph-partitioning" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Graph Partitioning<a href="descriptive-statistics-of-networks.html#graph-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Partitioning</strong>은 elements의 집합을 “발생이 자연스러운” 부분집합으로 분할하는 과정. 더 이론적으로 말하자면, finite set <span class="math inline">\(S\)</span>의 partition <span class="math inline">\(C = \{ C_1, \cdots, C_K \}\)</span>는 <span class="math inline">\(S\)</span>를 <span class="math inline">\(K\)</span> 개의 disjoint로 decomposition 한 물건으로, 이인즉 <span class="math inline">\(\forall C_k \not = \emptyset: \cup_{k=1}^K C_k = S\)</span>.</p>
<p>네트워크 그래프 분석에서, partitioning은 겉으로 드러나지 않는 관계성 측면에서 vertex의 묶음이 <strong>cohesiveness</strong>를 가지고 있는지를 확인하기에 유용한 방법이다. vertex의 <strong>cohesive</strong>한 subset은 일반적으로 이하와 같은 걸 일컬음:</p>
<ol style="list-style-type: decimal">
<li>subset 내부에서, <strong>동시에</strong>, 잘 connected 되어 있어야 한다</li>
<li>subset 외부, 즉 남아있는 vertex들과 잘 seperated - 연결성이 없음</li>
</ol>
<p>Graph partitioning algorithms 은 보통 그래프 <span class="math inline">\(G(V, E)\)</span>의 vertex set <span class="math inline">\(V\)</span> 의 partition <span class="math inline">\(C = \{ C_1, \cdots, C_K \}\)</span>를 찾는 것을 그 목표로 함. 이를 위한 방법으로 <span class="math inline">\(C_k\)</span> 안의 vertex에서 <span class="math inline">\(C_k&#39;\)</span>로의 vertex로 잇는 edge의 sets <span class="math inline">\(E(C_k, C_k &#39;)\)</span>는 <span class="math inline">\(C_k\)</span> 내에서 vertex 를 잇는 edge들의 set <span class="math inline">\(E(C_k) = E(C_k , C_k)\)</span>보다 작다는 점을 활용함.</p>
<p>그래프 partitioning의 이 문제는 complex networks 문헌에서의 community detection에서도 동일하게 발생함. 이에 대한 해결책으로 큰 틀에서 <strong>2가지 접근법</strong> 이 존재.</p>
<p><br>
<br>
<br></p>
<div id="hierarchical-clustering-1" class="section level4 hasAnchor" number="7.2.3.1">
<h4><span class="header-section-number">7.2.3.1</span> Hierarchical Clustering<a href="descriptive-statistics-of-networks.html#hierarchical-clustering-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>그래프 파티셔닝에 사용되는 대부분의 방법은 본질적으로 Hierarchical Clustering의 변용에 불과함. 여러가지 방법론이 제시되었지만, 그 차이는 결국 이하가 다를 뿐임.</p>
<ol style="list-style-type: decimal">
<li>proposed clusterings의 quality를 어떻게 측정하는가</li>
<li>연구자가 찾고 있는 해당 quality를 어떻게 최적화하는가. 보통 greedy algorithm 으로 모든 가능한 partition <span class="math inline">\(C\)</span>의 space를 탐색하는 식으로 한다. 이 과정에서 계속해서 후보 partition을 갱신하고.</li>
</ol>
<p>Hierarchical methods 는 다음 둘로 분류됨.</p>
<ol style="list-style-type: decimal">
<li><strong>agglomerative</strong>, 파티션을 합쳐나가는 것을 계속해나가는 것으로 크기를 키워가는 것에 기반 (coarsen)</li>
<li><strong>divisive</strong>, 파티션을 쪼개나가는 것을 계속해나가는 것으로 연속으로 다듬어나가는 것</li>
</ol>
<p>각 단계에서 현재의 후보 partition은 지정된 비용 측정값을 최소화한다는 목적으로 계속해서 정제되어 갑니다.</p>
<ol style="list-style-type: decimal">
<li>agglomerative 방법에서는, 2개의 이전의 partition elements 중 가장 저렴한 merge 방법이 실행된다</li>
<li>divisive 방법에서는, 1개의 이전의 partition 중 가장 저렴하게 2개로 split 할 수 있는 방법이 실행된다</li>
</ol>
<p>비용측정의 기준은 vertex의 <strong>cohesive</strong> subset을 뭘 기준으로 판정할지 하는 연구자의 주관이 개입됨. 메이저한 기준은 <strong>modularity</strong>. 계산은 이하와 같다:</p>
<ul>
<li><span class="math inline">\(C = \{ C_1, \cdots, C_K \}\)</span>를 주어진 (given) 후보 (candidate) partition 으로 하자</li>
<li><span class="math inline">\(f_{ij} = f_{ij}(\mathcal C)\)</span>는 <span class="math inline">\(C_i\)</span> 에 있던 vertex 들을 <span class="math inline">\(C_j\)</span> 에 있는 vertex 들과 연결시키는 (오리지널 네트워크의) edge 들의 fraction</li>
</ul>
<p>이때 <span class="math inline">\(\mathcal C\)</span>의 <strong>modularity</strong>는</p>
<p><span class="math display">\[
\mod(\mathcal C) = \sum_{k=1}^K \left[ f_{kk}(\mathcal C) - f_{kk}^\ast \right]
\tag{modularity}
\]</span></p>
<ul>
<li><span class="math inline">\(f_{kk}\)</span> 는 within <span class="math inline">\(C_k\)</span> 에서의 observed connections.</li>
<li><span class="math inline">\(f_{kk}^\ast\)</span>는 random edge assignment의 몇몇 모델 이하에서의 <span class="math inline">\(f_{kk}\)</span>의 기댓값.
<ul>
<li><span class="math inline">\(f_{kk}^\ast\)</span>는 <span class="math inline">\(f_{k+} \cdot f_{+k}\)</span>이며 각각 <span class="math inline">\(f\)</span>의 k번째 rowsum과 colsum. 즉 <span class="math inline">\(f_{ij}\)</span>를 entry로 하는 <span class="math inline">\(K \times K\)</span> 매트릭스가 만들어짐.</li>
</ul></li>
</ul>
<p><mark>
This choice corresponds to a model in which a graph is constructed to have the same degree distribution as <span class="math inline">\(G\)</span>, but with edges otherwise placed at random, without respect to the underlying partition elements dictated by <span class="math inline">\(C\)</span>.
</mark></p>
<p>In principle the optimization of the modularity requires a search over all possible partitions C, which is prohibitively expensive in networks of moderate size and larger.
• A fast, greedy approach to optimization has been proposed, in the form of an agglomerative hierarchical clustering algorithm, and implemented in igraph as fastgreedy.community.
• The result of this and related community detection methods in igraph is to produce an object of the class communities, which can then serve as input to various other functions.</p>
<p>Applying this method to the karate network,</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="descriptive-statistics-of-networks.html#cb27-1" aria-hidden="true" tabindex="-1"></a>kc <span class="ot">=</span> <span class="fu">fastgreedy.community</span>(karate)</span>
<span id="cb27-2"><a href="descriptive-statistics-of-networks.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(kc)</span>
<span id="cb27-3"><a href="descriptive-statistics-of-networks.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3</span></span>
<span id="cb27-4"><a href="descriptive-statistics-of-networks.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sizes</span>(kc)</span>
<span id="cb27-5"><a href="descriptive-statistics-of-networks.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Community sizes</span></span>
<span id="cb27-6"><a href="descriptive-statistics-of-networks.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  1  2  3 </span></span>
<span id="cb27-7"><a href="descriptive-statistics-of-networks.html#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 18 11  5</span></span>
<span id="cb27-8"><a href="descriptive-statistics-of-networks.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">membership</span>(kc))</span>
<span id="cb27-9"><a href="descriptive-statistics-of-networks.html#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="do">##   Mr Hi Actor 2 Actor 3 Actor 4 Actor 5 Actor 6 </span></span>
<span id="cb27-10"><a href="descriptive-statistics-of-networks.html#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="do">##       2       2       2       2       3       3</span></span></code></pre></div>
<p><mark>high modularity value nontrivial group ???</mark></p>
<p>The largest community of 18 members is centered around the administrator (i.e., John A, vertex ID 34).
• The second largest community of 11 members is centered around the head instructor (i.e., Mr Hi, vertex ID 1).</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="descriptive-statistics-of-networks.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(kc, karate)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="_main_files/figure-html/unnamed-chunk-24-1.png" alt="Partitioning of the Karate network obtained from hierarchical clustering" width="672" />
<p class="caption">
FIGURE 7.6: Partitioning of the Karate network obtained from hierarchical clustering
</p>
</div>
<p>Figure 9: Partitioning of the Karate network obtained from hierarchical clustering</p>
<p>• Whether agglomerative or divisive, when used for network graph partitioning, hierarchical clustering methods actually produce, as the name indicates, an entire hierarchy of nested partitions of the graph, not just a single partition.
• The resulting hierarchy typically is represented in the form of a tree, called a dendrogram.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="descriptive-statistics-of-networks.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ape)</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="descriptive-statistics-of-networks.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dendPlot</span>(kc, <span class="at">mode =</span> <span class="st">&quot;phylo&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="_main_files/figure-html/unnamed-chunk-26-1.png" alt="The corresponding dendrogram for this partitioning" width="672" />
<p class="caption">
FIGURE 7.7: The corresponding dendrogram for this partitioning
</p>
</div>
<p><br>
<br>
<br></p>
</div>
<div id="spectral-partitioning" class="section level4 hasAnchor" number="7.2.3.2">
<h4><span class="header-section-number">7.2.3.2</span> Spectral Partitioning<a href="descriptive-statistics-of-networks.html#spectral-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>Calculate Laplacian Grpah</li>
</ol>
<p><span class="math display">\[
L = \underbrace{D}_{\text{diagonal matrix with degree}}-\underbrace{A}_{\text{Adjacency Matrix}}
\\
=
\begin{bmatrix}
degree(n_1) &amp;  &amp; 0 \\
&amp; \ddots &amp;  \\
0 &amp;  &amp; degree(n_r)
\end{bmatrix}
-A
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Eigenvalue decomposition of <span class="math inline">\(L\)</span></li>
</ol>
<p><span class="math display">\[
L = \underbrace{\begin{bmatrix} v_1 &amp; \cdots &amp; v_n\end{bmatrix}}_{eigenvector}
\underbrace{\begin{bmatrix} \lambda_1 &amp;  &amp; 0 \\ &amp; \ddots &amp; \\ 0&amp; &amp; \lambda_n \end{bmatrix}}_{eigenvalue}
\begin{bmatrix} v_1 \\ \vdots \\ v_n\end{bmatrix}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p><mark>sort eigenvalue <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \cdots &gt; \lambda_n\)</span>. Is <span class="math inline">\(L\)</span> full rank? No. Why? cause <span class="math inline">\(L\)</span> is a representation of a network. Smallest eigenvalue can be shown to be identically zero and its corresponding eigenvecotr 1. Then? # of component in a grpah is directly related to # of non-zero eigenvalue. Which means <span class="math inline">\(\lambda_{N-1} \approx 0 \Rightarrow K=2\)</span>, <span class="math inline">\(\lambda_{N-1} \approx \lambda_{N-2} \approx 0 \Rightarrow K=3\)</span></mark><a href="section-8.html#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p></li>
<li><p>select <span class="math inline">\(K\)</span> eigenvector. <span class="math inline">\(v_{n-1}, \cdots, v_{n-k}\)</span>.</p></li>
<li><p>apply <span class="math inline">\(k\)</span>-means clustering to <span class="math inline">\(k\)</span> selected eigenvector.</p></li>
</ol>
<p>spectral graph theory의 연구결과를 응용하여 그래프 <span class="math inline">\(G\)</span>의 connectivity를 특정 매트릭스의 eigen-analysis와 연관짓는 것.</p>
<p>adjacency matrix <span class="math inline">\(A\)</span>에 대한 그래프 <span class="math inline">\(G\)</span>의 그래프 Laplacian 은 <span class="math inline">\(L = D − A\)</span>이며, 이때 <span class="math inline">\(D = diag[(D_{vv} = d_v)]\)</span>, <span class="math inline">\(d_v\)</span>는 <span class="math inline">\(G\)</span>의 entries of the degree sequences.</p>
<p>spectral graph theory의 결과를 통해 우리는 다음을 파악 가능.</p>
<p>그래프 <span class="math inline">\(G\)</span>는 <span class="math inline">\(K\)</span> 개의 connected components로 구성 <span class="math inline">\(\iff\)</span> <span class="math inline">\(\lambda_1 (L) = \cdots = \lambda_K(L) = 0\)</span> 이며 <span class="math inline">\(\lambda_{K+1}(L)&gt;0\)</span>, where <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_{N_v}\)</span>들은 L의 (not necessarily distinct) eigenvalue이며, <mark>ordered from small to large</mark>.</p>
<p>그래프 안의 component의 숫자는 그래프 Laplacian의 non-zero eigenvalue의 숫자과 직접적으로 연관되어 있음. <span class="math inline">\(L\)</span>의 최소 eigenvalue는 0임을 바로 보일 수 있다. eigenvector <span class="math inline">\(x_1 = (1,\cdots,1)&#39;\)</span>에 대응하므로. 따라서 우리가 그래프 <span class="math inline">\(G\)</span>가 “거의” <span class="math inline">\(K=2\)</span> 개의 component들로 구성되어 있다고 추론한다면, 즉슨 2개로 쪼개기에 적합하다고 생각한다면, 이는 곧 우리는 해당 대상에 대해 <span class="math inline">\(\lambda_2(L)\)</span>가 0에 가까울 것이라고 추론할 것이라는 것과 동치이다. 이러한 추론은 <span class="math inline">\(\lambda_2\)</span>가 그래프 connectivity와 structure의 측정치의 값과 깊은 연관이 있기에 합리적이다. 특히 이러한 관계성은 <span class="math inline">\(\lambda_2\)</span>가 0에 가까울 수록 서브그래프 A과 서브그래프 B 사이를 통과하는 edge가 적을 것이기에 이렇게 둘로 쪼개는 것이 합리적일 것임을 보여준다. <span class="math inline">\(\lambda_2\)</span>를 그래프의 connectivity와 연관지은 제언자는 대응하는 eigenvector <span class="math inline">\(x_2\)</span> 안의 entries들의 부호에 따라 vertex들을 쪼개는 것을 주장했다. 결과는 다음과 같다:</p>
<p><span class="math display">\[
S = \{v \in V: x_2 (v) \ge 0 \}
\\
\bar S = \{v \in V: x_2 (v) &lt; 0 \}
\]</span></p>
<p>즉, 2개의 vertex의 subset이 생산되며 (이를 보통 <strong>cut</strong>이라고 부름), 이 벡터 <span class="math inline">\(x_2\)</span>는 보통 <strong>Fiedler Vector</strong>라고 불리며 이에 대응하는 eigenvalue <span class="math inline">\(\lambda_2\)</span>는 <strong>Fiedler Value</strong>라고 부른다.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="descriptive-statistics-of-networks.html#cb31-1" aria-hidden="true" tabindex="-1"></a>k.lap <span class="ot">=</span> <span class="fu">graph.laplacian</span>(karate)</span>
<span id="cb31-2"><a href="descriptive-statistics-of-networks.html#cb31-2" aria-hidden="true" tabindex="-1"></a>eig.anal <span class="ot">=</span> <span class="fu">eigen</span>(k.lap)</span>
<span id="cb31-3"><a href="descriptive-statistics-of-networks.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(eig.anal<span class="sc">$</span>values, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Eigenvalues of Graph Laplacian&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-27"></span>
<img src="_main_files/figure-html/unnamed-chunk-27-1.png" alt="Eigenvalues of Graph Laplacian" width="672" />
<p class="caption">
FIGURE 7.8: Eigenvalues of Graph Laplacian
</p>
</div>
<p>We plot the eigenvalues of the graph Laplacian.</p>
<ol style="list-style-type: decimal">
<li>0인 eigenvalue는 딱 하나. (해당 네트워크는 connected이므로 예상한 결과)</li>
<li>2번째로 작은 eigenvalue인 <span class="math inline">\(\lambda_2\)</span>는 0에 매우 가까움.</li>
</ol>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="descriptive-statistics-of-networks.html#cb32-1" aria-hidden="true" tabindex="-1"></a>f.vec <span class="ot">=</span> eig.anal<span class="sc">$</span>vectors[, <span class="dv">33</span>]  <span class="co">#Extracting the Fiedler vector</span></span>
<span id="cb32-2"><a href="descriptive-statistics-of-networks.html#cb32-2" aria-hidden="true" tabindex="-1"></a>faction <span class="ot">=</span> <span class="fu">get.vertex.attribute</span>(karate, <span class="st">&quot;Faction&quot;</span>)</span>
<span id="cb32-3"><a href="descriptive-statistics-of-networks.html#cb32-3" aria-hidden="true" tabindex="-1"></a>f.colors <span class="ot">=</span> <span class="fu">as.character</span>(<span class="fu">length</span>(faction))</span>
<span id="cb32-4"><a href="descriptive-statistics-of-networks.html#cb32-4" aria-hidden="true" tabindex="-1"></a>f.colors[faction <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">=</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb32-5"><a href="descriptive-statistics-of-networks.html#cb32-5" aria-hidden="true" tabindex="-1"></a>f.colors[faction <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">=</span> <span class="st">&quot;cyan&quot;</span></span>
<span id="cb32-6"><a href="descriptive-statistics-of-networks.html#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(f.vec, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">xlab =</span> <span class="st">&quot;Actor Number&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Fiedler Vector Entry&quot;</span>,</span>
<span id="cb32-7"><a href="descriptive-statistics-of-networks.html#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> f.colors)</span>
<span id="cb32-8"><a href="descriptive-statistics-of-networks.html#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;lightgray&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="_main_files/figure-html/unnamed-chunk-28-1.png" alt="Fiedler vector and its corresponding partition" width="672" />
<p class="caption">
FIGURE 7.9: Fiedler vector and its corresponding partition
</p>
</div>
<p>Fiedler vector를 생산하고 해당 vector의 요소들을 실제 actor number에 따라 배정한 그래프를 보면 이 spectral 방법이 faction label에 의해 네트워크 partitioning 을 획득할 수 있다는 것을 확인된다.</p>
<p>보통 우리는 네트워크가 서브그래프 2개보다는 더 잘게 쪼개질 수 있으리라고 예상 가능. spectral 방법을 iterative하게 적용하는 것으로 2개 이상으로 쪼갤 수 있음. 하지만 이러한 반복이 특정 목적 함수를 최적화할 수 있도록 목표하는 것이 바람직함. Newman은 spectral bisection method와 논리적 흐름이 유사하나 Laplacian <span class="math inline">\(L\)</span>이 아니라 이를 대체해서 modularity와 연관된 매트릭스를 사용하는 방법을 제안했다.(leading.eigenvector.community)</p>
<p><br>
<br>
<br></p>
</div>
<div id="validation-of-graph-partitioning" class="section level4 hasAnchor" number="7.2.3.3">
<h4><span class="header-section-number">7.2.3.3</span> Validation of Graph Partitioning<a href="descriptive-statistics-of-networks.html#validation-of-graph-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>validation 문제는 그래프 partitioning에 항상 중요하지만, 대부분의 경우 nontrivial 문제이다. 네트워크 그래프에 vertex의 cohesive subset 이 존재한다면, 이러한 subset의 기저에는 vertex에게 있어 vertex 간에 특정한 연관적인 특성 (또는 속성)에 일부 공통성이 있을 것으로 일반적으로 예상한다. 그래프 partitioning은 이러한 성질에 대한 지식이 없을때 그러한 subset을 발견하기 위한 도구로 인식될 수도 있다. 우리가 그래프 외부에서 정의된 클래스 멤버쉽에 대한 subset 정의를 알고 있다면, 그래프 내부에서의 partitioning으로 얻은 분절들과 비교하는 것도 흥미로움.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="descriptive-statistics-of-networks.html#cb33-1" aria-hidden="true" tabindex="-1"></a>func.class <span class="ot">=</span> <span class="fu">get.vertex.attribute</span>(yeast.gc, <span class="st">&quot;Class&quot;</span>)</span>
<span id="cb33-2"><a href="descriptive-statistics-of-networks.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(func.class)</span>
<span id="cb33-3"><a href="descriptive-statistics-of-networks.html#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="do">## func.class</span></span>
<span id="cb33-4"><a href="descriptive-statistics-of-networks.html#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   A   B   C   D   E   F   G   M   O   P   R   T   U </span></span>
<span id="cb33-5"><a href="descriptive-statistics-of-networks.html#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  51  98 122 238  95 171  96 278 171 248  45 240 483</span></span></code></pre></div>
<p>해당 예시는 cell 구축에 있어 protein이 역할하는 바로 분절했음. 단백질들이 서로 다른 단백질들과 얼마나 유사한지는 특정 세포 역할에 해당 단백질이 무슨 일을 하는지와 연관되어 있다고 알려져 있음. 그래프 외부에서 이러한 단백질들을 분류하려는 시도는 분류된 결과가 그래프 내부에서 합리적은 partitioning 과정을 걸쳐 나온 결과물과 어느정도는 연관이 있는게 맞다. 아니면 partitioning이 잘못됐던가 그래프 외부 분절이 잘못됐던가.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="descriptive-statistics-of-networks.html#cb34-1" aria-hidden="true" tabindex="-1"></a>yc <span class="ot">=</span> <span class="fu">fastgreedy.community</span>(yeast.gc)</span>
<span id="cb34-2"><a href="descriptive-statistics-of-networks.html#cb34-2" aria-hidden="true" tabindex="-1"></a>c.m <span class="ot">=</span> <span class="fu">membership</span>(yc)</span>
<span id="cb34-3"><a href="descriptive-statistics-of-networks.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">table</span>(c.m, func.class, <span class="at">useNA =</span> <span class="fu">c</span>(<span class="st">&quot;no&quot;</span>)))</span>
<span id="cb34-4"><a href="descriptive-statistics-of-networks.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="do">##    func.class</span></span>
<span id="cb34-5"><a href="descriptive-statistics-of-networks.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="do">## c.m   A   B   C   D   E   F   G   M   O   P   R   T   U</span></span>
<span id="cb34-6"><a href="descriptive-statistics-of-networks.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="do">##   1   0   0   0   1   3   7   0   6   3 110   2  35  14</span></span>
<span id="cb34-7"><a href="descriptive-statistics-of-networks.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   2   0   2   2   7   1   1   1   4  39   5   0   4  27</span></span>
<span id="cb34-8"><a href="descriptive-statistics-of-networks.html#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   3   1   9   7  18   4   8   4  20  10  23   8  74  64</span></span>
<span id="cb34-9"><a href="descriptive-statistics-of-networks.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="do">##   4  25  11  10  22  72  84  81 168  14  75  16  27 121</span></span>
<span id="cb34-10"><a href="descriptive-statistics-of-networks.html#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="do">##   5   1   7   5  14   0   4   0   2   3   6   1  34  68</span></span>
<span id="cb34-11"><a href="descriptive-statistics-of-networks.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="do">##   6   1  24   1   4   1   4   0   7   0   1   0  19  16</span></span></code></pre></div>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="assortativity-and-mixing" class="section level3 hasAnchor" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Assortativity and Mixing<a href="descriptive-statistics-of-networks.html#assortativity-and-mixing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Assortative mixing</strong></li>
</ul>
<p>특정 성질에 따라서 vertex 중에 선별적으로 연결.</p>
<ul>
<li>Assortativity coefficients</li>
</ul>
<p>assortative mixing의 정도를 량화하는 측도. 이는 correlation coefficients의 변용. vertex 특성은 categorical, ordinal, or continuous 다 가능. categorical 케이스를 가정하고, 그래프 <span class="math inline">\(G\)</span>의 각 vertex가 <span class="math inline">\(M\)</span>개의 카테고리 중에 label 될 수 있다고 생각하자. 이 세팅에서의 Assortativity coefficients <span class="math inline">\(r_a\)</span>는 아래와 같다.</p>
<p><span class="math display">\[
r_a = \frac{\sum_{i}f_{ii} - \sum_i f_{x+}f_{+y}}{1 - \sum_if_{x+}f_{+y}}
\]</span></p>
<p><mark>where <span class="math inline">\(f_{ij}\)</span> is the fraction of edges in <span class="math inline">\(G\)</span> that join a vertex in the <span class="math inline">\(i\)</span>-th category with a vertex in the jth category, and <span class="math inline">\(f_{i+}\)</span> and <span class="math inline">\(f_{+i}\)</span> denote the ith marginal row and column sums, respectively, of the resulting matrix <span class="math inline">\(f\)</span>.</mark></p>
<p>이때 <span class="math inline">\(-1 \le r_a \le 1\)</span></p>
<p>– It is equal to zero when the mixing in the graph is no different from that obtained through a random assignment of edges that preserves the marginal degree distribution.
– It is equal to one when there is perfect assortative mixing (i.e., when edges only connect vertices of the same category).
– Howeigenvalueer, in the eigenvalueent that the mixing is perfectly disassortative, in the sense that eigenvalueery edge in the graph connects vertices of two different categories, the coefficient need not take the value −1.
• The fact that physical binding of proteins is known to be directly releigenvalueant to functional classes suggests that there will frequently be strong assortative mixing in protein-protein interaction networks with respective to these classes as attributes.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="descriptive-statistics-of-networks.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">assortativity.nominal</span>(yeast, (<span class="fu">replace</span>(<span class="fu">V</span>(yeast)<span class="sc">$</span>Class, <span class="fu">is.na</span>(<span class="fu">V</span>(yeast)<span class="sc">$</span>Class),</span>
<span id="cb35-2"><a href="descriptive-statistics-of-networks.html#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>) <span class="sc">==</span> <span class="st">&quot;P&quot;</span>) <span class="sc">+</span> <span class="dv">1</span>, <span class="at">directed =</span> <span class="cn">FALSE</span>)</span>
<span id="cb35-3"><a href="descriptive-statistics-of-networks.html#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.5232879</span></span>
<span id="cb35-4"><a href="descriptive-statistics-of-networks.html#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">assortativity.degree</span>(yeast)</span>
<span id="cb35-5"><a href="descriptive-statistics-of-networks.html#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4610798</span></span></code></pre></div>
<p>• When the vertex characteristic of interest is continuous, rather than discrete, denote by (xe, ye) the
values of that characteristic for the vertices joined by an edge e ∈ E.
• A natural candidate for quantifying the assortativity in this characteristic is just th e Pearson correlation coefficient of the pairs (xe, ye),</p>
<p><span class="math display">\[
r = \frac{\sum_{x,y}xy(f_{xy} - f_{x+}f_{+y})}{\sigma_x \sigma_y}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-collection-and-sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/212102_DescriptiveStats.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
