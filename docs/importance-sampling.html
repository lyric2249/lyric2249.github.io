<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 Importance Sampling | Self-Study</title>
  <meta name="description" content="4.1 Importance Sampling | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 Importance Sampling | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="https://github.com/lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 Importance Sampling | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mcmc.html"/>
<link rel="next" href="markov-chain-monte-carlo.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="part"><span><b>I 20-02</b></span></li>
<li class="chapter" data-level="1" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>1</b> Categorical</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Data Type and Statistical Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian</a>
<ul>
<li class="chapter" data-level="2.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>2.1</b> Abstract</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>2.1.1</b> 변수의 독립성</a></li>
<li class="chapter" data-level="2.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>2.1.2</b> 교환가능성</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>2.2</b> Continual Aeassessment Method</a></li>
<li class="chapter" data-level="2.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>2.3</b> Horseshoe Prior</a></li>
</ul></li>
<li class="part"><span><b>II 21-01</b></span></li>
<li class="chapter" data-level="3" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>3</b> Mathematical Stats</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3.1</b> Inference</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>3.1.1</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="completeness.html"><a href="completeness.html"><i class="fa fa-check"></i><b>3.2</b> Completeness</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="completeness.html"><a href="completeness.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>3.2.1</b> 레만-쉐페 thm.</a></li>
<li class="chapter" data-level="3.2.2" data-path="completeness.html"><a href="completeness.html#rao-blackwell-thm.-1"><i class="fa fa-check"></i><b>3.2.2</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Test</a></li>
<li class="chapter" data-level="3.4" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>3.4</b> Power Fucntion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>3.4.1</b> Significance Probability (p-value)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>3.5</b> Optimal Testing Method</a></li>
<li class="chapter" data-level="3.6" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>3.6</b> Data Reduction</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>3.6.1</b> Sufficiency Principle</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>3.7</b> Borel Paradox</a></li>
<li class="chapter" data-level="3.8" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>3.8</b> Neyman–Pearson lemma</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>3.8.1</b> Overview</a></li>
<li class="chapter" data-level="3.8.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>3.8.2</b> Generalized LRT</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>3.9</b> 개념</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>4</b> MCMC</a>
<ul>
<li class="chapter" data-level="4.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Importance Sampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Independent Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>4.2</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>4.2.1</b> MH Algorithm</a></li>
<li class="chapter" data-level="4.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>4.2.2</b> Random Walk Chains (Most Widely Used)</a></li>
<li class="chapter" data-level="4.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>4.2.3</b> Basic Gibbs Sampler</a></li>
<li class="chapter" data-level="4.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>4.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>4.3</b> Advanced MCMC (wk08)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>4.3.1</b> 1. Data Augmentation</a></li>
<li class="chapter" data-level="4.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>4.3.2</b> 2. Hit-and-Run Algorithm</a></li>
<li class="chapter" data-level="4.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>4.3.3</b> 3. Metropolis-Adjusted Langevin Algorithm</a></li>
<li class="chapter" data-level="4.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>4.3.4</b> 4. Multiple-Try Metropolis Algorithm</a></li>
<li class="chapter" data-level="4.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>4.3.5</b> 5. Reversible Jump MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>4.4</b> Auxiliary Variable MCMC</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Multimodal Target Distribution</a></li>
<li class="chapter" data-level="4.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>4.4.3</b> Doubly-intractable Normalizing Constants</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>4.5.1</b> Simulator-Based Models</a></li>
<li class="chapter" data-level="4.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>4.5.2</b> ABCifying Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>4.5.3</b> ABC-MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>4.6</b> Hamiltonian Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.6.1</b> Introduction to Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>4.7</b> Population Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>4.7.1</b> Adaptive Direction Sampling</a></li>
<li class="chapter" data-level="4.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>4.7.2</b> Conjugate Gradient MC</a></li>
<li class="chapter" data-level="4.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>4.7.3</b> Parallel Tempering</a></li>
<li class="chapter" data-level="4.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>4.7.4</b> Evolutionary MC</a></li>
<li class="chapter" data-level="4.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>4.7.5</b> Sequential Parallel Tempering</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>4.8</b> Stochastic Approximation Monte Carlo</a></li>
<li class="chapter" data-level="4.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>4.9</b> Review</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>4.9.1</b> Wk01</a></li>
<li class="chapter" data-level="4.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>4.9.2</b> wk03</a></li>
<li class="chapter" data-level="4.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>4.9.3</b> wk04, 05</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>4.10</b> Else</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>4.10.1</b> Hw4. Rasch Model</a></li>
<li class="chapter" data-level="4.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>4.10.2</b> DA) Example: MVN</a></li>
<li class="chapter" data-level="4.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>4.10.3</b> Bayesian adaptive clinical trial with delayed outcomes</a></li>
<li class="chapter" data-level="4.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>4.10.4</b> NMAR의 종류</a></li>
<li class="chapter" data-level="4.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>4.10.5</b> wk10) Bayesian Model Selection</a></li>
<li class="chapter" data-level="4.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>4.10.6</b> Autologistic model</a></li>
<li class="chapter" data-level="4.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>4.10.7</b> wk10) Bayesian Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>5</b> MVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>5.1</b> Overview of mva (not ended)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>5.1.1</b> Notation</a></li>
<li class="chapter" data-level="5.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>5.1.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Inference on Correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>5.1.4</b> Standardization</a></li>
<li class="chapter" data-level="5.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>5.1.5</b> Missing Value Treatment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>5.2</b> Multivariate Nomral (wk2)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>5.2.1</b> Overview</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>5.2.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="5.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>5.2.3</b> Properties of MVN</a></li>
<li class="chapter" data-level="5.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>5.2.5</b> Linear Combination of Random Vectors</a></li>
<li class="chapter" data-level="5.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Normal Likelihood</a></li>
<li class="chapter" data-level="5.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>5.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span></a></li>
<li class="chapter" data-level="5.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>5.2.8</b> Assessing Normality</a></li>
<li class="chapter" data-level="5.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>5.2.9</b> Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>5.3</b> Inference about Mean Vector (wk3)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>5.3.2</b> 1. Confidence Region</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>5.3.3</b> 2. Simultaneous CI</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>5.3.4</b> 3. Note: Bonferroni Multiple Comparison</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>5.3.5</b> 4. Large Sample Inferences about a Mean Vector</a></li>
<li class="chapter" data-level="5.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>5.3.6</b> 1. Profile Analysis (wk4, 5)</a></li>
<li class="chapter" data-level="5.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>5.3.7</b> 2. Test for Linear Trend</a></li>
<li class="chapter" data-level="5.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>5.3.8</b> 3. Inferences about a Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>5.4</b> Comparison of Several MV Means (wk5)</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>5.4.1</b> Paired Comparison</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>5.4.2</b> Comparing Mean Vectors from Two Populations</a></li>
<li class="chapter" data-level="5.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>5.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)</a></li>
<li class="chapter" data-level="5.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>5.4.4</b> Comparing Several Multivariate Population Means</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>5.5</b> Multivariate Multiple Regression (wk6)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>5.5.1</b> Overview</a></li>
<li class="chapter" data-level="5.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>5.5.2</b> Multivariate Multiple Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>5.6</b> PCA</a></li>
<li class="chapter" data-level="5.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>5.7</b> Factor</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>5.7.1</b> Method of Estimation</a></li>
<li class="chapter" data-level="5.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>5.7.2</b> Factor Rotation</a></li>
<li class="chapter" data-level="5.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>5.7.3</b> Varimax Criterion</a></li>
<li class="chapter" data-level="5.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>5.7.4</b> Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>5.8</b> Discrimination and Classification</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>5.8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="5.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>5.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations</a></li>
<li class="chapter" data-level="5.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>5.8.3</b> Evaluating Classification Functions</a></li>
<li class="chapter" data-level="5.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>5.8.4</b> Classification with several Populations (wk13)</a></li>
<li class="chapter" data-level="5.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>5.8.5</b> Other Discriminant Analysis Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>5.9</b> Clustering, Distance Methods, and Ordination</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>5.9.1</b> Overview</a></li>
<li class="chapter" data-level="5.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.9.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>5.9.3</b> K-means Clustering</a></li>
<li class="chapter" data-level="5.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>5.9.4</b> 군집의 평가방법</a></li>
<li class="chapter" data-level="5.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>5.9.5</b> Clustering using Density Estimation (wk14)</a></li>
<li class="chapter" data-level="5.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>5.9.6</b> Multidimensional Scaling (MDS)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>6.1</b> SVD</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="svd.html"><a href="svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>6.1.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="6.1.2" data-path="svd.html"><a href="svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>6.1.2</b> Singular value Decomposition: General-version</a></li>
<li class="chapter" data-level="6.1.3" data-path="svd.html"><a href="svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>6.1.3</b> Singular value Decomposition: Another-version</a></li>
<li class="chapter" data-level="6.1.4" data-path="svd.html"><a href="svd.html#quadratic-forms"><i class="fa fa-check"></i><b>6.1.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="6.1.5" data-path="svd.html"><a href="svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>6.1.5</b> Partitioned Matrices</a></li>
<li class="chapter" data-level="6.1.6" data-path="svd.html"><a href="svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>6.1.6</b> Geometrical Aspects</a></li>
<li class="chapter" data-level="6.1.7" data-path="svd.html"><a href="svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>6.1.7</b> Column, Row and Null Space</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>6.2</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-1.html"><a href="introduction-1.html#what"><i class="fa fa-check"></i><b>6.2.1</b> What</a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-1.html"><a href="introduction-1.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>6.2.2</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="6.2.3" data-path="introduction-1.html"><a href="introduction-1.html#multivariate-normal-distributions"><i class="fa fa-check"></i><b>6.2.3</b> Multivariate Normal Distributions</a></li>
<li class="chapter" data-level="6.2.4" data-path="introduction-1.html"><a href="introduction-1.html#distributions-of-quadratic-forms"><i class="fa fa-check"></i><b>6.2.4</b> Distributions of Quadratic Forms</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>6.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>6.3.1</b> Identifiability and Estimability</a></li>
<li class="chapter" data-level="6.3.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>6.3.2</b> Estimation: Least Squares</a></li>
<li class="chapter" data-level="6.3.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>6.3.3</b> Estimation: Best Linear Unbiased</a></li>
<li class="chapter" data-level="6.3.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>6.3.4</b> Estimation: Maximum Likelihood</a></li>
<li class="chapter" data-level="6.3.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>6.3.5</b> Estimation: Minimum Variance Unbiased</a></li>
<li class="chapter" data-level="6.3.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>6.3.6</b> Sampling Distributions of Estimates</a></li>
<li class="chapter" data-level="6.3.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>6.3.7</b> Generalized Least Squares(GLS)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>6.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>6.4.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="6.4.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>6.4.2</b> More About Models</a></li>
<li class="chapter" data-level="6.4.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> Estimating and Testing Contrasts</a></li>
<li class="chapter" data-level="6.4.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>6.4.4</b> Cochran’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>6.5</b> Testing</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>6.5.1</b> More About Models: Two approaches for linear model</a></li>
<li class="chapter" data-level="6.5.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>6.5.2</b> Testing Models</a></li>
<li class="chapter" data-level="6.5.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>6.5.3</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>6.5.4</b> Testing Linear Parametric Functions</a></li>
<li class="chapter" data-level="6.5.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>6.5.5</b> Theoretical Complements</a></li>
<li class="chapter" data-level="6.5.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>6.5.6</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>6.5.7</b> Testing Single Degrees of Freedom in a Given Subspace</a></li>
<li class="chapter" data-level="6.5.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>6.5.8</b> Breaking SS into Independent Components</a></li>
<li class="chapter" data-level="6.5.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>6.5.9</b> General Theory</a></li>
<li class="chapter" data-level="6.5.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>6.5.10</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="6.5.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>6.5.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="6.5.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>6.5.12</b> Tests for Generalized Least Squares Models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>6.6</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>6.6.1</b> A direct solution via inner products</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>6.7</b> Flat</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>6.7.1</b> 1.Flat</a></li>
<li class="chapter" data-level="6.7.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>6.7.2</b> 2. Solutions to systems of linear equations</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>6.8</b> Unified Approach to Balanced ANOVA Models</a></li>
</ul></li>
<li class="part"><span><b>III 21-02</b></span></li>
<li class="chapter" data-level="7" data-path="network-stats.html"><a href="network-stats.html"><i class="fa fa-check"></i><b>7</b> Network Stats</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="introduction-2.html"><a href="introduction-2.html#types-of-network-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Types of Network Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="introduction-2.html"><a href="introduction-2.html#network-modeling-and-inference"><i class="fa fa-check"></i><b>7.1.2</b> Network Modeling and Inference</a></li>
<li class="chapter" data-level="7.1.3" data-path="introduction-2.html"><a href="introduction-2.html#network-processes"><i class="fa fa-check"></i><b>7.1.3</b> Network Processes</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html"><i class="fa fa-check"></i><b>7.2</b> Descriptive Statistics of Networks</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics"><i class="fa fa-check"></i><b>7.2.1</b> Vertex and Edge Characteristics</a></li>
<li class="chapter" data-level="7.2.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion"><i class="fa fa-check"></i><b>7.2.2</b> Characterizing Network Cohesion</a></li>
<li class="chapter" data-level="7.2.3" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#graph-partitioning"><i class="fa fa-check"></i><b>7.2.3</b> Graph Partitioning</a></li>
<li class="chapter" data-level="7.2.4" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing"><i class="fa fa-check"></i><b>7.2.4</b> Assortativity and Mixing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html"><i class="fa fa-check"></i><b>7.3</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#sampling-procedures"><i class="fa fa-check"></i><b>7.3.1</b> Sampling Procedures</a></li>
<li class="chapter" data-level="7.3.2" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#sampling-designs"><i class="fa fa-check"></i><b>7.3.2</b> Sampling Designs</a></li>
<li class="chapter" data-level="7.3.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#coping-strategies"><i class="fa fa-check"></i><b>7.3.3</b> Coping Strategies</a></li>
<li class="chapter" data-level="7.3.4" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#big-data-solves-nothing"><i class="fa fa-check"></i><b>7.3.4</b> Big Data Solves Nothing</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html"><i class="fa fa-check"></i><b>7.4</b> Mathematical Models for Network Graphs</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models"><i class="fa fa-check"></i><b>7.4.1</b> Classical Random Graph Models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html"><i class="fa fa-check"></i><b>7.5</b> Introduction to ERGM</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#exponential-random-graph-models"><i class="fa fa-check"></i><b>7.5.1</b> Exponential Random Graph Models</a></li>
<li class="chapter" data-level="7.5.2" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation"><i class="fa fa-check"></i><b>7.5.2</b> Difficulty in Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html"><i class="fa fa-check"></i><b>7.6</b> Parameter Estimation of ERGM</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm"><i class="fa fa-check"></i><b>7.6.1</b> Approximation-based Algorithm</a></li>
<li class="chapter" data-level="7.6.2" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches"><i class="fa fa-check"></i><b>7.6.2</b> Auxiliary Variable MCMC-based Approaches</a></li>
<li class="chapter" data-level="7.6.3" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#goodness-of-fit-plots"><i class="fa fa-check"></i><b>7.6.3</b> Goodness-of-fit Plots</a></li>
<li class="chapter" data-level="7.6.4" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc"><i class="fa fa-check"></i><b>7.6.4</b> Varying Trunction Stochastic Approximation MCMC</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html"><i class="fa fa-check"></i><b>7.7</b> ERGM for Dynamic Networks</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#temporal-ergm"><i class="fa fa-check"></i><b>7.7.1</b> Temporal ERGM</a></li>
<li class="chapter" data-level="7.7.2" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm"><i class="fa fa-check"></i><b>7.7.2</b> Separable Temporal ERGM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>8</b> Survival Analysis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>8.2</b> </a></li>
<li class="chapter" data-level="8.3" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>8.3</b> </a></li>
<li class="chapter" data-level="8.4" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>8.4</b> </a></li>
<li class="chapter" data-level="8.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>8.5</b> Cox Regression</a></li>
<li class="chapter" data-level="8.6" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html"><i class="fa fa-check"></i><b>8.6</b> Filtration의 개념을 정복하자!</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약"><i class="fa fa-check"></i><b>8.6.1</b> Random Process를 이야기 하기까지의 긴 여정의 요약</a></li>
<li class="chapter" data-level="8.6.2" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#ft-measurable"><i class="fa fa-check"></i><b>8.6.2</b> Ft-measurable</a></li>
<li class="chapter" data-level="8.6.3" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#epilogue"><i class="fa fa-check"></i><b>8.6.3</b> EPILOGUE</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>8.7</b> Concepts</a></li>
</ul></li>
<li class="appendix"><span><b>00-00</b></span></li>
<li class="chapter" data-level="A" data-path="r-bookdown.html"><a href="r-bookdown.html"><i class="fa fa-check"></i><b>A</b> R Bookdown</a>
<ul>
<li class="chapter" data-level="A.1" data-path="tutorial.html"><a href="tutorial.html"><i class="fa fa-check"></i><b>A.1</b> Tutorial</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="tutorial.html"><a href="tutorial.html#about"><i class="fa fa-check"></i><b>A.1.1</b> About</a></li>
<li class="chapter" data-level="A.1.2" data-path="tutorial.html"><a href="tutorial.html#hello-bookdown"><i class="fa fa-check"></i><b>A.1.2</b> Hello bookdown</a></li>
<li class="chapter" data-level="A.1.3" data-path="tutorial.html"><a href="tutorial.html#cross-references"><i class="fa fa-check"></i><b>A.1.3</b> Cross-references</a></li>
<li class="chapter" data-level="A.1.4" data-path="tutorial.html"><a href="tutorial.html#parts"><i class="fa fa-check"></i><b>A.1.4</b> Parts</a></li>
<li class="chapter" data-level="A.1.5" data-path="tutorial.html"><a href="tutorial.html#footnotes-and-citations"><i class="fa fa-check"></i><b>A.1.5</b> Footnotes and citations</a></li>
<li class="chapter" data-level="A.1.6" data-path="tutorial.html"><a href="tutorial.html#blocks"><i class="fa fa-check"></i><b>A.1.6</b> Blocks</a></li>
<li class="chapter" data-level="A.1.7" data-path="tutorial.html"><a href="tutorial.html#sharing-your-book"><i class="fa fa-check"></i><b>A.1.7</b> Sharing your book</a></li>
<li class="chapter" data-level="" data-path="tutorial.html"><a href="tutorial.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="noname.html"><a href="noname.html"><i class="fa fa-check"></i><b>B</b> NoName</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="importance-sampling" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Importance Sampling</h2>
<div id="independent-monte-carlo" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Independent Monte Carlo</h3>
<p>타겟분포 <span class="math inline">\(f\)</span>로부터의 시뮬레이션의 랜덤 draws $ X_1 , , X_n $.</p>
<p>적분 범위에 걸쳐 (over) support가 펼쳐져 있는 분포로부터 무작위로 포인트를 추출해서 해당 포인트들의 적분값을 종합하여 만들어내는, 적분값에 대한 통계적 측정.</p>
<p>let <span class="math inline">\(f\)</span>는 <span class="math inline">\(X\)</span>의 density, <span class="math inline">\(\mu = E_f \left[ h(X) \right]\)</span>. 이때</p>
<p>$</p>
<p><em>{MC} =  </em>{i=1}^n h(X_i ) h(x)f(X) dx =; ; ; ; ;  n </p>
<p>$</p>
<p>let $ v(x) = { h(x)-}^2$, $ E_f { ^2 } &lt; $. Then, sampling <span class="math inline">\(Var\)</span> of $ _{MC} $ is $ = E {  }. This can be written as</p>
<p>$</p>
<p> (<em>{MC}) =  {n-1} </em>{i=1}^n ^2</p>
<p>$</p>
<p>when ^2 exists, by CLT, <span class="math inline">\(\hat \mu_MC \overset {\cdot} {\sim} N\)</span>, for large <span class="math inline">\(n\)</span>.</p>
<p>수치해석은 다차원 문제에는 적용하기 어렵다. 하지만, MC integration은 <span class="math inline">\(p\)</span>차원의 <span class="math inline">\(f\)</span>의 support에 걸쳐서 <span class="math inline">\(f\)</span>에서 랜덤하게 샘플링 한 후 이 영역에 대한 그 어떤 체계적인 탐색도 시도하지 않는다. 샘플링 후에는 그냥 냅둬버림. 따라서 MC는 고차원에서도 덜 피곤함.</p>
<p><br>
<br>
<br></p>
<div id="inverse-cdf" class="section level5" number="4.1.1.0.1">
<h5><span class="header-section-number">4.1.1.0.1</span> Inverse-cdf</h5>
<p><span class="math inline">\(\forall F, X=F^{-1}(U) = \text{inf}\{ x:F(x) \ge U \}\)</span>는 <span class="math inline">\(F\)</span>와 같은 cdf를 가짐. 이때 <span class="math inline">\(F\)</span>는 continuous distribution function, <span class="math inline">\(U \sim U(0, 1)\)</span>.</p>
<p>이때, linear interpolation을 활용해, <span class="math inline">\(F^{-1}\)</span> 계산 없이 <span class="math inline">\(F\)</span>만으로 난수 샘플링 가능.
1. <span class="math inline">\(f\)</span>의 supoprt를 span하는 grid <span class="math inline">\(x_1 , \cdots, x_m\)</span> 정의
2. 각 grid point에서 <span class="math inline">\(u_i = F(x_i)\)</span> 계산하거나 approximate
3. 가장 가까운 grid points <span class="math inline">\(u_i , u_j\)</span>에 대해, <span class="math inline">\(u_i \le U \le u_j\)</span>에 해당하는 영역을 이하에 따라 linearly interpotate. <span class="math inline">\(X = \dfrac{u_j-U}{u_j - u_i}x_i + \dfrac{U-u_i}{u_j - u_i}x_j\)</span>. 이때 <span class="math inline">\(U \sim U(0, 1)\)</span>.
- illustration of Rejection Sampling for a target distribution <span class="math inline">\(f\)</span> using a Rejection Sampling envelope <span class="math inline">\(e\)</span>.</p>
<p><br>
<br>
<br></p>
</div>
<div id="rejection-sampling" class="section level5" number="4.1.1.0.2">
<h5><span class="header-section-number">4.1.1.0.2</span> Rejection Sampling</h5>
<p><span class="math inline">\(f(x)\)</span>의 상수배 (proportionality constant) 만이라도 계산될 수 있다면, 정확한 타겟분포 <span class="math inline">\(f(x)\)</span>로부터의 샘플링을 위하여 <strong>Rejection Sampling</strong> 사용 가능. 이는 더 간단한 후보 (candidate) 분포로부터 샘플링한 후 이렇게 샘플링한 것 중 일부를 확률에 기반하여 랜덤하게 쳐냄으로써 샘플링 확률을 보정하는 것.
* <span class="math inline">\(g\)</span>는 우리가 분포의 형태를 정확히 알고 있고 <span class="math inline">\(g(x)\)</span> 계산도 쉬운 덴시티라고 정의.
* <span class="math inline">\(e\)</span>는 <strong>envelope</strong>, 이하의 성질을 갖는다. <span class="math inline">\(\forall x \; \; \; \text{s.t. for a given constant } \alpha \le 1, f(x)&gt;0 \; \; \; : \; \; \; e(x) = \dfrac {g(x)}{\alpha} \ge f(x)\)</span>.</p>
<p>방법은 이하와 같다.
1. <span class="math inline">\(Y \sim g\)</span>에서 샘플링.
2. <span class="math inline">\(U&gt;\dfrac {f(y)}{e(Y)}\)</span>일 경우 <span class="math inline">\(Y\)</span>를 기각. 기각된다면 <span class="math inline">\(Y\)</span>값을 target random sample의 요소로 기록하지 않음. step 1으로.
3. <span class="math inline">\(U \le \dfrac {f(y)}{e(Y)}\)</span>일 경우 set <span class="math inline">\(X=Y\)</span>로 한 후 <span class="math inline">\(X\)</span>를 타겟 랜덤샘플의 요소로 넣음. step 1으로.</p>
<p>여기서 <span class="math inline">\(\alpha\)</span>는 채택될 후보들의 expected 비율로 해석될 수 있다.</p>
<p>good RS envelope의 요건:
* 간단하게 제작되거나, 모든 값에서 타겟분포를 넘김이 간단하게 확인되어야 한다.
* 샘플링이 쉬어야 한다.
* rejected draws가 적어야 한다.</p>
<blockquote>
<p>Example: Normal From Double Exponential, Sampling a Bayesian Posterior</p>
</blockquote>
<p><br>
<br>
<br></p>
</div>
<div id="variants-of-the-rs-squeeze-rs" class="section level5" number="4.1.1.0.3">
<h5><span class="header-section-number">4.1.1.0.3</span> Variants of the RS: Squeeze RS</h5>
<p><span class="math inline">\(f\)</span> 계산해내는 게 비용이 많이 들고 RS가 매력적인 상황이면 <strong>Squeeze RS</strong>에 의해 더 빠른 연산속도를 획득할 수 있음. nonnegative squeezing function <span class="math inline">\(s(x)\)</span>를 정의하고 이를 사용함. 이때 <span class="math inline">\(s\)</span>가 적합한 squeezing function이기 위해선 <span class="math inline">\(f\)</span>의 모든 support에서 <span class="math inline">\(s&lt;f\)</span>.
* illustration of squeezed Rs for a target distribution <span class="math inline">\(f\)</span>, using envelope <span class="math inline">\(e\)</span> and squeezing function <span class="math inline">\(s\)</span>. Keep First and Keep Later correspond to steps 3 and 4 of the algorithm, respectively.</p>
<p>proceeds:
1. <span class="math inline">\(Y \sim g\)</span>에서 샘플링.
2. if <span class="math inline">\(U \le \dfrac {s(Y)}{e(Y)}\)</span>, keep <span class="math inline">\(Y\)</span>.
3. if not, whether if <span class="math inline">\(U \le \dfrac {f(Y)}{e(Y)}\)</span>, keep <span class="math inline">\(Y\)</span>.
4. both are not, reject <span class="math inline">\(Y\)</span>.</p>
<p>2번에선 <span class="math inline">\(s\)</span>, 3번에선 <span class="math inline">\(f\)</span>임에 주목. 샘플링 쉬운 <span class="math inline">\(s\)</span>에서 먼저 비교해서 우선권 시드 주고, 그 후에 <span class="math inline">\(f\)</span>로 본선 해보는거.</p>
<blockquote>
<p>Example: Lower Bound for Normal Generation</p>
</blockquote>
<p><br>
<br>
<br></p>
<div id="variants-of-the-rs-adaptive-rs" class="section level6" number="4.1.1.0.3.1">
<h6><span class="header-section-number">4.1.1.0.3.1</span> Variants of the RS: Adaptive RS</h6>
<p>적절한 envelope <span class="math inline">\(e\)</span>를 어떻게 만들 것인가? squeezed RS를 위해, support의 connected region에 대해 continuous, differentiable, log-concave인 덴시티를 만드는 자동화된 envelope 생산 전략에 해당함. 패키지로 실행.
* envelopes <span class="math inline">\(e\)</span> and squeezing function <span class="math inline">\(s\)</span> for adaptive RS. The target density <span class="math inline">\(f\)</span> is smooth, nearly bell-shaped curve. The first method discussed in the text, using the derivative of <span class="math inline">\(l\)</span>, produces the envelope <span class="math inline">\(e\)</span> shown as upper boundary of the lighter shaded region. This correponds to Equation (6.9) and Figure 6.4. Later in the text, a derivative-free method is presented. That envelope is the upper bound of the darker shaed region and corresponds to (6.11) and Figure 6.6. The squeezing function <span class="math inline">\(s\)</span> for both approaches is given by the dotted curve.</p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="importance-sampling-1" class="section level5" number="4.1.1.0.4">
<h5><span class="header-section-number">4.1.1.0.4</span> Importance Sampling</h5>
<p><strong>Importance Sampling</strong> 접근법은 <span class="math inline">\(E\{h(x)\}\)</span> w.r.t. its density는 이하처럼 alternative form으로 쓰일 수 있다는 것에 기반한다. 이때 <span class="math inline">\(g\)</span>는 envelope의 importance sampling function.</p>
<p>$
<span class="math display">\[\begin{align*}



\mu &amp;= \int h(x)f(x)dx &amp;= \int \left( h(x) \dfrac {f(x)}{g(x)} \right)g(x)dx \tag{1} \\

\\
\\

&amp;= \dfrac {\int h(x)f(x) dx}{\int f(x) dx} &amp;= \dfrac {\int \left( h(x) \dfrac {f(x)}{g(x)} \right) g(x) dx}{\int \left( \dfrac {f(x)}{g(x)} \right) g(x) dx} \tag{2}

\end{align*}\]</span></p>
<p>$</p>
<ul>
<li>(1)은 <span class="math inline">\(E \{ h(X) \}\)</span>를 측정하기 위한 MC 접근법이 이하임을 제시한다. <span class="math inline">\(X_1 , \cdots, X_n \overset {\text{iid}}{\sim} g\)</span>처럼 <span class="math inline">\(g\)</span>에서 랜덤샘플을 뽑고, 이의 (이를 활용한) estimator는 이하. 이때 <span class="math inline">\(w^{\ast} (X_i)\)</span>는 <strong>unstandardized weights</strong>, i.e., <strong>importance ratios</strong>.</li>
</ul>
<p>$</p>
<p><em>{IS}^{} =  </em>{i=1}^n h(X_i) w^{}(X_i) =  _{i=1}^n h(X_i) </p>
<p>$</p>
<p>(2)는 <span class="math inline">\(g\)</span>에서 <span class="math inline">\(X_1 , \cdots, X_n \overset {\text{iid}}{\sim} g\)</span>의 랜덤샘플을 뽑고 이하를 계산. 이때 <span class="math inline">\(w(X_i)\)</span>는 standardized weight. <br>이 (2)는 <span class="math inline">\(f\)</span>의 상수배 (proportionality constant) 까지만 알 수 있더라도 적용할 수 있다는 점에서 매우 중요함. <span class="math inline">\(f\)</span>의 상수배까지만 알 수 있는 상황은 베이지안 분석의 post에서 빈번하게 발생함. <strong><em>Both estimators converge by the same argument applied to the simple Monte Carlo estimator.</em></strong></p>
<p>$</p>
<p><em>{IS} = </em>{i=1}^n h(X_i) w(X_i) = _{i=1}^n h(X_i) </p>
<p>$</p>
<p>Proceeds:
1. Sample <span class="math inline">\(X_j \sim g(\cdot)\)</span>.
2. Calculate <span class="math inline">\(w(X_j) = \dfrac {f(X_j)}{g(X_j)}\)</span>
3. 지정 샘플 갯수까지 반복</p>
<p>then,</p>
<p>$
<span class="math display">\[\begin{align*}

E\{\hat h(x)\} &amp;= \dfrac {1}{n} \sum_{j=1}^n w(X_j)h(X_j) \\

\hat \sigma^2 &amp;= \dfrac {1}{n-1}  \sum_{j=1}^n \left\{ h(X_j) - E\left[ \hat h(x) \right] \right\}^2

\end{align*}\]</span>
$</p>
<p>과도한 변동성을 회피하기 위해, <span class="math inline">\(\dfrac {f(x)} {g(x)}\)</span>는 bounded여야 하며 또한 <span class="math inline">\(g\)</span>는 <span class="math inline">\(f\)</span>보다 heavier tail을 가져야 한다. 이것이 만족되지 않는다면 standardized importance weight는 제법 커질 수 있음.</p>
<p>함수 <span class="math inline">\(g\)</span>는, <span class="math inline">\(h(x)\)</span>가 매우매우 작을 경우에만 <span class="math inline">\(\dfrac {f(x)} {g(x)}\)</span>가 커지게 만드는 녀석으로 잘 골라야 한다. 가령 <span class="math inline">\(h\)</span>가 아주 드문 상황에서만 1을 반환하는 indicator function이라면, 우리는 <span class="math inline">\(g\)</span>로 하여금 샘플링의 편의성을 위해 해당 사건을 좀 더 빈번하게 발생시키도록 하는 녀석을 고를 수도 있을 것이다. 이를 택한다면 우리는 우리의 관심사가 아닌 사건, 가령 <span class="math inline">\(h(x)=0\)</span>에 대한 적절한 샘플링 power을 어느정도 희생하게 된다. <strong><em>이는 낮은 확률에 해당하는 case의 측정에 특히 잘 들어맞는 방법론이다.</em></strong></p>
<p>$_{IS}^$ 자체는 unbiased지만, 이를 importance weight로 standardize 하는 과정에서 <span class="math inline">\(\hat \mu_{IS}\)</span>에 다소 bias가 생겨버린다.</p>
<p>standardized weight를 쓰는 건 <span class="math inline">\(w^\ast(X)\)</span>와 <span class="math inline">\(h(X)w^\ast(X)\)</span>가 서로 강하게 상관관계가 있는 상황에서 더욱 우수한 estimator를 반환한다.</p>
<p>standardized weight는 <span class="math inline">\(f\)</span>의 비례상수를 요구하지 않는다. (우리가 갖고 있는 덴시티가 <span class="math inline">\(f\)</span>의 얼마만큼의 상수배인지를 알지 않아도 된다)</p>
<p>IS 방법론의 매력은 시뮬레이션의 reusability이다. 같은 sample points들과 weight들이 다양한 다른 quantity의 MC 적분 estimates를 구하는데 사용될 수 있다. (<strong>컴퓨팅 파워가 증가한 오늘날에 와서는 유의미한 장점은 아니다.</strong>)</p>
<blockquote>
<p>Example: Small Tail Probabilities</p>
</blockquote>
<p><br>
<br>
<br></p>
</div>
<div id="antithetic-sampling" class="section level5" number="4.1.1.0.5">
<h5><span class="header-section-number">4.1.1.0.5</span> Antithetic Sampling</h5>
<p>let <span class="math inline">\(\hat \mu_1, \hat \mu_2\)</span>. 이 둘은 identically distributed, UE, and <span class="math inline">\(Corr(\hat \mu_1, \hat \mu_2)&lt;0\)</span>.</p>
<p>이 estimator 둘을 평균한 <span class="math inline">\(\hat \mu_{AS} = \dfrac{\hat \mu_1 + \hat \mu_2}{2}\)</span>는 각 estimator들의 샘플을 2배 한 것보다 우월함. <strong><span class="math inline">\(Corr(\hat \mu_1, \hat \mu_2)&lt;0\)</span>이기 때문에 성립한다는 것을 유의.</strong></p>
<p>$</p>
<p>Var(_{AS}) =  ( Var(_1) + Var(_2) ) +  Cov(_1, _2) =   {n} (1+)</p>
<p>$</p>
<p><span class="math inline">\(\hat \mu_1 (X)\)</span>를 MC integral estimate로 잡는다면, 이는</p>
<p>$</p>
<p><em>1 (X) =  </em>{i=1}^n h_1 { F_1^{-1}(U_{i1}), , F_m^{-1}(U_{im}) }</p>
<p>$</p>
<p>이때 <span class="math inline">\(h_1\)</span>은 그의 <strong>arguments</strong>에 monotone이며, <span class="math inline">\(F_j\)</span>는 각 <span class="math inline">\(X_{ij}, \; j=1,\cdots,m\)</span>의 cdf이며 <span class="math inline">\(U_{ij} \sim U(0,1)\)</span>. 이에 의해 <span class="math inline">\(1-U_{ij} \sim U(0,1)\)</span>이기도 하며, 이에 의해 이하도 성립.</p>
<p>$</p>
<p><em>2 (X) =  </em>{i=1}^n h_1 { F_1^{-1}(1-U_{i1}), , F_m^{-1}(1-U_{im}) }</p>
<p>$</p>
<p>이는 <span class="math inline">\(\mu\)</span>의 2번째 estimator이며, 이는 <span class="math inline">\(\hat \mu_1 (X)\)</span>와 같은 분포를 가짐.</p>
<p>따라서 <span class="math inline">\(\hat \mu_{AS} = \dfrac{\hat \mu_1 + \hat \mu_2}{2}\)</span>는 <span class="math inline">\(\hat \mu_1\)</span>의 샘플을 2배 한 것(2n)보다 더 작은 <span class="math inline">\(Var\)</span>을 가지며, 따라서 더 우월함.</p>
<p><br>
<br>
<br></p>
</div>
<div id="control-variates" class="section level5" number="4.1.1.0.6">
<h5><span class="header-section-number">4.1.1.0.6</span> Control Variates</h5>
<p>우리는 알지 못하는 quantity <span class="math inline">\(\mu = E \{ h(X) \}\)</span>를 알고자 하며, 이에 연관된 quantity <span class="math inline">\(\theta = E[c(Y)]\)</span>에 대해서는 알고 있음. 후자는 수치적으로 획득 가능. <span class="math inline">\((X_1 , Y_1 ) ,\cdots, (X_n , Y_n )\)</span>은 simulation outcom에서 독립적으로 관측된 pairs of rv.</p>
<p>이때 MC estimator는 이하와 같다. <span class="math inline">\(\hat \mu_{MC}, \hat \theta_{MC}\)</span> 간에 상호연관이 있음을 유의.</p>
<p>$
<span class="math display">\[\begin{align*}

\hat \mu_{MC} = \dfrac {1}{n} \sum_{i=1}^n h(X_i), &amp; \; \; \; \; \; \; \; \; \; \; \hat \theta_{MC} = \dfrac {1}{n} \sum_{i=1}^n c(Y_i)

\end{align*}\]</span>
$</p>
<p>즉 우리는 여기서</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(\mu = E[h(x)]\)</span></th>
<th align="center"><span class="math inline">\(\theta = E[c(Y)]\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">MC (ex. <span class="math inline">\(\theta_{MC}\)</span>)</td>
<td align="center">able</td>
<td align="center">able</td>
</tr>
<tr class="even">
<td align="center">itself</td>
<td align="center"></td>
<td align="center">able</td>
</tr>
</tbody>
</table>
<p>즉 <span class="math inline">\(\theta\)</span>와 <span class="math inline">\(\theta_{MC}\)</span> 간의 차이를 알아내고, 이 차이를 적당히 스케일링해서 <span class="math inline">\(\mu\)</span>에 적용한다는 것이 기본 메커니즘.</p>
<p>여기서 Control Variate Estimator는 <span class="math inline">\(\hat \mu_{CV} = \hat \mu_{MC} + \lambda(\hat \theta_{MC} - \theta)\)</span>. <span class="math inline">\(\lambda\)</span>는 사용자에 의해 정해지는 임의의 parameter. 이에 의해</p>
<p>$</p>
<p>Var(<em>{CV} ) = Var (</em>{MC}) + ^2 Var (<em>{MC}) + 2 Cov(</em>{MC}, _{MC})</p>
<p>$</p>
<p>이며 이가 최소가 된 경우의 분산은 아래와 같으며, 이를 최소로 하는 <span class="math inline">\(\lambda\)</span>는 아래와 같다.</p>
<p>when <span class="math inline">\(\lambda = - \dfrac {Cov(\hat \mu_{MC}, \hat \theta_{MC})}{Var(\hat \theta_{MC})}\)</span>, <span class="math inline">\(\min_\lambda \left( Var(\hat \mu_{CV} ) \right) = Var(\hat \mu_{MC}) - \dfrac{\left[ Cov(\hat \mu_{MC}, \hat \theta_{MC}) \right]^2} {Var(\hat \theta_{MC})}\)</span>.</p>
<p><br>
<br>
<br></p>
</div>
<div id="rao-blackwellizaiton" class="section level5" number="4.1.1.0.7">
<h5><span class="header-section-number">4.1.1.0.7</span> Rao-Blackwellizaiton</h5>
<p>rs <span class="math inline">\(X_1 , \cdots, X_n \overset {\text{iid}}{\sim} f\)</span>를 활용해 <span class="math inline">\(\mu = E \{ h(X) \}\)</span>를 estimation.</p>
<p>각각의 <span class="math inline">\(X_i = (X_{i1}, X_{i2})\)</span>라고 가정하고, 조건부 기댓값 <span class="math inline">\(E\{ h(X_i) \rvert x_{i2} \}\)</span>가 수치적으로 풀릴 수 있다고 가정하자.</p>
<p>$ E{h(X_i)} = E_{X_{i2}}{ E }$라는 사실을 활용하여 $ _{MC} $ 에 대한 다른 estimator를 구축해보자.</p>
<p>Rao-Blackwellized estimator <span class="math inline">\(\hat \mu_{RB} = \dfrac 1 n \sum_{i=1}^n E \{ h(X_i) \rvert X_{i2} \}\)</span>. 이는 ordinary MC estimator <span class="math inline">\(\hat \mu_{MC}\)</span>와 같은 mean을 갖는다. Note that</p>
<p>$</p>
<p>Var(<em>{MC}) =  {n^2} Var { E} +  {n^2} E { Var } Var (</em>{RB})</p>
<p>$</p>
<p>따라서 Mean Squared Error, MSE 관점에서 <span class="math inline">\(\hat \mu_{RB}\)</span>는 <span class="math inline">\(\hat \mu_{MC}\)</span> 보다 우수하다.</p>
<p><br>
<br>
<br></p>
</div>
<div id="sampling-importance-resampling" class="section level5" number="4.1.1.0.8">
<h5><span class="header-section-number">4.1.1.0.8</span> Sampling Importance Resampling</h5>
<p>SIR 알고리즘은 몇 타겟분포에서 실현값을 모사적으로 시뮬레이트한다. SIR은 Importance Sampling의 개념에 기초하고 있다. IS에서 우리는 IS function <span class="math inline">\(g\)</span>에서 샘플링하는 식으로 진행했었다. 샘플의 각 point는 샘플링 확률을 보정 (correct)하기 위해 weighted 되었었으며, 이에 의해 weighted 샘플들은 타겟분포 <span class="math inline">\(f\)</span>와 연결지어질 수 있었다. 타겟분포 <span class="math inline">\(f\)</span>를 획득하기 위해 샘플링 확률 보정 목적으로 가해지는 weight는 <strong>standardized importance weight</strong> <span class="math inline">\(w(x_i)\)</span>로 불렸으며,</p>
<p>$
w(x_i) = 이 만으로 난수 샘플링 가능.

{}
{_{i=1}^m }</p>
<p>$</p>
<p>이렇게 획득했던 standardized weight는 이후에 출신 density가 아닌 다른 타겟 <span class="math inline">\(f\)</span>에서 다른 샘플을 생산할 때 재사용되는 것이 가능하다.</p>
<p>for a collection of values, <span class="math inline">\(x_i , \cdots, x_m \overset {\text{iid}} {\sim} g\)</span>, 이때 <span class="math inline">\(g\)</span>는 Importance Sampling Function.</p>
<p>proceeds:
1. sample candidates <span class="math inline">\(Y_1 , \cdots, Y_m \overset {\text{iid}} {\sim}\)</span> 타겟분포 <span class="math inline">\(g\)</span>. <strong><span class="math inline">\(g\)</span>가 타겟분포라고?????? 수업발언</strong>
2. caculate the standardized importance weights, <span class="math inline">\(w(Y_1) , \cdots, w(Y_m)\)</span>.
3. resample <span class="math inline">\(X_1 , \cdots, X_m\)</span> from <span class="math inline">\(Y_1 , \cdots, Y_m\)</span> with probabilities, <span class="math inline">\(w(Y_1) , \cdots, w(Y_m)\)</span>.</p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">for <span class="math inline">\(n\)</span> samples</th>
<th align="center">Rejection Sampling</th>
<th align="center">SIR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center">perfect</td>
<td align="center">not perfect</td>
</tr>
<tr class="even">
<td align="center">distribution of generation draw is</td>
<td align="center">exactly <span class="math inline">\(f\)</span></td>
<td align="center">random degree of approxiamtion to <span class="math inline">\(f\)</span></td>
</tr>
<tr class="odd">
<td align="center">required number of draws</td>
<td align="center">random</td>
<td align="center">determined</td>
</tr>
</tbody>
</table>
<p>It is important to consider the relative sizes of the initial sample and the resample. In principle, we require <span class="math inline">\(\dfrac n m \rightarrow 0\)</span> for distributional convergence of the sample.</p>
<p>1만개를 생산해놓고 이 안에서 추가적으로 공정을 진행해서 목표했던 랜덤한 샘플을 뽑아내는 것이 SIR. 그러나 전 영역에서 체크하는것과 생산해놓은 1만개에 randomness를 첨가하여 만들어낸 샘플은 퍼포먼스 차이가 당연히 존재. 그러나 전 영역 대비 1만개라는 한정된 영역에서 추가공정을 진행하므로 cost down.</p>
<p>기존에 만들어두었던 weight를 재사용하므로 시뮬레이션을 다시 할 필요가 없음. 시간 down.</p>
<div class="line-block">Rejection Sampling | envelope <span class="math inline">\(e\)</span>를 만들고 이 안에서 뽑음. 이는 continuous point. | perfect, exact |<br />
SIR | n개의 candidate point를 이미 선택해놓고 이 안에서 뽑음. discrete. | approximate sampling |</div>
<p>candidate <span class="math inline">\(m\)</span>개, 샘플 <span class="math inline">\(n\)</span>개. 당연하지만 candidate <span class="math inline">\(m\)</span>의 숫자가 커질수록 효율성 (approximate 성능) 은 높아짐. The maximum tolerable ratio <span class="math inline">\(\dfrac n m\)</span> depends on the quality of the envelope, bsed on <span class="math inline">\(m\)</span> candidate samples and their weights. 이상적으로는 <span class="math inline">\(m\)</span>이 무한해지면 SIR 조차도 exact sampling일 수 있다.</p>
<p>The SIR algorithm can be sensitive to the choice of <span class="math inline">\(g\)</span>.
* The support of <span class="math inline">\(g\)</span> must include the entire support of <span class="math inline">\(f\)</span>, for a reweighted sample from <span class="math inline">\(g\)</span> is to approximate a sample from <span class="math inline">\(f\)</span>.
* <span class="math inline">\(g\)</span> should have heavier tails than <span class="math inline">\(f\)</span>, or more generally <span class="math inline">\(g\)</span> should be chosen to ensure that $ $ never grows to o large.
* If <span class="math inline">\(g(x)\)</span> is nearly zero anywhere where <span class="math inline">\(g(x)\)</span> is positive, then a draw from this region will happen only extremely rarely, but when it does it will receive a huge weight.
* weight-degeneracy problem</p>
<blockquote>
<p>Example: Slash Distribution
Example: Sampling a Bayesian Posterior</p>
</blockquote>
<p><br>
<br>
<br></p>
</div>
<div id="sequential-monte-carlo" class="section level5" number="4.1.1.0.9">
<h5><span class="header-section-number">4.1.1.0.9</span> Sequential Monte Carlo</h5>
<p>When the target density <span class="math inline">\(f\)</span> becomes high dimensional, SIR is increasingly inefficient and can be difficult to implement. Specifying a very good high-dimensional envelope that closely approximates the target with sufficiently heavy tails but little waste can be challenging.</p>
<p>Sequential Monte Carlo methods address the problem by splitting the high-dimensional task into a sequence of simpler steps, each of which updates the previous one.</p>
<p><span class="math inline">\(\pmb X_{1:t} = (X_1 , \cdots, X_t )\)</span> represents a discrete time stochastic process with <span class="math inline">\(X_t\)</span> being the observation at time <span class="math inline">\(t\)</span>.</p>
<p><span class="math inline">\(\pmb X_{1:t}\)</span> represents the entire history of the sequence.</p>
<p>Suppose the density of <span class="math inline">\(\pmb X_{1:t}\)</span> is <span class="math inline">\(f_t\)</span> and we wish to estimate the expected value of <span class="math inline">\(h(\)</span>X_{1:t}<span class="math inline">\()\)</span> w.r.t. <span class="math inline">\(f_t\)</span>.</p>
<p>A direct application of the SIR approach would be to draw a sample <span class="math inline">\(\pmb x_{1:t}\)</span> sequences from an envelope gt and then calculate the importance weighted average of this sample of <span class="math inline">\(h(\pmb X_{1:t})\)</span> values.</p>
<p>This SIR approach overlooks a key aspect of the problem.
* As t increases, <span class="math inline">\(\pmb X_{1:t}\)</span> and the expected value of <span class="math inline">\(h(\pmb x_{1:t})\)</span> evlove.
* At time <span class="math inline">\(t\)</span> it would be better to update previous inferences than to act as if we had no previous information. <strong>Inefficient !!!</strong></p>
<p>Need to develop a strategy that will simulate <span class="math inline">\(X_t\)</span> from previously simulated <span class="math inline">\(\pmb X_{1:t-1}\)</span> and adjust the previous importance weights in order to estimate the expected value of <span class="math inline">\(h(\pmb X_{1:t})\)</span> . <strong>Sequential Importance Sampling</strong>.</p>
<p><br>
<br>
<br></p>
</div>
<div id="sis-for-markov-process" class="section level5" number="4.1.1.0.10">
<h5><span class="header-section-number">4.1.1.0.10</span> SIS for Markov Process</h5>
<p>Simplify assumption that <span class="math inline">\(\pmb X_{1:t}\)</span> is a Markov process.</p>
<p>The target density <span class="math inline">\(f_t (\pmb x_{1:t})\)</span> may be expressed as</p>
<p>$
<span class="math display">\[\begin{align*}

f_t (\pmb x_{1:t}) &amp;= f_1 (x_1) &amp;\ast f_2 (x_2 \rvert \pmb x_{1:1}) &amp;\ast f_3 (x_3 \rvert \pmb x_{1:2}) &amp;\cdots &amp;\ast f_t (x_t \rvert \pmb x_{1:t-1}) \\


&amp;= f_1 (x_1) &amp;\ast f_2 (x_2 \rvert x_1) &amp;\ast f_3 (x_3 \rvert x_2) &amp;\cdots &amp;\ast f_t (x_t \rvert x_{t-1})

\end{align*}\]</span>
$</p>
<p>Suppose that we adopt the same Markov form for the envelope, namely</p>
<p>$</p>
<p>g_t (x_{1:t})= g_1 (x_1) g_2 (x_2 x_1) g_3 (x_3 x_2) g_t (x_t x_{t-1})</p>
<p>$</p>
<p>Sample from <span class="math inline">\(g_t (\pmb x_{1:t})\)</span> and reweight each <span class="math inline">\(\pmb x_{1:t}\)</span> value by <span class="math inline">\(w_t = \dfrac {f_t (\pmb x_{1:t})}{g_t (\pmb x_{1:t})}\)</span>.</p>
<p>The weight at time <span class="math inline">\(t\)</span> is <span class="math inline">\(w_t = \dfrac {f_1 (x_1) \ast f_2 (x_2 \rvert x_1) \ast \cdots} {g_1 (x_1) \ast g_2 (x_2 \rvert x_1) \ast \cdots}\)</span>.</p>
<p>A sample of <span class="math inline">\(n\)</span> such points and their weights can be used to approximate <span class="math inline">\(f_t (\pmb x_{1:t} )\)</span> and calculate the expected value of <span class="math inline">\(h(\pmb x_{1:t} )\)</span>.</p>
<p>The sequential Monte Carlo algorithm for generating one sample is
1. Sample $X_1 g_1 $. Let <span class="math inline">\(w_1 = u_1 = \dfrac {f_1(x_1)}{g_1(x_1)}\)</span>. Set <span class="math inline">\(t = 2\)</span>.
2. Sample <span class="math inline">\(X_t \rvert x_{t-1} \sim g_t (x_t \rvert x_{t-1})\)</span>.
3. Append <span class="math inline">\(x_t\)</span> to <span class="math inline">\(\pmb x_{1:t-1}\)</span>, obtaining <span class="math inline">\(\pmb x_{1:t}\)</span>.
4. <span class="math inline">\(u_t = \dfrac{f_t (x_t \rvert x_{t-1})}{g_t (x_t \rvert x_{t-1})}\)</span>.
5. let <span class="math inline">\(w_t = w_{t-1}u_t\)</span>. <span class="math inline">\(w_t\)</span> is the importance weight for <span class="math inline">\(\pmb x_{1:t}\)</span> .
6. Increment t and return to step 2.</p>
<p>The weighted average <span class="math inline">\(\sum_{i=1}^n \left( \dfrac {w_t^{(i)}}{\sum_{i=1}^n w_t^{(i)}} \right) \ast h(\pmb X_{1:t}^{(i)})\)</span> serves as the estimate of <span class="math inline">\(E_{f_T} h(\pmb X_{1:t})\)</span>.</p>
<p><br>
<br>
<br></p>
</div>
<div id="generalized-sequential-importance-sampling" class="section level5" number="4.1.1.0.11">
<h5><span class="header-section-number">4.1.1.0.11</span> Generalized Sequential Importance Sampling</h5>
<p>Assume that <span class="math inline">\(\pmb X_{1:t}\)</span> is not a Markov process.</p>
<p>target density <span class="math inline">\(f_t (\pmb x_{1:t})\)</span> and envelope <span class="math inline">\(g_t (\pmb x_{1:t})\)</span> may be expressed as</p>
<p>$
<span class="math display">\[\begin{align*}

f_t (\pmb x_{1:t}) &amp;= f_1 (x_1) \ast f_2 (x_2 \rvert \pmb x_{1:1}) \ast f_3 (x_3 \rvert \pmb x_{1:2}) &amp;\cdots &amp;\ast f_t (x_t \rvert \pmb x_{1:t-1}) \\

g_t (\pmb x_{1:t}) &amp;= g_1 (x_1) \ast g_2 (x_2 \rvert \pmb x_{1:1}) \ast g_3 (x_3 \rvert \pmb x_{1:2}) &amp;\cdots &amp;\ast g_t (x_t \rvert \pmb x_{1:t-1})
\end{align*}\]</span>
$</p>
<p>the importance weight at time <span class="math inline">\(t\)</span> is</p>
<p>$</p>
<p>w_t (x_{1:t}) =  {g_1 (x_1) g_2 (x_2 x_{1:1}) g_3 (x_3 x_{1:2}) g_t (x_t x_{1:t-1})}
$</p>
<p>and the recursive updating expression for the importance weights is</p>
<p><span class="math inline">\(w_t(\pmb x_{1:t}) = w_t(\pmb x_{1:t}) \dfrac {f_t (x_t \rvert \pmb x_{1:t-1})}{g_t (x_t \rvert \pmb x_{1:t-1})}, \; \; \; \; \; \; \; \; \text{for }t&gt;1\)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mcmc.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="markov-chain-monte-carlo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/211201_ImportanceSampling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
