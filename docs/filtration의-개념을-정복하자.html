<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.6 Filtration의 개념을 정복하자! | Self-Study</title>
  <meta name="description" content="9.6 Filtration의 개념을 정복하자! | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="9.6 Filtration의 개념을 정복하자! | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.6 Filtration의 개념을 정복하자! | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cox-regression.html"/>
<link rel="next" href="concepts.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li><a href="index.html#intro" id="toc-intro">Intro<span></span></a></li>
<li><a href="#part-20-02" id="toc-part-20-02">(PART) 20-02<span></span></a></li>
<li><a href="categorical.html#categorical" id="toc-categorical"><span class="toc-section-number">1</span> Categorical<span></span></a>
<ul>
<li><a href="overview.html#overview" id="toc-overview"><span class="toc-section-number">1.1</span> Overview<span></span></a>
<ul>
<li><a href="overview.html#data-type-and-statistical-analysis" id="toc-data-type-and-statistical-analysis"><span class="toc-section-number">1.1.1</span> Data Type and Statistical Analysis<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="bayesian.html#bayesian" id="toc-bayesian"><span class="toc-section-number">2</span> Bayesian<span></span></a>
<ul>
<li><a href="abstract.html#abstract" id="toc-abstract"><span class="toc-section-number">2.1</span> Abstract<span></span></a>
<ul>
<li><a href="abstract.html#변수의-독립성" id="toc-변수의-독립성"><span class="toc-section-number">2.1.1</span> 변수의 독립성<span></span></a></li>
<li><a href="abstract.html#교환가능성" id="toc-교환가능성"><span class="toc-section-number">2.1.2</span> 교환가능성<span></span></a></li>
</ul></li>
<li><a href="continual-aeassessment-method.html#continual-aeassessment-method" id="toc-continual-aeassessment-method"><span class="toc-section-number">2.2</span> Continual Aeassessment Method<span></span></a></li>
<li><a href="horseshoe-prior.html#horseshoe-prior" id="toc-horseshoe-prior"><span class="toc-section-number">2.3</span> Horseshoe Prior<span></span></a></li>
</ul></li>
<li><a href="#part-21-01" id="toc-part-21-01">(PART) 21-01<span></span></a></li>
<li><a href="mathematical-stats.html#mathematical-stats" id="toc-mathematical-stats"><span class="toc-section-number">3</span> Mathematical Stats<span></span></a>
<ul>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">3.1</span> Inference<span></span></a>
<ul>
<li><a href="inference.html#rao-blackwell-thm." id="toc-rao-blackwell-thm."><span class="toc-section-number">3.1.1</span> Rao-Blackwell thm.<span></span></a></li>
<li><a href="inference.html#completeness" id="toc-completeness"><span class="toc-section-number">3.1.2</span> Completeness<span></span></a></li>
<li><a href="inference.html#레만-쉐페-thm." id="toc-레만-쉐페-thm."><span class="toc-section-number">3.1.3</span> 레만-쉐페 thm.<span></span></a></li>
<li><a href="inference.html#raoblack" id="toc-raoblack"><span class="toc-section-number">3.1.4</span> Rao-Blackwell thm.<span></span></a></li>
</ul></li>
<li><a href="hypothesis-test.html#hypothesis-test" id="toc-hypothesis-test"><span class="toc-section-number">3.2</span> Hypothesis Test<span></span></a></li>
<li><a href="power-fucntion.html#power-fucntion" id="toc-power-fucntion"><span class="toc-section-number">3.3</span> Power Fucntion<span></span></a>
<ul>
<li><a href="power-fucntion.html#significance-probability-p-value" id="toc-significance-probability-p-value"><span class="toc-section-number">3.3.1</span> Significance Probability (p-value)<span></span></a></li>
</ul></li>
<li><a href="optimal-testing-method.html#optimal-testing-method" id="toc-optimal-testing-method"><span class="toc-section-number">3.4</span> Optimal Testing Method<span></span></a></li>
<li><a href="data-reduction.html#data-reduction" id="toc-data-reduction"><span class="toc-section-number">3.5</span> Data Reduction<span></span></a>
<ul>
<li><a href="data-reduction.html#sufficiency-principle" id="toc-sufficiency-principle"><span class="toc-section-number">3.5.1</span> Sufficiency Principle<span></span></a></li>
</ul></li>
<li><a href="borel-paradox.html#borel-paradox" id="toc-borel-paradox"><span class="toc-section-number">3.6</span> Borel Paradox<span></span></a></li>
<li><a href="neymanpearson-lemma.html#neymanpearson-lemma" id="toc-neymanpearson-lemma"><span class="toc-section-number">3.7</span> Neyman–Pearson lemma<span></span></a>
<ul>
<li><a href="neymanpearson-lemma.html#overview-1" id="toc-overview-1"><span class="toc-section-number">3.7.1</span> Overview<span></span></a></li>
<li><a href="neymanpearson-lemma.html#generalized-lrt" id="toc-generalized-lrt"><span class="toc-section-number">3.7.2</span> Generalized LRT<span></span></a></li>
</ul></li>
<li><a href="개념.html#개념" id="toc-개념"><span class="toc-section-number">3.8</span> 개념<span></span></a></li>
</ul></li>
<li><a href="mcmc.html#mcmc" id="toc-mcmc"><span class="toc-section-number">4</span> MCMC<span></span></a>
<ul>
<li><a href="importance-sampling.html#importance-sampling" id="toc-importance-sampling"><span class="toc-section-number">4.1</span> Importance Sampling<span></span></a>
<ul>
<li><a href="importance-sampling.html#independent-monte-carlo" id="toc-independent-monte-carlo"><span class="toc-section-number">4.1.1</span> Independent Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo"><span class="toc-section-number">4.2</span> Markov Chain Monte Carlo<span></span></a>
<ul>
<li><a href="markov-chain-monte-carlo.html#mh-algorithm" id="toc-mh-algorithm"><span class="toc-section-number">4.2.1</span> MH Algorithm<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used" id="toc-random-walk-chains-most-widely-used"><span class="toc-section-number">4.2.2</span> Random Walk Chains (Most Widely Used)<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler" id="toc-basic-gibbs-sampler"><span class="toc-section-number">4.2.3</span> Basic Gibbs Sampler<span></span></a></li>
<li><a href="markov-chain-monte-carlo.html#implementation" id="toc-implementation"><span class="toc-section-number">4.2.4</span> Implementation<span></span></a></li>
</ul></li>
<li><a href="advanced-mcmc-wk08.html#advanced-mcmc-wk08" id="toc-advanced-mcmc-wk08"><span class="toc-section-number">4.3</span> Advanced MCMC (wk08)<span></span></a>
<ul>
<li><a href="advanced-mcmc-wk08.html#data-augmentation" id="toc-data-augmentation"><span class="toc-section-number">4.3.1</span> Data Augmentation<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm" id="toc-hit-and-run-algorithm"><span class="toc-section-number">4.3.2</span> Hit-and-Run Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm" id="toc-metropolis-adjusted-langevin-algorithm"><span class="toc-section-number">4.3.3</span> Metropolis-Adjusted Langevin Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm" id="toc-multiple-try-metropolis-algorithm"><span class="toc-section-number">4.3.4</span> Multiple-Try Metropolis Algorithm<span></span></a></li>
<li><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm" id="toc-reversible-jump-mcmc-algorithm"><span class="toc-section-number">4.3.5</span> Reversible Jump MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="auxiliary-variable-mcmc.html#auxiliary-variable-mcmc" id="toc-auxiliary-variable-mcmc"><span class="toc-section-number">4.4</span> Auxiliary Variable MCMC<span></span></a>
<ul>
<li><a href="auxiliary-variable-mcmc.html#introduction" id="toc-introduction"><span class="toc-section-number">4.4.1</span> Introduction<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution" id="toc-multimodal-target-distribution"><span class="toc-section-number">4.4.2</span> Multimodal Target Distribution<span></span></a></li>
<li><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants" id="toc-doubly-intractable-normalizing-constants"><span class="toc-section-number">4.4.3</span> Doubly-intractable Normalizing Constants<span></span></a></li>
</ul></li>
<li><a href="approximate-bayesian-computation.html#approximate-bayesian-computation" id="toc-approximate-bayesian-computation"><span class="toc-section-number">4.5</span> Approximate Bayesian Computation<span></span></a>
<ul>
<li><a href="approximate-bayesian-computation.html#simulator-based-models" id="toc-simulator-based-models"><span class="toc-section-number">4.5.1</span> Simulator-Based Models<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods" id="toc-abcifying-monte-carlo-methods"><span class="toc-section-number">4.5.2</span> ABCifying Monte Carlo Methods<span></span></a></li>
<li><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm" id="toc-abc-mcmc-algorithm"><span class="toc-section-number">4.5.3</span> ABC-MCMC Algorithm<span></span></a></li>
</ul></li>
<li><a href="hamiltonian-monte-carlo.html#hamiltonian-monte-carlo" id="toc-hamiltonian-monte-carlo"><span class="toc-section-number">4.6</span> Hamiltonian Monte Carlo<span></span></a>
<ul>
<li><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo" id="toc-introduction-to-hamiltonian-monte-carlo"><span class="toc-section-number">4.6.1</span> Introduction to Hamiltonian Monte Carlo<span></span></a></li>
</ul></li>
<li><a href="population-monte-carlo.html#population-monte-carlo" id="toc-population-monte-carlo"><span class="toc-section-number">4.7</span> Population Monte Carlo<span></span></a>
<ul>
<li><a href="population-monte-carlo.html#adaptive-direction-sampling" id="toc-adaptive-direction-sampling"><span class="toc-section-number">4.7.1</span> Adaptive Direction Sampling<span></span></a></li>
<li><a href="population-monte-carlo.html#conjugate-gradient-mc" id="toc-conjugate-gradient-mc"><span class="toc-section-number">4.7.2</span> Conjugate Gradient MC<span></span></a></li>
<li><a href="population-monte-carlo.html#parallel-tempering" id="toc-parallel-tempering"><span class="toc-section-number">4.7.3</span> Parallel Tempering<span></span></a></li>
<li><a href="population-monte-carlo.html#evolutionary-mc" id="toc-evolutionary-mc"><span class="toc-section-number">4.7.4</span> Evolutionary MC<span></span></a></li>
<li><a href="population-monte-carlo.html#sequential-parallel-tempering" id="toc-sequential-parallel-tempering"><span class="toc-section-number">4.7.5</span> Sequential Parallel Tempering<span></span></a></li>
</ul></li>
<li><a href="stochastic-approximation-monte-carlo.html#stochastic-approximation-monte-carlo" id="toc-stochastic-approximation-monte-carlo"><span class="toc-section-number">4.8</span> Stochastic Approximation Monte Carlo<span></span></a></li>
<li><a href="review.html#review" id="toc-review"><span class="toc-section-number">4.9</span> Review<span></span></a>
<ul>
<li><a href="review.html#wk01" id="toc-wk01"><span class="toc-section-number">4.9.1</span> Wk01<span></span></a></li>
<li><a href="review.html#wk03" id="toc-wk03"><span class="toc-section-number">4.9.2</span> wk03<span></span></a></li>
<li><a href="review.html#wk04-05" id="toc-wk04-05"><span class="toc-section-number">4.9.3</span> wk04, 05<span></span></a></li>
</ul></li>
<li><a href="else.html#else" id="toc-else"><span class="toc-section-number">4.10</span> Else<span></span></a>
<ul>
<li><a href="else.html#hw4.-rasch-model" id="toc-hw4.-rasch-model"><span class="toc-section-number">4.10.1</span> Hw4. Rasch Model<span></span></a></li>
<li><a href="else.html#da-example-mvn" id="toc-da-example-mvn"><span class="toc-section-number">4.10.2</span> DA) Example: MVN<span></span></a></li>
<li><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes" id="toc-bayesian-adaptive-clinical-trial-with-delayed-outcomes"><span class="toc-section-number">4.10.3</span> Bayesian adaptive clinical trial with delayed outcomes<span></span></a></li>
<li><a href="else.html#nmar의-종류" id="toc-nmar의-종류"><span class="toc-section-number">4.10.4</span> NMAR의 종류<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-selection" id="toc-wk10-bayesian-model-selection"><span class="toc-section-number">4.10.5</span> wk10) Bayesian Model Selection<span></span></a></li>
<li><a href="else.html#autologistic-model" id="toc-autologistic-model"><span class="toc-section-number">4.10.6</span> Autologistic model<span></span></a></li>
<li><a href="else.html#wk10-bayesian-model-averaging" id="toc-wk10-bayesian-model-averaging"><span class="toc-section-number">4.10.7</span> wk10) Bayesian Model Averaging<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="mva.html#mva" id="toc-mva"><span class="toc-section-number">5</span> MVA<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#overview-of-mva-not-ended" id="toc-overview-of-mva-not-ended"><span class="toc-section-number">5.1</span> Overview of mva (not ended)<span></span></a>
<ul>
<li><a href="overview-of-mva-not-ended.html#notation" id="toc-notation"><span class="toc-section-number">5.1.1</span> Notation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">5.1.2</span> Summary Statistics<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation" id="toc-statistical-inference-on-correlation"><span class="toc-section-number">5.1.3</span> Statistical Inference on Correlation<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#standardization" id="toc-standardization"><span class="toc-section-number">5.1.4</span> Standardization<span></span></a></li>
<li><a href="overview-of-mva-not-ended.html#missing-value-treatment" id="toc-missing-value-treatment"><span class="toc-section-number">5.1.5</span> Missing Value Treatment<span></span></a></li>
</ul></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-nomral-wk2" id="toc-multivariate-nomral-wk2"><span class="toc-section-number">5.2</span> Multivariate Nomral (wk2)<span></span></a>
<ul>
<li><a href="multivariate-nomral-wk2.html#overview-2" id="toc-overview-2"><span class="toc-section-number">5.2.1</span> Overview<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">5.2.2</span> Spectral Decomposition<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">5.2.3</span> Properties of MVN<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#chi2-distribution" id="toc-chi2-distribution"><span class="toc-section-number">5.2.4</span> <span class="math inline">\(\Chi^2\)</span> distribution<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors" id="toc-linear-combination-of-random-vectors"><span class="toc-section-number">5.2.5</span> Linear Combination of Random Vectors<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood" id="toc-multivariate-normal-likelihood"><span class="toc-section-number">5.2.6</span> Multivariate Normal Likelihood<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s" id="toc-sampling-distribtion-of-bar-pmb-y-s"><span class="toc-section-number">5.2.7</span> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span><span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#assessing-normality" id="toc-assessing-normality"><span class="toc-section-number">5.2.8</span> Assessing Normality<span></span></a></li>
<li><a href="multivariate-nomral-wk2.html#power-transformation" id="toc-power-transformation"><span class="toc-section-number">5.2.9</span> Power Transformation<span></span></a></li>
</ul></li>
<li><a href="inference-about-mean-vector-wk3.html#inference-about-mean-vector-wk3" id="toc-inference-about-mean-vector-wk3"><span class="toc-section-number">5.3</span> Inference about Mean Vector (wk3)<span></span></a>
<ul>
<li><a href="inference-about-mean-vector-wk3.html#overview-3" id="toc-overview-3"><span class="toc-section-number">5.3.1</span> Overview<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#confidence-region" id="toc-confidence-region"><span class="toc-section-number">5.3.2</span> 1. Confidence Region<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#simultaneous-ci" id="toc-simultaneous-ci"><span class="toc-section-number">5.3.3</span> 2. Simultaneous CI<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison" id="toc-note-bonferroni-multiple-comparison"><span class="toc-section-number">5.3.4</span> 3. Note: Bonferroni Multiple Comparison<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector" id="toc-large-sample-inferences-about-a-mean-vector"><span class="toc-section-number">5.3.5</span> 4. Large Sample Inferences about a Mean Vector<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5" id="toc-profile-analysis-wk4-5"><span class="toc-section-number">5.3.6</span> 1. Profile Analysis (wk4, 5)<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend" id="toc-test-for-linear-trend"><span class="toc-section-number">5.3.7</span> 2. Test for Linear Trend<span></span></a></li>
<li><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix" id="toc-inferences-about-a-covariance-matrix"><span class="toc-section-number">5.3.8</span> 3. Inferences about a Covariance Matrix<span></span></a></li>
</ul></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparison-of-several-mv-means-wk5" id="toc-comparison-of-several-mv-means-wk5"><span class="toc-section-number">5.4</span> Comparison of Several MV Means (wk5)<span></span></a>
<ul>
<li><a href="comparison-of-several-mv-means-wk5.html#paired-comparison" id="toc-paired-comparison"><span class="toc-section-number">5.4.1</span> Paired Comparison<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations" id="toc-comparing-mean-vectors-from-two-populations"><span class="toc-section-number">5.4.2</span> Comparing Mean Vectors from Two Populations<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2" id="toc-profile-analysis-for-g2"><span class="toc-section-number">5.4.3</span> Profile Analysis (for <span class="math inline">\(g=2\)</span>)<span></span></a></li>
<li><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means" id="toc-comparing-several-multivariate-population-means"><span class="toc-section-number">5.4.4</span> Comparing Several Multivariate Population Means<span></span></a></li>
</ul></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression-wk6" id="toc-multivariate-multiple-regression-wk6"><span class="toc-section-number">5.5</span> Multivariate Multiple Regression (wk6)<span></span></a>
<ul>
<li><a href="multivariate-multiple-regression-wk6.html#overview-4" id="toc-overview-4"><span class="toc-section-number">5.5.1</span> Overview<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression" id="toc-multivariate-multiple-regression"><span class="toc-section-number">5.5.2</span> Multivariate Multiple Regression<span></span></a></li>
<li><a href="multivariate-multiple-regression-wk6.html#example" id="toc-example"><span class="toc-section-number">5.5.3</span> Example)<span></span></a></li>
</ul></li>
<li><a href="pca.html#pca" id="toc-pca"><span class="toc-section-number">5.6</span> PCA<span></span></a></li>
<li><a href="factor.html#factor" id="toc-factor"><span class="toc-section-number">5.7</span> Factor<span></span></a>
<ul>
<li><a href="factor.html#method-of-estimation" id="toc-method-of-estimation"><span class="toc-section-number">5.7.1</span> Method of Estimation<span></span></a></li>
<li><a href="factor.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">5.7.2</span> Factor Rotation<span></span></a></li>
<li><a href="factor.html#varimax-criterion" id="toc-varimax-criterion"><span class="toc-section-number">5.7.3</span> Varimax Criterion<span></span></a></li>
<li><a href="factor.html#factor-scores" id="toc-factor-scores"><span class="toc-section-number">5.7.4</span> Factor Scores<span></span></a></li>
</ul></li>
<li><a href="discrimination-and-classification.html#discrimination-and-classification" id="toc-discrimination-and-classification"><span class="toc-section-number">5.8</span> Discrimination and Classification<span></span></a>
<ul>
<li><a href="discrimination-and-classification.html#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">5.8.1</span> Bayes Rule<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations" id="toc-classification-with-two-mv-n-populations"><span class="toc-section-number">5.8.2</span> Classification with Two mv <span class="math inline">\(N\)</span> Populations<span></span></a></li>
<li><a href="discrimination-and-classification.html#evaluating-classification-functions" id="toc-evaluating-classification-functions"><span class="toc-section-number">5.8.3</span> Evaluating Classification Functions<span></span></a></li>
<li><a href="discrimination-and-classification.html#classification-with-several-populations-wk13" id="toc-classification-with-several-populations-wk13"><span class="toc-section-number">5.8.4</span> Classification with several Populations (wk13)<span></span></a></li>
<li><a href="discrimination-and-classification.html#other-discriminant-analysis-methods" id="toc-other-discriminant-analysis-methods"><span class="toc-section-number">5.8.5</span> Other Discriminant Analysis Methods<span></span></a></li>
</ul></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-distance-methods-and-ordination" id="toc-clustering-distance-methods-and-ordination"><span class="toc-section-number">5.9</span> Clustering, Distance Methods, and Ordination<span></span></a>
<ul>
<li><a href="clustering-distance-methods-and-ordination.html#overview-5" id="toc-overview-5"><span class="toc-section-number">5.9.1</span> Overview<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">5.9.2</span> Hierarchical Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">5.9.3</span> K-means Clustering<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법" id="toc-군집의-평가방법"><span class="toc-section-number">5.9.4</span> 군집의 평가방법<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14" id="toc-clustering-using-density-estimation-wk14"><span class="toc-section-number">5.9.5</span> Clustering using Density Estimation (wk14)<span></span></a></li>
<li><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds" id="toc-multidimensional-scaling-mds"><span class="toc-section-number">5.9.6</span> Multidimensional Scaling (MDS)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linear.html#linear" id="toc-linear"><span class="toc-section-number">6</span> Linear<span></span></a>
<ul>
<li><a href="overview-svd.html#overview-svd" id="toc-overview-svd"><span class="toc-section-number">6.1</span> Overview &amp; SVD<span></span></a>
<ul>
<li><a href="overview-svd.html#spectral-decomposition-1" id="toc-spectral-decomposition-1"><span class="toc-section-number">6.1.1</span> Spectral Decomposition<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-general-version" id="toc-singular-value-decomposition-general-version"><span class="toc-section-number">6.1.2</span> Singular value Decomposition: General-version<span></span></a></li>
<li><a href="overview-svd.html#singular-value-decomposition-another-version" id="toc-singular-value-decomposition-another-version"><span class="toc-section-number">6.1.3</span> Singular value Decomposition: Another-version<span></span></a></li>
<li><a href="overview-svd.html#quadratic-forms" id="toc-quadratic-forms"><span class="toc-section-number">6.1.4</span> Quadratic Forms<span></span></a></li>
<li><a href="overview-svd.html#partitioned-matrices" id="toc-partitioned-matrices"><span class="toc-section-number">6.1.5</span> Partitioned Matrices<span></span></a></li>
<li><a href="overview-svd.html#geometrical-aspects" id="toc-geometrical-aspects"><span class="toc-section-number">6.1.6</span> Geometrical Aspects<span></span></a></li>
<li><a href="overview-svd.html#column-row-and-null-space" id="toc-column-row-and-null-space"><span class="toc-section-number">6.1.7</span> Column, Row and Null Space<span></span></a></li>
</ul></li>
<li><a href="introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">6.2</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-1.html#what" id="toc-what"><span class="toc-section-number">6.2.1</span> What<span></span></a></li>
<li><a href="introduction-1.html#random-vectors-and-matrices" id="toc-random-vectors-and-matrices"><span class="toc-section-number">6.2.2</span> Random Vectors and Matrices<span></span></a></li>
<li><a href="introduction-1.html#multivariate-normal-distributions" id="toc-multivariate-normal-distributions"><span class="toc-section-number">6.2.3</span> Multivariate Normal Distributions<span></span></a></li>
<li><a href="introduction-1.html#distributions-of-quadratic-forms" id="toc-distributions-of-quadratic-forms"><span class="toc-section-number">6.2.4</span> Distributions of Quadratic Forms<span></span></a></li>
</ul></li>
<li><a href="estimation.html#estimation" id="toc-estimation"><span class="toc-section-number">6.3</span> Estimation<span></span></a>
<ul>
<li><a href="estimation.html#identifiability-and-estimability" id="toc-identifiability-and-estimability"><span class="toc-section-number">6.3.1</span> Identifiability and Estimability<span></span></a></li>
<li><a href="estimation.html#estimation-least-squares" id="toc-estimation-least-squares"><span class="toc-section-number">6.3.2</span> Estimation: Least Squares<span></span></a></li>
<li><a href="estimation.html#estimation-best-linear-unbiased" id="toc-estimation-best-linear-unbiased"><span class="toc-section-number">6.3.3</span> Estimation: Best Linear Unbiased<span></span></a></li>
<li><a href="estimation.html#estimation-maximum-likelihood" id="toc-estimation-maximum-likelihood"><span class="toc-section-number">6.3.4</span> Estimation: Maximum Likelihood<span></span></a></li>
<li><a href="estimation.html#estimation-minimum-variance-unbiased" id="toc-estimation-minimum-variance-unbiased"><span class="toc-section-number">6.3.5</span> Estimation: Minimum Variance Unbiased<span></span></a></li>
<li><a href="estimation.html#sampling-distributions-of-estimates" id="toc-sampling-distributions-of-estimates"><span class="toc-section-number">6.3.6</span> Sampling Distributions of Estimates<span></span></a></li>
<li><a href="estimation.html#generalized-least-squaresgls" id="toc-generalized-least-squaresgls"><span class="toc-section-number">6.3.7</span> Generalized Least Squares(GLS)<span></span></a></li>
</ul></li>
<li><a href="one-way-anova.html#one-way-anova" id="toc-one-way-anova"><span class="toc-section-number">6.4</span> One-Way ANOVA<span></span></a>
<ul>
<li><a href="one-way-anova.html#one-way-anova-1" id="toc-one-way-anova-1"><span class="toc-section-number">6.4.1</span> One-Way ANOVA<span></span></a></li>
<li><a href="one-way-anova.html#more-about-models" id="toc-more-about-models"><span class="toc-section-number">6.4.2</span> More About Models<span></span></a></li>
<li><a href="one-way-anova.html#estimating-and-testing-contrasts" id="toc-estimating-and-testing-contrasts"><span class="toc-section-number">6.4.3</span> Estimating and Testing Contrasts<span></span></a></li>
<li><a href="one-way-anova.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">6.4.4</span> Cochran’s Theorem<span></span></a></li>
</ul></li>
<li><a href="testing.html#testing" id="toc-testing"><span class="toc-section-number">6.5</span> Testing<span></span></a>
<ul>
<li><a href="testing.html#more-about-models-two-approaches-for-linear-model" id="toc-more-about-models-two-approaches-for-linear-model"><span class="toc-section-number">6.5.1</span> More About Models: Two approaches for linear model<span></span></a></li>
<li><a href="testing.html#testing-models" id="toc-testing-models"><span class="toc-section-number">6.5.2</span> Testing Models<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure" id="toc-a-generalized-test-procedure"><span class="toc-section-number">6.5.3</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-linear-parametric-functions" id="toc-testing-linear-parametric-functions"><span class="toc-section-number">6.5.4</span> Testing Linear Parametric Functions<span></span></a></li>
<li><a href="testing.html#theoretical-complements" id="toc-theoretical-complements"><span class="toc-section-number">6.5.5</span> Theoretical Complements<span></span></a></li>
<li><a href="testing.html#a-generalized-test-procedure-1" id="toc-a-generalized-test-procedure-1"><span class="toc-section-number">6.5.6</span> A Generalized Test Procedure<span></span></a></li>
<li><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace" id="toc-testing-single-degrees-of-freedom-in-a-given-subspace"><span class="toc-section-number">6.5.7</span> Testing Single Degrees of Freedom in a Given Subspace<span></span></a></li>
<li><a href="testing.html#breaking-ss-into-independent-components" id="toc-breaking-ss-into-independent-components"><span class="toc-section-number">6.5.8</span> Breaking SS into Independent Components<span></span></a></li>
<li><a href="testing.html#general-theory" id="toc-general-theory"><span class="toc-section-number">6.5.9</span> General Theory<span></span></a></li>
<li><a href="testing.html#two-way-anova" id="toc-two-way-anova"><span class="toc-section-number">6.5.10</span> Two-Way ANOVA<span></span></a></li>
<li><a href="testing.html#confidence-regions" id="toc-confidence-regions"><span class="toc-section-number">6.5.11</span> Confidence Regions<span></span></a></li>
<li><a href="testing.html#tests-for-generalized-least-squares-models" id="toc-tests-for-generalized-least-squares-models"><span class="toc-section-number">6.5.12</span> Tests for Generalized Least Squares Models<span></span></a></li>
</ul></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">6.6</span> Generalized Least Squares<span></span></a>
<ul>
<li><a href="generalized-least-squares.html#a-direct-solution-via-inner-products" id="toc-a-direct-solution-via-inner-products"><span class="toc-section-number">6.6.1</span> A direct solution via inner products<span></span></a></li>
</ul></li>
<li><a href="flat.html#flat" id="toc-flat"><span class="toc-section-number">6.7</span> Flat<span></span></a>
<ul>
<li><a href="flat.html#flat-1" id="toc-flat-1"><span class="toc-section-number">6.7.1</span> 1.Flat<span></span></a></li>
<li><a href="flat.html#solutions-to-systems-of-linear-equations" id="toc-solutions-to-systems-of-linear-equations"><span class="toc-section-number">6.7.2</span> 2. Solutions to systems of linear equations<span></span></a></li>
</ul></li>
<li><a href="unified-approach-to-balanced-anova-models.html#unified-approach-to-balanced-anova-models" id="toc-unified-approach-to-balanced-anova-models"><span class="toc-section-number">6.8</span> Unified Approach to Balanced ANOVA Models<span></span></a></li>
</ul></li>
<li><a href="#part-21-02" id="toc-part-21-02">(PART) 21-02<span></span></a></li>
<li><a href="network-stats.html#network-stats" id="toc-network-stats"><span class="toc-section-number">7</span> Network Stats<span></span></a>
<ul>
<li><a href="introduction-2.html#introduction-2" id="toc-introduction-2"><span class="toc-section-number">7.1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction-2.html#types-of-network-analysis" id="toc-types-of-network-analysis"><span class="toc-section-number">7.1.1</span> Types of Network Analysis<span></span></a></li>
<li><a href="introduction-2.html#network-modeling-and-inference" id="toc-network-modeling-and-inference"><span class="toc-section-number">7.1.2</span> Network Modeling and Inference<span></span></a></li>
<li><a href="introduction-2.html#network-processes" id="toc-network-processes"><span class="toc-section-number">7.1.3</span> Network Processes<span></span></a></li>
</ul></li>
<li><a href="descriptive-statistics-of-networks.html#descriptive-statistics-of-networks" id="toc-descriptive-statistics-of-networks"><span class="toc-section-number">7.2</span> Descriptive Statistics of Networks<span></span></a>
<ul>
<li><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics" id="toc-vertex-and-edge-characteristics"><span class="toc-section-number">7.2.1</span> Vertex and Edge Characteristics<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion" id="toc-characterizing-network-cohesion"><span class="toc-section-number">7.2.2</span> Characterizing Network Cohesion<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#graph-partitioning" id="toc-graph-partitioning"><span class="toc-section-number">7.2.3</span> Graph Partitioning<span></span></a></li>
<li><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing" id="toc-assortativity-and-mixing"><span class="toc-section-number">7.2.4</span> Assortativity and Mixing<span></span></a></li>
</ul></li>
<li><a href="data-collection-and-sampling.html#data-collection-and-sampling" id="toc-data-collection-and-sampling"><span class="toc-section-number">7.3</span> Data Collection and Sampling<span></span></a>
<ul>
<li><a href="data-collection-and-sampling.html#sampling-designs" id="toc-sampling-designs"><span class="toc-section-number">7.3.1</span> Sampling Designs<span></span></a></li>
<li><a href="data-collection-and-sampling.html#coping-strategies" id="toc-coping-strategies"><span class="toc-section-number">7.3.2</span> Coping Strategies<span></span></a></li>
<li><a href="data-collection-and-sampling.html#big-data-solves-nothing" id="toc-big-data-solves-nothing"><span class="toc-section-number">7.3.3</span> Big Data Solves Nothing<span></span></a></li>
</ul></li>
<li><a href="mathematical-models-for-network-graphs.html#mathematical-models-for-network-graphs" id="toc-mathematical-models-for-network-graphs"><span class="toc-section-number">7.4</span> Mathematical Models for Network Graphs<span></span></a>
<ul>
<li><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models" id="toc-classical-random-graph-models"><span class="toc-section-number">7.4.1</span> Classical Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models" id="toc-generalized-random-graph-models"><span class="toc-section-number">7.4.2</span> Generalized Random Graph Models<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms" id="toc-network-graph-models-based-on-mechanisms"><span class="toc-section-number">7.4.3</span> Network Graph Models Based on Mechanisms<span></span></a></li>
<li><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics" id="toc-assessing-significance-of-network-graph-characteristics"><span class="toc-section-number">7.4.4</span> Assessing Significance of Network Graph Characteristics<span></span></a></li>
</ul></li>
<li><a href="introduction-to-ergm.html#introduction-to-ergm" id="toc-introduction-to-ergm"><span class="toc-section-number">7.5</span> Introduction to ERGM<span></span></a>
<ul>
<li><a href="introduction-to-ergm.html#exponential-random-graph-models" id="toc-exponential-random-graph-models"><span class="toc-section-number">7.5.1</span> Exponential Random Graph Models<span></span></a></li>
<li><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation" id="toc-difficulty-in-parameter-estimation"><span class="toc-section-number">7.5.2</span> Difficulty in Parameter Estimation<span></span></a></li>
</ul></li>
<li><a href="parameter-estimation-of-ergm.html#parameter-estimation-of-ergm" id="toc-parameter-estimation-of-ergm"><span class="toc-section-number">7.6</span> Parameter Estimation of ERGM<span></span></a>
<ul>
<li><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm" id="toc-current-methods-for-ergm"><span class="toc-section-number">7.6.1</span> Current Methods for ERGM<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm" id="toc-approximation-based-algorithm"><span class="toc-section-number">7.6.2</span> Approximation-based Algorithm<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches" id="toc-auxiliary-variable-mcmc-based-approaches"><span class="toc-section-number">7.6.3</span> Auxiliary Variable MCMC-based Approaches<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc" id="toc-varying-trunction-stochastic-approximation-mcmc"><span class="toc-section-number">7.6.4</span> Varying Trunction Stochastic Approximation MCMC<span></span></a></li>
<li><a href="parameter-estimation-of-ergm.html#conclusion" id="toc-conclusion"><span class="toc-section-number">7.6.5</span> Conclusion<span></span></a></li>
</ul></li>
<li><a href="ergm-for-dynamic-networks.html#ergm-for-dynamic-networks" id="toc-ergm-for-dynamic-networks"><span class="toc-section-number">7.7</span> ERGM for Dynamic Networks<span></span></a>
<ul>
<li><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm" id="toc-temporal-ergm-tergm-t-ergm"><span class="toc-section-number">7.7.1</span> Temporal ERGM (TERGM, T ERGM)<span></span></a></li>
<li><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm" id="toc-separable-temporal-ergm-stergm-st-ergm"><span class="toc-section-number">7.7.2</span> Separable Temporal ERGM (STERGM, ST ERGM)<span></span></a></li>
</ul></li>
<li><a href="latent-network-models.html#latent-network-models" id="toc-latent-network-models"><span class="toc-section-number">7.8</span> Latent Network Models<span></span></a>
<ul>
<li><a href="latent-network-models.html#latent-position-model" id="toc-latent-position-model"><span class="toc-section-number">7.8.1</span> Latent Position Model<span></span></a></li>
<li><a href="latent-network-models.html#latent-position-cluster-model" id="toc-latent-position-cluster-model"><span class="toc-section-number">7.8.2</span> Latent Position Cluster Model<span></span></a></li>
</ul></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#additive-and-multiplicative-effects-network-models" id="toc-additive-and-multiplicative-effects-network-models"><span class="toc-section-number">7.9</span> Additive and Multiplicative Effects Network Models<span></span></a>
<ul>
<li><a href="additive-and-multiplicative-effects-network-models.html#introduction-3" id="toc-introduction-3"><span class="toc-section-number">7.9.1</span> Introduction<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression" id="toc-social-relations-regression"><span class="toc-section-number">7.9.2</span> Social Relations Regression<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models" id="toc-multiplicative-effects-models"><span class="toc-section-number">7.9.3</span> Multiplicative Effects Models<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation" id="toc-inference-via-posterior-approximation"><span class="toc-section-number">7.9.4</span> Inference via Posterior Approximation<span></span></a></li>
<li><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r" id="toc-discussion-and-example-with-r"><span class="toc-section-number">7.9.5</span> Discussion and Example with R<span></span></a></li>
</ul></li>
<li><a href="stochastic-block-models.html#stochastic-block-models" id="toc-stochastic-block-models"><span class="toc-section-number">7.10</span> Stochastic Block Models<span></span></a>
<ul>
<li><a href="stochastic-block-models.html#stochastic-block-model" id="toc-stochastic-block-model"><span class="toc-section-number">7.10.1</span> Stochastic Block Model<span></span></a></li>
<li><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm" id="toc-mixed-membership-block-model-mmbm"><span class="toc-section-number">7.10.2</span> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="high-dimension.html#high-dimension" id="toc-high-dimension"><span class="toc-section-number">8</span> High Dimension<span></span></a>
<ul>
<li><a href="introduction-4.html#introduction-4" id="toc-introduction-4"><span class="toc-section-number">8.1</span> Introduction<span></span></a></li>
<li><a href="concentration-inequalities.html#concentration-inequalities" id="toc-concentration-inequalities"><span class="toc-section-number">8.2</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities.html#motivation" id="toc-motivation"><span class="toc-section-number">8.2.1</span> Motivation<span></span></a></li>
<li><a href="concentration-inequalities.html#from-markov-to-chernoff" id="toc-from-markov-to-chernoff"><span class="toc-section-number">8.2.2</span> From Markov to Chernoff<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-variables" id="toc-sub-gaussian-random-variables"><span class="toc-section-number">8.2.3</span> sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables" id="toc-properties-of-sub-gaussian-random-variables"><span class="toc-section-number">8.2.4</span> Properties of sub-Gaussian random variables<span></span></a></li>
<li><a href="concentration-inequalities.html#equivalent-definitions" id="toc-equivalent-definitions"><span class="toc-section-number">8.2.5</span> Equivalent definitions<span></span></a></li>
<li><a href="concentration-inequalities.html#sub-gaussian-random-vectors" id="toc-sub-gaussian-random-vectors"><span class="toc-section-number">8.2.6</span> Sub-Gaussian random vectors<span></span></a></li>
<li><a href="concentration-inequalities.html#hoeffdings-inequality" id="toc-hoeffdings-inequality"><span class="toc-section-number">8.2.7</span> Hoeffding’s inequality<span></span></a></li>
<li><a href="concentration-inequalities.html#maximal-inequalities" id="toc-maximal-inequalities"><span class="toc-section-number">8.2.8</span> Maximal inequalities<span></span></a></li>
<li><a href="concentration-inequalities.html#section" id="toc-section"><span class="toc-section-number">8.2.9</span> </a></li>
</ul></li>
<li><a href="concentration-inequalities-1.html#concentration-inequalities-1" id="toc-concentration-inequalities-1"><span class="toc-section-number">8.3</span> Concentration inequalities<span></span></a>
<ul>
<li><a href="concentration-inequalities-1.html#sub-exponential-random-variables" id="toc-sub-exponential-random-variables"><span class="toc-section-number">8.3.1</span> Sub-exponential random variables<span></span></a></li>
<li><a href="concentration-inequalities-1.html#bernsteins-condition" id="toc-bernsteins-condition"><span class="toc-section-number">8.3.2</span> Bernstein’s condition<span></span></a></li>
<li><a href="concentration-inequalities-1.html#mcdiarmids-inequality" id="toc-mcdiarmids-inequality"><span class="toc-section-number">8.3.3</span> McDiarmid’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#levys-inequality" id="toc-levys-inequality"><span class="toc-section-number">8.3.4</span> Levy’s inequality<span></span></a></li>
<li><a href="concentration-inequalities-1.html#quadratic-form" id="toc-quadratic-form"><span class="toc-section-number">8.3.5</span> Quadratic form<span></span></a></li>
<li><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma" id="toc-the-johnsonlindenstrauss-lemma"><span class="toc-section-number">8.3.6</span> The Johnson–Lindenstrauss Lemma<span></span></a></li>
</ul></li>
<li><a href="metric-entropy-and-its-uses.html#metric-entropy-and-its-uses" id="toc-metric-entropy-and-its-uses"><span class="toc-section-number">8.4</span> Metric entropy and its uses<span></span></a>
<ul>
<li><a href="metric-entropy-and-its-uses.html#metric-space" id="toc-metric-space"><span class="toc-section-number">8.4.1</span> Metric space<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy" id="toc-covering-numbers-and-metric-entropy"><span class="toc-section-number">8.4.2</span> Covering numbers and metric entropy<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#packing-numbers" id="toc-packing-numbers"><span class="toc-section-number">8.4.3</span> Packing numbers<span></span></a></li>
<li><a href="metric-entropy-and-its-uses.html#section-1" id="toc-section-1"><span class="toc-section-number">8.4.4</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-2" id="toc-section-2"><span class="toc-section-number">8.4.5</span> </a></li>
<li><a href="metric-entropy-and-its-uses.html#section-3" id="toc-section-3"><span class="toc-section-number">8.4.6</span> </a></li>
</ul></li>
<li><a href="covariance-estimation.html#covariance-estimation" id="toc-covariance-estimation"><span class="toc-section-number">8.5</span> Covariance estimation<span></span></a>
<ul>
<li><a href="covariance-estimation.html#matrix-algebra-review" id="toc-matrix-algebra-review"><span class="toc-section-number">8.5.1</span> Matrix algebra review<span></span></a></li>
<li><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm" id="toc-covariance-matrix-estimation-in-the-operator-norm"><span class="toc-section-number">8.5.2</span> Covariance matrix estimation in the operator norm<span></span></a></li>
<li><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices" id="toc-bounds-for-structured-covariance-matrices"><span class="toc-section-number">8.5.3</span> Bounds for structured covariance matrices<span></span></a></li>
</ul></li>
<li><a href="matrix-concentration-inequalities.html#matrix-concentration-inequalities" id="toc-matrix-concentration-inequalities"><span class="toc-section-number">8.6</span> Matrix concentration inequalities<span></span></a>
<ul>
<li><a href="matrix-concentration-inequalities.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">8.6.1</span> Matrix calculus<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#matrix-chernoff" id="toc-matrix-chernoff"><span class="toc-section-number">8.6.2</span> Matrix Chernoff<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices" id="toc-sub-gaussian-and-sub-exponential-matrices"><span class="toc-section-number">8.6.3</span> Sub-Gaussian and sub-exponential matrices<span></span></a></li>
<li><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds" id="toc-랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><span class="toc-section-number">8.6.4</span> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds<span></span></a></li>
</ul></li>
<li><a href="principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.7</span> Principal Component Analysis<span></span></a>
<ul>
<li><a href="principal-component-analysis.html#pca-1" id="toc-pca-1"><span class="toc-section-number">8.7.1</span> PCA<span></span></a></li>
<li><a href="principal-component-analysis.html#matrix-perturbation" id="toc-matrix-perturbation"><span class="toc-section-number">8.7.2</span> Matrix Perturbation<span></span></a></li>
<li><a href="principal-component-analysis.html#spiked-cov-model" id="toc-spiked-cov-model"><span class="toc-section-number">8.7.3</span> Spiked Cov Model<span></span></a></li>
<li><a href="principal-component-analysis.html#sparse-pca" id="toc-sparse-pca"><span class="toc-section-number">8.7.4</span> sparse PCA<span></span></a></li>
</ul></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">8.8</span> Linear Regression<span></span></a>
<ul>
<li><a href="linear-regression.html#problem-formulation" id="toc-problem-formulation"><span class="toc-section-number">8.8.1</span> Problem formulation<span></span></a></li>
<li><a href="linear-regression.html#least-squares-estimator-in-high-dimensions" id="toc-least-squares-estimator-in-high-dimensions"><span class="toc-section-number">8.8.2</span> Least Squares Estimator in high dimensions<span></span></a></li>
<li><a href="linear-regression.html#sparse-linear-regression" id="toc-sparse-linear-regression"><span class="toc-section-number">8.8.3</span> Sparse linear regression<span></span></a></li>
</ul></li>
<li><a href="uniform-laws-of-large-numbers.html#uniform-laws-of-large-numbers" id="toc-uniform-laws-of-large-numbers"><span class="toc-section-number">8.9</span> Uniform laws of large numbers<span></span></a>
<ul>
<li><a href="uniform-laws-of-large-numbers.html#motivation-1" id="toc-motivation-1"><span class="toc-section-number">8.9.1</span> Motivation<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity" id="toc-a-uniform-law-via-rademacher-complexity"><span class="toc-section-number">8.9.2</span> A uniform law via Rademacher complexity<span></span></a></li>
<li><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity" id="toc-upper-bounds-on-the-rademacher-complexity"><span class="toc-section-number">8.9.3</span> Upper bounds on the Rademacher complexity<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="survival-analysis.html#survival-analysis" id="toc-survival-analysis"><span class="toc-section-number">9</span> Survival Analysis<span></span></a>
<ul>
<li><a href="introduction-5.html#introduction-5" id="toc-introduction-5"><span class="toc-section-number">9.1</span> Introduction<span></span></a></li>
<li><a href="section-4.html#section-4" id="toc-section-4"><span class="toc-section-number">9.2</span> </a></li>
<li><a href="counting-processes-and-martingales.html#counting-processes-and-martingales" id="toc-counting-processes-and-martingales"><span class="toc-section-number">9.3</span> Counting Processes and Martingales<span></span></a>
<ul>
<li><a href="counting-processes-and-martingales.html#conditional-expectation" id="toc-conditional-expectation"><span class="toc-section-number">9.3.1</span> Conditional Expectation<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#martingale" id="toc-martingale"><span class="toc-section-number">9.3.2</span> Martingale<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#key-martingales-properties" id="toc-key-martingales-properties"><span class="toc-section-number">9.3.3</span> Key Martingales Properties<span></span></a></li>
<li><a href="counting-processes-and-martingales.html#section-5" id="toc-section-5"><span class="toc-section-number">9.3.4</span> </a></li>
<li><a href="counting-processes-and-martingales.html#section-6" id="toc-section-6"><span class="toc-section-number">9.3.5</span> </a></li>
</ul></li>
<li><a href="section-7.html#section-7" id="toc-section-7"><span class="toc-section-number">9.4</span> </a></li>
<li><a href="cox-regression.html#cox-regression" id="toc-cox-regression"><span class="toc-section-number">9.5</span> Cox Regression<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#filtration의-개념을-정복하자" id="toc-filtration의-개념을-정복하자"><span class="toc-section-number">9.6</span> Filtration의 개념을 정복하자!<span></span></a>
<ul>
<li><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약" id="toc-random-process를-이야기-하기까지의-긴-여정의-요약"><span class="toc-section-number">9.6.1</span> Random Process를 이야기 하기까지의 긴 여정의 요약<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#ft-measurable" id="toc-ft-measurable"><span class="toc-section-number">9.6.2</span> Ft-measurable<span></span></a></li>
<li><a href="filtration의-개념을-정복하자.html#epilogue" id="toc-epilogue"><span class="toc-section-number">9.6.3</span> EPILOGUE<span></span></a></li>
</ul></li>
<li><a href="concepts.html#concepts" id="toc-concepts"><span class="toc-section-number">9.7</span> Concepts<span></span></a></li>
</ul></li>
<li><a href="#part-22-01" id="toc-part-22-01">(PART) 22-01<span></span></a></li>
<li><a href="scikit.html#scikit" id="toc-scikit"><span class="toc-section-number">10</span> scikit<span></span></a>
<ul>
<li><a href="linear-models.html#linear-models" id="toc-linear-models"><span class="toc-section-number">10.1</span> Linear Models<span></span></a>
<ul>
<li><a href="linear-models.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">10.1.1</span> Ordinary Least Squares<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-00-00" id="toc-appendix-00-00">(APPENDIX) 00-00<span></span></a></li>
<li><a href="concepts-1.html#concepts-1" id="toc-concepts-1"><span class="toc-section-number">11</span> Concepts<span></span></a>
<ul>
<li><a href="autologistic.html#autologistic" id="toc-autologistic"><span class="toc-section-number">11.1</span> Autologistics<span></span></a></li>
<li><a href="orderlogit.html#orderlogit" id="toc-orderlogit"><span class="toc-section-number">11.2</span> Ordered Logit<span></span></a></li>
<li><a href="concepts-questions.html#concepts-questions" id="toc-concepts-questions"><span class="toc-section-number">11.3</span> Concepts Questions<span></span></a>
<ul>
<li><a href="concepts-questions.html#통계-및-수학" id="toc-통계-및-수학"><span class="toc-section-number">11.3.1</span> 통계 및 수학<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="about-cluster-gcn.html#about-cluster-gcn" id="toc-about-cluster-gcn"><span class="toc-section-number">12</span> About Cluster-GCN<span></span></a>
<ul>
<li><a href="about-cluster-gcn.html#ann" id="toc-ann"><span class="toc-section-number">12.0.1</span> ANN<span></span></a></li>
<li><a href="about-cluster-gcn.html#cnn" id="toc-cnn"><span class="toc-section-number">12.0.2</span> CNN<span></span></a></li>
<li><a href="about-cluster-gcn.html#graph-convolution-network" id="toc-graph-convolution-network"><span class="toc-section-number">12.0.3</span> Graph Convolution Network<span></span></a></li>
<li><a href="about-cluster-gcn.html#cluster-gcn" id="toc-cluster-gcn"><span class="toc-section-number">12.0.4</span> Cluster-GCN<span></span></a></li>
</ul></li>
<li><a href="cnn-1.html#cnn-1" id="toc-cnn-1"><span class="toc-section-number">13</span> CNN<span></span></a></li>
<li><a href="cnn-2.html#cnn-2" id="toc-cnn-2"><span class="toc-section-number">14</span> CNN<span></span></a></li>
<li><a href="cnn-3.html#cnn-3" id="toc-cnn-3"><span class="toc-section-number">15</span> CNN<span></span></a></li>
<li><a href="section-8.html#section-8" id="toc-section-8"><span class="toc-section-number">16</span> 01<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="filtration의-개념을-정복하자" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Filtration의 개념을 정복하자!<a href="filtration의-개념을-정복하자.html#filtration의-개념을-정복하자" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>도대체 Filtration(Filtration event)을 정복해야 하는 이유가 무엇인가?</p>
<p>금융공학 특히 금융수학을 공부하는데 있어서 가장 핵심이 되는 것 중에 하나가 Stochastic Differencial Equation 이다. 금융수학책들을 보면 다들 처음에는 확률(이 바닥에서는 측도론을 건드리는 것이 일반적)을 다룬 다음에 Stochastic Process를 다루고 Ito’s lemma를 거쳐 Black Sholes PDE 한번 찍어주고 Girsanov를 돌아서 Feynmman-Kac Theory로 끝내곤 한다.</p>
<p>특히 표준 금융수학 전반에 걸쳐서 Stochastic Process가 들어가는데 개인적으로는 이거 처음에 개념잡기가 힘들었다.. 왜냐하면 확률을 측도론(Measure Theory)에서 접근을 해야 하니 측도론을 조금은 알아야 하고(덧붙여 약간의 함수해석학…) Random process, Stopping Time… 등등… 이것들의 성질이 Ft-measurable이므로 Ft-measurable이라는 의미를 잘 알아야 할 필요성까지 있기 때문이다.</p>
<p>뭐 Ft-measurable, 좀더 분해해서 Ft라는 filtration 과 measurable이라는 성질을 논리적으로 완벽하게 정의하는데 드는 노력은 A4용지에 반정도 끄적이면 충분하다. 그러니 어디서 누가 물으면 답변하는 정도로는 그냥 암기해 버리면 그만이 아닐까 생각한다.</p>
<p>그러나 정의를 주저리주저리 외우고 다닌다고 하는 것은 그 대상을 아는 것은 아니다.… 애매한 문제에 엄밀한 판결을 내릴 수 있도록 능력이 되려면 수학적 정의가 포함하는 세계가 머리속에 익숙해 져야 한다고 생각한다.</p>
<p>어째든지 Random process이던 Stopping Time 이건, Girsanov이건간에, 수학적인 내용이 나오면 적어도 머리속에는 간략화되고 가시적인 예제들이 Simulation되어야지 그 개념들이 ‘눈에’ 보이게 되는데 Filtration의 구체적인 모습들이 떠오르지 않으니까 나머지도 거기서 더 이상 이해가 되지 않았다. 이것이 본인이 Filtration을 한번 손봐야 겠다고 맘먹은 직접적인 동기이다.</p>
<div id="random-process를-이야기-하기까지의-긴-여정의-요약" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Random Process를 이야기 하기까지의 긴 여정의 요약<a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>확률공간 (Ω, F, P)의 정의에 대해서 이야기하자면 많은 사람들이 이미 알겠지만 측도론(Measure Theory)에 대한 고된 과정을 거쳐야지만 비로서 이야기할 수 있게 된다. Measure Theory는 미적분 함수해석학 등을 최신(?)관점에서 접근하도록 만들어진 이론체계인데 실해석학(Real Analysis)이라는 과목에서 주로 다루게 된다. 여기에서 Measure라는 개념 – 우리말로 하면 측도 라는 것은 말 그대로 측정한다는 것인데 이런 류로 쉽게 생각할 수 있는 것은 ‘길이’ 이다…. 혹은 넓이, 부피, 기타 등등. 여기에 쓰이는 측도의 개념을 확률에다가 접근 시킨 사람이 콜모고로프라는 라는 것은 이미 널리 알려진 바이다. 길이, 확률의 기초적 개념은 초딩때 이미 섭렵했는데 굳이 어렵게 이해할 필요가 있는가 할 수도 있겠지만 아무래도 무한대이고 연속의 세계에서는 별의별 해괴한 일이 벌어지기 때문에 유치한(유치하다는 것은 경멸적인 이야기가 아니라 덜 세련된… 정도로 생각을 해주면 된다.) 차원의 접근방법은 더 이상 통하지 않고, 집합론, 해석학의 고등 주제들을 총 동원해야 모호한 문제에 비로소 해답을 내릴 수 있다. 그러니 해석학을 하기 위해서는 집합론을 먼저 알아야 하는데 이것도 중고딩때 하던 겉핥기 식의 집합론이 아닌 무한차원을 다룬 진검승부의 집합론이 필요하다. 하여튼지 간에 이렇게 금융수학을 공부하기 위해서 선행적으로 해야 할 아주 높은 산들이 산적해 있는데.. 이걸 다 언급하는 것은 이 글의 주제를 넘어서는 것 같구… 차후에 ’금융수학을 공부하기 위한 로드맵’에 좀더 자세히 언급하겠다. 여기서 이야기 하고 싶은 것은 <strong>확률과정</strong>(Random Process)를 실해석학 위에서 가지고 노는 것은 나름대로 다 이유가 있다는 말을 하고 싶은 것이다. 기존의 유치한 확률론으로 접근해서 나름대로 이론을 쌓을 수 있다면 아주 happy한 case이지만 그렇다 하더라도 표준적으로 쓰여있는 paper나 text가 마팅게일이니, Girsanov니.. 이런 식으로 쓰여져 있으니 뭐라고 하는지 이해는 할 수 있어야 할 것 아닌가, 더욱이 수학이 어렵다고 하는데 솔직히 공부하는 것은 아주 어려운 것 맞다. 하지만 어려운 것은 익숙지 않다는 것뿐이다. 익숙해지면 세상이 참 쉽게 보인다. 복잡한 것은 수학이 아니라 세상이다. 공부하기 어려운 수학에 익숙해지면 복잡하게만 보였던 세상이 수학에 의해서 간략하게 보이게 된다. 세상 만만하게 보인다는 이야기이다. 어렵게 공부한 사람만 볼 수 있으니 개인적 보람도 아주 크다.</p>
<div id="sigma-algebrasigma-field" class="section level4 hasAnchor" number="9.6.1.1">
<h4><span class="header-section-number">9.6.1.1</span> Sigma-Algebra(Sigma-Field)<a href="filtration의-개념을-정복하자.html#sigma-algebrasigma-field" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sigma-Algebra 라는 것은 measure를 다루기 위한 기본 개념인데 Sigma-Field와 같은 개념이다. 확률을 다루는 바닥에서는 Sigma-algebra 대신에 Sigma-Field라는 말을 쓴다. 참고로 Algebra, Field 라는 것은 Abstract Algebra(추상대수학)이라는 수학의 큰 줄기와 그 안의 체(Field)를 연상시키는데 연결고리에 대해서 약간의 심증은 가지고 있지만 물증은 찾지 못했다. 하여튼 간에 집합 A 위의 Sigma-Algebra(본인은 이 용어를 쓰겠다)의 정의는 다음의 말이 되는 A의 부분집합 E 들의 모임이다.
1. 공집합과 A는 Sigma-Algebra에 속한다.
2. E가 Sigma-Algebra에 속하면 E의 여집합도 Sigma-Algebra에 속한다.
3. E1과 E2가 Sigma-Algebra에 속하면 E1과 E2의 합집합도 Sigma-Algebra에 속한다.</p>
<p>뭐 여기까지는 상식적으로 이해가 된다.</p>
<ol start="4" style="list-style-type: decimal">
<li>Ei(I는 집합의 첨수족이고 자연수N의 원소이다) 가 Sigma-Algebra에 속하면 모든 Ei 의 합집합도 Sigma-Algebra에 속한다.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>과 (4) 는 같은 이야기 인 것이 아닌가? 하는 의문이 있을 수 있겠다. 즉 (3)을 반복적으로 적용하면 결국 (4)가 아닌가 하는 생각이 바로 그것이다. 답은 “같지 않다”이다. 이것이 다르다는 것을 이해하는데 에는 집합론을 깊이 아는 것이 필요하다. 뭐 그것은 그러려니 하자…. 그런데 문제는 이것뿐만이 아니다. (4) 의 내용을 이렇게 바꾸어 말할 수 있는가?</li>
</ol>
<p>「Sigma-Algebra에 속하는 모든 E들을 무한히 합집합을 취한 것도 Sigma-Algebra에 속한다」</p>
<p>답의 yes, no 차원을 떠나서 위의 문제는 참으로 무책임한 질문이다. 왜냐하면 <strong>무한히</strong> 라는 것도 모호한 말이기 때문이다. 정확히는 <strong>가부번수만큼</strong> 합집합을 취한 것이 Sigma-algebra에 속한다. 가부번수라는 것은 자연수의 개수와 같으므로 위의 (4) 라는 표현을 쓴 것이다. 무한에는 가부번수라는 것도 있고 비가부번수가 있다. 짝수, 홀수, 자연수, 유리수등등 이것이 다 같은 개수를 가진다고 생각을 하고 그것을 가부번수라고 한다. 그리고 실수 무리수 같은 수는 비가부번 수이다. 그러므로 <strong>(4) 에서 I 가 N 의 집합이 아닌 실수R의 집합이나. 무리수Qc 의 집합이라고 적어 놓으면 더 이상 Sigma-algebra가 아닌 다른 무엇이 되고 마는 것이다</strong>.
참고로 (4)의 조건이 없어지면 Sigma-algebra가 아닌 Algebra가 된다. 참고로 컴퓨터 이론의 기초를 이루는 2진수 연산인 <strong>Bool algebra</strong>(불대수)도 따지고 보면 여기서 말하는 algebra이다. 그리고 (3) 의 조건을 비가부번수까지 포함해서의 합집합에 대해서 라는 조건으로 바꾸어주고 유한개의 교집합에 대해서 닫혀 있으면 <strong>위상(Topology)</strong>가 된다. 이렇듯 쉽게 보이는 것도 엄밀하게 따지지 않으면 안 되는 이유가 바로 이것이다.</p>
<p>무한이고 연속인 것을 머리 속에서 엄밀한 Simulation을 할 수가 없는데 이럴 경우 쉽게 볼 수 있는 간단한 discrete case를 가지고 머리 속에서 가지고 놀면 된다. 대상을 간단하게 한다고 하지만 그 대상에 대해서 무한과 연속을 다루는 규칙을 똑같이 적용하면 적어도 오류에 다다르지는 않는다. 그리고 때때로 그 결과는 그대로 연속일 때로 확장시켜 생각해도 된다. 물리학에서도 discrete로 모델을 만들고 모델을 미소변화량에 대한 연속모델로 만드는 것과 비슷하다. 게다가 <strong>Caratheodory Extension</strong>와 <strong>Pi-System</strong> 같은 도구들은 이론적으로 그러한 것이 타당하다는 보장까지 하니… 맘놓고 생각해도 된다. Caratheodory Extension 이니 Pi system이니 하는 것은 일단은 몰라도 된다. 이제 Sigma-Algebra에 대해서 조금 더 생각해 보자.</p>
<p>A={w1,w2, w3}</p>
<p>인 A의 <strong>(2개의 원소를 가진)</strong> Sigma Algebra <span class="math inline">\(F\)</span>들을 구해보면</p>
<p>F1={ Ø, A}</p>
<p>위 F1은 A위에서 Sigma Algebra의 1,2,3 을 만족시킨다. <strong>유한집합이므로 3을 만족시키는 것은 4도 만족시킨다</strong>. 하지만 <strong>이것은 무한집합에서는 일반적인 것은 아니다</strong>.</p>
<p>F2={ Ø, {w1}, {w2,w3}, {w1,w2, w3}}</p>
<p>F3={ Ø, {w1}, {w2}, {w3}, {w1, w2}, {w2, w3}, {w1, w3}, {w1, w2, w3}}</p>
<p>어째든지 F1, F2, F3 셋 다 A위의 Sigma – Algebra이다. 그리고 F1⊆F2⊆F3이다.</p>
<p>F1, F2, F3, 중에서 {w2, w3}을 포함하는 Sigma-Algebra 는 F2, F3이다. 그런데 F2와 F3중에서 크기가 작은 것은 F2이다. 즉 <strong>F2는 {w2, w3}을 포함하는 가장 작은 Sigma-Algebra</strong>이다. 이것은 {w2, w3}이 생성(generate)하는 Sigma-Algebra이다. 기호로는 Sigma({w2,w3}) 이다. 또한 Sigma({w1, w3})=F3이다. 그리고 짐작을 했겠지만 A의 모든 부분집합의 모임인 <strong>P(A)가 가장 큰 Sigma Field이다</strong>.</p>
<p>어떤 집합(A)의 모든 열린집합들이 Generate하는 Sigma algebra를 <strong>Borel Field</strong> 라고 하고 B(A) 라고 쓰고 그 원소를 <strong>Borel Set</strong>이라고 한다. 어떤 집합이 실수의 집합이면 B(A)는 실수의 모든 열린집합들이 Generate하는 Sigma algebra 가 된다. 이딴거 어따쓰나 생각할지 모르겠지만 이것이 바로 <strong>‘길이’</strong> 라고 부를 수 있는 대상들의 모임인 것이다. 길이measure도 어렵게 말하면 lebegue measure라고 한다.</p>
<p>머리에서 김 나겠지만 좀더 생각해 보자. **수직선 위의 어느 한 점으로 이뤄진 집합도, 그러한 점들이 자연수개수만큼씩 있는 집합도 다 __Borel set__이므로 길이를 생각할 수 있다<strong>. 그 길이는 무엇일까? 답은 0 이다. 0과 1 사이의 모든 유리수 집합도 그 길이는 0이다. 가부번 집합의 길이는 0이 된다. </strong>연속적인 실수의 구간은 역시 Borel set<strong>이고 </strong>양끝의 값의 차이가 당근 길이이다<strong>. 그리고 그 구간에서 가부번 집합을 뺀 – 예를 들어 실수에서 유리수를 뺀 무리수의– 길이도 양끝 길이의 차이와 같다 왜냐하면 가부번 집합의 길이가 0이기 때문이다. </strong>무리수구간의 길이<strong>는 같은 끝점을 가진 실수 구간의 길이에서 유리수 길이를 빼야 하는데 </strong>그 값, 즉 유리수의 길이,**이 0이므로 실수 구간의 길이와 같게 된다.</p>
<p><strong>가부번 집합</strong>이 아닌 경우에 반드시 길이가 있는가? 답은 No 이다. 칸토르 집합의 경우 <strong>비가부번 집합</strong>이지만 전체 길이는 0 인 황당한 case 가 있기는 있다.</p>
<p>가부번, 비가부번 집합이란 둘 다 무한집합(원소의 개수가 무한개인 집합)을 의미한다.</p>
<p>참고로 측도가 0인 집합을 영집합(Null Set)이라고 하는데 해석학이건 확률론이건 확률미분방정식(Stochastic Differential Equation) 등에서 의외로 중요한 개념이므로 반드시 한번 더 보고 가자.</p>
<p>또한 실수에서 길이를 생각할 때 <strong><strong>왜</strong> 실수집합의 <strong>모든 부분집합</strong> 위에서 길이를 생각하지 않고 꼭 Borel Set위에서 정의를 하냐</strong>고 생각할 수 있겠는데… 이것은 아마도 실수의 부분집합들 중에 길이의 대상이 되기에 부적합한 부분집합들이 존재하기 때문이라고 생각된다.</p>
<p>솔직하게 이쯤 되면 이제 머리 속에서 속속들이 Simulation을 하는 것은 포기해야 한다. 어떤 집합은 (보통) 무한집합인데 무한집합의 Sigma-algebra를 적는 것은 사실상 불가능 할뿐더러 열린집합들의 모임이라는 것도 그리 생각하는 것이 쉽지 않다. 그러니 그것이 generate하는 것을 머리에 어떻게 떠올린다는 것인가? 수학도들은 숱한 반복적인 증명연습과 연습문제 풀이로 장님 코끼리 만지듯이 <strong>감각을 키워나가지만</strong> 문제는 시간이다… 하루아침에 되는 것이 아니기 때문이다.</p>
<p>그렇지만 이것만 우선 먼저. 측도라는 것은 측정에 관한 이야기이다. 길이도 가장 간단한 측도이다. 길이의 대상이 되는 것들을 생각해 보자. 각각의 대상을 합해 놓아도 길이의 대상이 된다. 뿐만 아니라 <strong>두 측도 값의 합이 바로 합집합의 측도값이 된다. Sigma–Algebra의 특성인 ’합집합에 대해서 닫혀 있음’이 바로 이것을 의미한다. </strong></p>
<p>위의 A집합에 대해서 F2를 생각해 보자.</p>
<p>F2={ Ø, {w1}, {w2,w3}, {w1,w2, w3}}</p>
<p>Ø → 0
{w1} → 0.4
{w2, w3} → 0.6
{w1,w2,w3} → 1</p>
<p>이렇게 Sigma Algebra의 각 값에 대해서 측도 값을 부여하였다. <strong>물리적인 의미는 없고</strong>, <strong>측도의 논리</strong>에 틀리지 않게 구성이 되어 있다. 한번 보자 “어떠한 원소들의 여집합이 Sigma Algebra의 원소이다.”
확률의 경우를 생각해 보면 <strong>어떤 사건이 일어난 확률이 <strong>있다면</strong> 그것이 일어나지 않을 확률이 반드시 <strong>있다</strong></strong>는 의미와 같다.</p>
<p>있다면 = <span class="math inline">\(\sigma\)</span>-field에 속한다면
있다 = <span class="math inline">\(\sigma\)</span>-field에 속한다</p>
<p>“어떠한 원소들의 합집합이 Sigma Algebra의 원소이다.”</p>
<p>또한 <strong>두 독립적 사건의 확률을 각각 계산할 수 있다면 반드시 두 사건 중 하나가 일어날 확률도 계산할 수 있다</strong>는 것을 의미한다.
또한 <strong>두 독립적 사건의 동시에 일어날 확률(합집합)은 각 확률(measure)의 합</strong>이다.</p>
<p>이러한 생각으로 위 측도값들을 보면 전혀 모순이 없다는 것을 알 수 있다. 물론 이러한 측도값은 하나만 있는 것이 아니다.</p>
<p>Ø → 0
{w1} → 0.2
{w2, w3} → 0.8
{w1,w2,w3} → 1</p>
<p>이렇게 해도 부여한 측도값에는 전혀 문제 없다.</p>
<p><mark>
<strong>그런데 {w2}에 대해서 측도값(확률)을 부여할 수 있는가?</strong> 예를 들어 {w2}라는 사건이 존재한다면 뭐 억지로 어떤 값을 부여 했다고 하자. 그러면 {w2}가 일어나지 않는 경우에 대한 확률을 구할 수 있을까? 게다가 {w1} 또는 {w2}가 일어날 사건도 생각할 수 있는가? 대답할 수 없을 것이다. <strong>이러한 예는 왜 확률을 생각할 때 Sigma Algebra를 생각해야 하는지를 설명해 준다.</strong>
</mark></p>
<p>즉, {2}는 가측(measurable)인가?</p>
</div>
</div>
<div id="ft-measurable" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Ft-measurable<a href="filtration의-개념을-정복하자.html#ft-measurable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>위의 내용은 초보자에게는 접근할 수 있는 흥미를, 이미 내용을 알고 있는 사람들에게는 일종의 쉬어가는 페이지가 되었을 것 같다. 이제는 좀더 엄밀하게 접근을 하려고 한다.</p>
<p>Ft-measurable 이라는 것은 Ft라는 Filtration에 measurable 하다는 이야기이다. <strong>Filtration</strong>은 Sample Space Ω 의 Sigma Algebra 들의 모임인데 Ft1⊆Ft2⊆Ft3 (t1&lt;t2&lt;t3) 로 되어 있어서 시간이 지날수록 점점 Sigma Algebra들이 이전 <span class="math inline">\(\sigma\)</span>-Algebra를 포함하면서 커지도록 되어 있다. 이 절의 내용은 바로 이 Filtration의 몇 가지 가시적인 예제를 보여줄 것이고 그것이 엄밀한 수학적 정의와 어떻게 연결되는지를 따져 볼 것이다. 그렇게 해서 얻어진 가시적인 예제들은 장차 Stochastic Process에서 나오는 여러 가지 수학적인 개념들을 머리에서 Simulation 해서 각자의 이해의 영역으로 확보하는 과정에서 크나큰 역할을 하기만을 바랄 뿐이다.</p>
<div id="가측measurable이란" class="section level4 hasAnchor" number="9.6.2.1">
<h4><span class="header-section-number">9.6.2.1</span> 가측(measurable)이란<a href="filtration의-개념을-정복하자.html#가측measurable이란" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>먼저 measurable의 정의를 보자. Measurable의 의미를 따지기 전에 우선 임의의 하나의 함수를 생각해 보자. 그러한 함수는 집합A의 원소에서 실수로 간다고 하자. $f:A R</p>
<p><pics></p>
<p>위와 같은 예가 될 수 있겠다.</p>
<p><mark>
그 다음은 역상에 대해서 생각해 보자. (역상도 집합이다.) A의 부분집합 Ω 위로의 역상은 <span class="math inline">\(f^{-1} (E) = \{ x | x \in \Omega, \; f(x) \in E \subseteq B(R)\}\)</span> 으로 정의된다.</p>
<p><span class="math inline">\(f\)</span>의 정의역은 <span class="math inline">\(A\)</span>, 치역은 <span class="math inline">\(E\)</span>. 단, E는 보렐집합 (가측집합).
역함수, 역상과 <span class="math inline">\(\sigma\)</span>-algebra의 관계는?</p>
<p>조금만 생각해 보면 <span class="math inline">\(f^{-1}\)</span>는 결국 다음과 같은 하나의 함수이다.</p>
<p><span class="math display">\[
f^{-1} : B(R) \rightarrow \sigma
\]</span></p>
<p>Sigma(<span class="math inline">\(\sigma\)</span>)는 (Ω 위의 = <span class="math inline">\(\Omega\)</span>가 생성하는) Sigma algebra이다. 즉 정의구역은 Borel Set(<span class="math inline">\(B(R)\)</span>) 들이고 공변역(치역)은 Ω의 Sigma algebra이다.
</mark></p>
<p>E는 실수의 Borel set 의 원소이다. 모든 E를 다 여기서 명세할 수는 없지만 몇가지 경우로 분류할 수는 있다.
1. r1을 원소로 포함한 E들 — 이것들의 무리를 E1이라고 하자
2. r1를 원소로 포함하지 않는 E들 — 이것들의 무리를 E1’이라고 하자</p>
<p><span class="math display">\[
\begin{align}
E &amp;= E_1 \cup E_2
E_1 \cap E_2 &amp;= \emptyset
\end{align}
\]</span></p>
<p>E1에 속한 E의 경우 <span class="math inline">\(f^{-1}(E)\)</span> 는 Ω가 된다. <mark>Check it!</mark>
E1’에 속한 E의 경우 <span class="math inline">\(f^{-1}(E)\)</span> 는 Ø가 된다. <mark>Check it!</mark></p>
<p>이때 주의해야 할 점은 E의 원소로 r2, r3, r4, 가 있는 경우이다. 이 경우 이들에 대한 f의 역함수 값은 A에는 존재할 수도 있다. 하지만 Ω의 원소 중에는 그런 없기 때문에 Ø 되는 것이다. 다시 한번 정의를 살펴보자.
<span class="math inline">\(f^{-1} (E) = \{ x | x \in \Omega, \; f(x) \in E \subseteq B(R)\}\)</span> 이다. x는 Ω의 원소이어야만 한다. 그런 원소가 아니면 공집합이 된다.
Ω 와 Ø 의 모임인 {Ω , Ø} 는 Ω 위의 Sigma algebra 이다. 특별히 Ω 가 Generate하는 Sigma algebra 라고 한번 생각해 주고 넘어가자.</p>
<p>이제 measurable의 정의를 보자.</p>
<p>함수 f: Ω → R이 F – measurable 이면, <span class="math inline">\(f^{-1} (E) = \{ x | x \in \Omega, \; f(x) \in E \subseteq B(R)\}\)</span> 가 Ω 위에서 Sigma Algebra F의 원소이다.</p>
<p>그런데 어떤 책에서는</p>
<p>함수 f: Ω → R이 F – measurable 이면 <span class="math inline">\(f^{-1} (E) = \{ x | x \in \Omega, \; f(x) \in E \subseteq B(R)\}\)</span> 가 Ω 위에서 Sigma Algebra 를 이룬다.</p>
<p>라고 해서 사람을 헷갈리게 만든다. 이게 왜 사람 헷갈리게 만드는가 하면 F 안의 원소인 e1의 상이 B(R) 의 원소가 아닐 경우 위의 두 개의 정의는 동치가 되기 때문이다.
아직 이 문제는 개인적으로는 해결이 안되었고 학습에는 그다지 큰 문제는 아니므로 첫 번째 정의를 받아들이자.</p>
</div>
<div id="filtration" class="section level4 hasAnchor" number="9.6.2.2">
<h4><span class="header-section-number">9.6.2.2</span> Filtration<a href="filtration의-개념을-정복하자.html#filtration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="t-0-일때-ft" class="section level5 hasAnchor" number="9.6.2.2.1">
<h5><span class="header-section-number">9.6.2.2.1</span> t =0 일때 Ft<a href="filtration의-개념을-정복하자.html#t-0-일때-ft" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Ω={w1, w2, w3, w4} 라고 하자.</p>
<p><pics></p>
<p>B(R) 들의 원소 들 E 들은 r1 을 원소로 같는 E 들의 무리 E1 이 있고 그렇지 못한 무리 E2 가 있을 수 있다.
앞의 경우에서 해설한 바 몇 가지 점을 유의하면 다음과 같은 결과를 얻는다.</p>
<ol style="list-style-type: decimal">
<li>무리 E1 들의 역상은 Ω 이다.</li>
<li>무리 E2 들의 역상은 Ø 이다.</li>
<li>따라서 B(R)의 모든 역상들의 모임은 {Ω, Ø} 이다.</li>
<li>Ft(t=0) = F0 을 {Ω, Ø}이라고 하면 함수 f 는 위 정의에 의하여 F0 measurable이다.</li>
<li>결과적으로 보니까. F0는 partition{Ω, Ø} 이 generate하는 Sigma Algebra로 볼 수 있다.</li>
</ol>
</div>
<div id="t-1-일-때-ft" class="section level5 hasAnchor" number="9.6.2.2.2">
<h5><span class="header-section-number">9.6.2.2.2</span> t =1 일 때 Ft<a href="filtration의-개념을-정복하자.html#t-1-일-때-ft" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><pics></p>
<p>앞에서와 같이 B(R)을 다음과 같이 나눌 수 있다. 그리고 앞에서 한 분석을 다시 해보면 다음과 같은 결과가 나온다.</p>
<table>
<thead>
<tr class="header">
<th align="center">E의 그룹</th>
<th align="center">그룹의 성격</th>
<th align="center">그룹내 모든 E들의 역상</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">E1</td>
<td align="center">r1 을 원소로 갖는 E들</td>
<td align="center">{w1}</td>
</tr>
<tr class="even">
<td align="center">E2</td>
<td align="center">r2 을 원소로 갖는 E들</td>
<td align="center">{w2, w3, w4}</td>
</tr>
<tr class="odd">
<td align="center">E12</td>
<td align="center">r1과 r2 모두를 원소로 갖는 E들</td>
<td align="center">{w1, w2, w3, w4}</td>
</tr>
<tr class="even">
<td align="center">En</td>
<td align="center">r1과 r2 모두를 원소로 갖지 않는 E들</td>
<td align="center">Ø</td>
</tr>
</tbody>
</table>
<p>마지막 열의 집합들은 partition{{w1}, {w2, w3, w4}} 이 generate하는 F1 내에 속하게 된다.</p>
</div>
<div id="t-2-일때-ft" class="section level5 hasAnchor" number="9.6.2.2.3">
<h5><span class="header-section-number">9.6.2.2.3</span> t =2 일때 Ft<a href="filtration의-개념을-정복하자.html#t-2-일때-ft" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>앞의 분석을 여기서 다시 하면 다음과 같은 결과를 얻을 수 있다.</p>
<table>
<thead>
<tr class="header">
<th align="center">E의 그룹</th>
<th align="center">그룹의 성격</th>
<th align="center">그룹내 모든 E 들의 역상</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">E1</td>
<td align="center">r1 을 원소로 갖는 E들</td>
<td align="center">{w1}</td>
</tr>
<tr class="even">
<td align="center">E2</td>
<td align="center">r2 을 원소로 갖는 E들</td>
<td align="center">{w2}</td>
</tr>
<tr class="odd">
<td align="center">E3</td>
<td align="center">r3 을 원소로 갖는 E들</td>
<td align="center">{w3, w4}</td>
</tr>
<tr class="even">
<td align="center">E12</td>
<td align="center">r1과 r2를 동시에 원소로 갖는 E들</td>
<td align="center">{w1, w2}</td>
</tr>
<tr class="odd">
<td align="center">E13</td>
<td align="center">r1과 r3를 동시에 원소로 갖는 E들</td>
<td align="center">{w1, w3, w4}</td>
</tr>
<tr class="even">
<td align="center">E23</td>
<td align="center">r2과 r3를 동시에 원소로 갖는 E들</td>
<td align="center">{w2, w3, w4}</td>
</tr>
<tr class="odd">
<td align="center">E123</td>
<td align="center">r1, r2, r3를 동시에 원소로 갖는 E</td>
<td align="center">{w1, w2, w3, w4}</td>
</tr>
<tr class="even">
<td align="center">En</td>
<td align="center">r1, r2, r3를 원소로 갖지 않는 E</td>
<td align="center">Ø</td>
</tr>
</tbody>
</table>
<p>확인해보면 마지막 컬럼의 집합들은 {{1},{2},{3, 4} }이 Generate하는 Sigma algebra의 원소들이다.</p>
<p>이제 어느 정도 요령이 생기고 규칙 같은 것들이 생길 것이다. 쉽게 말하면 Sample space를 partition 해서 Sigma Algebra Ft들을 generate하는 것이다. <mark>t가 증가함에 따라 partition을 좀더 세분화 하면 Ft1⊆Ft2⊆Ft3… 이런식으로 Filtration 을 만들 수 있다.</mark></p>
</div>
</div>
<div id="좀더-현실적인-예" class="section level4 hasAnchor" number="9.6.2.3">
<h4><span class="header-section-number">9.6.2.3</span> 좀더 현실적인 예<a href="filtration의-개념을-정복하자.html#좀더-현실적인-예" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>좀더 많이 볼 수 있는 Random process 에 적합한 예를 만들어 보자. 동전을 던져서 앞이 나오면 1 뒤가 나오면 0을 점수에 더하는 것으로 하자. 점수는 random process(라고 보아도 되는 것)이다. 동전던지기는 (최대) 3회까지 하는 것으로 하자. Sample space Ω 는 다음과 같이 구성할 수 있다.</p>
<p><pics></p>
<p>동전던지기 첫회 시행에 위, 그 다음 시행에 아래, 그 다음 시행에 위가 나오는 경우는 Wudu 이라고 보면 된다. T=0 시점은 동전던지기를 시행하기 전이다.</p>
<p>각 동전던지기 수행 시점에서 Random process인 점수를 다음 테이블에 표시하였다.</p>
<p><pics></p>
<p>앞의 예제들로 요령이 생겼으니 각각의 시점에서 해당 Sigma – algebra를 얻을 수 있을 것이다. T=0 일 경우에는 { Ø, Ω } 가 generate하는 Sigma-algebra이고 t=1 인 경우에는 Ω 를 1인 값과 0인 값으로 partition을 하고 그것으로부터 Sigma-algebra를 생성(generate)할 수 있다.</p>
<p>각각에 대해서 증대하는 Sigma – algebra인 filtration을 볼 수 있으면 random process가 Ft에 measurable이라는 것이 어떠한 것인지 적어도 논리적으로 알 수 있을 것이다.</p>
</div>
<div id="정보-집합으로서의-filtration" class="section level4 hasAnchor" number="9.6.2.4">
<h4><span class="header-section-number">9.6.2.4</span> 정보 집합으로서의 Filtration<a href="filtration의-개념을-정복하자.html#정보-집합으로서의-filtration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Stochastic process에서 나오는 이론적인 case에서는 위의 예제를 머리 속에서 굴려보면 충분히 이해가 될 듯 싶다. 그런데 간간히 들리는 이야기는 Filtration을 시장을 움직이는 정보들의 모임이라고 보는 견해가 있다. 이것을 알면 증대해 나가는 Sigma algebra에 대한 색다른 관점을 가지게 되는 것이고 또한 금융수학이 목적이면 우리가 사는 세상과 이 수학적 모델이 어떤 연결고리를 가지고 있는 것인지를 알아볼 수 있는 기회가 된다고 생각된다.</p>
<p>3개의 현실세계에서 주식가격을 움직이는 사건이 있다고 치자.</p>
<p>A. 2000년 4월 금리인하조치 발표 – 발생될 경우 주식가격 10% 상승
B. 2001년 9월 11일 뉴욕 무역센터 건물 붕괴 – 발생될 경우 주식가격 30% 하락
C. 2002년 3월 예상을 웃도는 회계실적 발표 – 발생될 경우 주식가격 15% 상승</p>
<p>일어난다면 대문자 일어나지 않는다면 소문자로 표기하면 WABc는 A는 일어나고 B도 일어나고 c는 일어나지 않는다는 case이다.</p>
<p>위의 세 사건말고는 절대 주식가격이 움직이지 않는다고 가정한다.</p>
<p><pics></p>
<ol style="list-style-type: decimal">
<li>1999.12 에는 Sigma Algebra가 { Ø, Ω } 이었다. 모든 사건들의 case는 아직 일어나지 않았다.</li>
<li>2000년 4월에 Sigma Algebra는 { {WABC, WABc, WAbC, WAbc}, {WaBC, WaBc, WabC, Wabc}}이라는 partition이 Generate하는 것이다. W의 첨자 중에서 첫번째 첨자가 가격을 결정한 것이다. 100원이 될지 110원이 될지는 첫 번째 첨자에 해당하는 사건의 정보가 나타났기 때문이다. 다시 말하면 2000년 4월에 100원 또는 110원을 결정하는 것은 2000년 4월에 일어난 ’사건’이다. 이런 식으로 점점 잘게 partition 을 쪼개어 나가고 그것이 generate하는 것이 Filtration 이고 정보가 점점 증가된다.</li>
<li>위의 모델을 잘 살펴보면 특정시점의 주식가격의 결정은 그때까지의 모든 발생된 정보에 의해서 결정된다. 2001년 9월 직후의 주식가격은 첨자A와 첨자 B에 해당하는 사건발생여부의 ’정보’가 가격을 결정한다.</li>
</ol>
</div>
<div id="natural-filtration" class="section level4 hasAnchor" number="9.6.2.5">
<h4><span class="header-section-number">9.6.2.5</span> Natural Filtration<a href="filtration의-개념을-정복하자.html#natural-filtration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>실제로는 주식가격을 결정하는 정보는 무한이고 그에 따른 주식가격의 변화도 연속적으로 바뀐다. 하지만 모든 정보의 결과가 주식가격으로 나타나니까. t시점의 정보의 변화는 t시점의 가격으로 반영이 된다. Sigma algebra가 어떤 partition에 의해 generate되는 것이라면 그 partition들을 이루는 집합들은 마치 t시점의 주식가격을 index처럼 가지고 있는 것처럼 만들어야 한다. 다시 말하면 partition내의 집합마다 각각 다른 St값을 가지고 있어야 한다. 이를테면 { …{St=99, …}, {St=100…}, {St=101…}, …} 이런 식으로 partition을 하면 된다.</li>
<li>t에서 dt만큼 지난 t’ 시점에서는 위의 각각의 partition 들을 다시 각각 쪼갠다. 쪼개는 방법은 각각의 partition의 집합들이 St와 St’ 2개의 index로 가지도록 하면 된다. 이렇게… {…{St=99, St’=99…}, {St=99, St’=100…}, {St=99, St’=101},… {St=100, St’=99…}, {St=100, St’=100…}, {St=100, St’=101},… ,{St=101, St’=99…}, {St=100, St’=100…}, {St=99, St’=101}…} 이런 식으로 말이다.</li>
<li>위의 식을 간략하게 말해보자. 첫 번째 t에서의 Partition(또는 책에 따라서는 Decomposition이라고 하기도 하더구먼)을 D(St)라고 표현해본다. St가 generate 하는 Partition이라고 생각하면 된다. 구체적인 방법은 위에 설명했다. T’에서는 어떻게 될까? D(St, St’) 두 개의 주식가격(또는 Random variable)에 의해서 generate되는 partition인 것이다.</li>
<li>일반적으로 보면 D(S0, S1, S2,….St)가 된다. T시점의 Sigma algebra Ft는 D(S0, S1,S2, …St)가 generate하는 Sigma-algebra이다. 이것을 σ(S0, S1, S2…St)라고 표시한다.</li>
</ol>
<p>마지막 절을 다룬 것은 몇몇의 다른 책에 위와 같은 notation이 있기 때문이다.</p>
</div>
</div>
<div id="epilogue" class="section level3 hasAnchor" number="9.6.3">
<h3><span class="header-section-number">9.6.3</span> EPILOGUE<a href="filtration의-개념을-정복하자.html#epilogue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>하여튼지 수학책에는 St까지의 Random process가 generate하는 partition이니 Decomposition이니 또 그것이 Generate하는 Sigma-algebra니… 이러니까. 논리적으로는 깔끔해 보이지만 그게 눈으로 들어오기까지는 참으로 죽을 맛이더라. 하긴 이렇게 깔끔한 표현이 도움이 되긴 한다. 앞에서도 말했지만 앞에서 13페이지가 넘게 주저리주저리 써 놓은 것을 수학책에서는 몇 줄로 표현을 하니까.. 깔끔스럽기는 이보다 더하지는 못할 것이다. 많이 생각한 수학의 정의는 대부분 암기하게 되는데… 그게 외우려고 외워지는 것이 아니다. 애매한 예들을 적용하려고 Definition을 수십 번 들춰보게 되는데 그러면 자연스럽게 토씨 하나까지 암기 하게 된다. 이것은 토씨 하나에 따라 정말 많은 것이 좌우되기 때문인데 이때마다. 수학적 정의가 얼마나 정교하게 만들어진 것인지를 새삼 깨닫게 된다. 그렇다 하더라도 개인적으로는 수학자들에게 불만이 많다. 깔끔하게 써 놓은 것도 이해는 가지만, 좀 이해하기 좋게 주저리주저리 쓰는 개인교습교재를 함께 만들었으면 하는 바람 때문이다. 말이 나왔으니 말인데, 수학책들은 두께는 얇으면서도 내용은 많고 비싸기로 유명하다. 비싼 것은 대중적이지 않으니까 그렇다 쳐도, 내용이 많은 것은 그 많은 내용을 압축해서 썼기쓰여졌기 때문이다. 고딩 때 정석책도 그런 식으로 쓰면 아마 책 내용이 1/7로 줄을 것 같다. 반대로 중편소설책 같은 분량의 수학책도 고딩 때 정석책처럼 주저리주저리 설명을 하려면 아마도 백과사전 책의 몇 권이 될 것 같다. 실제로 수학책의 강의노트들은 두꺼운 파일로 정리해도 네댓 개는 나온다. 강의노트 없이 그리고 정식 수학과정을 밟지 않은 공돌이가 맨땅에 헤딩해보니까. 나오는 것은 머리에 피나는 일밖에 없더라. 방법은 그저 조금 더 생각해 보고 써보고 낮은 포복이라도 멈추지 않고 기어가는 길일 것이다. 그래도 이렇게 쓰는 일로 해서 다른 사람들이 좀더 빨리 이해하고 도움이 되면 그간의 고통이 보람이 될 것이라고 믿는다. 여기의 모든 예들은 본인이 직접 고안한 것이다. 뭐, 저작권을 이야기 하려는 것은 아니고 다만 공인된 내용이 아니라(그래도 곡학아세로 혹세무민하는 일은 결코 없다.)그래도 수학이니 엄밀성을 요하는 부분에 부족한 점이 있을 수 있다는 것을 말하려는 것이다. 해서 보다 고수님들이 이 부분을 지적해 주면 더할 나위 없이 고맙겠다. 또한 충분히 설명을 하려고 했으나, 생각지 못한 부분이 있을 수 있으니 보다가 모르는 부분이 있으면 같이 생각하는 것도 또한 쌍방의 유익이 될 것도 같다. 하나 알아둘 것은 본 내용은 Self-Contained이지 않다. 무슨 이야긴고 하면 Sigma-algebra이고 measure고 Random Process이고 생판 모르는 사람이 본 이 내용을 보고 예수가 장님 눈뜨게 하듯 번쩍 뜨일 것 같지는 않을 것 같다는 말이다. 적어도 한번은 수학책을 보고 고민해서 이 생각 저 생각쯤은 한번쯤 한 사람들에게 도움이 되지 않을까?</p>
<p>아직은 장마다. 주룩주룩 내리는 비속에서 스타벅스같은 데서 노트북이나 두드리는 한가함을 기대해보면서 이만 펜을 놓는다.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cox-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="concepts.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/212598_Filtration.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
