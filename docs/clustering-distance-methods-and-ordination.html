<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.9 Clustering, Distance Methods, and Ordination | Self-Study</title>
  <meta name="description" content="5.9 Clustering, Distance Methods, and Ordination | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.9 Clustering, Distance Methods, and Ordination | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="https://github.com/lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.9 Clustering, Distance Methods, and Ordination | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="discrimination-and-classification.html"/>
<link rel="next" href="linear.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="part"><span><b>I 20-02</b></span></li>
<li class="chapter" data-level="1" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>1</b> Categorical</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Data Type and Statistical Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian</a>
<ul>
<li class="chapter" data-level="2.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>2.1</b> Abstract</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>2.1.1</b> 변수의 독립성</a></li>
<li class="chapter" data-level="2.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>2.1.2</b> 교환가능성</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>2.2</b> Continual Aeassessment Method</a></li>
<li class="chapter" data-level="2.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>2.3</b> Horseshoe Prior</a></li>
</ul></li>
<li class="part"><span><b>II 21-01</b></span></li>
<li class="chapter" data-level="3" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>3</b> Mathematical Stats</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3.1</b> Inference</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>3.1.1</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="completeness.html"><a href="completeness.html"><i class="fa fa-check"></i><b>3.2</b> Completeness</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="completeness.html"><a href="completeness.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>3.2.1</b> 레만-쉐페 thm.</a></li>
<li class="chapter" data-level="3.2.2" data-path="completeness.html"><a href="completeness.html#rao-blackwell-thm.-1"><i class="fa fa-check"></i><b>3.2.2</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Test</a></li>
<li class="chapter" data-level="3.4" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>3.4</b> Power Fucntion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>3.4.1</b> Significance Probability (p-value)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>3.5</b> Optimal Testing Method</a></li>
<li class="chapter" data-level="3.6" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>3.6</b> Data Reduction</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>3.6.1</b> Sufficiency Principle</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>3.7</b> Borel Paradox</a></li>
<li class="chapter" data-level="3.8" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>3.8</b> Neyman–Pearson lemma</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>3.8.1</b> Overview</a></li>
<li class="chapter" data-level="3.8.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>3.8.2</b> Generalized LRT</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>3.9</b> 개념</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>4</b> MCMC</a>
<ul>
<li class="chapter" data-level="4.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Importance Sampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Independent Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>4.2</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>4.2.1</b> MH Algorithm</a></li>
<li class="chapter" data-level="4.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>4.2.2</b> Random Walk Chains (Most Widely Used)</a></li>
<li class="chapter" data-level="4.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>4.2.3</b> Basic Gibbs Sampler</a></li>
<li class="chapter" data-level="4.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>4.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>4.3</b> Advanced MCMC (wk08)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>4.3.1</b> 1. Data Augmentation</a></li>
<li class="chapter" data-level="4.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>4.3.2</b> 2. Hit-and-Run Algorithm</a></li>
<li class="chapter" data-level="4.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>4.3.3</b> 3. Metropolis-Adjusted Langevin Algorithm</a></li>
<li class="chapter" data-level="4.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>4.3.4</b> 4. Multiple-Try Metropolis Algorithm</a></li>
<li class="chapter" data-level="4.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>4.3.5</b> 5. Reversible Jump MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>4.4</b> Auxiliary Variable MCMC</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Multimodal Target Distribution</a></li>
<li class="chapter" data-level="4.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>4.4.3</b> Doubly-intractable Normalizing Constants</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>4.5.1</b> Simulator-Based Models</a></li>
<li class="chapter" data-level="4.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>4.5.2</b> ABCifying Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>4.5.3</b> ABC-MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>4.6</b> Hamiltonian Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.6.1</b> Introduction to Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>4.7</b> Population Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>4.7.1</b> Adaptive Direction Sampling</a></li>
<li class="chapter" data-level="4.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>4.7.2</b> Conjugate Gradient MC</a></li>
<li class="chapter" data-level="4.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>4.7.3</b> Parallel Tempering</a></li>
<li class="chapter" data-level="4.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>4.7.4</b> Evolutionary MC</a></li>
<li class="chapter" data-level="4.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>4.7.5</b> Sequential Parallel Tempering</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>4.8</b> Stochastic Approximation Monte Carlo</a></li>
<li class="chapter" data-level="4.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>4.9</b> Review</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>4.9.1</b> Wk01</a></li>
<li class="chapter" data-level="4.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>4.9.2</b> wk03</a></li>
<li class="chapter" data-level="4.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>4.9.3</b> wk04, 05</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>4.10</b> Else</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>4.10.1</b> Hw4. Rasch Model</a></li>
<li class="chapter" data-level="4.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>4.10.2</b> DA) Example: MVN</a></li>
<li class="chapter" data-level="4.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>4.10.3</b> Bayesian adaptive clinical trial with delayed outcomes</a></li>
<li class="chapter" data-level="4.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>4.10.4</b> NMAR의 종류</a></li>
<li class="chapter" data-level="4.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>4.10.5</b> wk10) Bayesian Model Selection</a></li>
<li class="chapter" data-level="4.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>4.10.6</b> Autologistic model</a></li>
<li class="chapter" data-level="4.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>4.10.7</b> wk10) Bayesian Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>5</b> MVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>5.1</b> Overview of mva (not ended)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>5.1.1</b> Notation</a></li>
<li class="chapter" data-level="5.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>5.1.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Inference on Correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>5.1.4</b> Standardization</a></li>
<li class="chapter" data-level="5.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>5.1.5</b> Missing Value Treatment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>5.2</b> Multivariate Nomral (wk2)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>5.2.1</b> Overview</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>5.2.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="5.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>5.2.3</b> Properties of MVN</a></li>
<li class="chapter" data-level="5.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>5.2.5</b> Linear Combination of Random Vectors</a></li>
<li class="chapter" data-level="5.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Normal Likelihood</a></li>
<li class="chapter" data-level="5.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>5.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span></a></li>
<li class="chapter" data-level="5.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>5.2.8</b> Assessing Normality</a></li>
<li class="chapter" data-level="5.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>5.2.9</b> Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>5.3</b> Inference about Mean Vector (wk3)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>5.3.2</b> 1. Confidence Region</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>5.3.3</b> 2. Simultaneous CI</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>5.3.4</b> 3. Note: Bonferroni Multiple Comparison</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>5.3.5</b> 4. Large Sample Inferences about a Mean Vector</a></li>
<li class="chapter" data-level="5.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>5.3.6</b> 1. Profile Analysis (wk4, 5)</a></li>
<li class="chapter" data-level="5.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>5.3.7</b> 2. Test for Linear Trend</a></li>
<li class="chapter" data-level="5.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>5.3.8</b> 3. Inferences about a Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>5.4</b> Comparison of Several MV Means (wk5)</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>5.4.1</b> Paired Comparison</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>5.4.2</b> Comparing Mean Vectors from Two Populations</a></li>
<li class="chapter" data-level="5.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>5.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)</a></li>
<li class="chapter" data-level="5.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>5.4.4</b> Comparing Several Multivariate Population Means</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>5.5</b> Multivariate Multiple Regression (wk6)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>5.5.1</b> Overview</a></li>
<li class="chapter" data-level="5.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>5.5.2</b> Multivariate Multiple Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>5.6</b> PCA</a></li>
<li class="chapter" data-level="5.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>5.7</b> Factor</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>5.7.1</b> Method of Estimation</a></li>
<li class="chapter" data-level="5.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>5.7.2</b> Factor Rotation</a></li>
<li class="chapter" data-level="5.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>5.7.3</b> Varimax Criterion</a></li>
<li class="chapter" data-level="5.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>5.7.4</b> Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>5.8</b> Discrimination and Classification</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>5.8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="5.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>5.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations</a></li>
<li class="chapter" data-level="5.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>5.8.3</b> Evaluating Classification Functions</a></li>
<li class="chapter" data-level="5.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>5.8.4</b> Classification with several Populations (wk13)</a></li>
<li class="chapter" data-level="5.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>5.8.5</b> Other Discriminant Analysis Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>5.9</b> Clustering, Distance Methods, and Ordination</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>5.9.1</b> Overview</a></li>
<li class="chapter" data-level="5.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.9.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>5.9.3</b> K-means Clustering</a></li>
<li class="chapter" data-level="5.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>5.9.4</b> 군집의 평가방법</a></li>
<li class="chapter" data-level="5.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>5.9.5</b> Clustering using Density Estimation (wk14)</a></li>
<li class="chapter" data-level="5.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>5.9.6</b> Multidimensional Scaling (MDS)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>6.1</b> SVD</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="svd.html"><a href="svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>6.1.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="6.1.2" data-path="svd.html"><a href="svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>6.1.2</b> Singular value Decomposition: General-version</a></li>
<li class="chapter" data-level="6.1.3" data-path="svd.html"><a href="svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>6.1.3</b> Singular value Decomposition: Another-version</a></li>
<li class="chapter" data-level="6.1.4" data-path="svd.html"><a href="svd.html#quadratic-forms"><i class="fa fa-check"></i><b>6.1.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="6.1.5" data-path="svd.html"><a href="svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>6.1.5</b> Partitioned Matrices</a></li>
<li class="chapter" data-level="6.1.6" data-path="svd.html"><a href="svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>6.1.6</b> Geometrical Aspects</a></li>
<li class="chapter" data-level="6.1.7" data-path="svd.html"><a href="svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>6.1.7</b> Column, Row and Null Space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="what.html"><a href="what.html"><i class="fa fa-check"></i><b>7.1</b> What</a></li>
<li class="chapter" data-level="7.2" data-path="random-vectors-and-matrices.html"><a href="random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>7.2</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="7.3" data-path="multivariate-normal-distributions.html"><a href="multivariate-normal-distributions.html"><i class="fa fa-check"></i><b>7.3</b> Multivariate Normal Distributions</a></li>
<li class="chapter" data-level="7.4" data-path="distributions-of-quadratic-forms.html"><a href="distributions-of-quadratic-forms.html"><i class="fa fa-check"></i><b>7.4</b> Distributions of Quadratic Forms</a></li>
<li class="chapter" data-level="7.5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>7.5</b> Estimation</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>7.5.1</b> Identifiability and Estimability</a></li>
<li class="chapter" data-level="7.5.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>7.5.2</b> Estimation: Least Squares</a></li>
<li class="chapter" data-level="7.5.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>7.5.3</b> Estimation: Best Linear Unbiased</a></li>
<li class="chapter" data-level="7.5.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>7.5.4</b> Estimation: Maximum Likelihood</a></li>
<li class="chapter" data-level="7.5.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>7.5.5</b> Estimation: Minimum Variance Unbiased</a></li>
<li class="chapter" data-level="7.5.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>7.5.6</b> Sampling Distributions of Estimates</a></li>
<li class="chapter" data-level="7.5.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>7.5.7</b> Generalized Least Squares(GLS)</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>7.6</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>7.6.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="7.6.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>7.6.2</b> More About Models</a></li>
<li class="chapter" data-level="7.6.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>7.6.3</b> Estimating and Testing Contrasts</a></li>
<li class="chapter" data-level="7.6.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.6.4</b> Cochran’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>7.7</b> Testing</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>7.7.1</b> More About Models: Two approaches for linear model</a></li>
<li class="chapter" data-level="7.7.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>7.7.2</b> Testing Models</a></li>
<li class="chapter" data-level="7.7.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>7.7.3</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="7.7.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>7.7.4</b> Testing Linear Parametric Functions</a></li>
<li class="chapter" data-level="7.7.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>7.7.5</b> Theoretical Complements</a></li>
<li class="chapter" data-level="7.7.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>7.7.6</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="7.7.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>7.7.7</b> Testing Single Degrees of Freedom in a Given Subspace</a></li>
<li class="chapter" data-level="7.7.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>7.7.8</b> Breaking SS into Independent Components</a></li>
<li class="chapter" data-level="7.7.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>7.7.9</b> General Theory</a></li>
<li class="chapter" data-level="7.7.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>7.7.10</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="7.7.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>7.7.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="7.7.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>7.7.12</b> Tests for Generalized Least Squares Models</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>7.8</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>7.8.1</b> A direct solution via inner products</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>7.9</b> Flat</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>7.9.1</b> 1.Flat</a></li>
<li class="chapter" data-level="7.9.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>7.9.2</b> 2. Solutions to systems of linear equations</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>7.10</b> Unified Approach to Balanced ANOVA Models</a></li>
</ul></li>
<li class="part"><span><b>III 21-02</b></span></li>
<li class="chapter" data-level="8" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>8</b> Survival Analysis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>8.2</b> </a></li>
<li class="chapter" data-level="8.3" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>8.3</b> </a></li>
<li class="chapter" data-level="8.4" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>8.4</b> </a></li>
<li class="chapter" data-level="8.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>8.5</b> Cox Regression</a></li>
<li class="chapter" data-level="8.6" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>8.6</b> Concepts</a></li>
</ul></li>
<li class="appendix"><span><b>00-00</b></span></li>
<li class="chapter" data-level="A" data-path="r-bookdown.html"><a href="r-bookdown.html"><i class="fa fa-check"></i><b>A</b> R Bookdown</a>
<ul>
<li class="chapter" data-level="A.1" data-path="tutorial.html"><a href="tutorial.html"><i class="fa fa-check"></i><b>A.1</b> Tutorial</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="tutorial.html"><a href="tutorial.html#about"><i class="fa fa-check"></i><b>A.1.1</b> About</a></li>
<li class="chapter" data-level="A.1.2" data-path="tutorial.html"><a href="tutorial.html#hello-bookdown"><i class="fa fa-check"></i><b>A.1.2</b> Hello bookdown</a></li>
<li class="chapter" data-level="A.1.3" data-path="tutorial.html"><a href="tutorial.html#cross-references"><i class="fa fa-check"></i><b>A.1.3</b> Cross-references</a></li>
<li class="chapter" data-level="A.1.4" data-path="tutorial.html"><a href="tutorial.html#parts"><i class="fa fa-check"></i><b>A.1.4</b> Parts</a></li>
<li class="chapter" data-level="A.1.5" data-path="tutorial.html"><a href="tutorial.html#footnotes-and-citations"><i class="fa fa-check"></i><b>A.1.5</b> Footnotes and citations</a></li>
<li class="chapter" data-level="A.1.6" data-path="tutorial.html"><a href="tutorial.html#blocks"><i class="fa fa-check"></i><b>A.1.6</b> Blocks</a></li>
<li class="chapter" data-level="A.1.7" data-path="tutorial.html"><a href="tutorial.html#sharing-your-book"><i class="fa fa-check"></i><b>A.1.7</b> Sharing your book</a></li>
<li class="chapter" data-level="" data-path="tutorial.html"><a href="tutorial.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="noname.html"><a href="noname.html"><i class="fa fa-check"></i><b>B</b> NoName</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering-distance-methods-and-ordination" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Clustering, Distance Methods, and Ordination</h2>
<div id="overview-5" class="section level3" number="5.9.1">
<h3><span class="header-section-number">5.9.1</span> Overview</h3>
<ul>
<li>Example: Customer Segmentation</li>
</ul>
<p><img src = "12-1.png"></p>
<hr />
<div id="clustering" class="section level5" number="5.9.1.0.1">
<h5><span class="header-section-number">5.9.1.0.1</span> Clustering</h5>
<ul>
<li>군집화의 기준</li>
</ul>
<p>동일한 군집에 속하는 개체 (또는 개인) 은 여러 속성이 비슷하고, 서로 다른 군집에 속한 관찰치는 그렇지 않도록 (여러 속성이 비슷하지 않도록) 군집을 구성</p>
<ul>
<li>군집화를 위한 변수: 전체 개체 (개인) 의 속성을 판단하기 위한 기준
<ul>
<li>인구통계적 변인 (성별, 나이, 거주지, 직업, 소득, 교육 등)</li>
<li>구매패턴 변인 (상품, 주기, 거래액 등)</li>
</ul></li>
</ul>
<p>군집분석에서는 관측값들이 서로 얼마나 유사한지, 또는 유사하지 않은지를 측정할 수 있는 측도가 필요하다.
- 군집분석에서는 보통 유사성(similarity)보다는 비유사성(dissimilarity)를 기준으로 하며, 거리(distance)를 사용한다.</p>
<p><span class="math inline">\(x\)</span>가 연속형일 때 CA의 위력이 최고로 발휘됨. 유사성의 척도로 거리가 사용되는데, 카테고리컬 변수에는 거리 계산이 불가능하기 때문. 꼭꼭 카테고리컬 변수로 CA를 해야겠다면 지시변수로 대체하여 CA를 시도할 수는 있겠으나, 이는 어느정도 억지로 하는 것이고 오점없는 CA는 아님.</p>
<hr />
</div>
<div id="distance-measures" class="section level5" number="5.9.1.0.2">
<h5><span class="header-section-number">5.9.1.0.2</span> Distance Measures</h5>
<p>거리 (Distance) 라는 함수. CA에서 사용되는 모든 거리는 pairwise 거리.</p>
<ol style="list-style-type: decimal">
<li>Euclid 거리 (Euclidean) : 가장 메이저함</li>
</ol>
<p>p차원 공간에서 주어진 두 점 <span class="math inline">\(\pmb x=(x_1 , \cdots, x_p), \; \; \pmb y=(y_1 , \cdots, y_p)\)</span> 사이의 유클리드 거리는</p>
<p>$
d(x, y) = 
$</p>
<p>if <span class="math inline">\(p=2\)</span>,</p>
<p><img src = "12-2.png"></p>
<p><br>
<br></p>
<p>{:start=“2”}</p>
<ol start="2" style="list-style-type: decimal">
<li>Minkowski 거리</li>
</ol>
<p>$
d(x, y) = { {_{i=1}^p (x_i - y_i)^m} }^{}
$</p>
<p><span class="math inline">\(m=2\)</span>일 때 이는 Euclidean과 같아진다. 보통은 m의 값으로 짝수를 많이 씀. 민코프는 결국 Euclidean의 일반화.</p>
<p><br>
<br></p>
<p>{:start=“3”}</p>
<ol start="3" style="list-style-type: decimal">
<li>Mahalanobis 거리</li>
</ol>
<p><img src = "12-3.png"></p>
<p>위에서 A와 B의 거리만을 보는 것이 아니라 위의 점들의 군집의 패턴 또한 고려함. x축과 y축에 해당하는 변수들 사이에 correlation이 있다는 것을 반영함. 중앙의 <span class="math inline">\(S^{-1}\)</span>으로 corr 구조를 반영하는 것. <strong>뭔 메커니즘으로?</strong> 위 케이스를 생각하면 A는 전체적인 패턴의 연장선 상에서 멀리 있는데, B는 패턴에서 직교해서 벗어나면서 가까이 있음. 따라서 A보다 B가 멀다고 평가 가능.</p>
<p>$
d(x, y) = 
$</p>
<p>{:start=“4”}</p>
<ol start="4" style="list-style-type: decimal">
<li>Manhattan 거리</li>
</ol>
<p>$
d_{Manhattan} (x, y) = {_{i=1}^p x_i - y_i }
$</p>
<hr />
<ul>
<li>Standardization</li>
</ul>
<p>CA는 자료 사이의 거리를 이용하여 수행되기 때문에, 각 자료의 단위가 결과에 큰 영향을 미친다. 이러한 문제를 해결하기 위하여, 가장 널리 쓰이는 방법이 <strong>표준화 방법</strong>이다. 표준화 방법이란 각 변수의 관찰값으로부터 그 변수의 평균을 빼고, 그 변수의 표준편차로 나누는 것이다. 표준화된 모든 변수가 평균이 0이고 표준편차가 1이 된다. <strong>사실상 필수</strong>.</p>
<p><br></p>
<ul>
<li>Graphical Tools
<ul>
<li>Scatter Plot</li>
<li>Scatter Plot using PCA</li>
<li>Andrews Plot</li>
<li>Star Plot</li>
<li>Chernoff Faces</li>
</ul></li>
</ul>
<hr />
<p><br>
<br>
<br></p>
</div>
</div>
<div id="hierarchical-clustering" class="section level3" number="5.9.2">
<h3><span class="header-section-number">5.9.2</span> Hierarchical Clustering</h3>
<ol style="list-style-type: decimal">
<li><p>Start with <span class="math inline">\(N\)</span> clusters, each containing a single entity and an <span class="math inline">\(N \times N\)</span> symmetric matrix of distances, <span class="math inline">\(D=\{d_{ik}\}\)</span>.</p></li>
<li><p>Search the distance Matrix <span class="math inline">\(D\)</span> for the nearest pair of clusters. Let the distance b/w the most similar (가장 거리가 작은) clusters <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> be <span class="math inline">\(d_{UV}\)</span>.</p></li>
<li><p>Mearge clusters <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. Label the newly formed cluster <span class="math inline">\((UV)\)</span>. Update the entries in the distance Matrix <span class="math inline">\(D\)</span> by squences below. The distance b/w <span class="math inline">\((UV)\)</span> and other cluster <span class="math inline">\(W\)</span> is denoted by <span class="math inline">\(d_{(UV)W}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>deleting rows and columns corresponding to clusters <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>, then</li>
<li>adding a row and a column giving the distance b/w <span class="math inline">\((UV)\)</span> and the remaining clusters.</li>
</ol></li>
<li><p>repeat steps 2 and 3 a total of <span class="math inline">\(N-1\)</span> times. Then, all observations will be in single clusters. Record the identity of clusters that are merged and the levels at which the mergers take place.</p></li>
</ol>
<hr />
<div id="계층적-군집분석-example" class="section level5" number="5.9.2.0.1">
<h5><span class="header-section-number">5.9.2.0.1</span> 계층적 군집분석 Example</h5>
<p>distance Matrix <span class="math inline">\(D\)</span>는 <span class="math inline">\(n^2\)</span>에 의존하여 변수 숫자가 증가하면 연산 시간도 기하급수적으로 증가.</p>
<p>{:start=“5”}
5. 계층적 분석에서만 덴드로그램을 그릴 수 있음. a graphical tool to illustrate the merges or divisions.</p>
<p><img src = "12-4-3.png"></p>
<p>python 라이브러리 함수 기준 총 distance의 70%에서 짤라서 clutser를 판정. color_threshold.</p>
<hr />
</div>
<div id="hca의-종류" class="section level5" number="5.9.2.0.2">
<h5><span class="header-section-number">5.9.2.0.2</span> HCA의 종류</h5>
<ol style="list-style-type: decimal">
<li>Single Linkage, 단일 연결 (mimum distance, or nearest neighbor)</li>
</ol>
<p>$
d_{(UV)W} = ( d_{UW}, d_{VW} )
$</p>
<p>{:start=“2”}
2. Complete Linkage, 완전 연결 (maximum distance, or farthest neighbor)</p>
<p>$
d_{(UV)W} = ( d_{UW}, d_{VW} )
$</p>
<p>{:start=“3”}
3. <strong>Average Linkage, 평균 연결</strong> (average distance)
- 위의 둘이 변동이 너무 심해서 이를 해결하기 위해 제시됨</p>
<p>$
d_{(UV)W} =  ( <em>{i=1}^{n</em>{UV}} <em>{j=1}^{n</em>{W}} d_{ij} )
$</p>
<p><img src = "12-4.png"></p>
<p>{:start=“4”}
4. <strong>Centriod Method, 중심점 연결</strong> (For each cluster, compute the centroid)</p>
<p>$
d_{(UV)W} =  U  V
$</p>
<p><img src = "12-4-2.png"></p>
<p>{:start=“5”}
5. <strong><del>Ward’s Method</del></strong></p>
<p>bold들이 무난함</p>
<hr />
</div>
<div id="hca의-장단점" class="section level5" number="5.9.2.0.3">
<h5><span class="header-section-number">5.9.2.0.3</span> HCA의 장단점</h5>
<p>Advantage:
- cluster의 수를 알 필요가 없음
- 덴드로그램 통해 군집화 프로세스와 결과물을 표현 가능</p>
<p>Disadvantage:
- 계산속도가 느림
- 아웃라이어 (이상치) 가 존재할 경우, 초기 단계에 잘못 분류된 군집은 분석이 끝날때까지 소속 cluster가 변하지 않음
- 아웃라이어에 대한 사전검토 필요, Centroid 방법이 아웃라이어에 덜 민감함</p>
<hr />
<p><br>
<br>
<br></p>
</div>
</div>
<div id="k-means-clustering" class="section level3" number="5.9.3">
<h3><span class="header-section-number">5.9.3</span> K-means Clustering</h3>
<p>K-평균 군집분석법. 사전에 결정된 군집수 <span class="math inline">\(k\)</span>에 기초하여 전체 데이터를 상대적으로 유사한 k개의 군집으로 구분한다.</p>
<p>Proceeds:
1. 군집수 k를 결정한다
2. 초기 k개 군집의 중심을 선택한다 (랜덤하게)
3. 각 관찰치를 그 중심과 가장 가까운 거리에 있는 군집에 할당한다.
4. 형성된 군집의 중심 (centroid) 을 계산한다.
5. 3-4의 과정을 기존의 중심과 새로운 중심의 차이가 없을 때까지 반복한다.</p>
<p><img src = "12-5.png">
<img src = "12-6.png"></p>
<div id="determination-of-k" class="section level5" number="5.9.3.0.1">
<h5><span class="header-section-number">5.9.3.0.1</span> Determination of K</h5>
<p>KCA의 결과는 초기 군집수 k의 결정에 민감하게 반응한다.</p>
<ol style="list-style-type: decimal">
<li>여러가지의 k값을 선택하여 CA를 수행한 후 가장 좋다고 생각되는 k값을 이용.
<ul>
<li>Elbow point 계산하여 k 선택</li>
<li>Silhouette plot으로 k 선택</li>
</ul></li>
<li>자료의 시각화를 통하여 K를 결정 (ex. star plot을 2차원 df로 바꾸어 평균 체크했었음)
<ul>
<li>자료의 시각화를 위해서는 차원축소가 필수적이고, 이를 위하여 PCA가 널리 사용된다.</li>
</ul></li>
<li>대용량 데이터에서 sampling한 데이터 (이것이 스몰데이터가 됨) 로 HCA를 우선 수행하여 (여기서 덴드로그램이 얻어짐) k의 값을 선택 (즉 HCA와 KCA를 둘 다 쓰므로 hybrid)</li>
</ol>
<p><img src="12-elbowplot.png"></p>
<hr />
<p><br>
<br>
<br></p>
</div>
</div>
<div id="군집의-평가방법" class="section level3" number="5.9.4">
<h3><span class="header-section-number">5.9.4</span> 군집의 평가방법</h3>
<ul>
<li>Silhouette Score (Silhouette Plot)</li>
</ul>
$
s(i) =  =
<span class="math display">\[\begin{cases} 1-\dfrac{a(i)}{b(i)}, &amp; if \; \; a(i) &lt; b(i) \\ 0, &amp; if \; \; a(i) = b(i) \\ \dfrac{b(i)}{a(i)} - 1, &amp; if \; \; a(i) &gt; b(i) \end{cases}\]</span>
<p>$</p>
<ul>
<li><span class="math inline">\(a(i)\)</span>: 개체 <span class="math inline">\(i\)</span>로부터 <strong>같은</strong> 군집 내에 있는 <strong>모든 다른</strong> 개체들 사이의 평균 거리. <strong>작을수록 좋다.</strong> 작을수록 해당하는 군집 안에서 중앙 부분에 components가 모여 있다는 소리이므로.</li>
<li><span class="math inline">\(b(i)\)</span>: 개체 <span class="math inline">\(i\)</span>로부터 <strong>다른</strong> 군집 내에 있는 개체들 사이의 평균 거리 <strong>중 가장 작은 값</strong>. <strong>클수록 좋다.</strong> 클수록 다른 군집에 헷갈려서 속할 일 없이 확실하게 현재 소속되어 있는 군집에 소속되어 구분된다는 소리이므로.</li>
</ul>
<p>1을 넘어갈 수 없으며, 1에 가까울수록 군집화가 잘 된 관찰값. 몇개의 cluster가 설정되었을 때 가장 해당 stat이 높게 나오는지를 통해 판정하는 것이 이 접근법. 평균 Silhouette Score는 모든 obs마다 <span class="math inline">\(s(i)\)</span>를 구하여 이를 평균낸 값이므로, 평균 Silhouette Score가 1에 가까울수록 군집분석이 잘됐다고 판단 가능.</p>
<p><img src="12-SilhouettePlot.png"></p>
<hr />
<p><br>
<br>
<br></p>
</div>
<div id="clustering-using-density-estimation-wk14" class="section level3" number="5.9.5">
<h3><span class="header-section-number">5.9.5</span> Clustering using Density Estimation (wk14)</h3>
<p>Based on <strong>nonparametric</strong> density estimation
The clusters may be viewed as high-density regions in the space separated by low-density regions between them.
No need to specify the number of clusters. It is determined by the method itself.</p>
<p>밀도기반 추정에 요구되는 (hyper) Parameter: bandwidth. 해당 값이 달라지면 결과도 달라짐.
Iris 데이터 예</p>
<div id="kernel-density-estimation-kde" class="section level5" number="5.9.5.0.1">
<h5><span class="header-section-number">5.9.5.0.1</span> Kernel Density Estimation (KDE)</h5>
<p>$
f(x_0) =  _{i=1}^N K (  ) , ; ; ; ; ; x R
$</p>
<p>N은 샘플사이즈, 람다는 밴드위스, K는 스무딩 커널, x_i는 obs</p>
<p>closed form처럼 보이지만 그냥 상징적인 공식일 뿐. closed form이 있는게 아니라 데이터 포인트마다 고유한 값이 추정되는 것으로 진행됨.</p>
<p>밀도추정에서 가장 많이 쓰는 방법. 추정하고 싶은 포인트는 <span class="math inline">\(x_0\)</span>. <span class="math inline">\(x_0\)</span>라는 포인트에 대해 density를 추정하고 싶다. <span class="math inline">\(x_0\)</span> 인근의 관찰치는 더 많은 가중치를 가짐. <span class="math inline">\(x_0\)</span> 로 부터 멀어질수록 가중치는 감소함. 각 obs 별로 커널함수 부여하고 최종적으로 그 커널함수 다 더한 다음에 스케일링하면 끝.</p>
<p>K의 가장 흔한 선택은 정규분포함수, 즉 Gaussian Kernel</p>
<p>Bandwidth 의 효과: 커널함수의 좌우 넓이에 해당하는 것으로서, 가우시안 커널에서는 표준편차에 해당함. Bandwith가 크면 x값들 간에 차별화가 덜되어서 추정 위력이 떨어짐</p>
<p>봉우리의 갯수는 군집의 갯수로 생각할 수 있음. 지나치게 밴드위스가 좁으면 뾰족한 부분이 다수 튀어나와 군집의 과다추정 발생</p>
<p>그래프는 1차원 밀도 추정에 해당
회색: 정답. 표준정규분포
붉은색: undersmoothed, 𝜆𝜆 = 0.05 (too small)
녹색: oversmoothed, 𝜆𝜆 =
2 (too large)
검정색: optimally smoothed, 𝜆𝜆 = 0.337</p>
<p>Bandwidth 추정</p>
<ul>
<li>2D Kernel Density Estimation: 2차원에서의 KDE는 어떻게 확장될 것인가?</li>
</ul>
<hr />
<p><br>
<br>
<br></p>
</div>
</div>
<div id="multidimensional-scaling-mds" class="section level3" number="5.9.6">
<h3><span class="header-section-number">5.9.6</span> Multidimensional Scaling (MDS)</h3>
<p>Dimension Reduction Methods
- PCA : x변수들끼리의 분산을 최대화시키는 방향으로 차원축소. 한 변수의 분산이 최대화되어야 함
- Factor: 변수간의 correlation을 최대한 깨트리지 않고 반영하는 방향으로 DR. Corr 구조가 최대한 유지
- MDS
- Canonical Discriminant Analysis</p>
<p>이중 위의 둘은 original data의 Variance 설명에 집중함. (ex. 1명이 401호, 1명이 501호에 있다고 하면, 둘의 직선 거리가 그렇게 크게 떨어져있다고 하기는 어렵지만 위의 두 분석법은 멀리 떨어져 있는 것처럼 그래프에 표현될 수 있음. 거리 개념이 없기 땨문)</p>
<ul>
<li>MDS
<ul>
<li>Fit (projection) the original data into a low-dimensional coordinate system such that any distortion caused by a reduction in dimensionality is minimized.</li>
<li><strong>Map the distances</strong> between points in a high dimensional space into a lower dimensional space.</li>
</ul></li>
<li>distortion이란? dissimilarity (distance) among the original data points
<ul>
<li>For a given set of observed similarities (or distances) between every pair of N items, find a representative of the items in as few dimensions as possible such that the similarities (or distances) in the lower dimensions match, as close as possible with the original similarities (or distances).</li>
</ul></li>
<li><ol style="list-style-type: decimal">
<li>Nonmetric MDS</li>
</ol>
<ul>
<li>Only the rank orders of the N(N-1)/2 original similarities are used to arrange N items in a lower-dimensional coordinate system. 거리 없이 rank만 주어져있음. rank만 안무너지도록</li>
</ul></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Metric MDS (자주씀. Principal Coordinate Analysis)</li>
</ol>
<ul>
<li>The actual magnitudes of the original similarities are used to obtain a geometric representation.</li>
</ul></li>
</ul>
<div id="kruskals-stress" class="section level5" number="5.9.6.0.1">
<h5><span class="header-section-number">5.9.6.0.1</span> <strong>Kruskal’s Stress</strong></h5>
<p>so-called <strong>Badness of fit</strong> criterion. MDS가 잘됐다면 기존 오리지널 차원의 거리나 차원축소된 이후의 거리나 비슷해야 함. 크루스칼 스트레스가 작으면 왜곡도 작은 것. 스트레스가 최소인 DR이 최고의 DR.</p>
<ul>
<li><p>Let <span class="math inline">\(D_{rs}\)</span> denote the actual distance (or dissimilarity) between item r and item s, then the ordered distances are $D_{r_1 s_1 } &lt;D_{r_2 s_2 } &lt; &lt; D_{r_M s_M }, ; ; ; M=</p>
<span class="math display">\[\begin{pmatrix} N \\ 2 \end{pmatrix}\]</span>
<p>$.</p></li>
<li><p>Let <span class="math inline">\(d_{rs}\)</span> denote the distance between item r and item s in the lower dimensional space.</p></li>
<li><p>MDS seeks (iteratively) to find a set of <span class="math inline">\(d\)</span>’s such that <span class="math inline">\(d_{r_1 s_1 } &lt;d_{r_2 s_2 } &lt; \cdots &lt; d_{r_M s_M }\)</span> and <span class="math inline">\(Stress = \left\{ \dfrac{\sum_{i=1}^N \sum_{j=1}^{i-1}(D_{ij} - d_{ij})^2} {\sum_{i=1}^N \sum_{j=1}^{i-1} \left( D_{ij} \right)^2} \right\}^{\tfrac{1}{2}}\)</span> is minimized.</p></li>
<li><p>Interpretation Guideline</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center">Stress</th>
<th align="center">Goodness of Fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">20%</td>
<td align="center">Poor</td>
</tr>
<tr class="even">
<td align="center">10%</td>
<td align="center">Fair</td>
</tr>
<tr class="odd">
<td align="center"><strong>5%</strong></td>
<td align="center"><strong>Good</strong></td>
</tr>
<tr class="even">
<td align="center">2.5%</td>
<td align="center">Excellent</td>
</tr>
<tr class="odd">
<td align="center">0%</td>
<td align="center">Perfect</td>
</tr>
</tbody>
</table>
<ul>
<li>Goodness of fit = monotonic relationship between the similarities and the final distances.</li>
</ul>
<p><strong>Takane’s Stress</strong></p>
<p>$</p>
<p>Stress = {  {<em>{i=1}^N </em>{j=1}<sup>{i-1}(D_{ij}</sup>2)^2} }^{}</p>
<p>$</p>
<p>Algorithm:
1. For N items, obtain <span class="math inline">\(M=\dfrac{N(N-1)}{2}\)</span> 개의 distances <span class="math inline">\(D_{r_1 s_1 }, D_{r_2 s_2 } , \cdots , D_{r_M s_M }\)</span>. Tehn an <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(D = \{D_{ij} \}\)</span> is constructed.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Using a trial configuration in q dimensions, determine distances <span class="math inline">\(d_{ij}^{(q)}\)</span>. The method to get initial <span class="math inline">\(d_{ij}^{(q)}\)</span> is given later.</p></li>
<li><p>Using the <span class="math inline">\(d_{ij}^{(q)}\)</span>, move the points around to obtain an improved configurations. <br> A new configuration: new <span class="math inline">\(d_{ij}^{(q)}\)</span> and smaller stress (e.g. Newton-Raphson method) The process is repeated until the best (minimum stress) representation is obtained.</p></li>
<li><p>Plot minimum stress (q) versus q and choose the best number of dimensions, <span class="math inline">\(q^\ast\)</span> from an examination of this plot. x축은 축소된 차원, y축은 stress. 차원이 작아질수록 Stress는 높고, 차원이 p라면 (original 차원과 같다면) Stress는 0. PCA와 달리 여기서는 <strong>elbow에서 멈춤</strong>.</p>
<ul>
<li>similar to scree plot</li>
</ul></li>
</ol>
<p>Note:
1. The larger the dimension, the better the fit.
2. Higher dimension means harder to visualize.</p>
</div>
<div id="algorithm-to-find-초기값-d_ijq" class="section level5" number="5.9.6.0.2">
<h5><span class="header-section-number">5.9.6.0.2</span> Algorithm to find 초기값 <span class="math inline">\(d_{ij}^{(q)}\)</span></h5>
<p>q값을 줄이려면 수치해석을 시작하기 전에 넣어줄 초기값에 해당하는 초기좌표들이 필요함. 그 값을 구하는 방법.</p>
<ol style="list-style-type: decimal">
<li><p>Construct the <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(A = \{ a_{ij} \} = \left\{ -\dfrac{1}{2} D_{ij}^2 \right\}\)</span>.</p></li>
<li><p>Construct the <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(B = \left(I - \dfrac{1}{N} J \right) A \left(I - \dfrac{1}{N} J \right) = \{ b_{ij} \} = \{ \bar a_{ij} - \bar a_{i.} - \bar a_{.j} + \bar a_{..} \}\)</span>.</p></li>
</ol>
where
$
a_{..} = ^N ^N , ; ; ; ; ; J =
<span class="math display">\[\begin{bmatrix} 1 &amp; \cdots &amp; 1 \\ \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; \cdots &amp; 1 \end{bmatrix}\]</span>
<p>$</p>
<p>{:start=“3”}</p>
<p>D행렬은 distance들의 SSE 행렬 정도에 해당.</p>
<ol start="3" style="list-style-type: decimal">
<li>Since <span class="math inline">\(B\)</span> is a symmetric matrix, use the <strong>spectral decomposition</strong> to write <span class="math inline">\(B\)</span> in the <span class="math inline">\(B = V \Lambda V&#39;\)</span>. <br> If B is positive semidefinite of rank <strong>q</strong> (p차원 아님!! <span class="math inline">\(q \le p\)</span>. 거리행렬이 일정 ev까지는 유의할 수 있는데 그 후로는 0만 튀어나올 수 있으며 DR은 바로 이상황에서 일어남. p는 위에서 보였던 유사 scree plot에서 original data의 차원으로 지정되었던 숫자) , there are q positive eigenvalues.</li>
</ol>
<p>if</p>
<p>$</p>
_1 =
<span class="math display">\[\begin{bmatrix} \lambda_1 &amp; \cdots &amp; \pmb 0 \\ &amp; \ddots &amp; \\ \pmb 0 &amp; \cdots &amp; \lambda_q \end{bmatrix}\]</span>
_{q q}, ; ; ; ; ; V_1 =
<span class="math display">\[\begin{bmatrix} \pmb v_1 ,  \pmb v_2 ,  \cdots, \pmb v_q \end{bmatrix}\]</span>
<p>_{N q}</p>
<p>$</p>
<p>then we can express</p>
<p>$</p>
<p>B = { V_1 }<em>{N q} { <em>1 }</em>{q q} { V_1 ’ }</em>{q N}</p>
<p>= V_1 _1^{1/2} _1^{1/2} V_1 ’ = ZZ’</p>
<p>$</p>
<p>where</p>
<p>$</p>
<p>Z = V_1 _1^{1/2}</p>
=
<span class="math display">\[\begin{bmatrix} \sqrt{\lambda_1} \pmb v_1 , \sqrt{\lambda_2} \pmb v_2 , \cdots, \sqrt{\lambda_q} \pmb v_q \end{bmatrix}\]</span>
=
<span class="math display">\[\begin{bmatrix} \pmb z_1 &#39; \\ \pmb z_2 &#39; \\ \vdots \\ \pmb z_q &#39; \end{bmatrix}\]</span>
<p>_{N q}</p>
<p>$</p>
<p>{:start=“4”}</p>
<ol start="4" style="list-style-type: decimal">
<li><p>The rows $z_1 ’ , z_2 ’ , , z_q $ of <span class="math inline">\(Z\)</span> are the points whose interpoint distance <span class="math inline">\(d_{ij}^{(q)} = (\pmb z_i - \pmb z_j)&#39;(\pmb z_i - \pmb z_j)\)</span> match $D_{ij} $s in the original distance matrix <span class="math inline">\(D\)</span>.</p></li>
<li><p>Since <span class="math inline">\(q\)</span> will typically be too large to be of practical interest and we would prefer a smaller dimension <span class="math inline">\(k\)</span> for plotting, we can use the first <span class="math inline">\(k\)</span> eigenvalues and corresponding eigenvectors to obtain <span class="math inline">\(N\)</span> points whose distances <span class="math inline">\(d_{ij}^{(k)}\)</span> are approximately equal to the corresponding <span class="math inline">\(D_{ij}\)</span>. 오리지널 데이터의 차원 p가 15개였다면, 이 차원을 SVD했을 때 ev 중 5개가 0이어서 q는 15개로 하였다. 여기서 차원을 더 줄이고 싶다면, 가령 k=5개까지 임의로 내려버리고 싶다면, 뒤쪽의 ev 10개에 해당하는 걸 쳐내는 것</p></li>
</ol>
<p>Rank is clearly rank 2. 즉 차원을 2차원까지 줄여도 손실되는 정보가 전혀 없다. 따라서 orginal data Distance Matrix에서 보였던 특성이 그대로 똑같이 드러난다.</p>

</div>
</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="discrimination-and-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/211312_ClusterAnalysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
