<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.8 Linear Regression | Self-Study</title>
  <meta name="description" content="8.8 Linear Regression | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="8.8 Linear Regression | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.8 Linear Regression | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="principal-component-analysis.html"/>
<link rel="next" href="uniform-laws-of-large-numbers.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="part"><span><b>I 20-02</b></span></li>
<li class="chapter" data-level="1" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>1</b> Categorical</a>
<ul>
<li class="chapter" data-level="1.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>1.1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>1.1.1</b> Data Type and Statistical Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>2</b> Bayesian</a>
<ul>
<li class="chapter" data-level="2.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>2.1</b> Abstract</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>2.1.1</b> 변수의 독립성</a></li>
<li class="chapter" data-level="2.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>2.1.2</b> 교환가능성</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>2.2</b> Continual Aeassessment Method</a></li>
<li class="chapter" data-level="2.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>2.3</b> Horseshoe Prior</a></li>
</ul></li>
<li class="part"><span><b>II 21-01</b></span></li>
<li class="chapter" data-level="3" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>3</b> Mathematical Stats</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>3.1</b> Inference</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>3.1.1</b> Rao-Blackwell thm.</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference.html"><a href="inference.html#completeness"><i class="fa fa-check"></i><b>3.1.2</b> Completeness</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference.html"><a href="inference.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>3.1.3</b> 레만-쉐페 thm.</a></li>
<li class="chapter" data-level="3.1.4" data-path="inference.html"><a href="inference.html#raoblack"><i class="fa fa-check"></i><b>3.1.4</b> Rao-Blackwell thm.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>3.2</b> Hypothesis Test</a></li>
<li class="chapter" data-level="3.3" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>3.3</b> Power Fucntion</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>3.3.1</b> Significance Probability (p-value)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>3.4</b> Optimal Testing Method</a></li>
<li class="chapter" data-level="3.5" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>3.5</b> Data Reduction</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>3.5.1</b> Sufficiency Principle</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>3.6</b> Borel Paradox</a></li>
<li class="chapter" data-level="3.7" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>3.7</b> Neyman–Pearson lemma</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>3.7.1</b> Overview</a></li>
<li class="chapter" data-level="3.7.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>3.7.2</b> Generalized LRT</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>3.8</b> 개념</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>4</b> MCMC</a>
<ul>
<li class="chapter" data-level="4.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Importance Sampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Independent Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>4.2</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>4.2.1</b> MH Algorithm</a></li>
<li class="chapter" data-level="4.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>4.2.2</b> Random Walk Chains (Most Widely Used)</a></li>
<li class="chapter" data-level="4.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>4.2.3</b> Basic Gibbs Sampler</a></li>
<li class="chapter" data-level="4.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>4.2.4</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>4.3</b> Advanced MCMC (wk08)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>4.3.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="4.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>4.3.2</b> Hit-and-Run Algorithm</a></li>
<li class="chapter" data-level="4.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>4.3.3</b> Metropolis-Adjusted Langevin Algorithm</a></li>
<li class="chapter" data-level="4.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>4.3.4</b> Multiple-Try Metropolis Algorithm</a></li>
<li class="chapter" data-level="4.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>4.3.5</b> Reversible Jump MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>4.4</b> Auxiliary Variable MCMC</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Multimodal Target Distribution</a></li>
<li class="chapter" data-level="4.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>4.4.3</b> Doubly-intractable Normalizing Constants</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>4.5.1</b> Simulator-Based Models</a></li>
<li class="chapter" data-level="4.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>4.5.2</b> ABCifying Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>4.5.3</b> ABC-MCMC Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>4.6</b> Hamiltonian Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.6.1</b> Introduction to Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>4.7</b> Population Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>4.7.1</b> Adaptive Direction Sampling</a></li>
<li class="chapter" data-level="4.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>4.7.2</b> Conjugate Gradient MC</a></li>
<li class="chapter" data-level="4.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>4.7.3</b> Parallel Tempering</a></li>
<li class="chapter" data-level="4.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>4.7.4</b> Evolutionary MC</a></li>
<li class="chapter" data-level="4.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>4.7.5</b> Sequential Parallel Tempering</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>4.8</b> Stochastic Approximation Monte Carlo</a></li>
<li class="chapter" data-level="4.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>4.9</b> Review</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>4.9.1</b> Wk01</a></li>
<li class="chapter" data-level="4.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>4.9.2</b> wk03</a></li>
<li class="chapter" data-level="4.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>4.9.3</b> wk04, 05</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>4.10</b> Else</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>4.10.1</b> Hw4. Rasch Model</a></li>
<li class="chapter" data-level="4.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>4.10.2</b> DA) Example: MVN</a></li>
<li class="chapter" data-level="4.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>4.10.3</b> Bayesian adaptive clinical trial with delayed outcomes</a></li>
<li class="chapter" data-level="4.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>4.10.4</b> NMAR의 종류</a></li>
<li class="chapter" data-level="4.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>4.10.5</b> wk10) Bayesian Model Selection</a></li>
<li class="chapter" data-level="4.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>4.10.6</b> Autologistic model</a></li>
<li class="chapter" data-level="4.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>4.10.7</b> wk10) Bayesian Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>5</b> MVA</a>
<ul>
<li class="chapter" data-level="5.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>5.1</b> Overview of mva (not ended)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>5.1.1</b> Notation</a></li>
<li class="chapter" data-level="5.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>5.1.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>5.1.3</b> Statistical Inference on Correlation</a></li>
<li class="chapter" data-level="5.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>5.1.4</b> Standardization</a></li>
<li class="chapter" data-level="5.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>5.1.5</b> Missing Value Treatment</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>5.2</b> Multivariate Nomral (wk2)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>5.2.1</b> Overview</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>5.2.2</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="5.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>5.2.3</b> Properties of MVN</a></li>
<li class="chapter" data-level="5.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>5.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="5.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>5.2.5</b> Linear Combination of Random Vectors</a></li>
<li class="chapter" data-level="5.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>5.2.6</b> Multivariate Normal Likelihood</a></li>
<li class="chapter" data-level="5.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>5.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span></a></li>
<li class="chapter" data-level="5.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>5.2.8</b> Assessing Normality</a></li>
<li class="chapter" data-level="5.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>5.2.9</b> Power Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>5.3</b> Inference about Mean Vector (wk3)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>5.3.2</b> 1. Confidence Region</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>5.3.3</b> 2. Simultaneous CI</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>5.3.4</b> 3. Note: Bonferroni Multiple Comparison</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>5.3.5</b> 4. Large Sample Inferences about a Mean Vector</a></li>
<li class="chapter" data-level="5.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>5.3.6</b> 1. Profile Analysis (wk4, 5)</a></li>
<li class="chapter" data-level="5.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>5.3.7</b> 2. Test for Linear Trend</a></li>
<li class="chapter" data-level="5.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>5.3.8</b> 3. Inferences about a Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>5.4</b> Comparison of Several MV Means (wk5)</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>5.4.1</b> Paired Comparison</a></li>
<li class="chapter" data-level="5.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>5.4.2</b> Comparing Mean Vectors from Two Populations</a></li>
<li class="chapter" data-level="5.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>5.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)</a></li>
<li class="chapter" data-level="5.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>5.4.4</b> Comparing Several Multivariate Population Means</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>5.5</b> Multivariate Multiple Regression (wk6)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>5.5.1</b> Overview</a></li>
<li class="chapter" data-level="5.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>5.5.2</b> Multivariate Multiple Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>5.5.3</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="5.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>5.6</b> PCA</a></li>
<li class="chapter" data-level="5.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>5.7</b> Factor</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>5.7.1</b> Method of Estimation</a></li>
<li class="chapter" data-level="5.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>5.7.2</b> Factor Rotation</a></li>
<li class="chapter" data-level="5.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>5.7.3</b> Varimax Criterion</a></li>
<li class="chapter" data-level="5.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>5.7.4</b> Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>5.8</b> Discrimination and Classification</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>5.8.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="5.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>5.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations</a></li>
<li class="chapter" data-level="5.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>5.8.3</b> Evaluating Classification Functions</a></li>
<li class="chapter" data-level="5.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>5.8.4</b> Classification with several Populations (wk13)</a></li>
<li class="chapter" data-level="5.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>5.8.5</b> Other Discriminant Analysis Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>5.9</b> Clustering, Distance Methods, and Ordination</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>5.9.1</b> Overview</a></li>
<li class="chapter" data-level="5.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>5.9.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="5.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>5.9.3</b> K-means Clustering</a></li>
<li class="chapter" data-level="5.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>5.9.4</b> 군집의 평가방법</a></li>
<li class="chapter" data-level="5.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>5.9.5</b> Clustering using Density Estimation (wk14)</a></li>
<li class="chapter" data-level="5.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>5.9.6</b> Multidimensional Scaling (MDS)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>6.1</b> SVD</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="svd.html"><a href="svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>6.1.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="6.1.2" data-path="svd.html"><a href="svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>6.1.2</b> Singular value Decomposition: General-version</a></li>
<li class="chapter" data-level="6.1.3" data-path="svd.html"><a href="svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>6.1.3</b> Singular value Decomposition: Another-version</a></li>
<li class="chapter" data-level="6.1.4" data-path="svd.html"><a href="svd.html#quadratic-forms"><i class="fa fa-check"></i><b>6.1.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="6.1.5" data-path="svd.html"><a href="svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>6.1.5</b> Partitioned Matrices</a></li>
<li class="chapter" data-level="6.1.6" data-path="svd.html"><a href="svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>6.1.6</b> Geometrical Aspects</a></li>
<li class="chapter" data-level="6.1.7" data-path="svd.html"><a href="svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>6.1.7</b> Column, Row and Null Space</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>6.2</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-1.html"><a href="introduction-1.html#what"><i class="fa fa-check"></i><b>6.2.1</b> What</a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-1.html"><a href="introduction-1.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>6.2.2</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="6.2.3" data-path="introduction-1.html"><a href="introduction-1.html#multivariate-normal-distributions"><i class="fa fa-check"></i><b>6.2.3</b> Multivariate Normal Distributions</a></li>
<li class="chapter" data-level="6.2.4" data-path="introduction-1.html"><a href="introduction-1.html#distributions-of-quadratic-forms"><i class="fa fa-check"></i><b>6.2.4</b> Distributions of Quadratic Forms</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>6.3</b> Estimation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>6.3.1</b> Identifiability and Estimability</a></li>
<li class="chapter" data-level="6.3.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>6.3.2</b> Estimation: Least Squares</a></li>
<li class="chapter" data-level="6.3.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>6.3.3</b> Estimation: Best Linear Unbiased</a></li>
<li class="chapter" data-level="6.3.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>6.3.4</b> Estimation: Maximum Likelihood</a></li>
<li class="chapter" data-level="6.3.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>6.3.5</b> Estimation: Minimum Variance Unbiased</a></li>
<li class="chapter" data-level="6.3.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>6.3.6</b> Sampling Distributions of Estimates</a></li>
<li class="chapter" data-level="6.3.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>6.3.7</b> Generalized Least Squares(GLS)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>6.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>6.4.1</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="6.4.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>6.4.2</b> More About Models</a></li>
<li class="chapter" data-level="6.4.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> Estimating and Testing Contrasts</a></li>
<li class="chapter" data-level="6.4.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>6.4.4</b> Cochran’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>6.5</b> Testing</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>6.5.1</b> More About Models: Two approaches for linear model</a></li>
<li class="chapter" data-level="6.5.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>6.5.2</b> Testing Models</a></li>
<li class="chapter" data-level="6.5.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>6.5.3</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>6.5.4</b> Testing Linear Parametric Functions</a></li>
<li class="chapter" data-level="6.5.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>6.5.5</b> Theoretical Complements</a></li>
<li class="chapter" data-level="6.5.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>6.5.6</b> A Generalized Test Procedure</a></li>
<li class="chapter" data-level="6.5.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>6.5.7</b> Testing Single Degrees of Freedom in a Given Subspace</a></li>
<li class="chapter" data-level="6.5.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>6.5.8</b> Breaking SS into Independent Components</a></li>
<li class="chapter" data-level="6.5.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>6.5.9</b> General Theory</a></li>
<li class="chapter" data-level="6.5.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>6.5.10</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="6.5.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>6.5.11</b> Confidence Regions</a></li>
<li class="chapter" data-level="6.5.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>6.5.12</b> Tests for Generalized Least Squares Models</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>6.6</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>6.6.1</b> A direct solution via inner products</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>6.7</b> Flat</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>6.7.1</b> 1.Flat</a></li>
<li class="chapter" data-level="6.7.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>6.7.2</b> 2. Solutions to systems of linear equations</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>6.8</b> Unified Approach to Balanced ANOVA Models</a></li>
</ul></li>
<li class="part"><span><b>III 21-02</b></span></li>
<li class="chapter" data-level="7" data-path="network-stats.html"><a href="network-stats.html"><i class="fa fa-check"></i><b>7</b> Network Stats</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="introduction-2.html"><a href="introduction-2.html#types-of-network-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Types of Network Analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="introduction-2.html"><a href="introduction-2.html#network-modeling-and-inference"><i class="fa fa-check"></i><b>7.1.2</b> Network Modeling and Inference</a></li>
<li class="chapter" data-level="7.1.3" data-path="introduction-2.html"><a href="introduction-2.html#network-processes"><i class="fa fa-check"></i><b>7.1.3</b> Network Processes</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html"><i class="fa fa-check"></i><b>7.2</b> Descriptive Statistics of Networks</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics"><i class="fa fa-check"></i><b>7.2.1</b> Vertex and Edge Characteristics</a></li>
<li class="chapter" data-level="7.2.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion"><i class="fa fa-check"></i><b>7.2.2</b> Characterizing Network Cohesion</a></li>
<li class="chapter" data-level="7.2.3" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#graph-partitioning"><i class="fa fa-check"></i><b>7.2.3</b> Graph Partitioning</a></li>
<li class="chapter" data-level="7.2.4" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing"><i class="fa fa-check"></i><b>7.2.4</b> Assortativity and Mixing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html"><i class="fa fa-check"></i><b>7.3</b> Data Collection and Sampling</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#sampling-designs"><i class="fa fa-check"></i><b>7.3.1</b> Sampling Designs</a></li>
<li class="chapter" data-level="7.3.2" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#coping-strategies"><i class="fa fa-check"></i><b>7.3.2</b> Coping Strategies</a></li>
<li class="chapter" data-level="7.3.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#big-data-solves-nothing"><i class="fa fa-check"></i><b>7.3.3</b> Big Data Solves Nothing</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html"><i class="fa fa-check"></i><b>7.4</b> Mathematical Models for Network Graphs</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models"><i class="fa fa-check"></i><b>7.4.1</b> Classical Random Graph Models</a></li>
<li class="chapter" data-level="7.4.2" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models"><i class="fa fa-check"></i><b>7.4.2</b> Generalized Random Graph Models</a></li>
<li class="chapter" data-level="7.4.3" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms"><i class="fa fa-check"></i><b>7.4.3</b> Network Graph Models Based on Mechanisms</a></li>
<li class="chapter" data-level="7.4.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics"><i class="fa fa-check"></i><b>7.4.4</b> Assessing Significance of Network Graph Characteristics</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html"><i class="fa fa-check"></i><b>7.5</b> Introduction to ERGM</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#exponential-random-graph-models"><i class="fa fa-check"></i><b>7.5.1</b> Exponential Random Graph Models</a></li>
<li class="chapter" data-level="7.5.2" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation"><i class="fa fa-check"></i><b>7.5.2</b> Difficulty in Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html"><i class="fa fa-check"></i><b>7.6</b> Parameter Estimation of ERGM</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm"><i class="fa fa-check"></i><b>7.6.1</b> Current Methods for ERGM</a></li>
<li class="chapter" data-level="7.6.2" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm"><i class="fa fa-check"></i><b>7.6.2</b> Approximation-based Algorithm</a></li>
<li class="chapter" data-level="7.6.3" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches"><i class="fa fa-check"></i><b>7.6.3</b> Auxiliary Variable MCMC-based Approaches</a></li>
<li class="chapter" data-level="7.6.4" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc"><i class="fa fa-check"></i><b>7.6.4</b> Varying Trunction Stochastic Approximation MCMC</a></li>
<li class="chapter" data-level="7.6.5" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#conclusion"><i class="fa fa-check"></i><b>7.6.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html"><i class="fa fa-check"></i><b>7.7</b> ERGM for Dynamic Networks</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm"><i class="fa fa-check"></i><b>7.7.1</b> Temporal ERGM (TERGM, T ERGM)</a></li>
<li class="chapter" data-level="7.7.2" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm"><i class="fa fa-check"></i><b>7.7.2</b> Separable Temporal ERGM (STERGM, ST ERGM)</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="latent-network-models.html"><a href="latent-network-models.html"><i class="fa fa-check"></i><b>7.8</b> Latent Network Models</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-model"><i class="fa fa-check"></i><b>7.8.1</b> Latent Position Model</a></li>
<li class="chapter" data-level="7.8.2" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-cluster-model"><i class="fa fa-check"></i><b>7.8.2</b> Latent Position Cluster Model</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html"><i class="fa fa-check"></i><b>7.9</b> Additive and Multiplicative Effects Network Models</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#introduction-3"><i class="fa fa-check"></i><b>7.9.1</b> Introduction</a></li>
<li class="chapter" data-level="7.9.2" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression"><i class="fa fa-check"></i><b>7.9.2</b> Social Relations Regression</a></li>
<li class="chapter" data-level="7.9.3" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models"><i class="fa fa-check"></i><b>7.9.3</b> Multiplicative Effects Models</a></li>
<li class="chapter" data-level="7.9.4" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation"><i class="fa fa-check"></i><b>7.9.4</b> Inference via Posterior Approximation</a></li>
<li class="chapter" data-level="7.9.5" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r"><i class="fa fa-check"></i><b>7.9.5</b> Discussion and Example with R</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html"><i class="fa fa-check"></i><b>7.10</b> Stochastic Block Models</a>
<ul>
<li class="chapter" data-level="7.10.1" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html#stochastic-block-model"><i class="fa fa-check"></i><b>7.10.1</b> Stochastic Block Model</a></li>
<li class="chapter" data-level="7.10.2" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm"><i class="fa fa-check"></i><b>7.10.2</b> Mixed Membership Block Model (MMBM)</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>7.11</b> 01</a></li>
<li class="chapter" data-level="7.12" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>7.12</b> 02</a></li>
<li class="chapter" data-level="7.13" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>7.13</b> 10.</a>
<ul>
<li class="chapter" data-level="7.13.1" data-path="section-2.html"><a href="section-2.html#stochastic-block-model-1"><i class="fa fa-check"></i><b>7.13.1</b> Stochastic Block Model</a></li>
<li class="chapter" data-level="7.13.2" data-path="section-2.html"><a href="section-2.html#likelihood-function-1"><i class="fa fa-check"></i><b>7.13.2</b> Likelihood function</a></li>
<li class="chapter" data-level="7.13.3" data-path="section-2.html"><a href="section-2.html#mixed-membership-block-model-mmbm-1"><i class="fa fa-check"></i><b>7.13.3</b> Mixed Membership Block Model (MMBM)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="high-dimension.html"><a href="high-dimension.html"><i class="fa fa-check"></i><b>8</b> High Dimension</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-4.html"><a href="introduction-4.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html"><i class="fa fa-check"></i><b>8.2</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#motivation"><i class="fa fa-check"></i><b>8.2.1</b> Motivation</a></li>
<li class="chapter" data-level="8.2.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#from-markov-to-chernoff"><i class="fa fa-check"></i><b>8.2.2</b> From Markov to Chernoff</a></li>
<li class="chapter" data-level="8.2.3" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-variables"><i class="fa fa-check"></i><b>8.2.3</b> sub-Gaussian random variables</a></li>
<li class="chapter" data-level="8.2.4" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables"><i class="fa fa-check"></i><b>8.2.4</b> Properties of sub-Gaussian random variables</a></li>
<li class="chapter" data-level="8.2.5" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#equivalent-definitions"><i class="fa fa-check"></i><b>8.2.5</b> Equivalent definitions</a></li>
<li class="chapter" data-level="8.2.6" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-vectors"><i class="fa fa-check"></i><b>8.2.6</b> Sub-Gaussian random vectors</a></li>
<li class="chapter" data-level="8.2.7" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#hoeffdings-inequality"><i class="fa fa-check"></i><b>8.2.7</b> Hoeffding’s inequality</a></li>
<li class="chapter" data-level="8.2.8" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#maximal-inequalities"><i class="fa fa-check"></i><b>8.2.8</b> Maximal inequalities</a></li>
<li class="chapter" data-level="8.2.9" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#section-3"><i class="fa fa-check"></i><b>8.2.9</b> </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html"><i class="fa fa-check"></i><b>8.3</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#sub-exponential-random-variables"><i class="fa fa-check"></i><b>8.3.1</b> Sub-exponential random variables</a></li>
<li class="chapter" data-level="8.3.2" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#bernsteins-condition"><i class="fa fa-check"></i><b>8.3.2</b> Bernstein’s condition</a></li>
<li class="chapter" data-level="8.3.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#mcdiarmids-inequality"><i class="fa fa-check"></i><b>8.3.3</b> McDiarmid’s inequality</a></li>
<li class="chapter" data-level="8.3.4" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#levys-inequality"><i class="fa fa-check"></i><b>8.3.4</b> Levy’s inequality</a></li>
<li class="chapter" data-level="8.3.5" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#quadratic-form"><i class="fa fa-check"></i><b>8.3.5</b> Quadratic form</a></li>
<li class="chapter" data-level="8.3.6" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma"><i class="fa fa-check"></i><b>8.3.6</b> The Johnson–Lindenstrauss Lemma</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html"><i class="fa fa-check"></i><b>8.4</b> Metric entropy and its uses</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#metric-space"><i class="fa fa-check"></i><b>8.4.1</b> Metric space</a></li>
<li class="chapter" data-level="8.4.2" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy"><i class="fa fa-check"></i><b>8.4.2</b> Covering numbers and metric entropy</a></li>
<li class="chapter" data-level="8.4.3" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#packing-numbers"><i class="fa fa-check"></i><b>8.4.3</b> Packing numbers</a></li>
<li class="chapter" data-level="8.4.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-4"><i class="fa fa-check"></i><b>8.4.4</b> </a></li>
<li class="chapter" data-level="8.4.5" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-5"><i class="fa fa-check"></i><b>8.4.5</b> </a></li>
<li class="chapter" data-level="8.4.6" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-6"><i class="fa fa-check"></i><b>8.4.6</b> </a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="covariance-estimation.html"><a href="covariance-estimation.html"><i class="fa fa-check"></i><b>8.5</b> Covariance estimation</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="covariance-estimation.html"><a href="covariance-estimation.html#matrix-algebra-review"><i class="fa fa-check"></i><b>8.5.1</b> Matrix algebra review</a></li>
<li class="chapter" data-level="8.5.2" data-path="covariance-estimation.html"><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm"><i class="fa fa-check"></i><b>8.5.2</b> Covariance matrix estimation in the operator norm</a></li>
<li class="chapter" data-level="8.5.3" data-path="covariance-estimation.html"><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices"><i class="fa fa-check"></i><b>8.5.3</b> Bounds for structured covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html"><i class="fa fa-check"></i><b>8.6</b> Matrix concentration inequalities</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-calculus"><i class="fa fa-check"></i><b>8.6.1</b> Matrix calculus</a></li>
<li class="chapter" data-level="8.6.2" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-chernoff"><i class="fa fa-check"></i><b>8.6.2</b> Matrix Chernoff</a></li>
<li class="chapter" data-level="8.6.3" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices"><i class="fa fa-check"></i><b>8.6.3</b> Sub-Gaussian and sub-exponential matrices</a></li>
<li class="chapter" data-level="8.6.4" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><i class="fa fa-check"></i><b>8.6.4</b> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>8.7</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-1"><i class="fa fa-check"></i><b>8.7.1</b> PCA</a></li>
<li class="chapter" data-level="8.7.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#matrix-perturbation"><i class="fa fa-check"></i><b>8.7.2</b> Matrix Perturbation</a></li>
<li class="chapter" data-level="8.7.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#spiked-cov-model"><i class="fa fa-check"></i><b>8.7.3</b> Spiked Cov Model</a></li>
<li class="chapter" data-level="8.7.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#sparse-pca"><i class="fa fa-check"></i><b>8.7.4</b> sparse PCA</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>8.8</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="linear-regression.html"><a href="linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>8.8.1</b> Problem formulation</a></li>
<li class="chapter" data-level="8.8.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimator-in-high-dimensions"><i class="fa fa-check"></i><b>8.8.2</b> Least Squares Estimator in high dimensions</a></li>
<li class="chapter" data-level="8.8.3" data-path="linear-regression.html"><a href="linear-regression.html#sparse-linear-regression"><i class="fa fa-check"></i><b>8.8.3</b> Sparse linear regression</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html"><i class="fa fa-check"></i><b>8.9</b> Uniform laws of large numbers</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#motivation-1"><i class="fa fa-check"></i><b>8.9.1</b> Motivation</a></li>
<li class="chapter" data-level="8.9.2" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity"><i class="fa fa-check"></i><b>8.9.2</b> A uniform law via Rademacher complexity</a></li>
<li class="chapter" data-level="8.9.3" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity"><i class="fa fa-check"></i><b>8.9.3</b> Upper bounds on the Rademacher complexity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>9</b> Survival Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>9.2</b> </a></li>
<li class="chapter" data-level="9.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html"><i class="fa fa-check"></i><b>9.3</b> Counting Processes and Martingales</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#conditional-expectation"><i class="fa fa-check"></i><b>9.3.1</b> Conditional Expectation</a></li>
<li class="chapter" data-level="9.3.2" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#martingale"><i class="fa fa-check"></i><b>9.3.2</b> Martingale</a></li>
<li class="chapter" data-level="9.3.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#key-martingales-properties"><i class="fa fa-check"></i><b>9.3.3</b> Key Martingales Properties</a></li>
<li class="chapter" data-level="9.3.4" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#section-8"><i class="fa fa-check"></i><b>9.3.4</b> </a></li>
<li class="chapter" data-level="9.3.5" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#section-9"><i class="fa fa-check"></i><b>9.3.5</b> </a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>9.4</b> </a></li>
<li class="chapter" data-level="9.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>9.5</b> Cox Regression</a></li>
<li class="chapter" data-level="9.6" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html"><i class="fa fa-check"></i><b>9.6</b> Filtration의 개념을 정복하자!</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약"><i class="fa fa-check"></i><b>9.6.1</b> Random Process를 이야기 하기까지의 긴 여정의 요약</a></li>
<li class="chapter" data-level="9.6.2" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#ft-measurable"><i class="fa fa-check"></i><b>9.6.2</b> Ft-measurable</a></li>
<li class="chapter" data-level="9.6.3" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#epilogue"><i class="fa fa-check"></i><b>9.6.3</b> EPILOGUE</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>9.7</b> Concepts</a></li>
</ul></li>
<li class="appendix"><span><b>00-00</b></span></li>
<li class="chapter" data-level="A" data-path="concepts-1.html"><a href="concepts-1.html"><i class="fa fa-check"></i><b>A</b> Concepts</a>
<ul>
<li class="chapter" data-level="A.1" data-path="autologistic.html"><a href="autologistic.html"><i class="fa fa-check"></i><b>A.1</b> Autologistics</a></li>
<li class="chapter" data-level="A.2" data-path="orderlogit.html"><a href="orderlogit.html"><i class="fa fa-check"></i><b>A.2</b> Ordered Logit</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="abstract-1.html"><a href="abstract-1.html"><i class="fa fa-check"></i><b>B</b> ABSTRACT</a></li>
<li class="chapter" data-level="C" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>C</b> CNN</a></li>
<li class="chapter" data-level="D" data-path="cnn-1.html"><a href="cnn-1.html"><i class="fa fa-check"></i><b>D</b> CNN</a></li>
<li class="chapter" data-level="E" data-path="cnn-2.html"><a href="cnn-2.html"><i class="fa fa-check"></i><b>E</b> CNN</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level2" number="8.8">
<h2><span class="header-section-number">8.8</span> Linear Regression</h2>
<p>이것때문이라고 설마?</p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
<div id="problem-formulation" class="section level3" number="8.8.1">
<h3><span class="header-section-number">8.8.1</span> Problem formulation</h3>
<p>unknown vector 인 <strong>regression vector</strong> <span class="math inline">\(\theta^\ast \in \mathbb R^d\)</span> 설정.</p>
<p>벡터 <span class="math inline">\(Y = (Y_1 , \cdots, Y_n)&#39; \in \mathbb R^n\)</span> 를 관측했으며, linear model <span class="math inline">\(Y=X\theta^{*}+\epsilon\)</span> 를 통해 이와 관계되어 있는 <span class="math inline">\(X \in \mathbb R^{n \times d}\)</span> 를 가정하자. 이때 <span class="math inline">\(\epsilon\;=\;\left(\epsilon_{1},\cdot\cdot\cdot,\epsilon_{n}\right)^{\top}~\in~\mathbb{R}^{n}\;\)</span> 이며, 각각은 independent zero-mean <span class="math inline">\(\epsilon_1 , \cdots, \epsilon_n \in SG(\sigma^2)\)</span>.</p>
<p>이제 <span class="math inline">\(\hat \theta\)</span> 를 <span class="math inline">\(\theta^\ast\)</span> 의 estimator 로 잡는다. 이하의 2가지가 주된 관심사.</p>
<ol style="list-style-type: decimal">
<li>Prediction</li>
</ol>
<p><span class="math inline">\(X \theta^\ast + \tilde \epsilon = \tilde Y \overset{iid} \sim Y\)</span> 라고 설정하자. 우리의 목적은 <span class="math inline">\(\theta^\ast\)</span> 에 대한 우리의 estimate <span class="math inline">\(\hat \theta\)</span> 의 구현값을 사용해서 <span class="math inline">\(\tilde Y\)</span> 를 predict. performance 에 대한 natural measure 는 이하와 같다. 이때 unavoidable error 는 말 그대로 unavoidable 이므로, 우리는 후자인 MSE, 즉 <span class="math inline">\(\mathbb{E}\left [\Bigg \|X(\theta^{*}-{\widehat{\theta}})\Bigg \|_{2}^{2}\right ]\)</span>, 를 조사하고자 한다.</p>
<p><span class="math display">\[
\frac{1}{n}\mathbb{E}\left [\Bigg \|{\tilde{Y}}-X{\widehat{\theta}}\Bigg \|_{2}^{2}\right ]
=
\frac{1}{n}\mathbb{E}\left [\Bigg \|{\tilde{\epsilon}}+X(\theta^{*}-{\widehat{\theta}})\Bigg \|_{2}^{2}\right ]
=
\underbrace{\frac{1}{n}\mathbb{E}\left [\Bigg \|\tilde \epsilon\Bigg \|_{2}^{2}\right ]}_{\text{unavoidable error}}
+
\underbrace{\frac{1}{n}\mathbb{E}\left [\Bigg \|X(\theta^{*}-{\widehat{\theta}})\Bigg \|_{2}^{2}\right ]}_{\text{Mean Squared Error}}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Parameter Estimation</li>
</ol>
<p>위와는 다른 케이스로, 몇몇 경우에 우리는 regression vector <span class="math inline">\(\theta*\ast\)</span> 를 조사하고 싶어하는 경우가 있으며, 이 경우에 관심사 (조사대상) 는</p>
<p><span class="math display">\[
\mathbb{E}\left [\Bigg \|\theta^{*}-{\widehat{\theta}}\Bigg \|_{2}^{2}\right ]
\]</span></p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
<div id="least-squares-estimator-in-high-dimensions" class="section level3" number="8.8.2">
<h3><span class="header-section-number">8.8.2</span> Least Squares Estimator in high dimensions</h3>
<p>고전적인 LR 은 LS 문제를 풀어내는 것과 같다. 이하의 형을 구한다는 것과 equivalent 이며, 이는 보통 <strong>Ordinary Least Squares (OLS) estimator</strong> 로 통칭됨.</p>
<p><span class="math inline">\({\widehat{\theta}}_{\mathrm{LS}}=(X^{\top}X)^{-1}X^{\top}Y=\arg \min\limits_{\theta \in \mathbb R^d}\|Y-X\theta\|_{2}^{2}\)</span></p>
<p>Gauss–Markov thm 에 의해 우리는 OLS Estimator 가 Best Linear Unbiased Estimator (BLUE) 임을 알고 있음. 왜냐고 특정 condition 하에서의 Linear Unbiased Estimator 들의 class 내에서 OLS Estimator 가 가장 작은 Var 을 가지고 있으니까. 하지만 이 estimator 는 <span class="math inline">\(X&#39;X\)</span> 가 uninvertible 하면 존재할 수 없음. 다른 말로, <span class="math inline">\(n \ge d\)</span> 인 경우에만 존재할 수 있다는 거임. <span class="math inline">\(d&gt;n\)</span> 라도, <span class="math inline">\(\operatorname*{min}_{\theta\in\mathbb{R}^{d}}\|Y-X\theta\|_{2}^{2}\)</span> 라는 문제에 대한 해를 찾아내는 건 가능함. 다음과 같이 매핑하는 function <span class="math inline">\(\theta\mapsto\|Y=X\theta\|_{2}^{2}\)</span> 는 convex 이므로, 1차 optimality condition 인 <span class="math inline">\(\nabla_{\theta}\|Y-X\theta\|_{2}^{2}=0\quad\Longleftrightarrow\quad X^{\top}X\theta=X^{\top}Y\)</span> 를 체크하는 것만으로 충분함. 이의 해는 이하에 제시된 MP-pseudo Inverse <span class="math inline">\(X&#39;X\)</span> 의 형으로 서술될 수 있음.</p>
<p><br>
<br>
<br></p>
<div id="mean-squared-error-of-the-least-squares-estimator" class="section level4" number="8.8.2.1">
<h4><span class="header-section-number">8.8.2.1</span> Mean Squared Error of the Least Squares Estimator</h4>
<div class="theorem">
<p><span id="thm:unlabeled-div-22" class="theorem"><strong>Theorem 8.13  (Least Squares Estimator) </strong></span>let linear model <span class="math inline">\(Y=X\theta^{*}+\epsilon\)</span>, 이때 <span class="math inline">\(\epsilon\)</span> 의 elements 각각은 independent zero-mean <span class="math inline">\(\epsilon_1 , \cdots, \epsilon_n \in SG(\sigma^2)\)</span>.</p>
<p>이때 <span class="math inline">\(\hat \theta_{LS}\)</span> 는 이하를 만족하며, 2번째 ineq는 with probability at least <span class="math inline">\(1-\delta\)</span> 로 만족.</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
&amp; &amp;&amp;\frac{1}{n}E \Bigg ( &amp;&amp; \Bigg \| X\bigl(\widehat{\theta}_{\mathrm{LS}}-\theta^{*}\bigr) \Bigg \|_{2}^{2} \Bigg) &amp;&amp;\lesssim \sigma^{2}\frac{r}{m} &amp;&amp; \; \; \; \; \; \; \; \; \;\;r= rank(X&#39;X)

\\

&amp;\forall \delta&gt;0: &amp;&amp; {\frac{1}{n}} &amp;&amp;\|X(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})\|_{2}^{2} &amp;&amp;\lesssim\sigma^{2} \left({\frac{r+\log(\frac 1 \delta)}{n}} \right) 
\end{alignat}\]</span>
$$</p>
</div>
<ul>
<li>Remark</li>
</ul>
<p><span class="math inline">\(d \le n\)</span> 이며 <span class="math inline">\(rank(B) = rank \left( \frac{X&#39;X}{n}\right) =d\)</span> 일 때, 이하가 성립한다. 이때 <span class="math inline">\(\lambda_{\mathrm{min}}(B)\)</span> 는 <span class="math inline">\(B\)</span> 의 ev 중 최소인 값이며, 따라서 이에 <span class="math inline">\(\|\hat \theta_{LS} - \theta^\ast \|^2_2\)</span> 를 탐색하기 위해 thm 8.2. 를 바로 적용하는 것이 가능하다.</p>
<p>그러나 고차원 케이스에서는 <span class="math inline">\(d&gt;n\)</span> 일 때 <span class="math inline">\(\lambda_{\mathrm{min}}(B) = 0\)</span> 이므로 이 방법론을 쓸 수 없다. 따라서 MSE 의 형으로 <span class="math inline">\(\lambda_{\mathrm{min}}(B)\)</span> 의 bound 를 설정할 필요가 있기 때문에 가정이 추가적으로 필요하다.</p>
<p><span class="math display">\[
\begin{alignat}{2}
&amp; &amp;&amp; \lambda_{\mathrm{min}}(B) &amp;&amp;\Bigg \|\widehat{\theta}_{\mathrm{LS}}-\theta^{*}\Bigg \|_{2}^{2} &amp;&amp;\leq\left(\widehat{\theta}_{\mathrm{LS}}-\theta^{*}\right)^{\top}B&amp;&amp;(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})
\\
&amp; &amp;&amp; \lambda_{\mathrm{min}}(B) &amp;&amp;\Bigg \|\widehat{\theta}_{\mathrm{LS}}-\theta^{*}\Bigg \|_{2}^{2} &amp;&amp;\leq\left(\widehat{\theta}_{\mathrm{LS}}-\theta^{*}\right)^{\top}\frac{X&#39;X}{n}&amp;&amp;(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})
\\
&amp;\iff &amp;&amp; &amp;&amp; \Bigg \|{\widehat\theta}_{\mathrm{LS}}-\theta^{*} \Bigg \|_{2}^{2} &amp;&amp;\leq{\frac{1}{\lambda_{\mathrm{min}}(B)}}\cdot\frac{1}{n} &amp;&amp;\Bigg \|X({\widehat\theta}_{\mathrm{LS}}-\theta^{*}) \Bigg \|_{2}^{2}
\end{alignat}
\]</span></p>
<ul>
<li>Proof</li>
</ul>
<p>8.2 의 증명은 <strong>basic inequality</strong> 와 <strong>sup-out</strong> 테크닉에 의존.</p>
<ol style="list-style-type: decimal">
<li>Basic ineq.</li>
</ol>
<p>첫줄의 ineq. 는 의 정의에 의해 성립. 이때 <span class="math inline">\(\epsilon\)</span> 과 <span class="math inline">\(\hat \theta_{LS}\)</span> 는 dependent 하기 때문에 <span class="math inline">\(\frac{\epsilon^{\top}X(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})}{||X(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})||_{2}}\)</span> 를 control 하기가 어렵다는 것을 note. dependence structure 가 complicated 하다면 더더욱 그럴 것. 이 dependence 를 지워 없애기 위해 sup-out tachnique 를 사용. 이의 maximal ineq. 를 적용하는 것이 해당 문제 해결의 열쇠가 된다.</p>
$$
<span class="math display">\[\begin{array}
&amp;
&amp;
&amp;\|Y-X{\widehat{\theta}}_{\mathrm{LS}}\|_{2}^{2}
&amp;\leq
&amp;\|Y-X\theta^{*}\|_{2}^{2} 
&amp;=
\|\epsilon\|_{2}^{2}

\\
&amp;=
&amp; \| \epsilon + X(\theta^\ast - \hat \theta_{LS})\| &amp;
\\
&amp;=
&amp;\|\epsilon\|_{2}^{2}+2\epsilon^{\mathsf{T}}X(\theta^{*}-{\widehat{\theta}}_{\mathsf{L S}})+\|X(\theta^{*}-{\widehat{\theta}}_{\mathsf{L S}})\|_{2}^{2} &amp;&amp;

\\

\iff
&amp;
&amp;\|X(\theta^{*}-\hat{\theta}_{\mathrm{LS}})\|_{2}^{2}\;
&amp;\leq
&amp;\;2\epsilon^{\top}X(\hat{\theta}_{\mathrm{LS}}-\theta^{*})
&amp;
&amp;=
&amp;2\|X(\hat{\theta}_{\mathrm{LS}}-\theta^{*})\|_{2}\times\frac{\epsilon^{\top}X(\hat{\theta}_{\mathrm{LS}}-\theta^{*})}{\|X(\hat{\theta}_{\mathrm{LS}}-\theta^{*})\|_{2}}



\end{array}\]</span>
<p>$$</p>
<ol start="2" style="list-style-type: decimal">
<li>Sup-Out</li>
</ol>
<p><span class="math inline">\(X\)</span>의 column span 의 orthonormal basis 를 <span class="math inline">\(\Psi = [\psi_1, \cdots, \psi_r] \in \mathbb R^{n \times r}\)</span> 라고 하자. (SVD 를 생각하자.) 특히, <span class="math inline">\(\exists \nu \in \mathbb R^r : X(\hat \theta_{LS} - \theta^\ast) = \Psi \nu\)</span>. 이때 이 <span class="math inline">\(\nu\)</span> 에 대해</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
&amp;
&amp;&amp;\frac{\epsilon^{\top}X(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})}{||X(\widehat{\theta}_{\mathrm{LS}}-\theta^{*})||_{2}}
=\frac{\epsilon^{\top}\Psi\nu}{||\Psi \nu||_{2}}
&amp;&amp;=\frac{\epsilon^{\top}\Psi\nu}{||\nu||_{2}}

\\
&amp;
&amp;&amp;
&amp;&amp;=\frac{\tilde \epsilon^{\top}\Psi}{||\nu||_{2}}
&amp;&amp;\le&amp;&amp;\sup\limits_{u\in\mathbb{R}^{r}:||u||_{2}=1}\tilde{\epsilon}^{\top}u

\\
&amp; \iff
&amp;&amp;
&amp;&amp;||X(\theta^{*}-\widehat{\theta}_{\mathrm{LS}})||_{2}^{2}
&amp;&amp;\leq4 \cdot &amp;&amp;\sup\limits_{u\in\mathrm{R}^{r}:\|u\|_{2}=1}(\tilde{\epsilon}^{\mathsf{T}}u)^{2}
\end{alignat}\]</span>
$$</p>
<p>since <span class="math inline">\(\forall u \in \mathbb S^{r-1} : u^{\textsf{T}}\Psi^{\textsf{T}}\Psi u=u^{\textsf{T}}u=1\)</span>,<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> <span class="math inline">\(\forall s \in \mathbb R: \mathbb{E}[e^{s{\tilde{\epsilon}}^{\top}}u]=\mathbb{E}[e^{s\epsilon^{\top}\Psi u}]\leq e^{{\frac{s^{2}\sigma^2}{2}}}\)</span>.</p>
<p>이는 곧 <span class="math inline">\(\tilde \epsilon \in SG(\sigma^2)\)</span> 이라는 소리. 이때 <span class="math inline">\(X\sim SG(\sigma^2) \Longrightarrow Var(X) \le \sigma^2\)</span> (Lecture 2). 따라서 CS ineq. 에 의해 <span class="math inline">\(\mathbb{E}\left[\sup\limits_{u\in\mathbb{R}^{r}:\|u\|_{2}=1} ({\tilde{\epsilon}}^{\mathsf{T}}u)^{2}\right] \leq\sum\limits_{i=1}^{r}\mathbb{E}\left[{\widetilde{\epsilon}}_{i}^{2}\right]\leq r\sigma^{2}\)</span>. 이것이 thm 8.2. 의 첫번째 claim 을 증명.</p>
<p>bound in Probability 를 보이기 위해, 우리는 standard discretization argument 를 사용. <span class="math inline">\(N_{\frac12}\)</span> 를 <span class="math inline">\(\mathbb S^{r-1}\)</span> 의 <span class="math inline">\(\frac12\)</span>-covering 이라고 하자. 그러면 Lecture 4 에서의 결과물을 사용하는 것으로 <span class="math inline">\(\sup\limits_{u\in\mathbb{R}^{r}:\|u\|_{2}=1} \widetilde{\epsilon}^{\top}u\le2\operatorname*{max}_{u\in{N}_{\frac12}}\widetilde{\epsilon}^{\top}u\)</span> 가 얻어진다.</p>
<p>따라서 이하가 성립하며 이를 만족시키는 임의의 <span class="math inline">\(\delta\)</span> 를 잡았을때 이에서 파생되는 임의의 <span class="math inline">\(t\)</span> 를 얻는다.</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
\mathbb{P}(\|X(\theta^{*}-{\widehat{\theta}}_{LS})\|_{2}^{2}\geq t)&amp;\leq\mathbb{P}{\Big(}\max\limits_{u\in{ N}_{\frac12}}({\tilde{\epsilon}}^{T}u)^{2}\geq \frac t {16}{\Big)}
\\
&amp;\leq \Bigg|{ N}_{\frac 12} \Bigg|e^{-{\frac{t}{32 \sigma^2}}}
\\
&amp;\leq5^{r}e^{-{\frac{t}{32 \sigma^2}}}

\\

\exists \delta: &amp;\le \delta &amp;&amp;\iff \exists t: t \ge 32 \sigma^2 \left \{ r \log 5 + \log \left( \frac {1} {\delta} \right) \right \}
\end{alignat}\]</span>
$$</p>
<p>따라서 with probability at least <span class="math inline">\(1-\delta\)</span>, <span class="math inline">\(||X(\theta^{*}-\hat{\theta}_{\mathrm{LS}})||_{2}^{2}\lesssim \sigma^{2}\{r+\log \left (\frac1 \delta \right)\}\)</span>.</p>
<p><br>
<br>
<br></p>
<p><br>
<br>
<br></p>
</div>
</div>
<div id="sparse-linear-regression" class="section level3" number="8.8.3">
<h3><span class="header-section-number">8.8.3</span> Sparse linear regression</h3>
<p>이상을 통해 우리는 <span class="math inline">\(\frac d n \righarrow 0\)</span> 일 때 <span class="math inline">\(\hat \theta_{LS}\)</span> 가 consistent 함을 확인. 따라서 <span class="math inline">\(\frac d n \righarrow 0\)</span> 가 우리가 바랄 수 있는 최적의 condition. 특히 <span class="math inline">\(\frac d n\)</span> 이 0에서 bounded away 된 채로 남는다면 consistent estimator 를 획득하는 건 <strong>불가능</strong>함. (minimax point of view 에서는.) 이러한 이유로 <span class="math inline">\(d &gt; n\)</span> 상황에서 작업을 할 때는 unknown regression vector <span class="math inline">\(\theta^\ast\)</span> 에 추가적인 structure 을 얹는 게 필수가 됨. 이제 <span class="math inline">\(\theta^\ast\)</span> 의 대다수가 0이라는 조건인 <strong>sparse condition</strong> 에 대해서 논해보자.</p>
<p><br>
<br>
<br></p>
<div id="lasso" class="section level4" number="8.8.3.1">
<h4><span class="header-section-number">8.8.3.1</span> Lasso</h4>
<p>linear model (8.1)에서 <span class="math inline">\(\theta\)</span> 의 support set <span class="math inline">\(S(\theta^{*})=\{j\in\{1,\ldots, d\}:\theta_{j}^{*}\not=0\}\)</span> 이 cardinality <span class="math inline">\(s=|S(\theta^\ast)| \overset{substantially}{&lt;} d\)</span>, 즉 s-sparse 인 상황 가정. regularized estimator 로서, <strong>lasso estimator</strong> <span class="math inline">\(\widehat{\theta}_{\mathrm{lasso}}=\arg\min\limits_{\theta\in\mathbb{R}^{d}}\left\{\frac{1}{2n}\|Y-X\theta\|_{2}^{2}+\lambda_{n}\|\theta\|_{1}\right\}\)</span> 는 이러한 sparse structural assumption 에 대한 설명력을 가진다. lasso esimator 는 for many <span class="math inline">\(j \in \{1, \cdots, d\}:\hat_{lasso, \; j} = 0\)</span> 라는 <strong>sparcity property</strong> 를 가지며 이건 <span class="math inline">\(\lambda_n\)</span> 을 무엇으로 골랐는지에 의존한다. 위의 등식 (8.3)은 <strong>lasso problem</strong> 이라고 불리며, 이는 convex optimization problem 이고, computationally tractable.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-23" class="lemma"><strong>Lemma 8.1  (Basic inequality) </strong></span>lasso estimator 에 대한 basic ineq.</p>
<p><span class="math display">\[
{\frac{1}{2n}}\|X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})\|_{2}^{2}\ \leq\ {\frac{\epsilon^{\top}X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})}{n}}+\lambda_{n}(\|\theta^{*}\|_{1}-\|\widehat{\theta}_{\mathrm{lasso}}\|_{1})
\]</span></p>
</div>
<ul>
<li>Proof.</li>
</ul>
<p>Lasso 의 정의에 의해,</p>
<p><span class="math display">\[
\frac{1}{2n}\Vert Y-X\widehat{\theta}_{\mathrm{lasso}}\Vert_{2}^{2}+\lambda_{n}\Vert\widehat{\theta}_{\mathrm{lasso}}\Vert_{1}\ \leq\ \frac{1}{2n}\Vert Y-X\theta^{*}\Vert_{2}^{2}+\lambda_{n}\Vert\theta^{*}\Vert_{1} = \frac{1}{2n} ||\epsilon||_2^2 + \lambda_n ||\theta^\ast ||_1
\]</span></p>
<p>그리고 MSE 를 확장하는 것으로</p>
<p><span class="math display">\[
{\frac{1}{2n}}\|Y-X{\widehat{\theta}}_{\mathrm{lasso}}\|_{2}^{2}\ =\ {\frac{1}{2n}}\|\epsilon\|_{2}^{2}+{\frac{1}{2n}}\|X({\widehat{\theta}}_{\mathrm{lasso}}-\theta^{*})\|_{2}^{2}+{\frac{\epsilon^{\top}X(\theta^{*}-{\widehat{\theta}}_{\mathrm{laaso}})}{n}}
\]</span></p>
<p>둘을 복합.</p>
<p><br>
<br>
<br></p>
</div>
<div id="slow-convergence-rate" class="section level4" number="8.8.3.2">
<h4><span class="header-section-number">8.8.3.2</span> Slow convergence rate</h4>
<p>위에서의 basic inequality (lemma 8.3) 이 주어졌을 때, 우리는 lasso estimator <span class="math inline">\(\hat \theta_{lasso}\)</span> 의 consistency 를 보장하는 sufficient condition 생성 가능. 이는 deterministic bound 를 구하는 것부터 시작함.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 8.14  (Slow Convergence Rate) </strong></span><span class="math inline">\(X_j\)</span> 를 <span class="math inline">\(X\)</span> 의 j-th column 이라고 하고, <span class="math inline">\(\lambda_{n}\,\geq\,\left|\left|\frac{X^{\top}\epsilon}{n}\right|\right|_{\infty}\ = \max_{i \le j \le d}\left|\frac{X_{j}^{\textsf{T}}\epsilon}{n}\right|\)</span> 라고 가정하자. 이 때 lasso estimator 는 이하를 만족.</p>
<p><span class="math display">\[
\frac{1}{n} \Bigg\| X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*}) \Bigg\| _{2}^{2}\leq4\lambda_{n} \Bigg\| \theta^{*} \Bigg\| _{1}
\]</span></p>
</div>
<ul>
<li>Proof:</li>
</ul>
<p>$$
<span class="math display">\[\begin{align}
\frac{1}{2n}||X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})||_{2}^{2}\ &amp;\leq\ \frac{\epsilon^{\top}X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})}{n}
&amp;&amp;+\lambda_{n}(||\theta^{*}||_{1}-||\widehat{\theta}_{\mathrm{lasso}}||_{1})

\\

&amp;\overset{(i)}{\le} {\frac{1}{n}}\|\epsilon^{\mathsf{T}}X\|_{\infty}\|\widehat{\theta}_{\mathrm{lasso}}-\theta^{*}\|_{1}
&amp;&amp;+\lambda_{n}(\|\theta^{*}\|_{1}-\|\widehat{\theta}_{\mathrm{lasso}}\|_{1})

\\

&amp;\overset{(ii)}{\le}\frac{1}{n}||\epsilon^{\top}X||_{\infty}(||\widehat{\theta}_{\mathrm{lasso}}||_{1}+||\theta^{*}||_{1})
&amp;&amp;+\lambda_{n}(||\theta^{*}||_{1}-||\widehat{\theta}_{\mathrm{lasso}}||_{1})

\\

&amp;= \|\widehat\theta_{\mathrm{lasso}}\|_{1}\left(\frac{1}{n}\|\epsilon^{\mathsf{T}}X\|_{\infty}-\lambda_{n}\right)
&amp;&amp;+\|\theta^{*}\|_{1}\left(\frac{1}{n}\|\epsilon^{\mathsf{T}}X\|_{\infty}+\lambda_{n}\right)

\\

&amp;\stackrel{(iii)}{\le}2\lambda_{n}\vert\vert\theta^{*}\|_{1}
\end{align}\]</span>
$$</p>
<ol style="list-style-type: decimal">
<li>휠더 부등식</li>
<li>triangle ineq.</li>
<li>(8.4) 의 <span class="math inline">\(\lambda_n\)</span> 에 대해 걸었던 condition.</li>
</ol>
<p>th, 8.4. 의 error bound 는 (8.4) 에서 <span class="math inline">\(\lambda_n\)</span> 에 대해 걸었던 condition 에 의존. 이제 좀 더 자세히 살펴보자.</p>
<ul>
<li><span class="math inline">\(lambda_n\)</span> 의 good choice 는 무엇인가?</li>
</ul>
<p>랜덤벡터 <span class="math inline">\(\epsilon \in SG(\sigma^2)\)</span> 이었음을 상기. 이제 <span class="math inline">\(\exists C&gt;0:\max\limits_{1\le j\le d} \|X_i\|_2 \le C\sqrt n\)</span> 라고 가정 assume 하자. 이때 <span class="math inline">\(\forall n, d:\max\limits_{ 1 \le j \le d}\left\{\frac{1}{n}\sum_{i=1}^{n}X_{i j}^{2}\right\}\le C\)</span> 가 성립한다. 이를 통해 standard argument 를 만들수 있다:</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
\mathbb{P}\!\left(\frac{1}{n}||\epsilon^{\textsf{T}}X||_{\infty}\geq t\right)\ 
&amp;= \mathbb{P}\!\left(\max\limits_{1\le j \le d} |X_{j}^{\textsf{T}}\epsilon|\geq\,t n\right)

\\
&amp;\leq\mathrm{~}\sum_{j=1}^{d}\mathbb{P}(|X_{j}^{\top}\epsilon|\geq\,t n)



\\

&amp;=\;\sum_{j=1}^{d}\mathbb{P}\left({\frac{|X_{j}^{\top}\epsilon|}{||X_{j}||_{2}}}\geq\,{\frac{t n}{||X_{j}||_{2}}}\right)


\\
&amp;\leq\ 2d\exp\left(-\,\frac{n^{2}t^{2}}{2\sigma^{2}\,\max\limits_{1\le j\le d}\,||X_{j}||^{2}}\right)



\\
&amp;\leq\ 2d\exp\left(-\,\frac{n t^{2}}{2C^{2}\sigma^{2}}\right) &amp;&amp;=\delta

\end{alignat}\]</span>
$$</p>
<p>마지막 ineq. 는 시작 전 더해둔 assumption <span class="math inline">\(\max\limits_{1\leq j\leq d}\|X_{j}\|_{2}\leq C{\sqrt{n}}.\)</span> 에 의해서 성립. 이제 <span class="math inline">\(t=\lambda_{n}^{*}=\sqrt{\frac{2\sigma^{2}C^{2}}{n} \left \{\log(\frac 1 \delta)+\log(\frac 2d) \right\}}\)</span> 를 하나 고르는 것으로, 우리는 with probability <span class="math inline">\(1-\delta\)</span> 에 의해 <span class="math inline">\({\frac{1}{n}}\|\epsilon^{\mathsf{T}}X\|_{\infty}\leq\lambda_{n}^{*}\)</span> 가 성립한다는 사실을 파악할 수 있다.</p>
<p>따라서 <span class="math inline">\(\delta = \frac 1n\)</span> 으로 잡는 것으로, thm 8.4 를 적용하는 것으로 <span class="math inline">\(\lambda_n^\ast\)</span> 가 주어진 lasso estimator 는 이하를 보장한다.</p>
<p><span class="math display">\[
{\frac{1}{n}}\|X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})\|_{2}^{2}\lesssim \|\theta^{*}\|_{1} \cdot \sigma\sqrt{\frac{\log(d)+\log(n)}{n}}
\]</span></p>
<p>여기에 추가로 <span class="math inline">\(\max\limits_{1\leq j\leq d}\left|\theta_{j}^{\ast}\right|\)</span> 가 uniformly bounded 되어 있다고 suppose 한다면? 그 경우 <span class="math inline">\(\theta^\ast\)</span> 의 <span class="math inline">\(s\)</span>-sparcity 하에서, <span class="math inline">\(s \sqrt{\frac {\log(d)} n} \rightarrow 0\)</span> 이 1번이라도 발생한 순간 MSE 는 0으로 간다.</p>
<ul>
<li>Parameter Estimation</li>
</ul>
<p>위에서 언급되었 듯이, <span class="math inline">\(\d \le n\)</span> 이며 <span class="math inline">\(\lambda_{\mathrm{min}} \left(\frac{X^{\textsf{T}}X}n \right)\;\geq\;C_{\mathrm{min}}\;\gt \;0,\)</span> 일 경우, 앞의 결과는 이하를 보장한다. 안타깝게도 이 전략은 <span class="math inline">\(d&gt;n\)</span> 인 경우에는 작동하지 않는다. <span class="math inline">\(X&#39;X\)</span> 가 rank-deficient 가 되어 버리므로.</p>
<p><span class="math display">\[
||{\widehat\theta}_{\mathrm{lasso}}-\theta^{*}||_{2}^{2} \lesssim {\frac{||\theta^{*}||_1}{C_{\mathrm{min}}}} \cdot \sigma\sqrt{\frac{\log(d)+\log(n)}{n}}
\]</span></p>
<p><br>
<br>
<br></p>
</div>
<div id="fast-convergence-rate" class="section level4" number="8.8.3.3">
<h4><span class="header-section-number">8.8.3.3</span> Fast Convergence Rate</h4>
<p>디자인 매트릭스 <span class="math inline">\(X\)</span> 에 추가적인 assumption 을 붙이는 것으로 좀 더 빠른 convergence rate 를 얻는 것이 가능. 여기선 Restricted Ev (RE) condition 을 사용할 것. <span class="math inline">\(\{ 1, \cdots, d\}\)</span> 의 subset <span class="math inline">\(S\)</span> 에 대해 <span class="math inline">\(Z_S = (Z_{S,1}, \cdots, Z_{S, d}) \in \mathbb R^d)\)</span> 이며 <span class="math inline">\(\forall j \in S:Z_{S,j} = Z_j\)</span>, o.w. <span class="math inline">\(Z_{S, j} = 0\)</span> 으로 정의.</p>
<p><span class="math inline">\(S^c\)</span> 를 <span class="math inline">\(S\)</span> 의 complement 로 잡자. 즉슨 <span class="math inline">\(Z_{S^c}\)</span> 도 <span class="math inline">\(Z_S\)</span> 와 유사하게 정의. 이때 <span class="math inline">\(\forall \alpha\ge1:C(\alpha,S)=\{\Delta\in\mathbb{R}^{d}:||\Delta_{S^{c}}||_{1}\leq\alpha\|\Delta_{S}\|_{1}\}\)</span>. 이 notation 들을 써서 이하 정의.</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 8.8  (RE condition) </strong></span>매트릭스 <span class="math inline">\(X\)</span> 는 이하를 만족할 경우, 패러미터 <span class="math inline">\((\alpha, \kappa)\)</span> 와 함께 <span class="math inline">\(S\)</span> 에 대해 <strong>RE condition</strong> 을 만족한다.</p>
<p><span class="math display">\[
forall \Delta\in C(\alpha,S):{\frac{1}{n}}\|X\Delta\|_{2}^{2}\geq\kappa\|\Delta\|_{2}^{2}
\]</span></p>
</div>
<ul>
<li>Remark.</li>
</ul>
<p>RE condition 에 대한 직관을 좀 얻어보자. cost difference <span class="math inline">\(\mathcal{L}_{n}(\widehat{\theta}_{\mathrm{lasso}})-\mathcal{L}_{n}(\theta^{*})=\frac{1}{2n}\vert\vert Y-X\widehat{\theta}_{\mathrm{lasso}}\vert\vert_{2}^{2}-\frac{1}{2n}\vert\vert Y-X\theta^{*}\vert\vert_{2}^{2}\)</span> 가 작다면, error vector <span class="math inline">\(\hat \theta_{lasso} - \theta^\ast\)</span> 도 또한 작다는 것을 장담할 수 있을까? 일반적으로 이는 그렇다고 할 수 없다. 특히 cost function <span class="math inline">\(\mathcal L_n(\theta)\)</span> 가 <strong>flat</strong> 할 경우에는 더더욱. 이 flat 상황을 피하기 위해 cost function <span class="math inline">\(\mathcal L_n (\theta)\)</span> 로 하여금 이의 optimum <span class="math inline">\(\hat \theta_{lasso}\)</span> 주위에서 높은 <strong>curvature</strong> 를 갖도록 하는 것이 요구됨. <strong>curvature</strong> 는 Hessian MAtrix <span class="math inline">\(\nabla^{2}{\mathcal{L}}_{n}(\theta)={\frac{1}{n}}X^{\top}X\)</span> 의 구조에 의해 결정됨. 만약 우리가 이 Hessian Matrix 의 ev 가 0 에서 bounded away 되었다고 장담할 수 있다면, 즉, <span class="math inline">\(\forall \Delta\in\mathbb{R}^{d}\setminus\{0\}:\frac{1}{n}||X\Delta||_{2}^{2}\geq\kappa||\Delta||_{2}^{2}\gt 0\)</span> 라면, 우리는 모든 지점에서 curvature 를 갖는다는 것을 확신할 수 있을 것.</p>
<p>하지만 고차원 상황 <span class="math inline">\(d&gt;n\)</span> 에서는 Hessian Matrix 는 0 ev 를 가져야만 하고, condition (8.7) (바로 위 상황) 은 성립할 수 없다. 역으로 우리는 <span class="math inline">\(C(\alpha, S)\)</span> 로 정의된 특정한 지점에서 cost function 이 curved 한지를 고려한다. 이 직관을 써서 RE condition 하에서 lasso estimator 에 대한 deterministic bound 를 생산할 수 있다. 이것이 아래의 thm.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-26" class="theorem"><strong>Theorem 8.15  (Fast Convergence Rate) </strong></span>linear model (8.1) 을 살피고, <span class="math inline">\(S=\{i : \theta^\ast_i \not = 0 \}\)</span> 에 대해 패러미터 <span class="math inline">\((3, \kappa)\)</span> 를 통해 RE condition 을 만족하는 <span class="math inline">\(X\)</span> 를 assume. 이때, <span class="math inline">\(S\)</span> 의 cardinality 가 <span class="math inline">\(s\)</span> 를 쓰자. 여기서 만약 <span class="math inline">\(\lambda_{n}\geq2\left\| \frac{X^{\textsf{T}}\epsilon}{n}\right\|_{\infty}\)</span> 가 성립한다면 이하가 성립.</p>
<p><span class="math display">\[
{\frac{1}{n}}||X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})||_{2}^{2}\leq9{\frac{s\lambda_{n}^{2}}{\kappa}}, \; \; \; \; \; \; \; \; \; \; \; ||\widehat{\theta}_{\mathrm{lasso}}-\theta^{*}||_{2} \le3{\frac{\sqrt{s}\lambda_{n}}{n}}
\]</span></p>
</div>
<ul>
<li>Proof:</li>
</ul>
<p>편의를 위해 <span class="math inline">\(\hat \Delta = \hat \theta_{lasso} - \theta^\ast\)</span>. 주어진 condition 하에서 <span class="math inline">\(\hat \Delta \in C(3, S)\)</span> 임을 먼저 보이자. basic ineq. (lemma 8.3) 을 사용하면 <span class="math inline">\({\frac{1}{2n}}||X\hat{\Delta}||_{2}^{2}\; \leq \;{\frac{\epsilon^{\top}X\hat{\Delta}}{n}}+\lambda_{n}(||\theta^{*}||_{1}-||\hat{\theta}_{\mathrm{lasso}}||_{1})\)</span> 임을 보일 수 있다. <span class="math inline">\(\theta^\ast\)</span> 가 s-sparse 이며 <span class="math inline">\(\hat \Delta = \hat \theta_{lasso}-\theta^\ast\)</span> 이므로 이하가 성립.</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
\|\theta^{*}\|_{1}-\|{\widehat{\theta}}_{\mathrm{lasso}}\|_{1}

&amp;=\|\theta_{S}^{*}\|_{1}-\|\widehat{\theta}_{\mathrm{lasso}}\|_{1}

\\

&amp;= \| \theta_{S}^{*}\|_{1}-\|{\hat{\Delta}}+\theta^{*}\|_{1} 

\\

&amp;=\|\theta_{S}^{*}\|_{1}
&amp;&amp;-\|\widehat{\Delta}_{S}+\theta_{S}^{*}\|_{1} - \| \hat \Delta_{S^c} \|_1

\\

&amp;\leq \|\hat{\Delta}_{S}\|_{1}+\ \|\widehat{\Delta}_{S}+\theta_{S}^{*}\|_{1} 
&amp;&amp;-  \|\widehat{\Delta}_{S}+\theta_{S}^{*}\|_{1}\nonumber-\|\widehat{\Delta}_{S^{c}}\|_{1}\nonumber \tag{triangle ineq.}

\\

&amp;= \| \hat \Delta_S \|_1 - \| \hat \Delta_{S^c}\|_1
\end{alignat}\]</span>
$$</p>
<p>$$
<span class="math display">\[\begin{alignat}{2}
0\;\leq\;\frac{1}{n}\Vert X\hat{\Delta}\Vert_{2}^{2}
&amp;\stackrel{(i)}{\le}\;\frac{2}{n}\Vert\epsilon^{\top}X\Vert_{\infty}\Vert\hat{\Delta}\Vert_{1}
&amp;&amp;+2\lambda_{n}(\Vert\hat{\Delta}_{S}\Vert_{1}-\Vert\hat{\Delta}_{S^{c}}\Vert_{1})

\tag{Holder}

\\

&amp;
{\stackrel{\mathrm{(ii)}}{\leq}}\ \ \lambda_{n}\|{\widehat\Delta}\|_{1}
&amp;&amp;+2\lambda_{n}(\|{\hat\Delta}_{S}\|_{1}-\|{\hat\Delta}_{S^{c}}\|_{1})

\tag{condition (8.8) on λ_n}

\\


&amp;= \lambda_n (\| \hat \Delta_S \|_1 + \| \hat \Delta_{S^c}\|_1 ) 
&amp;&amp;+2\lambda_n (\| \hat \Delta_S \|_1 - \| \hat \Delta_{S^c}\|_1 )

\\


&amp;=\lambda_{n}(3||\widehat\Delta_{S}||_1-||\widehat\Delta_{S^{c}}||_1)
\end{alignat}\]</span>
$$</p>
<p>이를 통해 <span class="math inline">\(\hat \Delta \in C(3,S)\)</span> 라고 결론지을 수 있으며, 우리는 <span class="math inline">\(X\)</span> 가 패러미터 <span class="math inline">\((3, \kappa)\)</span> 와 함께 RE condition 을 만족한다고 assume 했으므로, 이는 곧 <span class="math inline">\({\frac{1}{n}}\|X{\hat{\Delta}}\|_{2}^{2}\geq\kappa\|{\hat{\Delta}}\|_{2}^{2}\)</span> 라는 것을 보여준다.</p>
<p>앞의 과정을 다시 사용해서 이제</p>
<p>$$
<span class="math display">\[\begin{align}
{\frac{1}{n}}\|X{\widehat{\Delta}}\|_{2}^{2}\;&amp;\leq\;\lambda_{n}(3\|\widehat{\Delta}_{S}\|_{1}-\|\widehat{\Delta}_{S^{c}}\|_{1})\leq3\lambda_{n}\|\widehat{\Delta}_{S}\|_{1}

\\

&amp;\overset{(i)}{\le} 3 \lambda_{n}\sqrt{s}||\hat{\Delta}_{S}||_{2} \tag{1}

\\

&amp;\leq\ 3\lambda_{n}\sqrt{s}||\hat{\Delta}||_{2}

\\

&amp;\overset{(ii)}{\le}\frac{3\lambda_{n}\sqrt{s}}{\sqrt{n\kappa}}\|X\widehat{\Delta}\|_{2} \tag{ineq. (8.9)}

\\

\iff {\frac{1}{n}}\|X(\widehat{\theta}_{\mathrm{lasso}}-\theta^{*})\|_{2}^{2} &amp; \leq9{\frac{s\lambda_{n}^{2}}{\kappa}}
\end{align}\]</span>
$$</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\forall x\in\mathbb{R}^{d},\,\|x\|_{1}\leq{\sqrt{d}}\|x\|_{2}\)</span></li>
</ol>
<p>따라서 위의 마지막 ineq. 와 ineq. (8.9) 를 다시 한번 적용하면 증명 완료.</p>
<ul>
<li>Remark.</li>
</ul>
<p><span class="math inline">\(\lambda_{n} \asymp \sigma\sqrt{\frac{\log(d)+\log(n)}{n}}\)</span> 상황이라고 가정하자. 이 경우 design 매트릭스에 (8.5) 와 유사한 condition 을 두고 같은 argument 를 적용하면 <span class="math inline">\(\lambda_{n}\geq2 \left \| \frac{X^{\top}\epsilon}{n} \right \|_{\infty}\)</span> with probability at least <span class="math inline">\(1-\frac {1} {n^c}\)</span> for some constant <span class="math inline">\(c&gt;0\)</span> 임을 보일 수 있다. 이는 또한 (<span class="math inline">\(d \gg n\)</span> 일 때) <span class="math inline">\({\frac{1}{n}}\|X({\widehat{\theta}}_{\mathrm{lasso}}-\theta^{*})\|_{2}^{2}\lesssim{\frac{s\log(d)}{n}}\)</span> 라는 것으로도 이어지며, 따라서 RE condition 하에서 once <span class="math inline">\(\frac{s \log(d)}{n}\rightarrow 0\)</span> 라면 lasso estimator 는 consistent 하다.</p>
<p>이와 별개로 <span class="math inline">\(\lim\limits_{n \rightarrow \infty} \lambda_n = 0\)</span> 를 가정하는 것으로, thm 8.6 의 결과가 8.4의 결과를 former의 upper bound 가 <span class="math inline">\(\lambda_n\)</span> 이 아니라 <span class="math inline">\(\labmda_n^2\)</span> 에 의존한다는 것으로 진화시킨다는 것을 발견할 수 있다.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p><span class="math inline">\(\mathbb S^{r-1} =\{x\in\mathbb{R}^{r}: \|x\|_2 = 1\}\)</span><a href="linear-regression.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="principal-component-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="uniform-laws-of-large-numbers.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/212308_LR.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
