<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B.1 ANN | Self-Study</title>
  <meta name="description" content="B.1 ANN | Self-Study" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="B.1 ANN | Self-Study" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="lyric2249/lyric2249.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B.1 ANN | Self-Study" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="about-cluster-gcn.html"/>
<link rel="next" href="cnn.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Self</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> url: your book url like <span>https://bookdown.org/yihui/bookdown</span><span></span></a></li>
<li class="part"><span><b>I 20-02<span></span></b></span></li>
<li class="chapter" data-level="2" data-path="categorical.html"><a href="categorical.html"><i class="fa fa-check"></i><b>2</b> Categorical<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>2.1</b> Overview<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="overview.html"><a href="overview.html#data-type-and-statistical-analysis"><i class="fa fa-check"></i><b>2.1.1</b> Data Type and Statistical Analysis<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>3</b> Bayesian<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>3.1</b> Abstract<span></span></a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="abstract.html"><a href="abstract.html#변수의-독립성"><i class="fa fa-check"></i><b>3.1.1</b> 변수의 독립성<span></span></a></li>
<li class="chapter" data-level="3.1.2" data-path="abstract.html"><a href="abstract.html#교환가능성"><i class="fa fa-check"></i><b>3.1.2</b> 교환가능성<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="continual-aeassessment-method.html"><a href="continual-aeassessment-method.html"><i class="fa fa-check"></i><b>3.2</b> Continual Aeassessment Method<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="horseshoe-prior.html"><a href="horseshoe-prior.html"><i class="fa fa-check"></i><b>3.3</b> Horseshoe Prior<span></span></a></li>
</ul></li>
<li class="part"><span><b>II 21-01<span></span></b></span></li>
<li class="chapter" data-level="4" data-path="mathematical-stats.html"><a href="mathematical-stats.html"><i class="fa fa-check"></i><b>4</b> Mathematical Stats<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4.1</b> Inference<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference.html"><a href="inference.html#rao-blackwell-thm."><i class="fa fa-check"></i><b>4.1.1</b> Rao-Blackwell thm.<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="inference.html"><a href="inference.html#completeness"><i class="fa fa-check"></i><b>4.1.2</b> Completeness<span></span></a></li>
<li class="chapter" data-level="4.1.3" data-path="inference.html"><a href="inference.html#레만-쉐페-thm."><i class="fa fa-check"></i><b>4.1.3</b> 레만-쉐페 thm.<span></span></a></li>
<li class="chapter" data-level="4.1.4" data-path="inference.html"><a href="inference.html#raoblack"><i class="fa fa-check"></i><b>4.1.4</b> Rao-Blackwell thm.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="hypothesis-test.html"><a href="hypothesis-test.html"><i class="fa fa-check"></i><b>4.2</b> Hypothesis Test<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="power-fucntion.html"><a href="power-fucntion.html"><i class="fa fa-check"></i><b>4.3</b> Power Fucntion<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="power-fucntion.html"><a href="power-fucntion.html#significance-probability-p-value"><i class="fa fa-check"></i><b>4.3.1</b> Significance Probability (p-value)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="optimal-testing-method.html"><a href="optimal-testing-method.html"><i class="fa fa-check"></i><b>4.4</b> Optimal Testing Method<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="data-reduction.html"><a href="data-reduction.html"><i class="fa fa-check"></i><b>4.5</b> Data Reduction<span></span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="data-reduction.html"><a href="data-reduction.html#sufficiency-principle"><i class="fa fa-check"></i><b>4.5.1</b> Sufficiency Principle<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="borel-paradox.html"><a href="borel-paradox.html"><i class="fa fa-check"></i><b>4.6</b> Borel Paradox<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html"><i class="fa fa-check"></i><b>4.7</b> Neyman–Pearson lemma<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#overview-1"><i class="fa fa-check"></i><b>4.7.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="neymanpearson-lemma.html"><a href="neymanpearson-lemma.html#generalized-lrt"><i class="fa fa-check"></i><b>4.7.2</b> Generalized LRT<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="개념.html"><a href="개념.html"><i class="fa fa-check"></i><b>4.8</b> 개념<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>5</b> MCMC<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.1</b> Importance Sampling<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="importance-sampling.html"><a href="importance-sampling.html#independent-monte-carlo"><i class="fa fa-check"></i><b>5.1.1</b> Independent Monte Carlo<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>5.2</b> Markov Chain Monte Carlo<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mh-algorithm"><i class="fa fa-check"></i><b>5.2.1</b> MH Algorithm<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#random-walk-chains-most-widely-used"><i class="fa fa-check"></i><b>5.2.2</b> Random Walk Chains (Most Widely Used)<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#basic-gibbs-sampler"><i class="fa fa-check"></i><b>5.2.3</b> Basic Gibbs Sampler<span></span></a></li>
<li class="chapter" data-level="5.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#implementation"><i class="fa fa-check"></i><b>5.2.4</b> Implementation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html"><i class="fa fa-check"></i><b>5.3</b> Advanced MCMC (wk08)<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#data-augmentation"><i class="fa fa-check"></i><b>5.3.1</b> Data Augmentation<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#hit-and-run-algorithm"><i class="fa fa-check"></i><b>5.3.2</b> Hit-and-Run Algorithm<span></span></a></li>
<li class="chapter" data-level="5.3.3" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#metropolis-adjusted-langevin-algorithm"><i class="fa fa-check"></i><b>5.3.3</b> Metropolis-Adjusted Langevin Algorithm<span></span></a></li>
<li class="chapter" data-level="5.3.4" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#multiple-try-metropolis-algorithm"><i class="fa fa-check"></i><b>5.3.4</b> Multiple-Try Metropolis Algorithm<span></span></a></li>
<li class="chapter" data-level="5.3.5" data-path="advanced-mcmc-wk08.html"><a href="advanced-mcmc-wk08.html#reversible-jump-mcmc-algorithm"><i class="fa fa-check"></i><b>5.3.5</b> Reversible Jump MCMC Algorithm<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html"><i class="fa fa-check"></i><b>5.4</b> Auxiliary Variable MCMC<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#introduction"><i class="fa fa-check"></i><b>5.4.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#multimodal-target-distribution"><i class="fa fa-check"></i><b>5.4.2</b> Multimodal Target Distribution<span></span></a></li>
<li class="chapter" data-level="5.4.3" data-path="auxiliary-variable-mcmc.html"><a href="auxiliary-variable-mcmc.html#doubly-intractable-normalizing-constants"><i class="fa fa-check"></i><b>5.4.3</b> Doubly-intractable Normalizing Constants<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html"><i class="fa fa-check"></i><b>5.5</b> Approximate Bayesian Computation<span></span></a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#simulator-based-models"><i class="fa fa-check"></i><b>5.5.1</b> Simulator-Based Models<span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#approximate-bayesian-computation-abc"><i class="fa fa-check"></i><b>5.5.2</b> Approximate Bayesian Computation (ABC)<span></span></a></li>
<li class="chapter" data-level="5.5.3" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abcifying-monte-carlo-methods"><i class="fa fa-check"></i><b>5.5.3</b> ABCifying Monte Carlo Methods<span></span></a></li>
<li class="chapter" data-level="5.5.4" data-path="approximate-bayesian-computation.html"><a href="approximate-bayesian-computation.html#abc-mcmc-algorithm"><i class="fa fa-check"></i><b>5.5.4</b> ABC-MCMC Algorithm<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>5.6</b> Hamiltonian Monte Carlo<span></span></a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#introduction-to-hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>5.6.1</b> Introduction to Hamiltonian Monte Carlo<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html"><i class="fa fa-check"></i><b>5.7</b> Population Monte Carlo<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#adaptive-direction-sampling"><i class="fa fa-check"></i><b>5.7.1</b> Adaptive Direction Sampling<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#conjugate-gradient-mc"><i class="fa fa-check"></i><b>5.7.2</b> Conjugate Gradient MC<span></span></a></li>
<li class="chapter" data-level="5.7.3" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#parallel-tempering"><i class="fa fa-check"></i><b>5.7.3</b> Parallel Tempering<span></span></a></li>
<li class="chapter" data-level="5.7.4" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#evolutionary-mc"><i class="fa fa-check"></i><b>5.7.4</b> Evolutionary MC<span></span></a></li>
<li class="chapter" data-level="5.7.5" data-path="population-monte-carlo.html"><a href="population-monte-carlo.html#sequential-parallel-tempering"><i class="fa fa-check"></i><b>5.7.5</b> Sequential Parallel Tempering<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="stochastic-approximation-monte-carlo.html"><a href="stochastic-approximation-monte-carlo.html"><i class="fa fa-check"></i><b>5.8</b> Stochastic Approximation Monte Carlo<span></span></a></li>
<li class="chapter" data-level="5.9" data-path="review.html"><a href="review.html"><i class="fa fa-check"></i><b>5.9</b> Review<span></span></a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="review.html"><a href="review.html#wk01"><i class="fa fa-check"></i><b>5.9.1</b> Wk01<span></span></a></li>
<li class="chapter" data-level="5.9.2" data-path="review.html"><a href="review.html#wk03"><i class="fa fa-check"></i><b>5.9.2</b> wk03<span></span></a></li>
<li class="chapter" data-level="5.9.3" data-path="review.html"><a href="review.html#wk04-05"><i class="fa fa-check"></i><b>5.9.3</b> wk04, 05<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="else.html"><a href="else.html"><i class="fa fa-check"></i><b>5.10</b> Else<span></span></a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="else.html"><a href="else.html#hw4.-rasch-model"><i class="fa fa-check"></i><b>5.10.1</b> Hw4. Rasch Model<span></span></a></li>
<li class="chapter" data-level="5.10.2" data-path="else.html"><a href="else.html#da-example-mvn"><i class="fa fa-check"></i><b>5.10.2</b> DA) Example: MVN<span></span></a></li>
<li class="chapter" data-level="5.10.3" data-path="else.html"><a href="else.html#bayesian-adaptive-clinical-trial-with-delayed-outcomes"><i class="fa fa-check"></i><b>5.10.3</b> Bayesian adaptive clinical trial with delayed outcomes<span></span></a></li>
<li class="chapter" data-level="5.10.4" data-path="else.html"><a href="else.html#nmar의-종류"><i class="fa fa-check"></i><b>5.10.4</b> NMAR의 종류<span></span></a></li>
<li class="chapter" data-level="5.10.5" data-path="else.html"><a href="else.html#wk10-bayesian-model-selection"><i class="fa fa-check"></i><b>5.10.5</b> wk10) Bayesian Model Selection<span></span></a></li>
<li class="chapter" data-level="5.10.6" data-path="else.html"><a href="else.html#autologistic-model"><i class="fa fa-check"></i><b>5.10.6</b> Autologistic model<span></span></a></li>
<li class="chapter" data-level="5.10.7" data-path="else.html"><a href="else.html#wk10-bayesian-model-averaging"><i class="fa fa-check"></i><b>5.10.7</b> wk10) Bayesian Model Averaging<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mva.html"><a href="mva.html"><i class="fa fa-check"></i><b>6</b> MVA<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html"><i class="fa fa-check"></i><b>6.1</b> Overview of mva (not ended)<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#notation"><i class="fa fa-check"></i><b>6.1.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#summary-statistics"><i class="fa fa-check"></i><b>6.1.2</b> Summary Statistics<span></span></a></li>
<li class="chapter" data-level="6.1.3" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#statistical-inference-on-correlation"><i class="fa fa-check"></i><b>6.1.3</b> Statistical Inference on Correlation<span></span></a></li>
<li class="chapter" data-level="6.1.4" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#standardization"><i class="fa fa-check"></i><b>6.1.4</b> Standardization<span></span></a></li>
<li class="chapter" data-level="6.1.5" data-path="overview-of-mva-not-ended.html"><a href="overview-of-mva-not-ended.html#missing-value-treatment"><i class="fa fa-check"></i><b>6.1.5</b> Missing Value Treatment<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html"><i class="fa fa-check"></i><b>6.2</b> Multivariate Nomral (wk2)<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#overview-2"><i class="fa fa-check"></i><b>6.2.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#spectral-decomposition"><i class="fa fa-check"></i><b>6.2.2</b> Spectral Decomposition<span></span></a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#properties-of-mvn"><i class="fa fa-check"></i><b>6.2.3</b> Properties of MVN<span></span></a></li>
<li class="chapter" data-level="6.2.4" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#chi2-distribution"><i class="fa fa-check"></i><b>6.2.4</b> <span class="math inline">\(\Chi^2\)</span> distribution<span></span></a></li>
<li class="chapter" data-level="6.2.5" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#linear-combination-of-random-vectors"><i class="fa fa-check"></i><b>6.2.5</b> Linear Combination of Random Vectors<span></span></a></li>
<li class="chapter" data-level="6.2.6" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#multivariate-normal-likelihood"><i class="fa fa-check"></i><b>6.2.6</b> Multivariate Normal Likelihood<span></span></a></li>
<li class="chapter" data-level="6.2.7" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#sampling-distribtion-of-bar-pmb-y-s"><i class="fa fa-check"></i><b>6.2.7</b> Sampling Distribtion of <span class="math inline">\(\bar {\pmb y}, S\)</span><span></span></a></li>
<li class="chapter" data-level="6.2.8" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#assessing-normality"><i class="fa fa-check"></i><b>6.2.8</b> Assessing Normality<span></span></a></li>
<li class="chapter" data-level="6.2.9" data-path="multivariate-nomral-wk2.html"><a href="multivariate-nomral-wk2.html#power-transformation"><i class="fa fa-check"></i><b>6.2.9</b> Power Transformation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html"><i class="fa fa-check"></i><b>6.3</b> Inference about Mean Vector (wk3)<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#overview-3"><i class="fa fa-check"></i><b>6.3.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#confidence-region"><i class="fa fa-check"></i><b>6.3.2</b> 1. Confidence Region<span></span></a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#simultaneous-ci"><i class="fa fa-check"></i><b>6.3.3</b> 2. Simultaneous CI<span></span></a></li>
<li class="chapter" data-level="6.3.4" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#note-bonferroni-multiple-comparison"><i class="fa fa-check"></i><b>6.3.4</b> 3. Note: Bonferroni Multiple Comparison<span></span></a></li>
<li class="chapter" data-level="6.3.5" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#large-sample-inferences-about-a-mean-vector"><i class="fa fa-check"></i><b>6.3.5</b> 4. Large Sample Inferences about a Mean Vector<span></span></a></li>
<li class="chapter" data-level="6.3.6" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#profile-analysis-wk4-5"><i class="fa fa-check"></i><b>6.3.6</b> 1. Profile Analysis (wk4, 5)<span></span></a></li>
<li class="chapter" data-level="6.3.7" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#test-for-linear-trend"><i class="fa fa-check"></i><b>6.3.7</b> 2. Test for Linear Trend<span></span></a></li>
<li class="chapter" data-level="6.3.8" data-path="inference-about-mean-vector-wk3.html"><a href="inference-about-mean-vector-wk3.html#inferences-about-a-covariance-matrix"><i class="fa fa-check"></i><b>6.3.8</b> 3. Inferences about a Covariance Matrix<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html"><i class="fa fa-check"></i><b>6.4</b> Comparison of Several MV Means (wk5)<span></span></a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#paired-comparison"><i class="fa fa-check"></i><b>6.4.1</b> Paired Comparison<span></span></a></li>
<li class="chapter" data-level="6.4.2" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-mean-vectors-from-two-populations"><i class="fa fa-check"></i><b>6.4.2</b> Comparing Mean Vectors from Two Populations<span></span></a></li>
<li class="chapter" data-level="6.4.3" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#profile-analysis-for-g2"><i class="fa fa-check"></i><b>6.4.3</b> Profile Analysis (for <span class="math inline">\(g=2\)</span>)<span></span></a></li>
<li class="chapter" data-level="6.4.4" data-path="comparison-of-several-mv-means-wk5.html"><a href="comparison-of-several-mv-means-wk5.html#comparing-several-multivariate-population-means"><i class="fa fa-check"></i><b>6.4.4</b> Comparing Several Multivariate Population Means<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html"><i class="fa fa-check"></i><b>6.5</b> Multivariate Multiple Regression (wk6)<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#overview-4"><i class="fa fa-check"></i><b>6.5.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#multivariate-multiple-regression"><i class="fa fa-check"></i><b>6.5.2</b> Multivariate Multiple Regression<span></span></a></li>
<li class="chapter" data-level="6.5.3" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5.3</b> Hypothesis Testing<span></span></a></li>
<li class="chapter" data-level="6.5.4" data-path="multivariate-multiple-regression-wk6.html"><a href="multivariate-multiple-regression-wk6.html#example"><i class="fa fa-check"></i><b>6.5.4</b> Example)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>6.6</b> PCA<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>6.7</b> Factor<span></span></a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="factor.html"><a href="factor.html#method-of-estimation"><i class="fa fa-check"></i><b>6.7.1</b> Method of Estimation<span></span></a></li>
<li class="chapter" data-level="6.7.2" data-path="factor.html"><a href="factor.html#factor-rotation"><i class="fa fa-check"></i><b>6.7.2</b> Factor Rotation<span></span></a></li>
<li class="chapter" data-level="6.7.3" data-path="factor.html"><a href="factor.html#varimax-criterion"><i class="fa fa-check"></i><b>6.7.3</b> Varimax Criterion<span></span></a></li>
<li class="chapter" data-level="6.7.4" data-path="factor.html"><a href="factor.html#factor-scores"><i class="fa fa-check"></i><b>6.7.4</b> Factor Scores<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html"><i class="fa fa-check"></i><b>6.8</b> Discrimination and Classification<span></span></a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#bayes-rule"><i class="fa fa-check"></i><b>6.8.1</b> Bayes Rule<span></span></a></li>
<li class="chapter" data-level="6.8.2" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-two-mv-n-populations"><i class="fa fa-check"></i><b>6.8.2</b> Classification with Two mv <span class="math inline">\(N\)</span> Populations<span></span></a></li>
<li class="chapter" data-level="6.8.3" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#evaluating-classification-functions"><i class="fa fa-check"></i><b>6.8.3</b> Evaluating Classification Functions<span></span></a></li>
<li class="chapter" data-level="6.8.4" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#classification-with-several-populations-wk13"><i class="fa fa-check"></i><b>6.8.4</b> Classification with several Populations (wk13)<span></span></a></li>
<li class="chapter" data-level="6.8.5" data-path="discrimination-and-classification.html"><a href="discrimination-and-classification.html#other-discriminant-analysis-methods"><i class="fa fa-check"></i><b>6.8.5</b> Other Discriminant Analysis Methods<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html"><i class="fa fa-check"></i><b>6.9</b> Clustering, Distance Methods, and Ordination<span></span></a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#overview-5"><i class="fa fa-check"></i><b>6.9.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="6.9.2" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#hierarchical-clustering"><i class="fa fa-check"></i><b>6.9.2</b> Hierarchical Clustering<span></span></a></li>
<li class="chapter" data-level="6.9.3" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#k-means-clustering"><i class="fa fa-check"></i><b>6.9.3</b> K-means Clustering<span></span></a></li>
<li class="chapter" data-level="6.9.4" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#군집의-평가방법"><i class="fa fa-check"></i><b>6.9.4</b> 군집의 평가방법<span></span></a></li>
<li class="chapter" data-level="6.9.5" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#clustering-using-density-estimation-wk14"><i class="fa fa-check"></i><b>6.9.5</b> Clustering using Density Estimation (wk14)<span></span></a></li>
<li class="chapter" data-level="6.9.6" data-path="clustering-distance-methods-and-ordination.html"><a href="clustering-distance-methods-and-ordination.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>6.9.6</b> Multidimensional Scaling (MDS)<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>7</b> Linear<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="overview-svd.html"><a href="overview-svd.html"><i class="fa fa-check"></i><b>7.1</b> Overview &amp; SVD<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="overview-svd.html"><a href="overview-svd.html#spectral-decomposition-1"><i class="fa fa-check"></i><b>7.1.1</b> Spectral Decomposition<span></span></a></li>
<li class="chapter" data-level="7.1.2" data-path="overview-svd.html"><a href="overview-svd.html#singular-value-decomposition-general-version"><i class="fa fa-check"></i><b>7.1.2</b> Singular value Decomposition: General-version<span></span></a></li>
<li class="chapter" data-level="7.1.3" data-path="overview-svd.html"><a href="overview-svd.html#singular-value-decomposition-another-version"><i class="fa fa-check"></i><b>7.1.3</b> Singular value Decomposition: Another-version<span></span></a></li>
<li class="chapter" data-level="7.1.4" data-path="overview-svd.html"><a href="overview-svd.html#quadratic-forms"><i class="fa fa-check"></i><b>7.1.4</b> Quadratic Forms<span></span></a></li>
<li class="chapter" data-level="7.1.5" data-path="overview-svd.html"><a href="overview-svd.html#partitioned-matrices"><i class="fa fa-check"></i><b>7.1.5</b> Partitioned Matrices<span></span></a></li>
<li class="chapter" data-level="7.1.6" data-path="overview-svd.html"><a href="overview-svd.html#geometrical-aspects"><i class="fa fa-check"></i><b>7.1.6</b> Geometrical Aspects<span></span></a></li>
<li class="chapter" data-level="7.1.7" data-path="overview-svd.html"><a href="overview-svd.html#column-row-and-null-space"><i class="fa fa-check"></i><b>7.1.7</b> Column, Row and Null Space<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7.2</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="introduction-1.html"><a href="introduction-1.html#what"><i class="fa fa-check"></i><b>7.2.1</b> What<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="introduction-1.html"><a href="introduction-1.html#random-vectors-and-matrices"><i class="fa fa-check"></i><b>7.2.2</b> Random Vectors and Matrices<span></span></a></li>
<li class="chapter" data-level="7.2.3" data-path="introduction-1.html"><a href="introduction-1.html#multivariate-normal-distributions"><i class="fa fa-check"></i><b>7.2.3</b> Multivariate Normal Distributions<span></span></a></li>
<li class="chapter" data-level="7.2.4" data-path="introduction-1.html"><a href="introduction-1.html#distributions-of-quadratic-forms"><i class="fa fa-check"></i><b>7.2.4</b> Distributions of Quadratic Forms<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>7.3</b> Estimation<span></span></a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="estimation.html"><a href="estimation.html#identifiability-and-estimability"><i class="fa fa-check"></i><b>7.3.1</b> Identifiability and Estimability<span></span></a></li>
<li class="chapter" data-level="7.3.2" data-path="estimation.html"><a href="estimation.html#estimation-least-squares"><i class="fa fa-check"></i><b>7.3.2</b> Estimation: Least Squares<span></span></a></li>
<li class="chapter" data-level="7.3.3" data-path="estimation.html"><a href="estimation.html#estimation-best-linear-unbiased"><i class="fa fa-check"></i><b>7.3.3</b> Estimation: Best Linear Unbiased<span></span></a></li>
<li class="chapter" data-level="7.3.4" data-path="estimation.html"><a href="estimation.html#estimation-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.4</b> Estimation: Maximum Likelihood<span></span></a></li>
<li class="chapter" data-level="7.3.5" data-path="estimation.html"><a href="estimation.html#estimation-minimum-variance-unbiased"><i class="fa fa-check"></i><b>7.3.5</b> Estimation: Minimum Variance Unbiased<span></span></a></li>
<li class="chapter" data-level="7.3.6" data-path="estimation.html"><a href="estimation.html#sampling-distributions-of-estimates"><i class="fa fa-check"></i><b>7.3.6</b> Sampling Distributions of Estimates<span></span></a></li>
<li class="chapter" data-level="7.3.7" data-path="estimation.html"><a href="estimation.html#generalized-least-squaresgls"><i class="fa fa-check"></i><b>7.3.7</b> Generalized Least Squares(GLS)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>7.4</b> One-Way ANOVA<span></span></a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="one-way-anova.html"><a href="one-way-anova.html#one-way-anova-1"><i class="fa fa-check"></i><b>7.4.1</b> One-Way ANOVA<span></span></a></li>
<li class="chapter" data-level="7.4.2" data-path="one-way-anova.html"><a href="one-way-anova.html#more-about-models"><i class="fa fa-check"></i><b>7.4.2</b> More About Models<span></span></a></li>
<li class="chapter" data-level="7.4.3" data-path="one-way-anova.html"><a href="one-way-anova.html#estimating-and-testing-contrasts"><i class="fa fa-check"></i><b>7.4.3</b> Estimating and Testing Contrasts<span></span></a></li>
<li class="chapter" data-level="7.4.4" data-path="one-way-anova.html"><a href="one-way-anova.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.4.4</b> Cochran’s Theorem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="testing.html"><a href="testing.html"><i class="fa fa-check"></i><b>7.5</b> Testing<span></span></a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="testing.html"><a href="testing.html#more-about-models-two-approaches-for-linear-model"><i class="fa fa-check"></i><b>7.5.1</b> More About Models: Two approaches for linear model<span></span></a></li>
<li class="chapter" data-level="7.5.2" data-path="testing.html"><a href="testing.html#testing-models"><i class="fa fa-check"></i><b>7.5.2</b> Testing Models<span></span></a></li>
<li class="chapter" data-level="7.5.3" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure"><i class="fa fa-check"></i><b>7.5.3</b> A Generalized Test Procedure<span></span></a></li>
<li class="chapter" data-level="7.5.4" data-path="testing.html"><a href="testing.html#testing-linear-parametric-functions"><i class="fa fa-check"></i><b>7.5.4</b> Testing Linear Parametric Functions<span></span></a></li>
<li class="chapter" data-level="7.5.5" data-path="testing.html"><a href="testing.html#theoretical-complements"><i class="fa fa-check"></i><b>7.5.5</b> Theoretical Complements<span></span></a></li>
<li class="chapter" data-level="7.5.6" data-path="testing.html"><a href="testing.html#a-generalized-test-procedure-1"><i class="fa fa-check"></i><b>7.5.6</b> A Generalized Test Procedure<span></span></a></li>
<li class="chapter" data-level="7.5.7" data-path="testing.html"><a href="testing.html#testing-single-degrees-of-freedom-in-a-given-subspace"><i class="fa fa-check"></i><b>7.5.7</b> Testing Single Degrees of Freedom in a Given Subspace<span></span></a></li>
<li class="chapter" data-level="7.5.8" data-path="testing.html"><a href="testing.html#breaking-ss-into-independent-components"><i class="fa fa-check"></i><b>7.5.8</b> Breaking SS into Independent Components<span></span></a></li>
<li class="chapter" data-level="7.5.9" data-path="testing.html"><a href="testing.html#general-theory"><i class="fa fa-check"></i><b>7.5.9</b> General Theory<span></span></a></li>
<li class="chapter" data-level="7.5.10" data-path="testing.html"><a href="testing.html#two-way-anova"><i class="fa fa-check"></i><b>7.5.10</b> Two-Way ANOVA<span></span></a></li>
<li class="chapter" data-level="7.5.11" data-path="testing.html"><a href="testing.html#confidence-regions"><i class="fa fa-check"></i><b>7.5.11</b> Confidence Regions<span></span></a></li>
<li class="chapter" data-level="7.5.12" data-path="testing.html"><a href="testing.html#tests-for-generalized-least-squares-models"><i class="fa fa-check"></i><b>7.5.12</b> Tests for Generalized Least Squares Models<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>7.6</b> Generalized Least Squares<span></span></a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#a-direct-solution-via-inner-products"><i class="fa fa-check"></i><b>7.6.1</b> A direct solution via inner products<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="flat.html"><a href="flat.html"><i class="fa fa-check"></i><b>7.7</b> Flat<span></span></a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="flat.html"><a href="flat.html#flat-1"><i class="fa fa-check"></i><b>7.7.1</b> 1.Flat<span></span></a></li>
<li class="chapter" data-level="7.7.2" data-path="flat.html"><a href="flat.html#solutions-to-systems-of-linear-equations"><i class="fa fa-check"></i><b>7.7.2</b> 2. Solutions to systems of linear equations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="unified-approach-to-balanced-anova-models.html"><a href="unified-approach-to-balanced-anova-models.html"><i class="fa fa-check"></i><b>7.8</b> Unified Approach to Balanced ANOVA Models<span></span></a></li>
</ul></li>
<li class="part"><span><b>III 21-02<span></span></b></span></li>
<li class="chapter" data-level="8" data-path="network-stats.html"><a href="network-stats.html"><i class="fa fa-check"></i><b>8</b> Network Stats<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>8.1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-2.html"><a href="introduction-2.html#types-of-network-analysis"><i class="fa fa-check"></i><b>8.1.1</b> Types of Network Analysis<span></span></a></li>
<li class="chapter" data-level="8.1.2" data-path="introduction-2.html"><a href="introduction-2.html#network-modeling-and-inference"><i class="fa fa-check"></i><b>8.1.2</b> Network Modeling and Inference<span></span></a></li>
<li class="chapter" data-level="8.1.3" data-path="introduction-2.html"><a href="introduction-2.html#network-processes"><i class="fa fa-check"></i><b>8.1.3</b> Network Processes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html"><i class="fa fa-check"></i><b>8.2</b> Descriptive Statistics of Networks<span></span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#vertex-and-edge-characteristics"><i class="fa fa-check"></i><b>8.2.1</b> Vertex and Edge Characteristics<span></span></a></li>
<li class="chapter" data-level="8.2.2" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#characterizing-network-cohesion"><i class="fa fa-check"></i><b>8.2.2</b> Characterizing Network Cohesion<span></span></a></li>
<li class="chapter" data-level="8.2.3" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#graph-partitioning"><i class="fa fa-check"></i><b>8.2.3</b> Graph Partitioning<span></span></a></li>
<li class="chapter" data-level="8.2.4" data-path="descriptive-statistics-of-networks.html"><a href="descriptive-statistics-of-networks.html#assortativity-and-mixing"><i class="fa fa-check"></i><b>8.2.4</b> Assortativity and Mixing<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html"><i class="fa fa-check"></i><b>8.3</b> Data Collection and Sampling<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#sampling-designs"><i class="fa fa-check"></i><b>8.3.1</b> Sampling Designs<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#coping-strategies"><i class="fa fa-check"></i><b>8.3.2</b> Coping Strategies<span></span></a></li>
<li class="chapter" data-level="8.3.3" data-path="data-collection-and-sampling.html"><a href="data-collection-and-sampling.html#big-data-solves-nothing"><i class="fa fa-check"></i><b>8.3.3</b> Big Data Solves Nothing<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html"><i class="fa fa-check"></i><b>8.4</b> Mathematical Models for Network Graphs<span></span></a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#classical-random-graph-models"><i class="fa fa-check"></i><b>8.4.1</b> Classical Random Graph Models<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#generalized-random-graph-models"><i class="fa fa-check"></i><b>8.4.2</b> Generalized Random Graph Models<span></span></a></li>
<li class="chapter" data-level="8.4.3" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#network-graph-models-based-on-mechanisms"><i class="fa fa-check"></i><b>8.4.3</b> Network Graph Models Based on Mechanisms<span></span></a></li>
<li class="chapter" data-level="8.4.4" data-path="mathematical-models-for-network-graphs.html"><a href="mathematical-models-for-network-graphs.html#assessing-significance-of-network-graph-characteristics"><i class="fa fa-check"></i><b>8.4.4</b> Assessing Significance of Network Graph Characteristics<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html"><i class="fa fa-check"></i><b>8.5</b> Introduction to ERGM<span></span></a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#exponential-random-graph-models"><i class="fa fa-check"></i><b>8.5.1</b> Exponential Random Graph Models<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="introduction-to-ergm.html"><a href="introduction-to-ergm.html#difficulty-in-parameter-estimation"><i class="fa fa-check"></i><b>8.5.2</b> Difficulty in Parameter Estimation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html"><i class="fa fa-check"></i><b>8.6</b> Parameter Estimation of ERGM<span></span></a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#current-methods-for-ergm"><i class="fa fa-check"></i><b>8.6.1</b> Current Methods for ERGM<span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#approximation-based-algorithm"><i class="fa fa-check"></i><b>8.6.2</b> Approximation-based Algorithm<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#auxiliary-variable-mcmc-based-approaches"><i class="fa fa-check"></i><b>8.6.3</b> Auxiliary Variable MCMC-based Approaches<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#varying-trunction-stochastic-approximation-mcmc"><i class="fa fa-check"></i><b>8.6.4</b> Varying Trunction Stochastic Approximation MCMC<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="parameter-estimation-of-ergm.html"><a href="parameter-estimation-of-ergm.html#conclusion"><i class="fa fa-check"></i><b>8.6.5</b> Conclusion<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html"><i class="fa fa-check"></i><b>8.7</b> ERGM for Dynamic Networks<span></span></a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#temporal-ergm-tergm-t-ergm"><i class="fa fa-check"></i><b>8.7.1</b> Temporal ERGM (TERGM, T ERGM)<span></span></a></li>
<li class="chapter" data-level="8.7.2" data-path="ergm-for-dynamic-networks.html"><a href="ergm-for-dynamic-networks.html#separable-temporal-ergm-stergm-st-ergm"><i class="fa fa-check"></i><b>8.7.2</b> Separable Temporal ERGM (STERGM, ST ERGM)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="latent-network-models.html"><a href="latent-network-models.html"><i class="fa fa-check"></i><b>8.8</b> Latent Network Models<span></span></a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-model"><i class="fa fa-check"></i><b>8.8.1</b> Latent Position Model<span></span></a></li>
<li class="chapter" data-level="8.8.2" data-path="latent-network-models.html"><a href="latent-network-models.html#latent-position-cluster-model"><i class="fa fa-check"></i><b>8.8.2</b> Latent Position Cluster Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html"><i class="fa fa-check"></i><b>8.9</b> Additive and Multiplicative Effects Network Models<span></span></a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#introduction-3"><i class="fa fa-check"></i><b>8.9.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="8.9.2" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#social-relations-regression"><i class="fa fa-check"></i><b>8.9.2</b> Social Relations Regression<span></span></a></li>
<li class="chapter" data-level="8.9.3" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#multiplicative-effects-models"><i class="fa fa-check"></i><b>8.9.3</b> Multiplicative Effects Models<span></span></a></li>
<li class="chapter" data-level="8.9.4" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#inference-via-posterior-approximation"><i class="fa fa-check"></i><b>8.9.4</b> Inference via Posterior Approximation<span></span></a></li>
<li class="chapter" data-level="8.9.5" data-path="additive-and-multiplicative-effects-network-models.html"><a href="additive-and-multiplicative-effects-network-models.html#discussion-and-example-with-r"><i class="fa fa-check"></i><b>8.9.5</b> Discussion and Example with R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html"><i class="fa fa-check"></i><b>8.10</b> Stochastic Block Models<span></span></a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html#stochastic-block-model"><i class="fa fa-check"></i><b>8.10.1</b> Stochastic Block Model<span></span></a></li>
<li class="chapter" data-level="8.10.2" data-path="stochastic-block-models.html"><a href="stochastic-block-models.html#mixed-membership-block-model-mmbm"><i class="fa fa-check"></i><b>8.10.2</b> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="high-dimension.html"><a href="high-dimension.html"><i class="fa fa-check"></i><b>9</b> High Dimension<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="introduction-4.html"><a href="introduction-4.html"><i class="fa fa-check"></i><b>9.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="9.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html"><i class="fa fa-check"></i><b>9.2</b> Concentration inequalities<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#motivation"><i class="fa fa-check"></i><b>9.2.1</b> Motivation<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#from-markov-to-chernoff"><i class="fa fa-check"></i><b>9.2.2</b> From Markov to Chernoff<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-variables"><i class="fa fa-check"></i><b>9.2.3</b> sub-Gaussian random variables<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#properties-of-sub-gaussian-random-variables"><i class="fa fa-check"></i><b>9.2.4</b> Properties of sub-Gaussian random variables<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#equivalent-definitions"><i class="fa fa-check"></i><b>9.2.5</b> Equivalent definitions<span></span></a></li>
<li class="chapter" data-level="9.2.6" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#sub-gaussian-random-vectors"><i class="fa fa-check"></i><b>9.2.6</b> Sub-Gaussian random vectors<span></span></a></li>
<li class="chapter" data-level="9.2.7" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#hoeffdings-inequality"><i class="fa fa-check"></i><b>9.2.7</b> Hoeffding’s inequality<span></span></a></li>
<li class="chapter" data-level="9.2.8" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#maximal-inequalities"><i class="fa fa-check"></i><b>9.2.8</b> Maximal inequalities<span></span></a></li>
<li class="chapter" data-level="9.2.9" data-path="concentration-inequalities.html"><a href="concentration-inequalities.html#section"><i class="fa fa-check"></i><b>9.2.9</b> </a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html"><i class="fa fa-check"></i><b>9.3</b> Concentration inequalities<span></span></a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#sub-exponential-random-variables"><i class="fa fa-check"></i><b>9.3.1</b> Sub-exponential random variables<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#bernsteins-condition"><i class="fa fa-check"></i><b>9.3.2</b> Bernstein’s condition<span></span></a></li>
<li class="chapter" data-level="9.3.3" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#mcdiarmids-inequality"><i class="fa fa-check"></i><b>9.3.3</b> McDiarmid’s inequality<span></span></a></li>
<li class="chapter" data-level="9.3.4" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#levys-inequality"><i class="fa fa-check"></i><b>9.3.4</b> Levy’s inequality<span></span></a></li>
<li class="chapter" data-level="9.3.5" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#quadratic-form"><i class="fa fa-check"></i><b>9.3.5</b> Quadratic form<span></span></a></li>
<li class="chapter" data-level="9.3.6" data-path="concentration-inequalities-1.html"><a href="concentration-inequalities-1.html#the-johnsonlindenstrauss-lemma"><i class="fa fa-check"></i><b>9.3.6</b> The Johnson–Lindenstrauss Lemma<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html"><i class="fa fa-check"></i><b>9.4</b> Metric entropy and its uses<span></span></a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#metric-space"><i class="fa fa-check"></i><b>9.4.1</b> Metric space<span></span></a></li>
<li class="chapter" data-level="9.4.2" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#covering-numbers-and-metric-entropy"><i class="fa fa-check"></i><b>9.4.2</b> Covering numbers and metric entropy<span></span></a></li>
<li class="chapter" data-level="9.4.3" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#packing-numbers"><i class="fa fa-check"></i><b>9.4.3</b> Packing numbers<span></span></a></li>
<li class="chapter" data-level="9.4.4" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-1"><i class="fa fa-check"></i><b>9.4.4</b> </a></li>
<li class="chapter" data-level="9.4.5" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-2"><i class="fa fa-check"></i><b>9.4.5</b> </a></li>
<li class="chapter" data-level="9.4.6" data-path="metric-entropy-and-its-uses.html"><a href="metric-entropy-and-its-uses.html#section-3"><i class="fa fa-check"></i><b>9.4.6</b> </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="covariance-estimation.html"><a href="covariance-estimation.html"><i class="fa fa-check"></i><b>9.5</b> Covariance estimation<span></span></a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="covariance-estimation.html"><a href="covariance-estimation.html#matrix-algebra-review"><i class="fa fa-check"></i><b>9.5.1</b> Matrix algebra review<span></span></a></li>
<li class="chapter" data-level="9.5.2" data-path="covariance-estimation.html"><a href="covariance-estimation.html#covariance-matrix-estimation-in-the-operator-norm"><i class="fa fa-check"></i><b>9.5.2</b> Covariance matrix estimation in the operator norm<span></span></a></li>
<li class="chapter" data-level="9.5.3" data-path="covariance-estimation.html"><a href="covariance-estimation.html#bounds-for-structured-covariance-matrices"><i class="fa fa-check"></i><b>9.5.3</b> Bounds for structured covariance matrices<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html"><i class="fa fa-check"></i><b>9.6</b> Matrix concentration inequalities<span></span></a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-calculus"><i class="fa fa-check"></i><b>9.6.1</b> Matrix calculus<span></span></a></li>
<li class="chapter" data-level="9.6.2" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#matrix-chernoff"><i class="fa fa-check"></i><b>9.6.2</b> Matrix Chernoff<span></span></a></li>
<li class="chapter" data-level="9.6.3" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#sub-gaussian-and-sub-exponential-matrices"><i class="fa fa-check"></i><b>9.6.3</b> Sub-Gaussian and sub-exponential matrices<span></span></a></li>
<li class="chapter" data-level="9.6.4" data-path="matrix-concentration-inequalities.html"><a href="matrix-concentration-inequalities.html#랜덤-매트릭스에-대한-hoeffding-and-bernstein-bounds"><i class="fa fa-check"></i><b>9.6.4</b> 랜덤 매트릭스에 대한 Hoeffding and Bernstein bounds<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>9.7</b> Principal Component Analysis<span></span></a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-1"><i class="fa fa-check"></i><b>9.7.1</b> PCA<span></span></a></li>
<li class="chapter" data-level="9.7.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#matrix-perturbation"><i class="fa fa-check"></i><b>9.7.2</b> Matrix Perturbation<span></span></a></li>
<li class="chapter" data-level="9.7.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#spiked-cov-model"><i class="fa fa-check"></i><b>9.7.3</b> Spiked Cov Model<span></span></a></li>
<li class="chapter" data-level="9.7.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#sparse-pca"><i class="fa fa-check"></i><b>9.7.4</b> sparse PCA<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>9.8</b> Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="linear-regression.html"><a href="linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>9.8.1</b> Problem formulation<span></span></a></li>
<li class="chapter" data-level="9.8.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimator-in-high-dimensions"><i class="fa fa-check"></i><b>9.8.2</b> Least Squares Estimator in high dimensions<span></span></a></li>
<li class="chapter" data-level="9.8.3" data-path="linear-regression.html"><a href="linear-regression.html#sparse-linear-regression"><i class="fa fa-check"></i><b>9.8.3</b> Sparse linear regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html"><i class="fa fa-check"></i><b>9.9</b> Uniform laws of large numbers<span></span></a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#motivation-1"><i class="fa fa-check"></i><b>9.9.1</b> Motivation<span></span></a></li>
<li class="chapter" data-level="9.9.2" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#a-uniform-law-via-rademacher-complexity"><i class="fa fa-check"></i><b>9.9.2</b> A uniform law via Rademacher complexity<span></span></a></li>
<li class="chapter" data-level="9.9.3" data-path="uniform-laws-of-large-numbers.html"><a href="uniform-laws-of-large-numbers.html#upper-bounds-on-the-rademacher-complexity"><i class="fa fa-check"></i><b>9.9.3</b> Upper bounds on the Rademacher complexity<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>10</b> Survival Analysis<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>10.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>10.2</b> </a></li>
<li class="chapter" data-level="10.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html"><i class="fa fa-check"></i><b>10.3</b> Counting Processes and Martingales<span></span></a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#conditional-expectation"><i class="fa fa-check"></i><b>10.3.1</b> Conditional Expectation<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#martingale"><i class="fa fa-check"></i><b>10.3.2</b> Martingale<span></span></a></li>
<li class="chapter" data-level="10.3.3" data-path="counting-processes-and-martingales.html"><a href="counting-processes-and-martingales.html#key-martingales-properties"><i class="fa fa-check"></i><b>10.3.3</b> Key Martingales Properties<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>10.4</b> </a></li>
<li class="chapter" data-level="10.5" data-path="cox-regression.html"><a href="cox-regression.html"><i class="fa fa-check"></i><b>10.5</b> Cox Regression<span></span></a></li>
<li class="chapter" data-level="10.6" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html"><i class="fa fa-check"></i><b>10.6</b> Filtration의 개념을 정복하자!<span></span></a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#random-process를-이야기-하기까지의-긴-여정의-요약"><i class="fa fa-check"></i><b>10.6.1</b> Random Process를 이야기 하기까지의 긴 여정의 요약<span></span></a></li>
<li class="chapter" data-level="10.6.2" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#ft-measurable"><i class="fa fa-check"></i><b>10.6.2</b> Ft-measurable<span></span></a></li>
<li class="chapter" data-level="10.6.3" data-path="filtration의-개념을-정복하자.html"><a href="filtration의-개념을-정복하자.html#epilogue"><i class="fa fa-check"></i><b>10.6.3</b> EPILOGUE<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>10.7</b> Concepts<span></span></a></li>
</ul></li>
<li class="part"><span><b>IV 22-01<span></span></b></span></li>
<li class="chapter" data-level="11" data-path="scikit.html"><a href="scikit.html"><i class="fa fa-check"></i><b>11</b> scikit<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>11.1</b> Linear Models<span></span></a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>11.1.1</b> Ordinary Least Squares<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>00-00<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="concepts-1.html"><a href="concepts-1.html"><i class="fa fa-check"></i><b>A</b> Concepts<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="autologistic.html"><a href="autologistic.html"><i class="fa fa-check"></i><b>A.1</b> Autologistics<span></span></a></li>
<li class="chapter" data-level="A.2" data-path="orderlogit.html"><a href="orderlogit.html"><i class="fa fa-check"></i><b>A.2</b> Ordered Logit<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="about-cluster-gcn.html"><a href="about-cluster-gcn.html"><i class="fa fa-check"></i><b>B</b> About Cluster-GCN<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>B.1</b> ANN<span></span></a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="ann.html"><a href="ann.html#training"><i class="fa fa-check"></i><b>B.1.1</b> Training<span></span></a></li>
<li class="chapter" data-level="B.1.2" data-path="ann.html"><a href="ann.html#problem"><i class="fa fa-check"></i><b>B.1.2</b> Problem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="cnn.html"><a href="cnn.html"><i class="fa fa-check"></i><b>B.2</b> CNN<span></span></a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="cnn.html"><a href="cnn.html#convolution-layer"><i class="fa fa-check"></i><b>B.2.1</b> Convolution Layer<span></span></a></li>
<li class="chapter" data-level="B.2.2" data-path="cnn.html"><a href="cnn.html#pooling"><i class="fa fa-check"></i><b>B.2.2</b> Pooling<span></span></a></li>
<li class="chapter" data-level="B.2.3" data-path="cnn.html"><a href="cnn.html#cnn-을-위한-back-propagation"><i class="fa fa-check"></i><b>B.2.3</b> CNN 을 위한 back-propagation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="graph-convolution-network.html"><a href="graph-convolution-network.html"><i class="fa fa-check"></i><b>B.3</b> Graph Convolution Network<span></span></a></li>
<li class="chapter" data-level="B.4" data-path="cluster-gcn.html"><a href="cluster-gcn.html"><i class="fa fa-check"></i><b>B.4</b> Cluster-GCN<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="cnn-1.html"><a href="cnn-1.html"><i class="fa fa-check"></i><b>C</b> CNN<span></span></a></li>
<li class="chapter" data-level="D" data-path="cnn-2.html"><a href="cnn-2.html"><i class="fa fa-check"></i><b>D</b> CNN<span></span></a></li>
<li class="chapter" data-level="E" data-path="cnn-3.html"><a href="cnn-3.html"><i class="fa fa-check"></i><b>E</b> CNN<span></span></a></li>
<li class="chapter" data-level="F" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>F</b> 01<span></span></a></li>
<li class="chapter" data-level="G" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>G</b> 02<span></span></a>
<ul>
<li class="chapter" data-level="G.1" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>G.1</b> 10.<span></span></a>
<ul>
<li class="chapter" data-level="G.1.1" data-path="section-8.html"><a href="section-8.html#stochastic-block-model-1"><i class="fa fa-check"></i><b>G.1.1</b> Stochastic Block Model<span></span></a></li>
<li class="chapter" data-level="G.1.2" data-path="section-8.html"><a href="section-8.html#likelihood-function-1"><i class="fa fa-check"></i><b>G.1.2</b> Likelihood function<span></span></a></li>
<li class="chapter" data-level="G.1.3" data-path="section-8.html"><a href="section-8.html#mixed-membership-block-model-mmbm-1"><i class="fa fa-check"></i><b>G.1.3</b> Mixed Membership Block Model (MMBM)<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Study</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ann" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">B.1</span> ANN<a href="ann.html#ann" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>딥러닝이란 수많은 머신러닝 방법론들 중의 한 갈래로, 실제 신경계를 모사하여 인간이 현실세계에서 데이터에 대한 labeling 을 수행하는 방법을 구현하는 것으로 이러한 프로세스를 컴퓨터로 하여금 진행하도록 하는 것을 의미한다. 인간이 세상을 인식할 때 인간의 감각세포가 받아들인 외부의 자극 input (입력값)은 신경계를 거쳐 해석되어 어떤 종류의 인식인지 output (결과값)으로 분류되어 뇌에 인식된다. 이 일 련의 프로세스를 모사적으로 컴퓨터로 구현하여 인간의 인식을 전뇌적으로 구현하고자 하는 것이 딥러닝의 기초적인 정의이다. 이러한 딥 러닝의 가장 핵심적인 알고리즘으로 꼽히는 것이 Artificial Neural Network (ANN) 으로, ANN 의 방법론을 수학의 언어를 빌려 설명하자면 이하와 같다.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{x} &amp;=\left[x_{1} x_{2} \ldots x_{M}\right]^{T} \\
s(\mathbf{x} ; \theta=\mathbf{w}, b) &amp;=\mathbf{w} \mathbf{x}+b \quad=\sum_{j=1}^{M} w_{j} x_{j}+b \\
y &amp;=\sigma\{s(\mathbf{x} ; \theta)\}
\end{aligned}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{x}\)</span> : 외부의 자극 input</p></li>
<li><p><span class="math inline">\(\mathbf{w}, b\)</span> : 신경계의 작동 메커니즘. <span class="math inline">\(\theta\)</span> 는 해당 구현에서의 패러미터인 <span class="math inline">\(\mathbf{w}, b\)</span> 의 실현값.</p></li>
<li><p><span class="math inline">\(y\)</span> : 뇌가 인식하는 결과값 output</p></li>
</ul>
<p>ANN 은 신경계의 분류 메커니즘이 linear transformation 이라는 가정을 깔고 시작한다. input 되는 외부의 자극을 linear 하게 조립하는 것 으로 인간이 해당 자극을 labeling 하기 직전에 마주하는 최종적인 자극의 형태를 만들어낼 수 있다는 것이 그 골자이다. 이때 자극 input 을 linear transformation 하므로, 자극 vector 를 linear transformation 할 coefficient 와 스칼라 연산을 할 constant 를 필요로 한다. 이때 coefficient 를 weight, w 로 명명하고, constant 를 bias, <span class="math inline">\(b\)</span> 로 명명하게 된다.</p>
<p>언급하였듯이 labeling 하기 전 인간의 신경계가 linear 하게 조립한 자극 자체는 아무런 의미를 갖지 않는다. 이렇게 도출된 계산결과를 무 엇으로 인식할 건지에 대한 규칙을 1 번 더 거친 후에야 비로소 인간이 어떻게 해당 자극을 labeling 하였는가에 대한 결과값을 얻을 수 있 다. 이 규칙이 바로 <span class="math inline">\(\sigma\)</span> 이며 이는 activation function 이라고 불린다. activation function 자체에는 unit step function, sigmoid function, cross entropy 등 다양한 종류가 존재하며 activation function 자체는 연구자가 임의로 선택하게 된다. sigmoid function 은 가장 자주 쓰였던 함수 중 하나이나 sigmoid 의 미분값은 <span class="math inline">\(0 \sim 0.25\)</span> 사이이기에 전달되는 weight 가 발산하거나 곡선의 기울기가 0이 되는 Vanishing Gradient Problem 이 발생한다. 해당 문제를 해결하기 위해 최근에는 임의로 설계된 함수인 ReLU 함수를 activation function 으로 많이 채용한다. 이 는 미분값이 0 혹은 1 로 연산자원을 적게 먹는다는 장점 때문에 자주 사용되는 함수이다.</p>
<p><img src="https://cdn.mathpix.com/cropped/2022_04_22_50eddabab3eaffe2ca79g-02.jpg?height=359&amp;width=615&amp;top_left_y=59&amp;top_left_x=308" /></p>
<p>기본적으로는 설명의 편의를 위해 sigmoid 함수를 사용하겠다.</p>
<p>이렇게 1 개의 인공적인 neuron 을 설계해냈지만 이것으로 문제가 한번에 해결될리는 없다. 연구자가 임의로 정한 weight 와 activation function 으로 문제가 한번에 풀린다면 이상적이겠으나 현실에서는 neuron 의 기능이 부족하여 문제를 한번에 해결하지 못하는 경우가 다 수이다. neuron 이 정확하게 작동하기 위해서는 이하의 2 가지 문제를 해결해야 한다.</p>
<ol style="list-style-type: decimal">
<li><p>Training: 주어진 training dataset <span class="math inline">\(\mathcal{X}\)</span> 를 이용하여 인공적으로 설계한 neuron 이 계산해내는 자극의 linear transformation 이 실제 목 표하는 neuron 의 작동기전과 일치하도록 패러미터 weight <span class="math inline">\(\mathbf{w}\)</span> 와 bias <span class="math inline">\(b\)</span> 를 조정.</p></li>
<li><p>Classification: neuron 이 작동 도중 계산해내는 자극의 linear transformation 을 activation function <span class="math inline">\(\sigma\)</span> 로 분류할 때 이것이 정답과 일 치할 수 있도록 <span class="math inline">\(\sigma\)</span> 를 조정</p></li>
</ol>
<p>이때 <span class="math inline">\(\sigma\)</span> 는 연구자가 직접 설정하므로 이터레이션을 거치며 성능향상을 시키는 과정에서 자동화를 시키기에 어려운 부분이 존재한다. 따라 서 ANN 에서 주요한 것은 training 과정에서 neuron 의 패러미터 weight <span class="math inline">\(\mathbf{w}\)</span> 와 bias <span class="math inline">\(b\)</span> 가 정답에 가까워지는 속도를 올리는 것이다.</p>
<div id="training" class="section level3 hasAnchor" number="13.1.1">
<h3><span class="header-section-number">B.1.1</span> Training<a href="ann.html#training" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>neuron 의 training 을 위해서는 우선 cost function <span class="math inline">\(C\)</span> 를 정의해야 한다. cost function <span class="math inline">\(C\)</span> 의 목적은 input <span class="math inline">\(x\)</span> 에 대한 정답인 <span class="math inline">\(t\)</span> 와 현재 보유하 고 있는 model 에서 계산해낸 output <span class="math inline">\(y\)</span> 사이의 차이를 계산해내는 것이다. 보편적으로 쓰이는 cost function 중 하나는 Sum of Squared Error (SSE) 이다. neuron 의 training 이란 곧 <span class="math inline">\(C\)</span> 의 값을 작게 만드는, 즉 실제값과 model 이 계산해낸 결과값 사이의 오차를 최소화시키는 <span class="math inline">\(\mathbf{w}, b\)</span> 를 찾아나가는 과정과 같다. 이러한 학습에는 주로 gradient descent method 가 사용된다. gradient descent method 를 사용해서 neuron 을 training 하는 상황을 가정할 경우 <span class="math inline">\(C\)</span> 를 <span class="math inline">\(\theta=\{\mathbf{w}, b\}\)</span> 각각에 대해 편미분 한 결과값을 얻어야 한다. 편의를 위해 activation function 은 sigmoid 로 사용하였다고 가정하자. 이 경우 이하가 성립한다.</p>
<p><span class="math display">\[
\begin{aligned}
w_{j} &amp;=w_{j}-\eta \frac{\partial C}{\partial w_{j}} \\
&amp;=w_{j}-\eta \frac{\partial C}{\partial y} \frac{\partial y}{\partial s} \frac{\partial s}{\partial w_{j}}=w_{j}+\eta \sum_{n=1}^{N}\left(t_{n}-y\right) y(1-y) x_{j} \\
b &amp;=b-\eta \frac{\partial C}{\partial b} \quad=b+\eta \sum_{n=1}^{N}\left(t_{n}-y\right) y(1-y)
\end{aligned}
\]</span></p>
<p>이렇게 구한 식들을 대입하는 것으로 gradient descent method 구현이 완료된다. 물론 training 방법론으로 gradient descent method 이외 의 다른 방법론을 채택했다면 update rule 은 달라진다. 이처럼 neuron 하나에 대한 update rule 은 위와 같은 과정을 걸쳐서 정의된다.</p>
<p>기본적으로 하나의 neuron 은 binary class 를 분류하도록 설계된다. 즉 하나의 neuron 은 <span class="math inline">\(\mathrm{A}\)</span> 인가 <span class="math inline">\(\mathrm{A}\)</span> 가 아닌가를 분류하는 기능만을 수행한 다. 그러나 현실의 labeling 은 1 개의 여부를 분별하는 binary 이기보다는 다양한 선택지 안에서 하나를 고르는 multinomial 문제가 절대다수 이다. 따라서 yes or not 을 분별하는 binary neuron 다수를 복합하여 이러한 multinomial class 에 대한 classifier 를 설계하게 된다. 이렇게 다수의 neuron 을 복합해서 구성한 하나의 classifier 를 layer 라고 명명한다. 하나의 neuron 에는 그 뉴런이 보유한 weight 와 activation function 이 세트이므로, 따라서 1 개의 layer 에 적층된 neuron 이 <span class="math inline">\(N\)</span> 개라면, weight vector 도 <span class="math inline">\(N\)</span> 개, activation function 도 <span class="math inline">\(N\)</span> 개라는 것과 동 의이다.</p>
<p>여기에 위에서 설명한 update rule 을 구하는 방법을 사용하여 이렇게 다수의 neuron 을 적층시킨 layer 1 개의 Cost function 을 정의하고 이 에 대한 update rule 을 따로 정의해주어야 한다. 이러한 layer 에 대한 cost function 으로서 SSE 를 사용하여 예시를 보일 수 있다.</p>
<p><span class="math display">\[
\mathcal{C}=\frac{1}{2} \sum_{n=1}^{N}\left\|t_{n}-y\right\|_{2}^{2}=\frac{1}{2} \sum_{n=1}^{N} \sum_{i=1}^{K}\left(t_{n i}-y_{i}\right)^{2}
\]</span></p>
<ul>
<li><p><span class="math inline">\(t_{n i}: n\)</span> 번째 데이터에 대한 label vector <span class="math inline">\(t_{n}\)</span> 의 <span class="math inline">\(i\)</span>-th element</p></li>
<li><p><span class="math inline">\(y_{i}\)</span> : output layer 에서의 <span class="math inline">\(i\)</span>-th neuron의 output</p></li>
</ul>
<p>위에서 구하였던 update rule 을 이 layer 에 대한 cost function 에 적용하면 아래와 같다. 이때 activation function 은 sigmoid 로 가정하였 다. single-layer ANN 은 서로 independent 한 neuron 여러개를 적층한 것일 뿐이므로, layer 1 개에 들어있는 각 neuron 에 대한 update rule 은 neuron 1 개에 대한 update rule 과 다를바 없음을 알 수 있다.</p>
<p><span class="math display">\[
\begin{aligned}
w_{i j}=w_{i j}-\eta \frac{\partial C}{\partial w_{i j}} &amp;=w_{i j}-\eta \frac{\partial C}{\partial y_{i}} \frac{\partial y_{i}}{\partial s_{i}} \frac{\partial s_{i}}{\partial w_{i j}} \\
&amp;=w_{i j}+\sum_{n=1}^{N}\left(t_{n i}-y_{i}\right) y_{i}\left(1-y_{i}\right) x_{j} \\
b_{i}=b_{i}-\eta \frac{\partial C}{\partial b_{i}} &amp;=b_{i}+\sum_{n=1}^{N}\left(t_{n i}-y_{i}\right) y_{i}\left(1-y_{i}\right)
\end{aligned}
\]</span></p>
<p>만약 데이터의 분포가 단순하다면 이런 layer 1 개만으로도 충분한 성능의 classification 을 획득하는 것이 가능하다. 그러나 현실세계의 문 제는 복잡하여 그렇지 않은 경우가 더 다수가 된다. 만약 데이터의 분포가 복잡하여 이를 분류하는데에 지나치게 복잡한 형태의 함수를 요 구로 한다면 linear function 으로 이들의 경계선을 만드는 것은 불가능할 것이다. 그러나 데이터의 본질을 건드리지 않는 선에서 데이터에 변환을 적용하여 데이터 class 간의 경계선을 선형에 가깝게 변환할 수 있다면 이러한 문제를 해결하는 것이 가능하다.</p>
<p>Multi-layer ANN 이 바로 이러한 발상을 구현한 것이다. 최종적인 결과물을 출력하는 layer 인 output layer 를 사용하기 전, input layer 와 output layer 사이에 hidden layer 를 다수 설치하여 hidden layer 를 거치면서 데이터의 classfication 을 최적화할 수 있는 데이터 변환을 연 산하자는 것이 그 골자이다. 이러한 최적화 변환에는 차원을 축소하거나, 혹은 증가시키는 것도 포함된다.</p>
<p>hidden layer 를 도입해도 output layer 에서 결과를 classification 하는 방법은 위에서 설명하였던 일련의 과정들과 동일하다. 단지 output layer 가 시간 <span class="math inline">\(L\)</span> 에서의 layer 로 정의된다는 것이 다를 뿐이다. 이를 반영하여 update rule 을 수식으로 서술하면 아래와 같다.</p>
<p><span class="math display">\[
\begin{aligned}
w_{i j}^{(L)} &amp;=w_{i j}^{(L)}-\eta \frac{\partial C}{\partial w_{i j}^{(L)}} \\
&amp;=w_{i j}^{(L)}+\sum_{n=1}^{N}\left(t_{n}-y_{i}^{(L)}\right) y_{i}^{(L)}\left(1-y_{i}^{(L)}\right) y_{j}^{(L-1)} \\
b_{i}^{(L)} &amp;=b_{i}^{(L)}-\eta \frac{\partial \mathcal{C}}{\partial b_{i}^{(L)}} \\
&amp;=b_{i}^{(L)}+\sum_{n=1}^{N}\left(t_{n}-y_{i}^{(L)}\right) y_{i}^{(L)}\left(1-y_{i}^{(L)}\right)
\end{aligned}
\]</span></p>
<p>그러나 hidden layer 는 이러한 cost function 접근법을 직접적으로 활용할 수 없다는 문제가 있다. hidden layer 에는 label 이 존재하지 않아 cost function 을 접근할 수 없기 때문이다. 이러한 문제를 해결하기 위해 제언된 것이 Backpropagation 알고리즘이다.</p>
<div id="backpropagation" class="section level4 hasAnchor" number="13.1.1.1">
<h4><span class="header-section-number">B.1.1.1</span> Backpropagation<a href="ann.html#backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>기본적인 아이디어는 output layer 의 output 과 cost function으로부터 계산된 오차 ( <span class="math inline">\(t\)</span> 와 <span class="math inline">\(y\)</span> 의 차이) 를 hidden layer 로 도로 역행시키며 각 layer 마다 패러미터를 재조정하는 것이다. 우선 <span class="math inline">\(\delta_{i}^{(l)}=\frac{\partial C}{\partial s_{i}^{(i)}}\)</span> 라는 변수를 정의한다. 이때 <span class="math inline">\(C\)</span> 는 output layer 의 cost function 이다. 이제 <span class="math inline">\(L-1\)</span> layer 를 생각하자. 이는 output layer 도, input layer 도 아니므로 hidden layer 이며, output layer 를 제외하면 layer 배치 중 최후미에 존재하므로 ‘최상층’ 이라고 불린다. 해당 <span class="math inline">\(L-1\)</span> layer 를 구성하는 neuron 들 중 <span class="math inline">\(i\)</span>-th neuron 의 <span class="math inline">\(j\)</span>-th weight, 즉 <span class="math inline">\(w_{i j}^{(L-1)}\)</span> 에 대한 update rule 은 이하와 같은 과정을 거쳐 유도된다.</p>
<p><span class="math display">\[
\begin{array}{rlr}
\frac{\partial C}{\partial w_{i j}^{(L-1)}} &amp; =\frac{\partial C}{\partial y_{i}^{(L-1)}} \cdot &amp; \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)}} \frac{\partial s_{i}^{(L-1)}}{\partial w_{i j}^{(L-1}} \\
&amp; =\left[\sum_{k=1}^{K_{L}} \frac{\partial C}{\partial s_{k}^{(L)}} \frac{\partial s_{k}^{(L)}}{\partial y_{i}^{(L-1)}}\right] &amp; \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)}} \frac{\partial s_{i}^{(L-1}}{\partial w_{i j}^{(L-1)}} \\
&amp; =\left[\sum_{k=1}^{K_{L}} \frac{\partial C}{\partial s_{k}^{(L)}} \frac{\partial}{\partial y_{i}^{(L-1)}}\left\{\sum_{q=1}^{M_{L}} w_{k \beta}^{(L)} v_{i}^{(L-1)}+b_{k}\right\}\right] \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)} \frac{\partial s_{i}^{(L-1)}}{\partial w_{i j}^{(L-1)}}} \\
&amp; =\left\{\sum_{k=1}^{K_{k}^{(L)} W_{k i}^{(L)}}\right\} &amp; \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)}} \frac{\partial s_{i}^{(L-1)}}{\partial w_{i j}^{(L-1)}} \\
&amp; =\left\{\sum_{k=1}^{K_{L}} \delta_{k}^{(L)} W_{k i}^{(L)}\right\}_{j}^{(L-2)} &amp; \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)}}
\end{array}
\]</span></p>
<ul>
<li><p><span class="math inline">\(K_{l}: l\)</span> layer 를 구성하는 neuron 의 수</p></li>
<li><p><span class="math inline">\(M_{l}: l\)</span> layer 에 입력되는 데이터의 차원</p></li>
</ul>
<p>따라서 <span class="math inline">\(w_{i j}^{(L-1)}\)</span> 는 아래와 같이 update 되며, 상기와 같이 activate function 으로 sigmoid 를 가정하였다면 마지막 등식이 성립한다.</p>
<p><span class="math display">\[
\begin{aligned}
w_{i j}^{(L-1)} &amp;=w_{i j}^{(L-1)}-\eta \frac{\partial C}{\partial w_{i j}^{(L-1)}} \\
&amp;=w_{i j}^{(L-1)}-\eta\left\{\sum_{k=1}^{K_{L}} \delta_{k}^{(L)} w_{k i}^{(L)}\right\} y_{j}^{(L-2)} \cdot \frac{\partial y_{i}^{(L-1)}}{\partial s_{i}^{(L-1)}} \\
&amp;=w_{i j}^{(L-1)}-\eta\left\{\sum_{k=1}^{K_{L}} \delta_{k}^{(L)} w_{k i}^{(L)}\right\} y_{j}^{(L-2)} \cdot y_{i}^{(L-1)}\left(1-y_{i}^{(L-1)}\right) \text { (if activ func. is sigmoid) }
\end{aligned}
\]</span></p>
<p>이를 종합한 hidden layer 의 update rule 은 아래와 같다. 이처럼 <span class="math inline">\(l\)</span>-th <span class="math inline">\(\delta \delta_{i}^{(l)}\)</span> 를 메모리에 저장한 후 <span class="math inline">\(l-1\)</span> layer 를 training 할 때 이 변수를 사용하는 것으로 계속해서 거슬러 올라가며 training 을 반복하게 된다.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;w_{i j}^{(l)}=w_{i j}^{(l)}-\eta\left\{\sum_{k=1}^{K_{l+1}} \delta_{k}^{(l+1)} w_{k i}^{(l+1)}\right\} y_{j}^{(l)}\left(1-y_{i}^{(l)}\right) \cdot y_{j}^{(l-1)} \\
&amp;b_{i}^{(l)}=b_{i}^{(l)}-\eta\left\{\sum_{k=1}^{K_{l+1}} \delta_{k}^{(k+1)} w_{k i}^{(l+1)}\right\} y_{j}^{(l)}\left(1-y_{i}^{(l)}\right)
\end{aligned}
\]</span></p>
</div>
</div>
<div id="problem" class="section level3 hasAnchor" number="13.1.2">
<h3><span class="header-section-number">B.1.2</span> Problem<a href="ann.html#problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>이렇게 강력한 성능을 발휘하는 ANN 알고리즘이지만 특정 영역에서는 문제가 발생하게 된다. 1번째로는 ANN 의 input layer 는 vector 데 이터만을 입력받는다는 점이 문제가 된다. 2차원 이상의 데이터를 input 하고자 할 경우 이를 slice 하여 1차원의 긴 데이터로 입력하는 것 으로 우선 입력시키는 것만은 가능하다. 그러나 다차원 데이터의 경우 개별 entry <span class="math inline">\(a\)</span> 주위에 위치한 entry 들이 가지는 속성이 <span class="math inline">\(a\)</span> 가 어떤 속 성을 가지고 있는지 짐작하는데 중요한 역할을 하는 경우가 존재한다는 것이 이러한 접근을 방해하게 된다. 일례로 이미지 데이터를 분석 하는 상황이라면 특정 픽셀이 가지고 있는 색은 주위 픽셀이 가지고 있는 색과 유사할 가능성이 높을 것이다. 또한 node 를 개인, edge 를 인간관계로 하는 그래프 데이터 분석 상황의 경우 유유상종이라는 말에 따라 어느정도 성질을 유사하게 가지는 사람들이 관계를 맺고 있을 가능성이 높으므로 이또한 neighbor 의 정보가 개별 node 분석에 있어 필수불가결하다. 이처럼 다차원 데이터에서는 위치 데이터가 중요 한 역할을 하는 경우가 많으며 다차원 데이터를 평탄화하여 vector 화 하는 것은 다차원 데이터의 위치정보를 유실시켜 결국 알고리즘의 성 능을 크게 떨어트리게 된다.</p>
<p>또한 ANN 의 경우 연산중에 지나치게 많은 model 패러미터를 요구한다. ANN 을 통해 RGB 룰을 따르는 <span class="math inline">\(1024 \times 1024\)</span> 크기의 이미지를 처리 하고자 할 경우 ANN 에 입력되는 벡터의 차원은 <span class="math inline">\(1024 \times 1024 \times 3\)</span> 으로 약 300 만에 달한다. 이러한 input 하나하나에 대해 weight 가 지정될 필요가 있으므로 요구되는 모델 패러미터들의 숫자가 지나치게 많아지게 되며 저장공간을 비효율적으로 활용하게 된다는 점에서 이또한 심각한 단점 중 하나이다. 이러한 ANN 들의 한계를 해결하기 위한 것으로 다양한 해결책들이 제시되었으며, 이들 중 가장 유명한 것은 Convolutional Neural Network, 이하 CNN 이다.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="about-cluster-gcn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/lyric2249/lyric2249.github.io/edit/main/990102_Cluster_GCN.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
