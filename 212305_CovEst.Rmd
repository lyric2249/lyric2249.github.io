## Covariance estimation

<br>
<br>
<br>


### Matrix algebra review

<br>
<br>
<br>

<br>
<br>
<br>


### Covariance matrix estimation in the operator norm

::: {.theorem name="Covariance estimation"}

$X_1 , \cdots, X_n \overset {iid}\sim SG(\sigma)$ s.t. $E(X_1) = 0, Var(X_1) = \Sigma_{d \times d}$.

Let sample Cov matrix $\hat \Sigma = \frac{1}{n} \sum X_i X_i '$ based on $X_1 , \cdots, X_n$.

Then there exists a universal constant $C >0$ s.t. below holds with probabilty at least $1-\sigma$.

$$
\forall \sigma \in (0,1): \frac{\|\hat \Sigma - \Sigma \|_{op}}{\sigma^2} \le C \max \left \{ \sqrt{\frac{d + \log(\frac{2}{\sigma})}{n}}, \; {\frac{d + \log(\frac{2}{\sigma})}{n}}\right \}
$$

:::





<br>




- ì´ê±´ ê²°êµ­ $\lim_{n \rightarrow \infty \frac{d}{n} \rightarrow 0}$ ì¼ ë•Œ operator norm ì•ˆì˜ $\Sigma$ë¥¼ ê³„ì†í•´ì„œ estimate í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë§í•¨. ì‹¤ì œë¡œ ì¶”ê°€ì ì¸ ê°€ì • ì—†ì´ëŠ” ì´ rate ì´ìƒìœ¼ë¡œ ì¸¡ì •ì„ ì •ë°€í™”í•  ìˆ˜ ì—†ìŒ.




<br>
<br>




ì¦ëª…ì„ 2ë‹¨ê³„ë¡œ ë¶„í• .

1. discretization argument ë¥¼ ì¨ì„œ ë¬¸ì œë¥¼ finitely ë§ì€ ëœë¤ë³€ìˆ˜ì˜ maximum ì„ ì œì–´í•˜ëŠ” ë¬¸ì œë¡œ ë³€ê²½. ì´í•˜ì˜ ì •ë³´ì™€ í•¨ê»˜ finite maximum ë¼ëŠ” ì‚¬ì‹¤ ì‚¬ìš©í•´ì„œ $\|\hat \Sigma - \Sigma \|_{op}$ ì— ëŒ€í•œ ìƒí•œ ìƒì‚°.

let $A = A' \in \mathbb R^{d \times d}$ ë¡œ í•˜ê³ , $N_\epsilon = \{ y_1 , \cdots, y_N \}$ ì„ $\mathbb S^{d-1}$ ì˜ $\epsilon$-covering ìœ¼ë¡œ í•¨.

ì´ë•Œ $\| A \|_{op} \le \frac{1}{1-2\epsilon} \cdot \max_{y \in N_\epsilon} | y' A y |$.

ì´ë¥¼ ì¦ëª…í•˜ì. $x \in \mathbb S^{d-1}$ ì— ëŒ€í•´ $\| x-y \|_2 \le \epsilon$ ë§Œì¡±í•˜ëŠ” $y \in N_\epsilon$ ì„ íƒ. ì´ë•Œ $A$ëŠ” symmetric Matrix ì´ë¯€ë¡œ,


ì—¬ê¸°ì„œ $\hat \Sigma - \Sigma$ ëŠ” symmetric Matrix ì´ë¯€ë¡œ, $\|\hat \Sigma - \Sigma \|_{op} \le \frac{1}{1-2\epsilon} \max_{y \in N_\epsilon} | y' (\hat \Sigma - \Sigma) y |$.

\tag{5.1}


$$
\vert x^{\textsf{T}}A x-y^{\textsf{T}}A y\vert\ =\ \vert x^{\textsf{T}}A(x-y)-y^{\textsf{T}}A(y-x)\vert
\leq\;\left|x^{\textsf{T}}A(x-y)\right|+\left|y^{\textsf{T}}A(y-x)\right| \tag{triangle ineq.}
$$


$$
|x^{\textsf{T}}A(x-y)|\ \stackrel{(1)}{\le}\ ||A(x-y)||_{2}||x||_{2} \tag{Cauchyâ€“Schwarz inequality}
\\
\overset{(2)}{\le} {{||A||_{\mathrm{op}}||x-y||_{2}}}
\\
\overset{(3)}{\le} \epsilon\|A\|_{\mathrm{op}}
$$

2. $|| x||_2 = 1$, and $\forall v \in \mathbb R^d:||A v ||_2 \le ||A||_{op} ||v||_2$
3. $||x-y||_2 \le \epsilon$

Applying the same argument to $|y^T A(y-x)|$ then gives $|x^T Ax - y^T Ay| \le 2 \epsilon ||A||_{op}$.

To complete the proof of inequality (5.1), note that


$$
|x^{\textsf{T}}A x|=|x^{\textsf{T}}A x-y^{\textsf{T}}A y+y^{\textsf{T}}A y| \leq\ \left|x^{\top}A x-y^{\top}A y\right|+\left|y^{\top}A y\right|
\leq 2\epsilon || A||_{\mathrm{op}}+|y^{T}A y \rvert
$$

This implies that $||A||_{op} \le 2 \epsilon ||A||_{op} + \max_{y \in N_\epsilon} |y^T A y|$ and rearranging the terms yields inequality (5.1).




<br>
<br>
 



2. standard concentration inequality ì‚¬ìš©.


Step 2. By choosing  = 1/4, inequality (5.2) becomes




$$
||\hat{\Sigma} - \Sigma||_\mathrm{op} \leq 
2 \max_{y\in{N_{1/4}}}
|\mathcal{y}^{\top} \big(\hat{\Sigma} - \Sigma\big)\mathcal{y}|
$$

Therefore by the union bound


$$
P(||\hat{\Sigma} - \Sigma||_\mathrm{op} \ge t) \leq 
P \left ( 2 \max_{y\in{N_{1/4}}}
|\mathcal{y}^{\top} \big(\hat{\Sigma} - \Sigma\big)\mathcal{y}|
\ge t \right )
 \leq 
\sum_{y\in{ N_{1/4}}} P \left( |\mathcal{y}^{\top} \big(\hat{\Sigma} - \Sigma\big)\mathcal{y}| \ge \frac{t}{2} \right)
$$
Note that we can write


$y^{\top}(\hat{\Sigma}-\Sigma)y=\frac{1}{n}\sum_{i=1}^{n}\left\{(y^{\top}X_{i})^{2}-\mathbb{E}[(y^{\top}X_{i})^{2}]\right\}$


We saw earlier in Lemma 3.3 that the square of a sub-Gaussian random variable is sub-exponential with parameters $(\nu, \alpha) = (16 \sigma^2, 16 \sigma^2)$. This property implies that $\left\{(y^{\top}X_{i})^{2}-\mathbb{E}[(y^{\top}X_{i})^{2}]\right\}$ is sub-exponential with $(16\sigma^2 , 16\sigma^2)$. Applying the sub-exponential tail bound, especially inequality (3.2), yields







$$
\mathbb{P}(\|\widehat\Sigma-\Sigma\|_{\mathrm{lop}}\geq t)\ \leq\ 2\ \underbrace{\mathrm{l}N_{1/4}}_{\mathrm{extrianitv}}
\times 
\exp\Biggl(-{\frac{1}{2}}\operatorname{min}\Biggl\{{\frac{n t}{16\sigma^{2}}},\ {\frac{n t^{2}}{16^{2}\sigma^{4}}}\Biggr\}\Biggr)
\\
\leq\ 2\times9^{d}\times\exp\biggl(-\frac{1}{2}\operatorname*{min}\Biggl\{\frac{n t}{16\sigma^{2}},\ \frac{n t^{2}}{16^{2}\sigma^{4}}\Biggr\}\biggr)
$$



where the last step uses the result in Lecture 4, which shows that the cardinality of $N_{1/4}$ is
bounded by $|N_{1/4}| â‰¤ 9$. (here note that $\mathbb{S}^{d-1}\subset\mathbb{B}=\{\theta\in\mathbb{R}^{d}:||\theta||_{2}\leq1\}\})$). Finally, inverting the bound gives the desired result.















<br>
<br>
<br>


<br>
<br>
<br>



###  Bounds for structured covariance matrices

ìš°ë¦¬ì˜ ì£¼ëœ ëª©ì ì€ ìƒ˜í”Œ Covë¥¼ ê²½ìœ í•˜ì—¬ unstructured Cov Matrixë¥¼ estimate í•˜ëŠ” ê²ƒ. Cov Matrix ê°€ ì¶”ê°€ì ì¸ structure ë¥¼ í’ˆê³  ìˆë‹¤ë©´, ìƒ˜í”Œ Cov ê°€ ì•„ë‹ˆë¼ ë‹¤ë¥¸ estimator ë¥¼ ì‚¬ìš©í•´ì„œ ì¢€ë” ì—°ì‚°ì´ ë¹ ë¥¸ estimate ê°€ ê°€ëŠ¥.




<br>

- **Diagonal matrix**

**Cov Matrix ê°€ diagonalì´ë¼ëŠ” ì •ë³´**ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•´ë³´ì. ì´ë•Œ $\hat \Sigma_{diag} = diag \{ \hat \Sigma_{11}, \cdots, \hat \Sigma_{dd}$ ë¡œ estimate í•˜ëŠ” ê²ƒì€ ìì—°ìŠ¤ëŸ½ë‹¤. ì´ ê²½ìš° sub-Gaussinianity ë¥¼ ê°€ì •í•œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ? unstructured ì¼€ì´ìŠ¤ì—ì„œ order ê°€ $\sqrt{\frac{d}{n}}$ rates (ë‹¨, $d \le n$) ì˜€ë˜ ê²ƒê³¼ ëŒ€ë¹„ë˜ê²Œ estimation error of the order $\sqrt{\frac{\log d}{n}} ê°€ ìƒì‚°ëœë‹¤.

ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ì. diagonal ì¼€ì´ìŠ¤ì—ì„œ,  $\hat \Sigma_{diag} - \Sigma$ ì˜ operator norm ì€ ë³¸ì§ˆì ìœ¼ë¡œ $d$ ê°œì˜ entryê°’ $\{| \hat \Sigma_{diag,11} - \Sigma_{11} |, \cdots, | \hat \Sigma_{diag,dd} - \Sigma_{dd} |\}$ ì¤‘ì˜ maximum ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ the union bound argument along with an exponential tail bound ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” $\sqrt{\frac{\log d}{n}} â†’ 0$ ì¼ ë•Œ operator norm ì´ 0ë¡œ decay ëœë‹¤ëŠ” ê²ƒì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. <mark> See Theorem 2.11 for a similar argument.</mark>




<br>

- **Unknown sparsity and thresholding**

ì¢€ë” ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤ë¥¼ ìƒê°í•´ë³´ì. Cov Matrixê°€ ìƒëŒ€ì ìœ¼ë¡œ sparse í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì´ ì•Œë ¤ì ¸ ìˆì§€ë§Œ, ì–´ëŠ entryê°€ non-zeroì¸ì§€ëŠ” ì•Œë ¤ì ¸ìˆì§€ ì•Šë‹¤. ì´ë•Œ estimatorê°€ thresholding ì— ê¸°ë°˜í•˜ê³  ìˆë‹¤ê³  ìƒê°€ê°€í” ë„‰ìŠ¹ ë‚˜ì ¼ìŠ¤ëŸ½ë‹¤. ì´ë•Œ $\lambda >0$ ë¼ëŠ” íŒ¨ëŸ¬ë¯¸í„°ê°€ ì£¼ì–´ì ¸ ìˆë‹¤ê³  ìƒê°í•  ë•Œ, hard-thresholding ì„ í†µí•´ ì–»ì–´ì§€ëŠ” Cov estimator ì˜ $(i,j)$ entry ëŠ” $[T_\lambda (\hat \Sigma)]_{ij} = \hat \Sigma_{ij} \cdot I(|\hat \Sigma_{ij}>\lambda)$.

let $\Sigma$ì˜ adjacency matrix $A \in \mathbb R^{d \times d}$, $A_{ij} = I(\Sigma_{ij}) \not = 0$. adjacency matrix ì˜ operator norm $\| A \|_{op}$ ëŠ” sparsity ì— ëŒ€í•œ natural measure ë¥¼ ì œê³µí•œë‹¤. ì´ë•Œ ìš°ë¦¬ëŠ” $\Sigma$ ê°€ row ë³„ë¡œ $s$ ê°œì˜ non-zero entryë¥¼ ê°–ê³  ìˆë‹¤ë©´ $\| A \|_{op} \le s$ ì„ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë˜í•œ thresholded ìƒ˜í”Œ Cov Matrix ëŠ” ë‹¤ìŒê³¼ ê°™ì€ concentration boundë¥¼ ê°€ì§.





<br>
<br>

::: {.theorem name="Thresholding-based covariance estimation"}

$X_1 , \cdots, X_n \overset {iid} \sim$, s.t. $E(X_1) = 0, Var(X_1) = \Sigma_{d \times d}$, and suppose each component $X_{ij}$ is sub-Gaussinian with íŒ¨ëŸ¬ë¯¸í„° at most $\sigma$.

ë§Œì•½ $n > 16 \log d$ ë¼ë©´, $\forall \delta>0$ì— ëŒ€í•´, thresholded ìƒ˜í”Œ Cov Matrix $T_{\lambda_n} (\hat \Sigma)$ with $\frac{\lambda_n}{\sigma^2} = 8 \sqrt{\frac{\log d}{n}} + \delta$ ëŠ” ì´í•˜ë¥¼ ë§Œì¡±í•œë‹¤.

$$
P \Big (
\| T_{\lambda_n} ( \hat \Sigma ) - \Sigma \|_{op} \ge 2 \| A \|_{op} \cdot \lambda_n
 \Big)
\le 8 \exp \Big( -\frac{n}{16} (\delta \wedge \delta^2)\Big)
$$

:::




<br>

- ìœ„ì˜ ë¶€ë“±ì‹ì€ ë†’ì€ í™•ë¥ ë¡œ $\| T_{\lambda_n} ( \hat \Sigma ) - \Sigma \|_{op} \lesssim \| A \|_{op} \sqrt{\log d}{n}$ ì„ì„ ë³´ì—¬ì¤Œ. ì´ì— ë”í•´ì„œ $\sigma$ ê°€ row ë‹¹ ìµœëŒ€ $s$ ê°œì˜ non-zero entry ë¥¼ ê°€ì§„ë‹¤ëŠ” ì¡°ê±´ì„ ìƒê°í•˜ì. ì´ëŠ” ê³§ $\|A\|_2 \le s$ ë¼ëŠ” ì˜ë¯¸ê°€ ë¨. ê·¸ë ‡ë‹¤ë©´ thresholded Cov MatrixëŠ” $s\sqrt{\frac{\log d}{n}}â†’0$ ì¼ ë•Œ consistent í•˜ë©°, ì´ëŠ” ê³§ íŠ¹íˆ $s$ ê°€ ì‘ì„ ë•Œ $\sqrt{\frac{d}{n}}$ ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë‹¤.

- thresholding íŒ¨ëŸ¬ë¯¸í„°ëŠ” sub-Gaussian íŒ¨ëŸ¬ë¯¸í„° $\sigma$ì— ì˜ì¡´í•˜ëŠ”ë°, ì´ëŠ” ì‹¤ì „ ìƒí™©ì—ì„œëŠ” ëŒ€ë¶€ë¶„ unknown.





<br>
<br>




**Proof**: Let us denote the elementwise infinity norm of the error matrix $\hat \Delta = \hat \Sigma - \Sigma$ by $|| \hat \Delta ||_\infty = \max_{1\le j , \; \; j \le d} |\hat \Delta_{ij}|$. The proof of the theorem is based on two intermediate results:



\


1. Under the assumptions of the theorem,

$\mathbb{P}(||\widehat{\Delta}||_{\infty}/\sigma^{2}\geq t)\leq8e^{-{\frac{n}{16}}\operatorname*{min}\{t,t^{2}\}+2\log d}\quad\mathrm{for~all~}t\gt 0 \tag{5.3}$

2. For any choice of $\lambda_n$ such that $|| \hat \Delta ||_\infty \le \lambda_n$ we are guaranteed that

$||\widehat{\Sigma}-\Sigma||_{\mathrm{lop}}\leq\ 2||A||_{\mathrm{op}}\lambda_{n} \tag{5.4}$


\


Having these results in place, the theorem follows by taking $t = \frac{\lambda_n}{\sigma^2} = 8 \sqrt{\frac{\log d}{n}} + \delta$in inequality (5.3) and see


$8e^{-\frac{n}{16}\operatorname*{min}\{t,t^{2}\}+2\log d}\lt 8e^{-\frac{n}{16}\operatorname*{min}\{\delta,\delta^{2}\}},$


, when $n > 16 log d$. Thus



It remains to prove inequality (5.3) and inequality (5.4), which are left as exercises (see Section 6.5 of Martinâ€™s book)

$$
|\mathbb{P} \Bigg (||T_{\lambda_{n}}(\hat{\Sigma})\to\Sigma||_{\mathrm{op}}\geq\ 2||{A}||_{\mathrm{op}}\lambda_{n} \Bigg)
 \le P (||\widehat\Delta||_{\infty}\ge\,\lambda_{n})\le8e^{-\frac n{16}\it\ m i n}\{\delta,\delta^{2}\}_{.}
$$


It remains to prove inequality (5.3) and inequality (5.4), which are left as exercises (see Section 6.5 of Martinâ€™s book)




















