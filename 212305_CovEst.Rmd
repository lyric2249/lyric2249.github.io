## Covariance estimation

### Matrix algebra review

### Covariance matrix estimation in the operator norm

::: {.theorem name="Covariance estimation"}

$X_1 , \cdots, X_n \overset {iid}\sim SG(\sigma)$ s.t. $E(X_1) = 0, Var(X_1) = \Sigma_{d \times d}$.

Let sample Cov matrix $\hat \Sigma = \frac{1}{n} \sum X_i X_i '$ based on $X_1 , \cdots, X_n$.

Then there exists a universal constant $C >0$ s.t. below holds with probabilty at least $1-\sigma$.

$$
\forall \sigma \in (0,1): \frac{\|\hat \Sigma - \Sigma \|_{op}}{\sigma^2} \le C \max \left \{ \sqrt{\frac{d + \log(\frac{2}{\sigma})}{n}}, \; {\frac{d + \log(\frac{2}{\sigma})}{n}}\right \}
$$

:::


- ì´ê±´ ê²°êµ­ $\lim_{n \rightarrow \infty \frac{d}{n} \rightarrow 0}$ ì¼ ë•Œoperator norm ì•ˆì˜ $\Sigma$ë¥¼ ê³„ì†í•´ì„œ estimate í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë§í•¨. ì‹¤ì œë¡œ ì¶”ê°€ì ì¸ ê°€ì • ì—†ì´ëŠ” ì´ rate ì´ìƒìœ¼ë¡œ ì¸¡ì •ì„ ì •ë°€í™”í•  ìˆ˜ ì—†ìŒ.

ì¦ëª…ì„ 2ë‹¨ê³„ë¡œ ë¶„í• .

1. discretization argument ë¥¼ ì¨ì„œ ë¬¸ì œë¥¼ finitely ë§ì€ ëœë¤ë³€ìˆ˜ì˜ maximum ì„ ì œì–´í•˜ëŠ” ë¬¸ì œë¡œ ë³€ê²½. ì´í•˜ì˜ ì •ë³´ì™€ í•¨ê»˜ finite maximum ë¼ëŠ” ì‚¬ì‹¤ ì‚¬ìš©í•´ì„œ $\|\hat \Sigma - \Sigma \|_{op}$ ì— ëŒ€í•œ ìƒí•œ ìƒì‚°.

let $A = A' \in \mathbb R^{d \times d}$ ë¡œ í•˜ê³ , $N_\epsilon = \{ y_1 , \cdots, y_N \}$ ì„ $\mathbb S^{d-1}$ ì˜ $\epsilon$-covering ìœ¼ë¡œ í•¨.

ì´ë•Œ $\| A \|_{op} \le \frac{1}{1-2\epsilon} \cdot \max_{y \in N_\epsilon} | y' A y |$.

ì´ë¥¼ ì¦ëª…í•˜ì. $x \in \mathbb S^{d-1}$ ì— ëŒ€í•´ $\| x-y \|_2 \le \epsilon$ ë§Œì¡±í•˜ëŠ” $y \in N_\epsilon$ ì„ íƒ. ì´ë•Œ $A$ëŠ” symmetric Matrix ì´ë¯€ë¡œ,

$$

$$



ì—¬ê¸°ì„œ $\hat \Sigma - \Sigma$ ëŠ” symmetric Matrix ì´ë¯€ë¡œ, $\|\hat \Sigma - \Sigma \|_{op} \le \frac{1}{1-2\epsilon} \max_{y \in N_\epsilon} | y' (\hat \Sigma - \Sigma) y |$.

y choosing  = 1/4, inequality (5.2) becomes
kÎ£b âˆ’ Î£kop â‰¤ 2 max
yâˆˆN1/4
|y
>(Î£b âˆ’ Î£)y|.
Therefore by the union bound
P(kÎ£b âˆ’ Î£kop â‰¥ t) â‰¤ P

2 max
yâˆˆN1/4
|y
>(Î£b âˆ’ Î£)y| â‰¥ t

â‰¤
X
yâˆˆN1/4
P(|y
>(Î£b âˆ’ Î£)y| â‰¥ t/2).
Note that we can write
y
>(Î£b âˆ’ Î£)y =
1
n
Xn
i=1

(y



2. standard concentration inequality ì‚¬ìš©.


y choosing  = 1/4, inequality (5.2) becomes
kÎ£b âˆ’ Î£kop â‰¤ 2 max
yâˆˆN1/4
|y
>(Î£b âˆ’ Î£)y|.
Therefore by the union bound
P(kÎ£b âˆ’ Î£kop â‰¥ t) â‰¤ P

2 max
yâˆˆN1/4
|y
>(Î£b âˆ’ Î£)y| â‰¥ t

â‰¤
X
yâˆˆN1/4
P(|y
>(Î£b âˆ’ Î£)y| â‰¥ t/2).
Note that we can write
y
>(Î£b âˆ’ Î£)y =
1
n
Xn
i=1

(y


###  Bounds for structured covariance matrices

ìš°ë¦¬ì˜ ì£¼ëœ ëª©ì ì€ ìƒ˜í”Œ Covë¥¼ ê²½ìœ í•˜ì—¬ unstructured Cov Matrixë¥¼ estimate í•˜ëŠ” ê²ƒ. Cov Matrix ê°€ ì¶”ê°€ì ì¸ structure ë¥¼ í’ˆê³  ìˆë‹¤ë©´, ìƒ˜í”Œ Cov ê°€ ì•„ë‹ˆë¼ ë‹¤ë¥¸ estimator ë¥¼ ì‚¬ìš©í•´ì„œ ì¢€ë” ì—°ì‚°ì´ ë¹ ë¥¸ estimate ê°€ ê°€ëŠ¥.

- **Diagonal matrix**

**Cov Matrix ê°€ diagonalì´ë¼ëŠ” ì •ë³´**ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•´ë³´ì. ì´ë•Œ $\hat \Sigma_{diag} = diag \{ \hat \Sigma_{11}, \cdots, \hat \Sigma_{dd}$ ë¡œ estimate í•˜ëŠ” ê²ƒì€ ìì—°ìŠ¤ëŸ½ë‹¤. ì´ ê²½ìš° sub-Gaussinianity ë¥¼ ê°€ì •í•œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ? unstructured ì¼€ì´ìŠ¤ì—ì„œ order ê°€ $\sqrt{\frac{d}{n}}$ rates (ë‹¨, $d \le n$) ì˜€ë˜ ê²ƒê³¼ ëŒ€ë¹„ë˜ê²Œ estimation error of the order $\sqrt{\frac{\log d}{n}} ê°€ ìƒì‚°ëœë‹¤.

ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ì. diagonal ì¼€ì´ìŠ¤ì—ì„œ,  $\hat \Sigma_{diag} - \Sigma$ ì˜ operator norm ì€ ë³¸ì§ˆì ìœ¼ë¡œ $d$ ê°œì˜ entryê°’ $\{| \hat \Sigma_{diag,11} - \Sigma_{11} |, \cdots, | \hat \Sigma_{diag,dd} - \Sigma_{dd} |\}$ ì¤‘ì˜ maximum ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ the union bound argument along with an exponential tail bound ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” $\sqrt{\frac{\log d}{n}} â†’ 0$ ì¼ ë•Œ operator norm ì´ 0ë¡œ decay ëœë‹¤ëŠ” ê²ƒì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. <mark> See Theorem 2.11 for a similar argument.</mark>

- **Unknown sparsity and thresholding**

ì¢€ë” ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤ë¥¼ ìƒê°í•´ë³´ì. Cov Matrixê°€ ìƒëŒ€ì ìœ¼ë¡œ sparse í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì´ ì•Œë ¤ì ¸ ìˆì§€ë§Œ, ì–´ëŠ entryê°€ non-zeroì¸ì§€ëŠ” ì•Œë ¤ì ¸ìˆì§€ ì•Šë‹¤. ì´ë•Œ estimatorê°€ thresholding ì— ê¸°ë°˜í•˜ê³  ìˆë‹¤ê³  ìƒê°€ê°€í” ë„‰ìŠ¹ ë‚˜ì ¼ìŠ¤ëŸ½ë‹¤. ì´ë•Œ $\lambda >0$ ë¼ëŠ” íŒ¨ëŸ¬ë¯¸í„°ê°€ ì£¼ì–´ì ¸ ìˆë‹¤ê³  ìƒê°í•  ë•Œ, hard-thresholding ì„ í†µí•´ ì–»ì–´ì§€ëŠ” Cov estimator ì˜ $(i,j)$ entry ëŠ” $[T_\lambda (\hat \Sigma)]_{ij} = \hat \Sigma_{ij} \cdot I(|\hat \Sigma_{ij}>\lambda)$.

let $\Sigma $ì˜ adjacency matrix $A \in \mathbb R^{d \times d}$, $A_{ij} = I(\Sigma_{ij}) \not = 0$. adjacency matrix ì˜ operator norm $\| A \|_{op}$ ëŠ” sparsity ì— ëŒ€í•œ natural measure ë¥¼ ì œê³µí•œë‹¤. ì´ë•Œ ìš°ë¦¬ëŠ” $\Sigma$ ê°€ row ë³„ë¡œ $s$ ê°œì˜ non-zero entryë¥¼ ê°–ê³  ìˆë‹¤ë©´ $\| A \|_{op} \le s$ ì„ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë˜í•œ thresholded ìƒ˜í”Œ Cov Matrix ëŠ” ë‹¤ìŒê³¼ ê°™ì€ concentration boundë¥¼ ê°€ì§.


::: {..theorem name="Thresholding-based covariance estimation"}

$X_1 , \cdots, X_n \overset {iid} \sim$, s.t. $E(X_1) = 0, Var(X_1) = \Sigma_{d \times d}$, and suppose each component $X_{ij}$ is sub-Gaussinian with íŒ¨ëŸ¬ë¯¸í„° at most $\sigma$.

ë§Œì•½ $n > 16 \log d$ ë¼ë©´, $\forall \delta>0$ì— ëŒ€í•´, thresholded ìƒ˜í”Œ Cov Matrix $T_{\lambda_n} (\hat \Sigma)$ with $\frac{\lambda_n}{\sigma^2} = 8 \sqrt{\frac{\log d}{n}} + \delta$ ëŠ” ì´í•˜ë¥¼ ë§Œì¡±í•œë‹¤.

$$
P \Big (

\| T_{\lambda_n} ( \hat \Sigma ) - \Sigma \|_{op} \ge 2 \| A \|_{op} \cdot \lambda_n

 \Big)

\le 8 \exp \Big( -\frac{n}{16} (\delta \wedge \delta^2)\Big)

$$

:::

- ìœ„ì˜ ë¶€ë“±ì‹ì€ ë†’ì€ í™•ë¥ ë¡œ $\| T_{\lambda_n} ( \hat \Sigma ) - \Sigma \|_{op} \lesssim \| A \|_{op} \sqrt{\log d}{n}$ ì„ì„ ë³´ì—¬ì¤Œ. ì´ì— ë”í•´ì„œ $\sigma$ ê°€ row ë‹¹ ìµœëŒ€ $s$ ê°œì˜ non-zero entry ë¥¼ ê°€ì§„ë‹¤ëŠ” ì¡°ê±´ì„ ìƒê°í•˜ì. ì´ëŠ” ê³§ $\|A\|_2 \le s$ ë¼ëŠ” ì˜ë¯¸ê°€ ë¨. ê·¸ë ‡ë‹¤ë©´ thresholded Cov MatrixëŠ” $s\sqrt{\frac{\log d}{n}}â†’0$ ì¼ ë•Œ consistent í•˜ë©°, ì´ëŠ” ê³§ íŠ¹íˆ $s$ ê°€ ì‘ì„ ë•Œ $\sqrt{\frac{d}{n}}$ ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë‹¤.

- thresholding íŒ¨ëŸ¬ë¯¸í„°ëŠ” sub-Gaussian íŒ¨ëŸ¬ë¯¸í„° $\sigma$ì— ì˜ì¡´í•˜ëŠ”ë°, ì´ëŠ” ì‹¤ì „ ìƒí™©ì—ì„œëŠ” ëŒ€ë¶€ë¶„ unknown.



Proof: Let us denote the elementwise infinity norm of the error matrix âˆ† = b Î£b âˆ’ Î£ by
kâˆ†bkâˆ = max1â‰¤i,jâ‰¤d |âˆ†bij |. The proof of the theorem is based on two intermediate results:
1. Under the assumptions of the theorem,
P(kâˆ†bkâˆ/Ïƒ2 â‰¥ t) â‰¤ 8e
âˆ’ n
16 min{t,t2}+2 log d
for all t > 0. (5.3)
2. For any choice of Î»n such that kâˆ†bkâˆ â‰¤ Î»n, we are guaranteed that
kÎ£b âˆ’ Î£kop â‰¤ 2kAkopÎ»n. (5.4)
Having these results in place, the theorem follows by taking t = Î»n/Ïƒ2 = 8p
(log d)/n + Î´
in inequality (5.3) and see
8e
âˆ’ n
16 min{t,t2}+2 log d â‰¤ 8e
âˆ’ n
16 min{Î´,Î´2}
,
when n > 16 log d. Thus
P(kTÎ»n
(Î£) b âˆ’ Î£kop â‰¥ 2kAkopÎ»n) â‰¤ P(kâˆ†bkâˆ â‰¥ Î»n) â‰¤ 8e
âˆ’ n
16 min{Î´,Î´2}
.
It remains to prove inequality (5.3) and inequality (5.4), which are left as exercises (see
Section 6.5 of Martinâ€™s book)

























