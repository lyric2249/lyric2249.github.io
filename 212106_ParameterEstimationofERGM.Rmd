## Parameter Estimation of ERGM

Current Methods for ERGM

1. Approximation-based Algorithm: Maximize the likelihood function with MCMC samples.
	- Maximum Pseudo-likelihood Estimation (MPLE).
	- Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMCMLE).
	- Markov Chain Monte Carlo Stochastic Approximation (MCMCSA).
	- Varying Truncation Stochastic Approximation MCMC Method with Trajectory Averaging (VTSAMCMC).

2. Auxiliary Variable Markov Chain Monte Carlo (MCMC) Algorithm: Introduce auxiliary variables to cancel the normalizing constant ratio $\frac{\kappa(\theta)} {\kappa(\theta')}$ or to approximate the normalizing constant. Used for the Bayesian Inference.
	- The Exchange Algorithm.
	- Auxiliary Variable Metropolis-Hasting Algorithm (AVMH).
	- Adaptive Exchange Monte Carlo Algorithm (AEXMC).

<br>
<br>
<br>



### Approximation-based Algorithm

#### Maximum Pseudo Likelihood Estimation


$A$ 내부의 성분 간의 의존성을 무시한 채로 조건부 likelihood 함수들의 series를 곱하는 것으로 likelihood 함수를 근사함. ERGMS 의 조건부이며 pseudo Likelihood 는 아래와 같음. 물론 이건 어디까지나 가장 간단한 방법으로서 제시되었을 뿐이고, 이 방법론은 의존성 구조를 완전히 무시했기 때문에 퍼포먼스가 구림.


$$
\begin{align}
logit \left \{ P_\theta \Big (X_{ij} = 1 \Big | X_{ij}^c = x_{ij}^c \right \} &= \theta ' g(x_{ij}^c)

\\

\log PL(\theta, x) &= \sum_{ij} \theta ' g(x_{ij}^c) \cdot x_{ij} - \sum_{ij}\log \left \{ 1+ \theta ' g(x_{ij}^c) \right \}

\end{align}
$$



<br>
<br>
<br>


#### MCMCMLE

Approximate $\kappa(θ)$ using Monte Carlo samples generated from a
distribution 


f(x|θ
(0)
), where θ
(0)
is an initial estimate of θ.
Draw x1, · · · , xm denote random samples drawn from f(xobs|θ
(0)
) via
MCMC simulations, the log-ratio of likelihood can be approximated by

$$
I(\theta) - I(\theta^{(0)}) = \left (\theta - \theta^{(0)} \right) ' g(x_{obs}) - \log \left [ \frac{1}{m} \sum_{i=1}^m \exp \left \{ \left (\theta - \theta^{(0)}\right )' g(x_i) \right \} \right ]
$$

) via θ will approximate θˆ (MLE).
The performance of MCMLE depends on the choice of θ
(0)
. If θ
(0)
does
not lie in the attraction region of true MLE, the method may converge to
a suboptimal solution or fail to converge.

<br>
<br>
<br>


#### MCMC Stochastic Approximation


exponential family 에 대한 이론을 통해, 우리는 ERGMs을 maximize 하는 것은 $E_{\hat \theta} \Big \{ g(X) \Big \} = g(x_{obs})$ 라는 system 을 푸는 것과 동일하다는 것을 알 수 있다. 이 system 의 성립 여부는 이하와 iff ($\iff$).

1. $\hat \theta = \hat \theta_{MLE}$
2. Independent network generation
3. Parameter estimation update with stochastic approximation

이 방법론은 independent 네트워크 샘플을 생산하는데에는 비효율적이다. 각 샘플 $x_{k+1}$을 생산하기 위해 요구되는 이터레이션 스텝의 갯수는 order of $100 n^2$. 이때 $n$은 node 의 갯수.



<br>
<br>
<br>


<br>
<br>
<br>










### Auxiliary Variable MCMC-based Approaches

#### Exchange Algorithm

normalizing constant ratio $\frac{\kappa(\theta)}{\kappa(\theta')}$ 따위의 auxiliary 변수로 분포 $f(x | \theta)$ 를 augment. 이는 시뮬레이션 중에 canceled.

1. 후보 point $\theta ' \sim q(\theta ' | \theta, x)$ 를 생산
2. perfect sampler 사용해서 auxiliary 변수 $y \sim f(y | \theta ')$ 를 생산
3. $\theta'$를 with probability $1 \wedge r(\theta, \theta ' \Big | x)$로 채택. 이때

$$
r(\theta, \theta ' \Big | x) = 


\frac{\pi(theta')}{\pi(theta)}
\cdot
\frac{f(x \Big | \theta ' )}{f(x \Big | \theta )}
\cdot
\frac{f(y \Big \theta)}{f(y \Big \theta')}
\cdot
\frac{q(\theta \Big | \theta ' , x)}{q(\theta ' \Big | \theta , x)}


$$

exchange 알고리즘은 **Ising** 이나 **autologistic** 과 같은 일부 discrete 모델에는 잘 작동. 하지만 **perfect sampling** 이 적용되지 않는 많은 다른 모델에는 적용할 수 없다. 우리는 MCMC 샘플을 통해 auxiliary 변수 $y$를 생산할 수 있지만, 수렴 문제 부분에서 이론적인 흠결이 있음. 만약 MCMC 샘플의 mixing이 매우 느리다면 딴놈이 아니라 $\theta_0$ 가 매우 높은 확률로 채택되어버림. 그리고 이 MCMC 샘플의 mixing 이 느린 것 자체가 ERGMs 의 일반적인 특징임. 이래서 문제.




<br>
<br>
<br>


#### Monte Carlo MH Algorithm

MH 알고리즘의 MC 버전. 각 이터레이션에서 **MCMH 알고리즘**은 unknown normalizing constant ratio $\frac{\kappa(\theta)}{\kappa(\theta')}$ 를 MC estimate와 importance 샘플링 접근법으로 대체한다. 

exchange 알고리즘과 다르게, MCMH 알고리즘은 perfect sampler 요구조건을 빗겨간다. 따라서 perfect sampler 를 못 쓰는 다수의 통계문제에 대해서도 얘를 쓸 수 있음. 하지만 얘도 여전히 수렴 문제가 존재함. 얘의 경우에는 importance sampling estimator 가 유한한 숫자의 샘플로는 ratio의 참값 $\frac{\kappa(\theta)}{\kappa(\theta')}$ 에 수렴하지 못할 수도 있음.




<br>
<br>
<br>




```
r pressure-plot, fig.asp=.7, fig.width=6, fig.cap='A figure example with the specified aspect ratio, width, and alignment.', fig.align='center', out.width='90%'
par(mar = c(4, 4, .1, .1))
plot(pressure, pch = 19, type = 'b')
```

```
r knitr-logo, out.width='32.8%', fig.show='hold', fig.cap='Goodness-of-fit Plots, for ERGMs'
knitr::include_graphics(rep('images/knit-logo.png', 3))
```










<br>
<br>
<br>










### Varying Trunction Stochastic Approximation MCMC

ERGM 의 likelihood 함수는

$$
\begin{align}
&f(y | \theta ) = \frac{1}{\kappa(\theta )} \exp \Big \{ \theta' S(y)\Big \},
&&\theta = (\theta_1 , \cdots, \theta_d)'$, 
&&$S(y) = \Big( S_1 (y), \cdots, S_d(y) \Big)'
\end{align}
$$

이때 **$\kappa(θ)$를 특정할 수 없다** (intractability)는 점과 **모델 degeneracy** 때문에, $\theta$를 정확하게 추정하는 것은 어렵다. 이 문제는 **varying truncation stochastic approximation MCMC** 를 사용하는 것으로 해결 가능.

Normalizing constant $\kappa(\theta) = \sum_{\text{all possible }y} \exp \Big \{ \theta ' S(y)\Big\}$.

이로 인해 촉발되는 문제는?

1. MPLE: dyadic 독립을 가정하는 것으로 해결해보고자 함. 이건 observed 네트워크가 무엇이냐에 대해 과하게 의존.
2. MCMLE: $\theta^{(0)}$ 을 어떻게 고르느냐에 대해 과하게 의존. local 최적해로 수렴해버리거나 모델 degeneracy 로 수렴 자체가 실패하기도.




<br>
<br>
<br>





#### MCMC Stochastic Approximation

$h(\theta) = 0$ 형의 system 을 풀자. 고전적인 **SA 알고리즘**은 이하와 같은 형태를 띈다. ($a_k$ , $h$, $\omgea_k$ 에 대한) 적절한 조건 하에서, 이 알고리즘은 해로 수렴한다는 것을 실제로 보이는 것이 가능하다.

$$
\begin{align}
\theta_{k+1} 
&= \theta_k + a_k Y_k 

\\

&=\theta_k + a_k \Big \{ h(\theta_k) + \omega_k \Big \}, && k \ge 0 
\end{align}
$$

- $Y = h(\theta) + \omega$ 는 noisy estimate
- $\omega$ 는 mean-zero noise.

$\theta_{MLE}$ 를 찾는 것은 exponential family 안에서 $E_\theta \{ S(Y) \} = S(\mathbf y_{obs})$ 의 해를 찾는 것과 equivalent. 이는 이하의 단계를 거친다. 다만 이는 independent 네트워크 샘플을 생산하는데 있어 비효율적일 수밖에 없음. order of $100n^2$ 이라 연산을 엄청 먹으니까.

1. $\mathbf y^{k+1} \sim f(\mathbf y | \theta^{(k)}$를 샘플링 (independence 네트워크 생산)

각 arc 변수 $Y_{ij}$가 독립적으로 정해지는 랜덤 그래프에서 시작. 이 변수에는 0 혹은 1이 0.5 확률로 할당. 이 랜덤 그래프를 Gibbs 샘플러든 MH 알고리즘이든 써서 업데이트.

2. **SA** 를 통해 패러미터 estimate 업데이트.

$$
\begin{align}
\theta^{(k+1)} &= \theta^{(k)} - a_k D^{-1} \Big \{ U( \mathbf y_{k+1}, \bar {\mathbf y}_{k+1} ) - \mathbf S (\mathbf y_{obs} ) \Big \},
\\
\\
\\
U( \mathbf y_{k+1}, \bar {\mathbf y}_{k+1} ) - \mathbf S (\mathbf y_{obs} ) \Big 
\} &= P(\bar {\mathbf y}_{k+1} \Big | \mathbf y_{k+1} ) \cdot \mathbf S (\mathbf {\bar y}_{k+1} ) 

\Big \{ 1-P(\mathbf {\bar y}_{k+1} \Big | \mathbf y_{k+1} ) \Big \} \cdot \mathbf S (\mathbf {\bar y}_{k+1} ) 

\\

\mathbf {\bar y}_{k+1} &= 1- \mathbf y_{k+1} 

\end{align}
$$



<br>
<br>
<br>


##### Model Degeneracy

$\theta$을 어떻게 정하느냐에 따라 모델은 그것의 확률을 (Complete (fully connected) 거나 empty (entirely unconnected) 네트워크와 같은) 1개 혹은 소수의 그래프에만 한정시켜서 부어버릴 수도 있다. (탐색 효율 쓰레기됨) 이 문제를 해결하기 위해선 탐색 전에 degeneracy 리전을 거의 포함하지 않는 패러미터 스페이스를 가지는 모델로 특정할 필요가 있다. 근데 이게 진짜 드럽게 어려움.






<br>
<br>
<br>


#### Varying Truncation SAMCMC for ERGMs 

##### Setup

$$
\begin{align}

&a_k = C_a \left( \frac{k_0}{(k_0 \vee k)}\right)^\eta, &&b_k = C_b \left( \frac{k_0}{(k_0 \vee k)}\right)^\xi \tag{C_1, set}

\end{align}
$$

$$
\bigcup_{s \ge 0} \mathcal K_s = \Theta, \; \; \; \text{where } \mathcal K_s \subset \text{int}(\mathcal K_{s+1}) \tag{C_2}
$$

- for some constants $k_0>1$, $\eta \in (\frac{1}{2},1)$, $\xi \in (\frac{1}{2},\eta)$, $C_a > 0$, $C_b >0$.

And also,

- $\mathcal X$: a space of social network
- $\mathcal T$: $\mathcal X \times \Theta \rightarrow \mathcal X_0 \times \mathcal K_0$ (reinitialization mechanism)

- $\sigma_k$: the number of reinitialization performed until iteration $k$. ($\sigma_0 = 0$)


<br>
<br>
<br>

##### Varying truncation SAMCMC algorithm for ERGMs

1. Gibbs 샘플러 사용해 $m$ sweeps 번 interate 해서 auxiliary network $y_{k+1} \sim f(y | \theta^{(k)}$ 생산

2. Set $\theta^{(k + \frac{1}{2})} = \theta^{(k)}  + a_k \Big \{ S(y_{k+1}) - S(y_{obs}) \Big \}$

3. Set

$$
\begin{cases}

\sigma_{k+1} = \sigma_k \; \text{ and } \left( y_{k+1}, \theta^{(k + {1})} \right) =  \left( y_{k+1}, \theta^{(k + \frac{1}{2})} \right)
\; \; \; \; \; \; \; \; \; \; 

& 
\| \theta^{(k + \frac{1}{2})} - \theta^{(k)} \| \le b_k, \; \; \theta^{(k + \frac{1}{2})} \in \mathcal K_{\sigma_k}
\\
\sigma_{k+1} = \sigma_k+1 \; \text{ and } \left( y_{k+1}, \theta^{(k + {1})} \right) =  \mathcal T \left( y_{k}, \theta^{(k)} \right)
&
o.w.
\end{cases}
$$



<br>
<br>
<br>

##### Trajectory averaging estimator

trajectory averaging estimator $\bar \theta_n = \frac{1}{n}\sum_{k=1}^n \theta^{(k)}$ 에 의해 $\theta$ 를 estimate 가능.

실전에서는 estimate 의 variation 을 줄이기 위해 대신 $\theta$ estimate 에 $\bar \theta (n_0 , n) = \frac{1}{n-n_0}\sum_{k=n_0+1}^n \theta^{(k)}$ 을 자주 사용함. 이 때 $n_0$ 는 burn-in 이터레이션의 숫자.

- Free parameters: $\{a_k\}$, $\{b_k\}$, $\{\mathcal K_s, \; s \le 0\}$, $m$
- $k_0 = 100$, $\eta = 0.65$, $\xi = \frac{0.5 + \eta}{2}$.
- $C_a$, $C_b$: adjusted for different examples
- choose $\mathcal K_0$ to be around MPLE.
- In this artical, we set $\mathcal K_{s, 1} = \Big [ -4(s+1), 4(s+1) \Big]$, $\mathcal K_{s, 2} = \cdots = \mathcal K_{s, d} = \Big [ -2(s+1), 2(s+1) \Big]$.
- $m=1$.

<br>
<br>
<br>

##### Numerical Examples

Methods: MCMLE, SAA (Stochastic Approximation Algorithm), Varying truncation SAMCMC
SAMCMC: independent 5 runs(each of 200,000 iterations) (m=1, Ca = 0.01, Cb = 1000, η, ξ, κs are defalut)
























































































































