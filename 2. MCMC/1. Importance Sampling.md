---
sort: 1
---

# Importance Sampling

## Dreaming

Wait, can not I write equation like this $$ \lambda $$ b/w text?

```
$$ \lambda $$
```

```note
how do we know? nothing. but we need to do it.

$$\dfrac {1} {2} > 1 $$ 
```

$$\dfrac {1} {2}$$

```mathjax
how do we know? nothing. but we need to do it.

$$\dfrac {1} {2}$$
```

```
[equation](http://www.sciweavers.org/tex2img.php?eq=1%2Bsin%28mc%5E2%29&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=)



$$
\dfrac {1} {2}
$$
```


## Independent Monte Carlo

$$ \textbf X $$가 타겟함수 $$ f $$의 랜덤샘플. 

적분영역 전체에 걸친 

MC integraition : 적분값에 대한 수치적 해답. 분포에서 획득한 

$$

\hat \mu_{MC} = \dfrac {1}{n} 

$$





----------------------------------



## Independent Monte Carlo

$$ f $$ 가 측정은 되는데 샘플화가 안되면, MC를 통해 유사한 샘플을 만들어낼 수 있었다. 이를 넘어서 MCMC는 $$ f $$ 의 모사함수에서 샘플링하는 게 가능하지만, 이 이상으로 이는 임의의 함수 $$p$$에 대해 $$E[p(X)]$$가 신뢰도 높게 측정되는 경우에만 샘플링 가능한 별개의 방법론으로 보는 게 정확하다.

| MC | MCMC | numerical integration approach |
| :-: | :-: | :-: |
|  | iterative nature | |
|  | can be customized to very diverse & <br> difficult problem ||
|  | 무관하며 implementation이 complex하지도 않음 | 문제가 고차원이면 수렴이 느려짐|



시퀀스 $$ { \textbf X^{(t)} } $$는 MC, $$ t = 0, 1, 2, …. $$. $$ \textbf X^{(t)} = (X_1^{t} , cdots, X_p^(t))$$ 와 __state space__ 는 양쪽 모두 연속이거나 discrete.

For the types of Markov chains, $$ { \textbf X^{(t)} } $$의 분포는 체인의 limiting stationary distribution으로 수렴한다. 체인이 irreducible, aperiodic 할 때.

MCMC의 샘플링 전략은 irreducible, aperiodic MC를 만드는 것. stationary distribution이 목표분포 $$ f $$ 와 일치하는.

t가 충분히 크다면 이 체인으로부터의 $$ \textbf X^{(t) $$의 실현값은 근사적으로 마지널 분포 $$ f $$ 를 갖는다.

이런 MCMC의 특성은 베이지안 추론에 크게 도움이 되며 자주 쓰인다.







Markov Chain 자체는 __어떤 상태에서 다른 상태로 넘어갈 때, 바로 전 단계의 상태에만 영향을 받는__ (Markov Property) 확률 과정을 의미한다. 
 * 보통 사람들은 전날 먹은 식사와 유사한 식사를 하지 않으려는 경향이 있다.
 * 가령, 어제 짜장면을 먹었다면 오늘은 면종류의 음식을 먹으려고 하지 않는다.

<br />
<br />
<br />
<br />
<br />


## MH Algorithm

MCMC 중 가장 유명한 적용법은 MH 알고리즘. $$ t=0 $$에서 시작. 시작 distribution $$ g $$에서 추출한, $$ f(\textbf x^{(0)} )> 0 $$ 을 만족하는 $$\textbf x^{(0)} $$를 $$ \textbf X^{(0)} = \textbf x^{(0)} $$ 로 잡고 개시한다.


이때 제안분포 $$ g $$ 에서 후보 $$ \textbf X^{( \star )} $$ 를 하나 만들고, MH ratio $$f (\textbf {x}^{(t)}, \textbf X^{\star} ) $$ 는


$$
f (\textbf {x}^{(t)}, \textbf X^{\star} ) 

= \dfrac 
{f(\textbf X^{( \star )}) g(\textbf x^{( t )} | \textbf X^{( \star )})} 
{f(\textbf x^{( t )}) g(\textbf X^{( \star )} | \textbf x^{( t )}) } 

=\dfrac
{\dfrac
{f(\textbf X^{( \star )})}
{g(\textbf X^{( \star )} | \textbf x^{( t )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf x^{( t )} | \textbf X^{( \star )})}
}

=\dfrac
{\dfrac
{f(\textbf X^{( t+1 )})}
{g(\textbf X^{( t+1 )} | \textbf x^{( t )})}
}
{\dfrac
{f(\textbf x^{( t )})}
{g(\textbf x^{( t )} | \textbf X^{( t+1 )})}
}
$$ 

$$\textgreater$$
$$\text{\textgreater }	$$

```warning

여기서 단순 Metropolis 알고리즘은 단순히 $$ \dfrac {f(x_1)} {f(x_0)} {>} {\le} 1 $$ 이기만 하면 새로운 샘플을 수용한다. 이인즉 $$g$$로 표준화해주는 것의 가장 주요한 요점은 둘의 시작 높이, 즉 쥐고 나온 수저가 다를 수 있으므로 이를 표준화해준다는 것이다. 아웃풋이 높은 $$x_i$$를 선택하는 것은 MLE 관점에 기반한다.

단 언제나 그렇듯 이렇게 샘플을 다쳐내면 오히려 음질의 결과가 나온다. 따라서 패자부활전으로서 $$ \dfrac {f(x_1)} {f(x_0)} > u \sim U_{(0,1)} $$ 을 한 번 더 거친다. 이조차도 실패하면 생산해두었던 샘플 $$\textbf X^{(t)}$$ 는 버려지고 새로운 샘플을 $$t+1$$으로 설정해 재진행한다.
```

이후 

$$
\[
    X(m,n) = \left\{\begin{array}{@{}lr@{}}
	\multirow {2}{*}{x(n),} & \text{for }0\leq n\leq 1\\
                               & \text{or }0\leq n\leq 1\\
        x(n-1), & \text{for }0\leq n\leq 1\\
        x(n-1), & \text{for }0\leq n\leq 1
        \end{array}\right\} = xy
		\]
$$










| revise | modify | amend | alter | change | edit |
| --- | --- | --- | --- | --- | --- |
| | | | | | |


| | --- |  |
| | --- |  |
