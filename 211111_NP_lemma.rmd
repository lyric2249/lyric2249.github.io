---
sort: 11
---

## Neyman–Pearson lemma 

rs $X_1 , \cdots, X_n \overset {iid}{\sim} f(x_1 , \cdots, x_n ; \theta)$이고, $H_0 : \theta=\theta_0, \; \; \; H_1 : \theta=\theta_1  $. 이때 이하를 만족하면 rejection region $R$은 MP test의 기각역.

$\exists k \ge 0$:

1. $\pmb x \in R$ if $f(\pmb x \vert \theta_1) > k f(\pmb x \vert \theta_0)$.
2. $\pmb x \in R^c$ if $f(\pmb x \vert \theta_1) < k f(\pmb x \vert \theta_0)$.
3. $\mathbb{P}_{\theta_0} \left( \pmb X \in R \right) = \alpha$ for the prefiexed significance level $\alpha$.



<br>
<br>

Proof:


\begin{align}

P(\pmb X \in A \vert \theta) &= \int_A L(\theta ; \pmb x) d \pmb x \\

&= \int_A f(\pmb x ; \theta) d \pmb x 

\end{align}






이므로, $A \subset C^\ast$라면 






\begin{alignat}{4}

\int_A f(\pmb x ; \theta) d \pmb x &\le \int_A && k \ast f(\pmb x ; \theta) d \pmb x \\

\\

P(\pmb X \in A \vert \theta_0) &\le && k \ast P(\pmb X \in A \vert \theta_1)

\end{alignat}









<br>

마찬가지 방법으로 $A \subset \left( C^\ast \right)^c$라면 $P(\pmb X \in A \vert \theta_0) \ge k \ast P(\pmb X \in A \vert \theta_1)$.


<br>

$C^\ast$의 유의수준이 $\alpha$라 하고, 유의수준이 동일한 임의의 RR $C$를 가정하자. 이때 두 RR은 각각





\begin{align}

C^\ast &= (C^\ast \cap C) \cup (C^\ast \cap C^c) \\


C &= (C^\ast \cap C) \cup ({C^\ast}^c \cap C) 

\end{align}







<br>

로 표현할 수 있으며, 두 RR에 대한 power function은 각각 







\begin{alignat}{4}

\pi^\ast(\theta) &= P(\pmb X \in C^\ast \vert \theta)



&&= P(\pmb X \in C^\ast \cap C \vert \theta) &&+ P(\pmb X \in C^\ast \cap C^c \vert \theta) \\


\pi(\theta) &= P(\pmb X \in C \vert \theta)

&&= P(\pmb X \in C^\ast \cap C \vert \theta) &&+ P(\pmb X \in {C^\ast}^c \cap C \vert \theta) \\


\end{alignat}







<br>

이때 $H_0$에서 두 power의 차이는












\begin{alignat}{4}

\pi^\ast(\theta_1) -\pi(\theta_1)

&= && P(\pmb X \in C^\ast \cap C^c \vert \theta_1) - P(\pmb X \in {C^\ast}^c \cap C \vert \theta_1) \\

&\ge \dfrac{1}{k} && \left\{ P(\pmb X \in C^\ast \cap C^c \vert \theta_0) - P(\pmb X \in {C^\ast}^c \cap C \vert \theta_0) \right\} \\

&=


\dfrac{1}{k} && \left\{

 P(\pmb X \in C^\ast \cap C^c \vert \theta_0) - P(\pmb X \in {C^\ast}^c \cap C \vert \theta_0) \\

+ P(\pmb X \in C^\ast \cap C \vert \theta_0) - P(\pmb X \in C^\ast \cap C \vert \theta_0) 

\right\} \\

&= \dfrac{1}{k} && \left\{ \pi^\ast (\theta_0) - \pi (\theta_0) \right \} \\

&=0 &&


\end{alignat}












<br>

이에 의해 MP test의 정의를 만족한다. 이때 $C$의 유의수준이 $< \alpha$인 경우, $\pi^\ast (\theta_1) > \pi (\theta_1)$이 되므로, $C^\ast$의 $H_1$에서의 power인 $\pi^\ast(\theta_1)$은 유의수준이 $\le \alpha$인 모든 RR의 power보다 크거나 같음을 알 수 있다. 

----

<br>
<br>
<br>

### Overview

- Example

| $x$ | 1 | 2 | 3 | 4 | 5 | 6 |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| $f(x \vert \theta_0)$ | .01 | .02 | .02 | .05 | .10 | .80 |
| $f(x \vert \theta_1)$ | .03 | .05 | .15 | .10 | 0 | .67 |
| $ \dfrac{f(x \vert \theta_0)}{f(x \vert \theta_1)}$ | .33 | .4 | .13 | .5 | $\infty$ | 1.19 |




유의수준이란 기본적으로 $H_0$이 사실인데 $H_1$을 선택할 확률. 선택한 RR에 해당하는 $H_0$와 $H_1$에서의 density가 각각 있다면, $H_0$에서의 density의 합이 된다. 기각을 해버렸는데 $H_0$가 발생해버렸다는 소리니까. 

power란 RR에서의 $H_1$이 발생할 확률. 

test 자체가 $H_1$에 마음을 두고 시작하는 거임. power는 무조건 $H_1$에만 직결. 실패하면 어쩌지? 무지성으로 $H_1$ 골라버리자. 이랬다가 $H_0$ 발생해버리면? 난 망하는거잖아. 이 망함의 risk를 고정해두자. 이게 $\alpha$.

power function은 $H_0$와 $H_1$ 각각에 대해서 존재한다. 이는 각각에서의 pdf이다.

즉, 표본을 통한 $ \dfrac{f(x \vert \theta_0)}{f(x \vert \theta_1)}$의 값이 크면 $H_0$를 기각할 이유가 없고, 작으면 기각할 근거를 갖는다. 이 값이 얼마나 작아야 기각할 수 있는가는 유의수준에 의해 결정. 이와 같이 rs의 LR을 통해 MP test의 RR을 찾을 수 있다. 이때 RR과 **검정법**은 실제로 동일한 것이므로 혼돈이 없다는 전제 하에 test라는 단어를 주로 사용한다. 

$LR(\theta_0, \theta_1 ; \pmb x) = \dfrac{L(\theta_0 ; \pmb x)} {L(\theta_1 ; \pmb x)}$ 는 표본의 $\theta_0$에 대한 지지 (그리고 $\theta_1$에 대한 반증)의 정도를 표현한다고 볼 수 있다.

----

<br>
<br>
<br>

### Generalized LRT

rs $\pmb X_n \overset {iid}{\sim} f(\pmb x ; \theta)$, $H_0: \theta \in \Omega_0$, $H_0: \theta \in \Omega_1 (=\Omega - \Omega_0)$.

$

\Lambda (\pmb x) = \dfrac{\max_{theta \in \Omega_0} L(\theta ; \pmb x) }{\max_{theta \in \Omega} L(\theta ; \pmb x) } = \dfrac{L(\hat \theta_0)}{L(\hat \theta_n )}



$



----

<br>
<br>
<br>


rs $X_1, \cdots, X_n$의 pdf가 $f(x ; \theta), \; \; \; \theta \in \Omega$라고 하자. 확률구간 $\left[ L(\pmb X_n ), U(\pmb X_n ) \right]$가 

$$
P \left[ L(\pmb X_n ) \le \theta \le U(\pmb X_n ) \right] = 1- \alpha
$$

를 만족하면 이를 패러미터 $\alpha$의 $100(1-\alpha) \%$ CI라 부른다.

----

<br>
<br>
<br>

rs $\pmb X_n$ 의 분포가 pdf  $f(x ; \theta), \; \; \; \theta \in \Omega$를 따른다 하자. 이때 샘플과 패러미터 $\theta$의 함수인 random quantity $T(\pmb X_n ; \theta)$의 분포가 패러미터 $\theta$에 의존하지 않으면 이는 **pivotal quantity**.

----

<br>
<br>
<br>

$H_0: \theta \in \Omega_0, H_1: \theta \in \Omega - \Omega_0$ 에 대한 RR $C^\ast$가 이하를 만족하면 이는 UMP test. $\pi^\ast$가 이 test의 power function이라면

$$

\max \{ \pi^\ast (\theta) \vert \theta \in \Omega_0 \} =\alpha,

$$


모든 다른 power function에 대해 위의 기각역에서의 power 가 최대.


----

<br>
<br>
<br>


rs $\pmb X_n $ 의 joint pdf가 $f(\pmb X_n ; \theta)$일 때, $LR( \theta_1 ,\theta_2 ; \pmb X_n) = \dfrac{L(\theta_1 ; \pmb X_n)}{L(\theta_1 ; \pmb X_n)}$가 $\theta_1 < \theta_2$에 대해 $T(\pmb X_n)$의 non-dec 혹은 non-inc라면 $L(\theta)$는 $T(\pmb X_n)$에 대해 monotone LR를 갖는다.


----

<br>
<br>
<br>

- Karlin-Rubin




$H_0: \theta \le \theta_0, H_1: \theta \ge \theta_0$. $T$가 $\theta$에 대한 SS임을 가정하고, $T$의 pdf의 family는 MLR을 가짐. then $\forall t_0$, reject $H_0 \; \; \; \iff \; \; \; T>t_0$ 하는 test는 level $\alpha$의 UMP test이다. 이때, $\alpha = P_{\theta_0} (T>t_0)$.

<br>
<br>
<br>

$L(\theta ; \pmb X_n)$이 $T(\pmb X_n)$에 대해 non-inc인 MLR. 이때

$H_0: \theta \le \theta_0, H_1: \theta \ge \theta_0$에 대한 level $\alpha$의 UMP test는 $C = \left\{ \pmb X_n : T(\pmb X_n) \ge k \right\}$ 이며, 상수 $k$는 $P[T(\pmb X_n) \ge k \vert H_0 ] = \alpha$에 의해 결정. 

$H_0: \theta \ge \theta_0, H_1: \theta \le \theta_0$는 $C = \left\{ \pmb X_n : T(\pmb X_n) \le k \right\}$.


<br>
<br>
<br>

----

MLE의 불변성

MLE의 함수는 MLE

<br>
<br>
<br>

----


서로 독립인 rv X Y의 공통된 성공 확률 p의 MLE. f(X)와 f(Y)를 곱해서 쓴다.


<br>
<br>
<br>

----

$\pmb X_n \sim U(\theta - \tfrac{1}{2}, \theta + \tfrac{1}{2})$. 이때 LF로 MLE 구하는 건 굳이 log 안 거쳐도 가능함. 안 거쳐야 증명이 깔끔한 부분이 있음.

<br>
<br>
<br>

----

$$

\dfrac{\partial f(x;\theta)}{\partial \theta}


= f(x;\theta) 

\dfrac{\partial \log f(x;\theta)}{\partial \theta}

$$

에 의해

$$

E \left\{ 


\dfrac{\partial}{\partial \theta} \log f(X;\theta)



\right\}^2


+

E \left\{ 


\dfrac{\partial^2}{\partial \theta^2} \log f(X;\theta)



\right\}



=0

$$



<br>
<br>
<br>

----

$X \sim U(0, \theta)$일 때, $\theta^2$의 UE는? $E(X^2) = \dfrac{\theta^2}{3}$이므로 $T(X)=3X^2$는 $\theta$의 UE.



<br>
<br>
<br>

----

$\pmb X_n \sim U(-\theta, \theta)$일 때, $c(X_{(n)}-X_{(1)}$가 $\theta$의 UE가 되기 위한 c의 값은?

<br>
<br>
<br>

----

$\pmb X_n \sim N(\mu, \sigma^2 )$. 이때 $cS = c \sqrt{\dfrac{\sum (X_i - \bar X)^2}{n-1}}$가 $\sigma$의 UE가 되도록 하는 c의 값은? 

$Y=(n-1)\dfrac{S^2}{sigma^2}$이 카이제곱을 따르는 rv임을 이용하여 $E(\sqrt{Y})$를 구하라.


<br>
<br>
<br>

----

$Var \left( \sum a_i \hat \theta_i \right)$는 $a_i = \dfrac{\tfrac{1}{\sigma^2_i}}{\sum \tfrac{1}{\sigma^2_i}}$일 때 최소화.



<br>
<br>
<br>

----


통계량 $S(X)$의 분포가 패러미터 $\theta$에 의존하지 않는다면 이는 **ancillary statistic**.




<br>
<br>
<br>

----

최소 SS가 존재한다면, 모든 CSS는 MSS이다.






<br>
<br>
<br>

----

## 개념


충분통계량

분해정리

Minimum 충분통계량

Completeness 6.3. 

ancillary 통계량 (분포가 모수에 의존 안함)

바수정리 complete고 minimum 충분통계량이면 모든 ancillary랑 독립

지수족 만족하면 뭐의 묶음은 complete 충분통계량 (추가조건, 6.6

minimum 충분통계량 존재하면 모든 Complete 통계량은 동시에 minimum 충분통계량

----

모먼트, MLE (2차까지 확인)

MLE 불변성

MSE를 통해 통계량 성능 비교 가능함
bias

MSE = precision + accuracy

UMVUE 7.5

크래머-라오 부등식 : 최저 분산 뽑아내는 수단

피셔 정보

2차원 피셔 정보

라오-블랙웰 : uniform better UE 뽑아내는 수단

unique best UE

best UE는 오직 하나뿐

(레만쉐페) CSS에 기반한 UE는 오직 유일함

W가 best UE면 W는 다른 모든 0에 대한 추정자들과 무연관 7.7

consistent (점근성)

충분통계량에 기반한 가설검정은 원본데이터 가설검정과 결과 동일

test으 unbaised 8.8

네이만 피어슨

카를린 루빈 8.3

빅 샘플 추정자들과 8.5 

스코어 스탯 8.12

왈드 테스트 8.13


1-a confidence iterval = acceptance region of level 알파 test

뒤집은 테스트의 성질은 컨피던스 인터벌에도 전이됨

pivotal 주어진 X랑 모수로 다른 변량 만들었을 때 이것이 오리지널 모수와 무관한 분포 따름. CLT.

MLE는 asymptotic 성질 갖음. MLE를 asymptotic 했을 때 이는 정규분포 따름. 따라서 MLE의 함수는 추축변량. 

cdf는 출신과 무관하게 U(0,1)을 따르므로 이를 추축변량으로 삼는게 가능. 이떄 자주 쓰이는건 알파/2.

감마와 포아송간 변환



유니모달 cdf가 이하의 조건을 지키면 shortest. 9.5.



dog-tired







Bubble Plot
3D Scatter Plot
Star Plot
Chernoff Faces
Parallel Coordinate Plot

1.Q-Q Plot
Shapiro-Wilks Test
Kolmogorov-Smirnov Test
Skewness Test ( )
Kurtosis Test: ( )
Lin and Mudholkar

Scatter Plot
Squared Generalized Distances
Chi-Square Plot (Gamma Plot)

nqplot
contour plot
cqplot

(Python – assumption check)



