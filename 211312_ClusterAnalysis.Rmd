---
sort: 12
---

## Clustering, Distance Methods, and Ordination

### Overview

- Example: Customer Segmentation

<img src = "12-1.png">


----


##### Clustering

- êµ°ì§‘í™”ì˜ ê¸°ì¤€ 

ë™ì¼í•œ êµ°ì§‘ì— ì†í•˜ëŠ” ê°œì²´ (ë˜ëŠ” ê°œì¸) ì€ ì—¬ëŸ¬ ì†ì„±ì´ ë¹„ìŠ·í•˜ê³ , ì„œë¡œ ë‹¤ë¥¸ êµ°ì§‘ì— ì†í•œ ê´€ì°°ì¹˜ëŠ” ê·¸ë ‡ì§€ ì•Šë„ë¡ (ì—¬ëŸ¬ ì†ì„±ì´ ë¹„ìŠ·í•˜ì§€ ì•Šë„ë¡) êµ°ì§‘ì„ êµ¬ì„±

- êµ°ì§‘í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜: ì „ì²´ ê°œì²´ (ê°œì¸) ì˜ ì†ì„±ì„ íŒë‹¨í•˜ê¸° ìœ„í•œ ê¸°ì¤€
	- ì¸êµ¬í†µê³„ì  ë³€ì¸ (ì„±ë³„, ë‚˜ì´, ê±°ì£¼ì§€, ì§ì—…, ì†Œë“, êµìœ¡ ë“±)
	- êµ¬ë§¤íŒ¨í„´ ë³€ì¸ (ìƒí’ˆ, ì£¼ê¸°, ê±°ë˜ì•¡ ë“±)

êµ°ì§‘ë¶„ì„ì—ì„œëŠ” ê´€ì¸¡ê°’ë“¤ì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€, ë˜ëŠ” ìœ ì‚¬í•˜ì§€ ì•Šì€ì§€ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ì¸¡ë„ê°€ í•„ìš”í•˜ë‹¤.
- êµ°ì§‘ë¶„ì„ì—ì„œëŠ” ë³´í†µ ìœ ì‚¬ì„±(similarity)ë³´ë‹¤ëŠ” ë¹„ìœ ì‚¬ì„±(dissimilarity)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë©°, ê±°ë¦¬(distance)ë¥¼ ì‚¬ìš©í•œë‹¤.

$x$ê°€ ì—°ì†í˜•ì¼ ë•Œ CAì˜ ìœ„ë ¥ì´ ìµœê³ ë¡œ ë°œíœ˜ë¨. ìœ ì‚¬ì„±ì˜ ì²™ë„ë¡œ ê±°ë¦¬ê°€ ì‚¬ìš©ë˜ëŠ”ë°, ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ì—ëŠ” ê±°ë¦¬ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸. ê¼­ê¼­ ì¹´í…Œê³ ë¦¬ì»¬ ë³€ìˆ˜ë¡œ CAë¥¼ í•´ì•¼ê² ë‹¤ë©´ ì§€ì‹œë³€ìˆ˜ë¡œ ëŒ€ì²´í•˜ì—¬ CAë¥¼ ì‹œë„í•  ìˆ˜ëŠ” ìˆê² ìœ¼ë‚˜, ì´ëŠ” ì–´ëŠì •ë„ ì–µì§€ë¡œ í•˜ëŠ” ê²ƒì´ê³  ì˜¤ì ì—†ëŠ” CAëŠ” ì•„ë‹˜.


----

##### Distance Measures

ê±°ë¦¬ (Distance) ë¼ëŠ” í•¨ìˆ˜. CAì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ê±°ë¦¬ëŠ” pairwise ê±°ë¦¬.

1. Euclid ê±°ë¦¬ (Euclidean) : ê°€ì¥ ë©”ì´ì €í•¨

pì°¨ì› ê³µê°„ì—ì„œ ì£¼ì–´ì§„ ë‘ ì  $\pmb x=(x_1 , \cdots, x_p), \; \; \pmb y=(y_1 , \cdots, y_p)$ ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ëŠ”

$
d(\pmb x, \pmb y) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}
$

if $p=2$,

<img src = "12-2.png">

<br>
<br>

{:start="2"}

2. Minkowski ê±°ë¦¬

$
d(\pmb x, \pmb y) = \left\{ {\sum_{i=1}^p (x_i - y_i)^m} \right\}^{\tfrac{1}{m}}
$

$m=2$ì¼ ë•Œ ì´ëŠ” Euclideanê³¼ ê°™ì•„ì§„ë‹¤. ë³´í†µì€ mì˜ ê°’ìœ¼ë¡œ ì§ìˆ˜ë¥¼ ë§ì´ ì”€. ë¯¼ì½”í”„ëŠ” ê²°êµ­ Euclideanì˜ ì¼ë°˜í™”.



<br>
<br>


{:start="3"}

3. Mahalanobis ê±°ë¦¬

<img src = "12-3.png">

ìœ„ì—ì„œ Aì™€ Bì˜ ê±°ë¦¬ë§Œì„ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìœ„ì˜ ì ë“¤ì˜ êµ°ì§‘ì˜ íŒ¨í„´ ë˜í•œ ê³ ë ¤í•¨. xì¶•ê³¼ yì¶•ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜ë“¤ ì‚¬ì´ì— correlationì´ ìˆë‹¤ëŠ” ê²ƒì„ ë°˜ì˜í•¨. ì¤‘ì•™ì˜ $S^{-1}$ìœ¼ë¡œ corr êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ëŠ” ê²ƒ. **ë­” ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ?** ìœ„ ì¼€ì´ìŠ¤ë¥¼ ìƒê°í•˜ë©´ AëŠ” ì „ì²´ì ì¸ íŒ¨í„´ì˜ ì—°ì¥ì„  ìƒì—ì„œ ë©€ë¦¬ ìˆëŠ”ë°, BëŠ” íŒ¨í„´ì—ì„œ ì§êµí•´ì„œ ë²—ì–´ë‚˜ë©´ì„œ ê°€ê¹Œì´ ìˆìŒ. ë”°ë¼ì„œ Aë³´ë‹¤ Bê°€ ë©€ë‹¤ê³  í‰ê°€ ê°€ëŠ¥.

$
d(\pmb x, \pmb y) = \sqrt{(\pmb x - \pmb y) ' S^{-1} (\pmb x - \pmb y) }
$




{:start="4"}

4. Manhattan ê±°ë¦¬

$
d_{Manhattan} (\pmb x, \pmb y) = {\sum_{i=1}^p \vert x_i - y_i \vert} 
$




----

- Standardization

CAëŠ” ìë£Œ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸ì—, ê° ìë£Œì˜ ë‹¨ìœ„ê°€ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬, ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²•ì´ **í‘œì¤€í™” ë°©ë²•**ì´ë‹¤. í‘œì¤€í™” ë°©ë²•ì´ë€ ê° ë³€ìˆ˜ì˜ ê´€ì°°ê°’ìœ¼ë¡œë¶€í„° ê·¸ ë³€ìˆ˜ì˜ í‰ê· ì„ ë¹¼ê³ , ê·¸ ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤. í‘œì¤€í™”ëœ ëª¨ë“  ë³€ìˆ˜ê°€ í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 1ì´ ëœë‹¤. **ì‚¬ì‹¤ìƒ í•„ìˆ˜**.

<br>

- Graphical Tools
	- Scatter Plot
	- Scatter Plot using PCA
	- Andrews Plot
	- Star Plot
	- Chernoff Faces


----

<br>
<br>
<br>

### Hierarchical Clustering

1. Start with $N$ clusters, each containing a single entity and an $N \times N$ symmetric matrix of distances, $D=\{d_{ik}\}$.

2. Search the distance Matrix $D$ for the nearest pair of clusters. Let the distance b/w the most similar (ê°€ì¥ ê±°ë¦¬ê°€ ì‘ì€) clusters $U$ and $V$ be $d_{UV}$.

3. Mearge clusters $U$ and $V$. Label the newly formed cluster $(UV)$. Update the entries in the distance Matrix $D$ by squences below. The distance b/w $(UV)$ and other cluster $W$ is denoted by $d_{(UV)W}$.
	1. deleting rows and columns corresponding to clusters $U$ and $V$, then
	2. adding a row and a column giving the distance b/w $(UV)$ and the remaining clusters.

4. repeat steps 2 and 3 a total of $N-1$ times. Then, all observations will be in single clusters. Record the identity of clusters that are merged and the levels at which the mergers take place.

----


##### ê³„ì¸µì  êµ°ì§‘ë¶„ì„ Example

distance Matrix $D$ëŠ” $n^2$ì— ì˜ì¡´í•˜ì—¬ ë³€ìˆ˜ ìˆ«ìê°€ ì¦ê°€í•˜ë©´ ì—°ì‚° ì‹œê°„ë„ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€.



{:start="5"}
5. ê³„ì¸µì  ë¶„ì„ì—ì„œë§Œ ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦´ ìˆ˜ ìˆìŒ. a graphical tool to illustrate the merges or divisions.

<img src = "12-4-3.png">

python ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ ê¸°ì¤€ ì´ distanceì˜ 70%ì—ì„œ ì§¤ë¼ì„œ clutserë¥¼ íŒì •. color_threshold.

----

##### HCAì˜ ì¢…ë¥˜

1. Single Linkage, ë‹¨ì¼ ì—°ê²° (mimum distance, or nearest neighbor)

$
d_{(UV)W} = \min \left( d_{UW}, d_{VW} \right)
$


{:start="2"}
2. Complete Linkage, ì™„ì „ ì—°ê²° (maximum distance, or farthest neighbor)

 $
d_{(UV)W} = \max \left( d_{UW}, d_{VW} \right)
$




{:start="3"}
3. **Average Linkage, í‰ê·  ì—°ê²°** (average distance)
	- ìœ„ì˜ ë‘˜ì´ ë³€ë™ì´ ë„ˆë¬´ ì‹¬í•´ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì‹œë¨


$
d_{(UV)W} = \dfrac{1}{n_{UV}n_W} \left( \sum_{i=1}^{n_{UV}} \sum_{j=1}^{n_{W}} d_{ij} \right)
$

<img src = "12-4.png">



{:start="4"}
4. **Centriod Method, ì¤‘ì‹¬ì  ì—°ê²°** (For each cluster, compute the centroid)



$
d_{(UV)W} = \text{ distance b/w the centroids of cluster } U \text{ and } V
$

<img src = "12-4-2.png">



{:start="5"}
5. **~~Ward's Method~~**

boldë“¤ì´ ë¬´ë‚œí•¨

----

##### HCAì˜ ì¥ë‹¨ì 

Advantage:
- clusterì˜ ìˆ˜ë¥¼ ì•Œ í•„ìš”ê°€ ì—†ìŒ
- ë´ë“œë¡œê·¸ë¨ í†µí•´ êµ°ì§‘í™” í”„ë¡œì„¸ìŠ¤ì™€ ê²°ê³¼ë¬¼ì„ í‘œí˜„ ê°€ëŠ¥


Disadvantage:
- ê³„ì‚°ì†ë„ê°€ ëŠë¦¼
- ì•„ì›ƒë¼ì´ì–´ (ì´ìƒì¹˜) ê°€ ì¡´ì¬í•  ê²½ìš°, ì´ˆê¸° ë‹¨ê³„ì— ì˜ëª» ë¶„ë¥˜ëœ êµ°ì§‘ì€ ë¶„ì„ì´ ëë‚ ë•Œê¹Œì§€ ì†Œì† clusterê°€ ë³€í•˜ì§€ ì•ŠìŒ
- ì•„ì›ƒë¼ì´ì–´ì— ëŒ€í•œ ì‚¬ì „ê²€í†  í•„ìš”, Centroid ë°©ë²•ì´ ì•„ì›ƒë¼ì´ì–´ì— ëœ ë¯¼ê°í•¨


----

<br>
<br>
<br>

### K-means Clustering

K-í‰ê·  êµ°ì§‘ë¶„ì„ë²•. ì‚¬ì „ì— ê²°ì •ëœ êµ°ì§‘ìˆ˜ $k$ì— ê¸°ì´ˆí•˜ì—¬ ì „ì²´ ë°ì´í„°ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ ìœ ì‚¬í•œ kê°œì˜ êµ°ì§‘ìœ¼ë¡œ êµ¬ë¶„í•œë‹¤.

Proceeds:
1. êµ°ì§‘ìˆ˜ kë¥¼ ê²°ì •í•œë‹¤
2. ì´ˆê¸° kê°œ êµ°ì§‘ì˜ ì¤‘ì‹¬ì„ ì„ íƒí•œë‹¤ (ëœë¤í•˜ê²Œ)
3. ê° ê´€ì°°ì¹˜ë¥¼ ê·¸ ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìˆëŠ” êµ°ì§‘ì— í• ë‹¹í•œë‹¤.
4. í˜•ì„±ëœ êµ°ì§‘ì˜ ì¤‘ì‹¬ (centroid) ì„ ê³„ì‚°í•œë‹¤.
5. 3-4ì˜ ê³¼ì •ì„ ê¸°ì¡´ì˜ ì¤‘ì‹¬ê³¼ ìƒˆë¡œìš´ ì¤‘ì‹¬ì˜ ì°¨ì´ê°€ ì—†ì„ ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.

<img src = "12-5.png">
<img src = "12-6.png">


##### Determination of K

KCAì˜ ê²°ê³¼ëŠ” ì´ˆê¸° êµ°ì§‘ìˆ˜ kì˜ ê²°ì •ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤.

1. ì—¬ëŸ¬ê°€ì§€ì˜ kê°’ì„ ì„ íƒí•˜ì—¬ CAë¥¼ ìˆ˜í–‰í•œ í›„ ê°€ì¥ ì¢‹ë‹¤ê³  ìƒê°ë˜ëŠ” kê°’ì„ ì´ìš©.
	- Elbow point ê³„ì‚°í•˜ì—¬ k ì„ íƒ
	- Silhouette plotìœ¼ë¡œ k ì„ íƒ

2. ìë£Œì˜ ì‹œê°í™”ë¥¼ í†µí•˜ì—¬ Kë¥¼ ê²°ì • (ex. star plotì„ 2ì°¨ì› dfë¡œ ë°”ê¾¸ì–´ í‰ê·  ì²´í¬í–ˆì—ˆìŒ)
	- ìë£Œì˜ ì‹œê°í™”ë¥¼ ìœ„í•´ì„œëŠ” ì°¨ì›ì¶•ì†Œê°€ í•„ìˆ˜ì ì´ê³ , ì´ë¥¼ ìœ„í•˜ì—¬ PCAê°€ ë„ë¦¬ ì‚¬ìš©ëœë‹¤.

3. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ samplingí•œ ë°ì´í„° (ì´ê²ƒì´ ìŠ¤ëª°ë°ì´í„°ê°€ ë¨) ë¡œ HCAë¥¼ ìš°ì„  ìˆ˜í–‰í•˜ì—¬ (ì—¬ê¸°ì„œ ë´ë“œë¡œê·¸ë¨ì´ ì–»ì–´ì§) kì˜ ê°’ì„ ì„ íƒ (ì¦‰ HCAì™€ KCAë¥¼ ë‘˜ ë‹¤ ì“°ë¯€ë¡œ hybrid)

<img src="12-elbowplot.png">

----

<br>
<br>
<br>

### êµ°ì§‘ì˜ í‰ê°€ë°©ë²•

- Silhouette Score (Silhouette Plot)

$
s(i) = \dfrac{b(i)-a(i)}{\max \left\{ a(i),b(i) \right\} } = \begin{cases} 1-\dfrac{a(i)}{b(i)}, & if \; \; a(i) < b(i) \\ 0, & if \; \; a(i) = b(i) \\ \dfrac{b(i)}{a(i)} - 1, & if \; \; a(i) > b(i) \end{cases}
$

- $a(i)$: ê°œì²´ $i$ë¡œë¶€í„° **ê°™ì€** êµ°ì§‘ ë‚´ì— ìˆëŠ” **ëª¨ë“  ë‹¤ë¥¸** ê°œì²´ë“¤ ì‚¬ì´ì˜ í‰ê·  ê±°ë¦¬. **ì‘ì„ìˆ˜ë¡ ì¢‹ë‹¤.** ì‘ì„ìˆ˜ë¡ í•´ë‹¹í•˜ëŠ” êµ°ì§‘ ì•ˆì—ì„œ ì¤‘ì•™ ë¶€ë¶„ì— componentsê°€ ëª¨ì—¬ ìˆë‹¤ëŠ” ì†Œë¦¬ì´ë¯€ë¡œ. 
- $b(i)$: ê°œì²´ $i$ë¡œë¶€í„° **ë‹¤ë¥¸** êµ°ì§‘ ë‚´ì— ìˆëŠ” ê°œì²´ë“¤ ì‚¬ì´ì˜ í‰ê·  ê±°ë¦¬ **ì¤‘ ê°€ì¥ ì‘ì€ ê°’**. **í´ìˆ˜ë¡ ì¢‹ë‹¤.** í´ìˆ˜ë¡ ë‹¤ë¥¸ êµ°ì§‘ì— í—·ê°ˆë ¤ì„œ ì†í•  ì¼ ì—†ì´ í™•ì‹¤í•˜ê²Œ í˜„ì¬ ì†Œì†ë˜ì–´ ìˆëŠ” êµ°ì§‘ì— ì†Œì†ë˜ì–´ êµ¬ë¶„ëœë‹¤ëŠ” ì†Œë¦¬ì´ë¯€ë¡œ.

1ì„ ë„˜ì–´ê°ˆ ìˆ˜ ì—†ìœ¼ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ°ì§‘í™”ê°€ ì˜ ëœ ê´€ì°°ê°’. ëª‡ê°œì˜ clusterê°€ ì„¤ì •ë˜ì—ˆì„ ë•Œ ê°€ì¥ í•´ë‹¹ statì´ ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€ë¥¼ í†µí•´ íŒì •í•˜ëŠ” ê²ƒì´ ì´ ì ‘ê·¼ë²•. í‰ê·  Silhouette ScoreëŠ” ëª¨ë“  obsë§ˆë‹¤ $s(i)$ë¥¼ êµ¬í•˜ì—¬ ì´ë¥¼ í‰ê· ë‚¸ ê°’ì´ë¯€ë¡œ, í‰ê·  Silhouette Scoreê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ°ì§‘ë¶„ì„ì´ ì˜ëë‹¤ê³  íŒë‹¨ ê°€ëŠ¥.

<img src="12-SilhouettePlot.png">


----

<br>
<br>
<br>

### Clustering using Density Estimation (wk14)











Based on **nonparametric** density estimation
The clusters may be viewed as high-density regions in the space separated by low-density regions between them.
No need to specify the number of clusters. It is determined by the method itself.


ë°€ë„ê¸°ë°˜ ì¶”ì •ì— ìš”êµ¬ë˜ëŠ” (hyper) Parameter: bandwidth. í•´ë‹¹ ê°’ì´ ë‹¬ë¼ì§€ë©´ ê²°ê³¼ë„ ë‹¬ë¼ì§. 
Iris ë°ì´í„° ì˜ˆ




##### Kernel Density Estimation (KDE) 


$
f(x_0) = \dfrac{1}{N \lambda} \sum_{i=1}^N K \left( \dfrac{x_0 - x_i}{\lambda} \right) , \; \; \; \; \; x \in R
$

Nì€ ìƒ˜í”Œì‚¬ì´ì¦ˆ, ëŒë‹¤ëŠ” ë°´ë“œìœ„ìŠ¤, KëŠ” ìŠ¤ë¬´ë”© ì»¤ë„, x_iëŠ” obs

closed formì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ê·¸ëƒ¥ ìƒì§•ì ì¸ ê³µì‹ì¼ ë¿. closed formì´ ìˆëŠ”ê²Œ ì•„ë‹ˆë¼ ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ê³ ìœ í•œ ê°’ì´ ì¶”ì •ë˜ëŠ” ê²ƒìœ¼ë¡œ ì§„í–‰ë¨. 


ë°€ë„ì¶”ì •ì—ì„œ ê°€ì¥ ë§ì´ ì“°ëŠ” ë°©ë²•. ì¶”ì •í•˜ê³  ì‹¶ì€ í¬ì¸íŠ¸ëŠ” $x_0$. $x_0$ë¼ëŠ” í¬ì¸íŠ¸ì— ëŒ€í•´ densityë¥¼ ì¶”ì •í•˜ê³  ì‹¶ë‹¤. $x_0$ ì¸ê·¼ì˜ ê´€ì°°ì¹˜ëŠ” ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§. $x_0$ ë¡œ ë¶€í„° ë©€ì–´ì§ˆìˆ˜ë¡ ê°€ì¤‘ì¹˜ëŠ” ê°ì†Œí•¨. ê° obs ë³„ë¡œ ì»¤ë„í•¨ìˆ˜ ë¶€ì—¬í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ ê·¸ ì»¤ë„í•¨ìˆ˜ ë‹¤ ë”í•œ ë‹¤ìŒì— ìŠ¤ì¼€ì¼ë§í•˜ë©´ ë.

Kì˜ ê°€ì¥ í”í•œ ì„ íƒì€ ì •ê·œë¶„í¬í•¨ìˆ˜, ì¦‰ Gaussian Kernel 

Bandwidth ì˜ íš¨ê³¼: ì»¤ë„í•¨ìˆ˜ì˜ ì¢Œìš° ë„“ì´ì— í•´ë‹¹í•˜ëŠ” ê²ƒìœ¼ë¡œì„œ, ê°€ìš°ì‹œì•ˆ ì»¤ë„ì—ì„œëŠ” í‘œì¤€í¸ì°¨ì— í•´ë‹¹í•¨. Bandwithê°€ í¬ë©´ xê°’ë“¤ ê°„ì— ì°¨ë³„í™”ê°€ ëœë˜ì–´ì„œ ì¶”ì • ìœ„ë ¥ì´ ë–¨ì–´ì§

ë´‰ìš°ë¦¬ì˜ ê°¯ìˆ˜ëŠ” êµ°ì§‘ì˜ ê°¯ìˆ˜ë¡œ ìƒê°í•  ìˆ˜ ìˆìŒ. ì§€ë‚˜ì¹˜ê²Œ ë°´ë“œìœ„ìŠ¤ê°€ ì¢ìœ¼ë©´ ë¾°ì¡±í•œ ë¶€ë¶„ì´ ë‹¤ìˆ˜ íŠ€ì–´ë‚˜ì™€ êµ°ì§‘ì˜ ê³¼ë‹¤ì¶”ì • ë°œìƒ

ê·¸ë˜í”„ëŠ” 1ì°¨ì› ë°€ë„ ì¶”ì •ì— í•´ë‹¹
íšŒìƒ‰: ì •ë‹µ. í‘œì¤€ì •ê·œë¶„í¬
ë¶‰ì€ìƒ‰: undersmoothed, ğœ†ğœ† = 0.05 (too small)
ë…¹ìƒ‰: oversmoothed, ğœ†ğœ† = 
2 (too large)
ê²€ì •ìƒ‰: optimally smoothed, ğœ†ğœ† = 0.337

Bandwidth ì¶”ì •



- 2D Kernel Density Estimation: 2ì°¨ì›ì—ì„œì˜ KDEëŠ” ì–´ë–»ê²Œ í™•ì¥ë  ê²ƒì¸ê°€?

----

<br>
<br>
<br>



### Multidimensional Scaling (MDS)
Dimension Reduction Methods
- PCA : xë³€ìˆ˜ë“¤ë¼ë¦¬ì˜ ë¶„ì‚°ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì°¨ì›ì¶•ì†Œ. í•œ ë³€ìˆ˜ì˜ ë¶„ì‚°ì´ ìµœëŒ€í™”ë˜ì–´ì•¼ í•¨
- Factor: ë³€ìˆ˜ê°„ì˜ correlationì„ ìµœëŒ€í•œ ê¹¨íŠ¸ë¦¬ì§€ ì•Šê³  ë°˜ì˜í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ DR. Corr êµ¬ì¡°ê°€ ìµœëŒ€í•œ ìœ ì§€
- MDS
- Canonical Discriminant Analysis

ì´ì¤‘ ìœ„ì˜ ë‘˜ì€ original dataì˜ Variance ì„¤ëª…ì— ì§‘ì¤‘í•¨.  (ex. 1ëª…ì´ 401í˜¸, 1ëª…ì´ 501í˜¸ì— ìˆë‹¤ê³  í•˜ë©´, ë‘˜ì˜ ì§ì„  ê±°ë¦¬ê°€ ê·¸ë ‡ê²Œ í¬ê²Œ ë–¨ì–´ì ¸ìˆë‹¤ê³  í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ ìœ„ì˜ ë‘ ë¶„ì„ë²•ì€ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” ê²ƒì²˜ëŸ¼ ê·¸ë˜í”„ì— í‘œí˜„ë  ìˆ˜ ìˆìŒ. ê±°ë¦¬ ê°œë…ì´ ì—†ê¸° ë•¨ë¬¸) 


- MDS
	- Fit (projection)  the original data into a low-dimensional coordinate system such that any distortion caused by a reduction in dimensionality is minimized.
	- **Map the distances**  between points in a high dimensional space into a lower dimensional space.

- distortionì´ë€? dissimilarity (distance) among the original data points
	- For a given set of observed similarities (or distances) between every pair of N items, find a representative of the items in as few dimensions as possible such that the similarities (or distances) in the lower dimensions match, as close as possible with the original similarities (or distances).

- 1. Nonmetric MDS
	- Only the rank orders of the N(N-1)/2 original similarities are used to arrange  N items in a lower-dimensional coordinate system. ê±°ë¦¬ ì—†ì´ rankë§Œ ì£¼ì–´ì ¸ìˆìŒ. rankë§Œ ì•ˆë¬´ë„ˆì§€ë„ë¡

- 2. Metric MDS (ìì£¼ì”€. Principal Coordinate Analysis)
	- The actual magnitudes of the original similarities are used to obtain a geometric representation.








##### **Kruskalâ€™s Stress**

so-called **Badness of fit** criterion. MDSê°€ ì˜ëë‹¤ë©´ ê¸°ì¡´ ì˜¤ë¦¬ì§€ë„ ì°¨ì›ì˜ ê±°ë¦¬ë‚˜ ì°¨ì›ì¶•ì†Œëœ ì´í›„ì˜ ê±°ë¦¬ë‚˜ ë¹„ìŠ·í•´ì•¼ í•¨. í¬ë£¨ìŠ¤ì¹¼ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ì‘ìœ¼ë©´ ì™œê³¡ë„ ì‘ì€ ê²ƒ. ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ìµœì†Œì¸ DRì´ ìµœê³ ì˜ DR.

- Let $D_{rs}$ denote the actual distance (or dissimilarity) between item r and item s, then the ordered distances are $D_{r_1 s_1 } <D_{r_2 s_2 } < \cdots < D_{r_M s_M }, \; \; \; M=\begin{pmatrix} N \\ 2 \end{pmatrix} $.


- Let $d_{rs}$ denote the distance between item r and item s in the lower dimensional space.

- MDS seeks (iteratively) to find a set of $d$â€™s such that $d_{r_1 s_1 } <d_{r_2 s_2 } < \cdots < d_{r_M s_M }$ and $Stress = \left\{ \dfrac{\sum_{i=1}^N \sum_{j=1}^{i-1}(D_{ij} - d_{ij})^2} {\sum_{i=1}^N \sum_{j=1}^{i-1} \left( D_{ij} \right)^2} \right\}^{\tfrac{1}{2}}$ is minimized.








- Interpretation Guideline

| Stress | Goodness of Fit |
| :-: | :-: |
| 20% | Poor |
| 10% | Fair |
| **5%** | **Good** |
| 2.5% | Excellent |
| 0% | Perfect |


- Goodness of fit = monotonic relationship between the similarities and the final distances.


**Takaneâ€™s Stress**

$

Stress = \left\{ \dfrac{\sum_{i=1}^N \sum_{j=1}^{i-1}(D_{ij}^2 - d_{ij}^2)^2} {\sum_{i=1}^N \sum_{j=1}^{i-1}\left(D_{ij}^2\right)^2} \right\}^{\tfrac{1}{2}}

$





Algorithm: 
1. For N items, obtain $M=\dfrac{N(N-1)}{2}$ ê°œì˜ distances $D_{r_1 s_1 }, D_{r_2 s_2 } , \cdots , D_{r_M s_M }$. Tehn an $N \times N$ matrix $D = \{D_{ij} \}$ is constructed.

2. Using a trial configuration in q dimensions, determine distances $d_{ij}^{(q)}$. The method to get initial $d_{ij}^{(q)}$ is given later.

3. Using the $d_{ij}^{(q)}$, move the points around to obtain an improved configurations. <br> A new configuration: new $d_{ij}^{(q)}$ and smaller stress (e.g. Newton-Raphson method) The process is repeated until the best (minimum stress) representation is obtained.

4. Plot minimum stress (q) versus q and choose the best number of dimensions, $q^\ast$ from an examination of this plot. xì¶•ì€ ì¶•ì†Œëœ ì°¨ì›, yì¶•ì€ stress. ì°¨ì›ì´ ì‘ì•„ì§ˆìˆ˜ë¡ StressëŠ” ë†’ê³ , ì°¨ì›ì´ pë¼ë©´ (original ì°¨ì›ê³¼ ê°™ë‹¤ë©´) StressëŠ” 0. PCAì™€ ë‹¬ë¦¬ ì—¬ê¸°ì„œëŠ” **elbowì—ì„œ ë©ˆì¶¤**.
	- similar to scree plot

Note:
1. The larger the dimension, the better the fit.
2. Higher dimension means harder to visualize.






##### Algorithm to find ì´ˆê¸°ê°’ $d_{ij}^{(q)}$

qê°’ì„ ì¤„ì´ë ¤ë©´ ìˆ˜ì¹˜í•´ì„ì„ ì‹œì‘í•˜ê¸° ì „ì— ë„£ì–´ì¤„ ì´ˆê¸°ê°’ì— í•´ë‹¹í•˜ëŠ” ì´ˆê¸°ì¢Œí‘œë“¤ì´ í•„ìš”í•¨. ê·¸ ê°’ì„ êµ¬í•˜ëŠ” ë°©ë²•. 

1. Construct the $N \times N$ matrix $A = \{ a_{ij} \} = \left\{ -\dfrac{1}{2} D_{ij}^2 \right\}$.





2. Construct the $N \times N$ matrix $B = \left(I - \dfrac{1}{N} J \right) A \left(I - \dfrac{1}{N} J \right) = \{ b_{ij} \} = \{ \bar a_{ij} - \bar a_{i.} - \bar a_{.j} + \bar a_{..} \}$.

where
$
\bar a_{..} = \sum{j=1}^N \sum{i=1}^N \dfrac{a_{ij}}{N^2}, \; \; \; \; \; J = \begin{bmatrix} 1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1 \end{bmatrix}
$



{:start="3"}

Dí–‰ë ¬ì€ distanceë“¤ì˜ SSE í–‰ë ¬ ì •ë„ì— í•´ë‹¹.

3. Since $B$ is a symmetric matrix, use the **spectral decomposition** to write $B$ in the $B = V \Lambda V'$. <br> If B is positive semidefinite of rank **q** (pì°¨ì› ì•„ë‹˜!! $q \le p$. ê±°ë¦¬í–‰ë ¬ì´ ì¼ì • evê¹Œì§€ëŠ” ìœ ì˜í•  ìˆ˜ ìˆëŠ”ë° ê·¸ í›„ë¡œëŠ” 0ë§Œ íŠ€ì–´ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë©° DRì€ ë°”ë¡œ ì´ìƒí™©ì—ì„œ ì¼ì–´ë‚¨. pëŠ” ìœ„ì—ì„œ ë³´ì˜€ë˜ ìœ ì‚¬ scree plotì—ì„œ original dataì˜ ì°¨ì›ìœ¼ë¡œ ì§€ì •ë˜ì—ˆë˜ ìˆ«ì) , there are q positive eigenvalues.

if

$

\Lambda_1 = \begin{bmatrix} \lambda_1 & \cdots & \pmb 0 \\ & \ddots & \\ \pmb 0 & \cdots & \lambda_q \end{bmatrix}_{q \times q}, \; \; \; \; \; V_1 = \begin{bmatrix} \pmb v_1 ,  \pmb v_2 ,  \cdots, \pmb v_q \end{bmatrix}_{N \times q}

$

then we can express

$

B = \{ V_1 \}_{N \times q} \{ \Lambda_1 \}_{q \times q} \{ V_1 ' \}_{q \times N}

= V_1 \Lambda_1^{1/2} \Lambda_1^{1/2} V_1 ' = ZZ'

$

where

$

Z = V_1 \Lambda_1^{1/2} 

= \begin{bmatrix} \sqrt{\lambda_1} \pmb v_1 , \sqrt{\lambda_2} \pmb v_2 , \cdots, \sqrt{\lambda_q} \pmb v_q \end{bmatrix} 

= \begin{bmatrix} \pmb z_1 ' \\ \pmb z_2 ' \\ \vdots \\ \pmb z_q ' \end{bmatrix}_{N \times q}

$








{:start="4"}








4. The rows $\pmb z_1 ' ,  \pmb z_2 ' , \cdots ,  \pmb z_q $  of $Z$ are the points whose interpoint distance $d_{ij}^{(q)} = (\pmb z_i - \pmb z_j)'(\pmb z_i - \pmb z_j)$ match $D_{ij} $s in the original distance matrix $D$.


5. Since $q$ will typically be too large to be of practical interest and we would prefer a smaller dimension $k$ for plotting, we can use the first $k$ eigenvalues and corresponding eigenvectors to obtain $N$ points whose distances $d_{ij}^{(k)}$ are approximately equal to the corresponding $D_{ij}$. ì˜¤ë¦¬ì§€ë„ ë°ì´í„°ì˜ ì°¨ì› pê°€ 15ê°œì˜€ë‹¤ë©´, ì´ ì°¨ì›ì„ SVDí–ˆì„ ë•Œ ev ì¤‘ 5ê°œê°€ 0ì´ì–´ì„œ qëŠ” 15ê°œë¡œ í•˜ì˜€ë‹¤. ì—¬ê¸°ì„œ ì°¨ì›ì„ ë” ì¤„ì´ê³  ì‹¶ë‹¤ë©´, ê°€ë ¹ k=5ê°œê¹Œì§€ ì„ì˜ë¡œ ë‚´ë ¤ë²„ë¦¬ê³  ì‹¶ë‹¤ë©´, ë’¤ìª½ì˜ ev 10ê°œì— í•´ë‹¹í•˜ëŠ” ê±¸ ì³ë‚´ëŠ” ê²ƒ






Rank is clearly rank 2. ì¦‰ ì°¨ì›ì„ 2ì°¨ì›ê¹Œì§€ ì¤„ì—¬ë„ ì†ì‹¤ë˜ëŠ” ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤. ë”°ë¼ì„œ orginal data Distance Matrixì—ì„œ ë³´ì˜€ë˜ íŠ¹ì„±ì´ ê·¸ëŒ€ë¡œ ë˜‘ê°™ì´ ë“œëŸ¬ë‚œë‹¤.








































































